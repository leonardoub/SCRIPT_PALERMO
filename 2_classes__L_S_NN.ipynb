{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_classes _L_S_NN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOUE7mOElYREcM1h+BbAGP0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/2_classes__L_S_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ucro1fblcxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGJk36soJp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "eaa274d8-d691-4b95-8aa4-d2e8844da838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qPpCYboRP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_LS = df_train[df_train['Histology'] != 'adenocarcinoma']\n",
        "df_test_LS = df_test[df_test['Histology'] != 'adenocarcinoma']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train_LS.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test_LS.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train_LS.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test_LS.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "ba6da89f-d32d-4ea7-a87d-97ff5879e072",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(89, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'large cell':0, 'squamous cell carcinoma':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYt5NQPG8fX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "train_big_labels_dec = [word_index[label] for label in y_train_big]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)\n",
        "one_hot_train_big_labels = to_categorical(train_big_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(107,)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #kernel_regularizer=regularizers.l2(l=0.001)\n",
        "  model.add(layers.Dense(20, activation='relu'))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4770166c-4ff2-4a0b-e329-b13d62075261",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "845bcfc1-5572-4cae-8096-d979c80d360b",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, one_hot_train_labels, validation_data=(val_data_stand, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=89, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 89 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "89/89 [==============================] - 2s 27ms/step - loss: 0.7366 - acc: 0.4831 - val_loss: 0.6841 - val_acc: 0.4615\n",
            "Epoch 2/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.7242 - acc: 0.4831 - val_loss: 0.6759 - val_acc: 0.5385\n",
            "Epoch 3/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.7145 - acc: 0.4719 - val_loss: 0.6690 - val_acc: 0.6154\n",
            "Epoch 4/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.7060 - acc: 0.5056 - val_loss: 0.6623 - val_acc: 0.6154\n",
            "Epoch 5/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6985 - acc: 0.5281 - val_loss: 0.6568 - val_acc: 0.6923\n",
            "Epoch 6/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6918 - acc: 0.5169 - val_loss: 0.6531 - val_acc: 0.6154\n",
            "Epoch 7/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6856 - acc: 0.5393 - val_loss: 0.6497 - val_acc: 0.6154\n",
            "Epoch 8/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6798 - acc: 0.5393 - val_loss: 0.6473 - val_acc: 0.6923\n",
            "Epoch 9/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.6746 - acc: 0.5730 - val_loss: 0.6454 - val_acc: 0.6923\n",
            "Epoch 10/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6699 - acc: 0.5955 - val_loss: 0.6433 - val_acc: 0.6923\n",
            "Epoch 11/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6655 - acc: 0.6180 - val_loss: 0.6393 - val_acc: 0.6923\n",
            "Epoch 12/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6610 - acc: 0.6404 - val_loss: 0.6349 - val_acc: 0.6923\n",
            "Epoch 13/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6567 - acc: 0.6292 - val_loss: 0.6300 - val_acc: 0.7692\n",
            "Epoch 14/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6529 - acc: 0.6292 - val_loss: 0.6250 - val_acc: 0.6923\n",
            "Epoch 15/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6491 - acc: 0.6292 - val_loss: 0.6199 - val_acc: 0.6923\n",
            "Epoch 16/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6453 - acc: 0.6404 - val_loss: 0.6150 - val_acc: 0.6923\n",
            "Epoch 17/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6413 - acc: 0.6517 - val_loss: 0.6104 - val_acc: 0.6923\n",
            "Epoch 18/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6374 - acc: 0.6517 - val_loss: 0.6062 - val_acc: 0.6923\n",
            "Epoch 19/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6335 - acc: 0.6517 - val_loss: 0.6028 - val_acc: 0.6923\n",
            "Epoch 20/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6297 - acc: 0.6629 - val_loss: 0.5997 - val_acc: 0.6923\n",
            "Epoch 21/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6262 - acc: 0.6742 - val_loss: 0.5965 - val_acc: 0.7692\n",
            "Epoch 22/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6228 - acc: 0.6742 - val_loss: 0.5934 - val_acc: 0.7692\n",
            "Epoch 23/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6196 - acc: 0.6854 - val_loss: 0.5903 - val_acc: 0.7692\n",
            "Epoch 24/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6163 - acc: 0.6966 - val_loss: 0.5876 - val_acc: 0.7692\n",
            "Epoch 25/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6128 - acc: 0.7079 - val_loss: 0.5849 - val_acc: 0.7692\n",
            "Epoch 26/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6093 - acc: 0.7191 - val_loss: 0.5824 - val_acc: 0.7692\n",
            "Epoch 27/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6058 - acc: 0.7416 - val_loss: 0.5802 - val_acc: 0.7692\n",
            "Epoch 28/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6022 - acc: 0.7416 - val_loss: 0.5782 - val_acc: 0.7692\n",
            "Epoch 29/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5987 - acc: 0.7528 - val_loss: 0.5762 - val_acc: 0.7692\n",
            "Epoch 30/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5954 - acc: 0.7303 - val_loss: 0.5736 - val_acc: 0.7692\n",
            "Epoch 31/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5921 - acc: 0.7416 - val_loss: 0.5707 - val_acc: 0.7692\n",
            "Epoch 32/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5887 - acc: 0.7303 - val_loss: 0.5675 - val_acc: 0.7692\n",
            "Epoch 33/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5853 - acc: 0.7303 - val_loss: 0.5643 - val_acc: 0.7692\n",
            "Epoch 34/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5817 - acc: 0.7303 - val_loss: 0.5609 - val_acc: 0.7692\n",
            "Epoch 35/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5780 - acc: 0.7416 - val_loss: 0.5576 - val_acc: 0.7692\n",
            "Epoch 36/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5742 - acc: 0.7416 - val_loss: 0.5543 - val_acc: 0.7692\n",
            "Epoch 37/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5705 - acc: 0.7416 - val_loss: 0.5506 - val_acc: 0.7692\n",
            "Epoch 38/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5666 - acc: 0.7640 - val_loss: 0.5468 - val_acc: 0.7692\n",
            "Epoch 39/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5626 - acc: 0.7640 - val_loss: 0.5429 - val_acc: 0.7692\n",
            "Epoch 40/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5584 - acc: 0.7640 - val_loss: 0.5390 - val_acc: 0.7692\n",
            "Epoch 41/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.5543 - acc: 0.7640 - val_loss: 0.5351 - val_acc: 0.8462\n",
            "Epoch 42/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5505 - acc: 0.7640 - val_loss: 0.5320 - val_acc: 0.8462\n",
            "Epoch 43/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5467 - acc: 0.7753 - val_loss: 0.5296 - val_acc: 0.8462\n",
            "Epoch 44/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5427 - acc: 0.7753 - val_loss: 0.5275 - val_acc: 0.8462\n",
            "Epoch 45/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5385 - acc: 0.7753 - val_loss: 0.5258 - val_acc: 0.8462\n",
            "Epoch 46/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5343 - acc: 0.7753 - val_loss: 0.5234 - val_acc: 0.8462\n",
            "Epoch 47/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5301 - acc: 0.7753 - val_loss: 0.5200 - val_acc: 0.8462\n",
            "Epoch 48/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5260 - acc: 0.7753 - val_loss: 0.5164 - val_acc: 0.8462\n",
            "Epoch 49/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.5219 - acc: 0.7753 - val_loss: 0.5126 - val_acc: 0.8462\n",
            "Epoch 50/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5178 - acc: 0.7865 - val_loss: 0.5085 - val_acc: 0.8462\n",
            "Epoch 51/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5135 - acc: 0.7865 - val_loss: 0.5044 - val_acc: 0.7692\n",
            "Epoch 52/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.5091 - acc: 0.7753 - val_loss: 0.5001 - val_acc: 0.7692\n",
            "Epoch 53/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5047 - acc: 0.7753 - val_loss: 0.4956 - val_acc: 0.7692\n",
            "Epoch 54/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5003 - acc: 0.7753 - val_loss: 0.4913 - val_acc: 0.7692\n",
            "Epoch 55/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4960 - acc: 0.7753 - val_loss: 0.4872 - val_acc: 0.7692\n",
            "Epoch 56/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4916 - acc: 0.7753 - val_loss: 0.4831 - val_acc: 0.7692\n",
            "Epoch 57/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.4874 - acc: 0.7753 - val_loss: 0.4787 - val_acc: 0.7692\n",
            "Epoch 58/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.4831 - acc: 0.7753 - val_loss: 0.4739 - val_acc: 0.7692\n",
            "Epoch 59/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.4788 - acc: 0.7753 - val_loss: 0.4687 - val_acc: 0.7692\n",
            "Epoch 60/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4744 - acc: 0.7978 - val_loss: 0.4633 - val_acc: 0.7692\n",
            "Epoch 61/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.4700 - acc: 0.8090 - val_loss: 0.4578 - val_acc: 0.7692\n",
            "Epoch 62/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.4656 - acc: 0.8202 - val_loss: 0.4519 - val_acc: 0.7692\n",
            "Epoch 63/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4611 - acc: 0.8202 - val_loss: 0.4467 - val_acc: 0.7692\n",
            "Epoch 64/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.4566 - acc: 0.8202 - val_loss: 0.4420 - val_acc: 0.7692\n",
            "Epoch 65/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4520 - acc: 0.8090 - val_loss: 0.4375 - val_acc: 0.7692\n",
            "Epoch 66/1000\n",
            "89/89 [==============================] - 0s 83us/step - loss: 0.4473 - acc: 0.8090 - val_loss: 0.4332 - val_acc: 0.7692\n",
            "Epoch 67/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.4426 - acc: 0.8090 - val_loss: 0.4281 - val_acc: 0.7692\n",
            "Epoch 68/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.4379 - acc: 0.8090 - val_loss: 0.4224 - val_acc: 0.7692\n",
            "Epoch 69/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.4331 - acc: 0.8202 - val_loss: 0.4163 - val_acc: 0.7692\n",
            "Epoch 70/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.4282 - acc: 0.8315 - val_loss: 0.4103 - val_acc: 0.7692\n",
            "Epoch 71/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.4234 - acc: 0.8427 - val_loss: 0.4055 - val_acc: 0.7692\n",
            "Epoch 72/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.4184 - acc: 0.8427 - val_loss: 0.4017 - val_acc: 0.7692\n",
            "Epoch 73/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4134 - acc: 0.8427 - val_loss: 0.3975 - val_acc: 0.7692\n",
            "Epoch 74/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.4083 - acc: 0.8427 - val_loss: 0.3925 - val_acc: 0.7692\n",
            "Epoch 75/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4032 - acc: 0.8427 - val_loss: 0.3870 - val_acc: 0.7692\n",
            "Epoch 76/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3980 - acc: 0.8427 - val_loss: 0.3819 - val_acc: 0.8462\n",
            "Epoch 77/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3927 - acc: 0.8427 - val_loss: 0.3777 - val_acc: 0.8462\n",
            "Epoch 78/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.3875 - acc: 0.8539 - val_loss: 0.3740 - val_acc: 0.8462\n",
            "Epoch 79/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3821 - acc: 0.8539 - val_loss: 0.3695 - val_acc: 0.8462\n",
            "Epoch 80/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3767 - acc: 0.8539 - val_loss: 0.3645 - val_acc: 0.8462\n",
            "Epoch 81/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3712 - acc: 0.8539 - val_loss: 0.3594 - val_acc: 0.8462\n",
            "Epoch 82/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3658 - acc: 0.8539 - val_loss: 0.3559 - val_acc: 0.8462\n",
            "Epoch 83/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3603 - acc: 0.8652 - val_loss: 0.3538 - val_acc: 0.8462\n",
            "Epoch 84/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.3549 - acc: 0.8764 - val_loss: 0.3513 - val_acc: 0.8462\n",
            "Epoch 85/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3495 - acc: 0.8764 - val_loss: 0.3486 - val_acc: 0.8462\n",
            "Epoch 86/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.3441 - acc: 0.8876 - val_loss: 0.3461 - val_acc: 0.8462\n",
            "Epoch 87/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3386 - acc: 0.8989 - val_loss: 0.3430 - val_acc: 0.8462\n",
            "Epoch 88/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.3334 - acc: 0.8989 - val_loss: 0.3393 - val_acc: 0.9231\n",
            "Epoch 89/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3281 - acc: 0.9101 - val_loss: 0.3351 - val_acc: 0.9231\n",
            "Epoch 90/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.3228 - acc: 0.9101 - val_loss: 0.3323 - val_acc: 0.9231\n",
            "Epoch 91/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3177 - acc: 0.9101 - val_loss: 0.3302 - val_acc: 0.9231\n",
            "Epoch 92/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3124 - acc: 0.9101 - val_loss: 0.3270 - val_acc: 0.9231\n",
            "Epoch 93/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3073 - acc: 0.9101 - val_loss: 0.3238 - val_acc: 0.9231\n",
            "Epoch 94/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3019 - acc: 0.9101 - val_loss: 0.3207 - val_acc: 0.9231\n",
            "Epoch 95/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.2965 - acc: 0.9213 - val_loss: 0.3168 - val_acc: 0.9231\n",
            "Epoch 96/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2908 - acc: 0.9213 - val_loss: 0.3128 - val_acc: 0.9231\n",
            "Epoch 97/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2853 - acc: 0.9213 - val_loss: 0.3104 - val_acc: 0.9231\n",
            "Epoch 98/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.2798 - acc: 0.9213 - val_loss: 0.3111 - val_acc: 0.9231\n",
            "Epoch 99/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.2744 - acc: 0.9326 - val_loss: 0.3120 - val_acc: 0.9231\n",
            "Epoch 100/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2692 - acc: 0.9326 - val_loss: 0.3115 - val_acc: 0.9231\n",
            "Epoch 101/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2640 - acc: 0.9326 - val_loss: 0.3093 - val_acc: 0.9231\n",
            "Epoch 102/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2589 - acc: 0.9326 - val_loss: 0.3072 - val_acc: 0.9231\n",
            "Epoch 103/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2537 - acc: 0.9326 - val_loss: 0.3069 - val_acc: 0.9231\n",
            "Epoch 104/1000\n",
            "89/89 [==============================] - 0s 87us/step - loss: 0.2485 - acc: 0.9326 - val_loss: 0.3074 - val_acc: 0.9231\n",
            "Epoch 105/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.2437 - acc: 0.9326 - val_loss: 0.3087 - val_acc: 0.9231\n",
            "Epoch 106/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2390 - acc: 0.9326 - val_loss: 0.3102 - val_acc: 0.9231\n",
            "Epoch 107/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2344 - acc: 0.9326 - val_loss: 0.3114 - val_acc: 0.9231\n",
            "Epoch 108/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.2299 - acc: 0.9326 - val_loss: 0.3124 - val_acc: 0.9231\n",
            "Epoch 109/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2255 - acc: 0.9326 - val_loss: 0.3125 - val_acc: 0.9231\n",
            "Epoch 110/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2211 - acc: 0.9326 - val_loss: 0.3132 - val_acc: 0.9231\n",
            "Epoch 111/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.2170 - acc: 0.9326 - val_loss: 0.3145 - val_acc: 0.9231\n",
            "Epoch 112/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.2128 - acc: 0.9326 - val_loss: 0.3157 - val_acc: 0.9231\n",
            "Epoch 113/1000\n",
            "89/89 [==============================] - 0s 101us/step - loss: 0.2088 - acc: 0.9326 - val_loss: 0.3173 - val_acc: 0.9231\n",
            "\n",
            "Epoch 00113: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 114/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2048 - acc: 0.9326 - val_loss: 0.3176 - val_acc: 0.9231\n",
            "Epoch 115/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2044 - acc: 0.9326 - val_loss: 0.3179 - val_acc: 0.9231\n",
            "Epoch 116/1000\n",
            "89/89 [==============================] - 0s 96us/step - loss: 0.2040 - acc: 0.9326 - val_loss: 0.3183 - val_acc: 0.9231\n",
            "Epoch 117/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2035 - acc: 0.9326 - val_loss: 0.3187 - val_acc: 0.9231\n",
            "Epoch 118/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2031 - acc: 0.9326 - val_loss: 0.3192 - val_acc: 0.9231\n",
            "Epoch 119/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2027 - acc: 0.9326 - val_loss: 0.3197 - val_acc: 0.9231\n",
            "Epoch 120/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2023 - acc: 0.9326 - val_loss: 0.3202 - val_acc: 0.9231\n",
            "Epoch 121/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2019 - acc: 0.9326 - val_loss: 0.3206 - val_acc: 0.9231\n",
            "Epoch 122/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2015 - acc: 0.9326 - val_loss: 0.3208 - val_acc: 0.9231\n",
            "Epoch 123/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2010 - acc: 0.9326 - val_loss: 0.3211 - val_acc: 0.9231\n",
            "\n",
            "Epoch 00123: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 124/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.2006 - acc: 0.9326 - val_loss: 0.3215 - val_acc: 0.9231\n",
            "Epoch 125/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2002 - acc: 0.9326 - val_loss: 0.3221 - val_acc: 0.9231\n",
            "Epoch 126/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1998 - acc: 0.9326 - val_loss: 0.3225 - val_acc: 0.9231\n",
            "Epoch 127/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1994 - acc: 0.9326 - val_loss: 0.3231 - val_acc: 0.9231\n",
            "Epoch 128/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1990 - acc: 0.9326 - val_loss: 0.3237 - val_acc: 0.9231\n",
            "Epoch 129/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1986 - acc: 0.9326 - val_loss: 0.3241 - val_acc: 0.9231\n",
            "Epoch 130/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1983 - acc: 0.9326 - val_loss: 0.3243 - val_acc: 0.9231\n",
            "Epoch 131/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1979 - acc: 0.9326 - val_loss: 0.3244 - val_acc: 0.9231\n",
            "Epoch 132/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1975 - acc: 0.9326 - val_loss: 0.3243 - val_acc: 0.9231\n",
            "Epoch 133/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1971 - acc: 0.9326 - val_loss: 0.3242 - val_acc: 0.9231\n",
            "Epoch 134/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1967 - acc: 0.9326 - val_loss: 0.3240 - val_acc: 0.9231\n",
            "Epoch 135/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1963 - acc: 0.9326 - val_loss: 0.3239 - val_acc: 0.9231\n",
            "Epoch 136/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1960 - acc: 0.9326 - val_loss: 0.3237 - val_acc: 0.9231\n",
            "Epoch 137/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1956 - acc: 0.9326 - val_loss: 0.3237 - val_acc: 0.9231\n",
            "Epoch 138/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1952 - acc: 0.9326 - val_loss: 0.3239 - val_acc: 0.9231\n",
            "Epoch 139/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1949 - acc: 0.9326 - val_loss: 0.3242 - val_acc: 0.9231\n",
            "Epoch 140/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1945 - acc: 0.9326 - val_loss: 0.3244 - val_acc: 0.9231\n",
            "Epoch 141/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1941 - acc: 0.9326 - val_loss: 0.3245 - val_acc: 0.9231\n",
            "Epoch 142/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1938 - acc: 0.9326 - val_loss: 0.3244 - val_acc: 0.9231\n",
            "Epoch 143/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1934 - acc: 0.9326 - val_loss: 0.3247 - val_acc: 0.9231\n",
            "Epoch 144/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1930 - acc: 0.9326 - val_loss: 0.3251 - val_acc: 0.9231\n",
            "Epoch 145/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1927 - acc: 0.9326 - val_loss: 0.3256 - val_acc: 0.9231\n",
            "Epoch 146/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1923 - acc: 0.9326 - val_loss: 0.3260 - val_acc: 0.9231\n",
            "Epoch 147/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1920 - acc: 0.9326 - val_loss: 0.3261 - val_acc: 0.9231\n",
            "Epoch 148/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1916 - acc: 0.9326 - val_loss: 0.3262 - val_acc: 0.9231\n",
            "Epoch 149/1000\n",
            "89/89 [==============================] - 0s 79us/step - loss: 0.1913 - acc: 0.9326 - val_loss: 0.3261 - val_acc: 0.9231\n",
            "Epoch 150/1000\n",
            "89/89 [==============================] - 0s 85us/step - loss: 0.1909 - acc: 0.9326 - val_loss: 0.3259 - val_acc: 0.9231\n",
            "Epoch 151/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1906 - acc: 0.9326 - val_loss: 0.3258 - val_acc: 0.9231\n",
            "Epoch 152/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.1902 - acc: 0.9326 - val_loss: 0.3259 - val_acc: 0.9231\n",
            "Epoch 153/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1899 - acc: 0.9326 - val_loss: 0.3259 - val_acc: 0.9231\n",
            "Epoch 154/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1895 - acc: 0.9326 - val_loss: 0.3261 - val_acc: 0.9231\n",
            "Epoch 155/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1892 - acc: 0.9326 - val_loss: 0.3263 - val_acc: 0.9231\n",
            "Epoch 156/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1888 - acc: 0.9326 - val_loss: 0.3263 - val_acc: 0.9231\n",
            "Epoch 157/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1885 - acc: 0.9326 - val_loss: 0.3265 - val_acc: 0.9231\n",
            "Epoch 158/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1881 - acc: 0.9326 - val_loss: 0.3264 - val_acc: 0.9231\n",
            "Epoch 159/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1878 - acc: 0.9326 - val_loss: 0.3264 - val_acc: 0.9231\n",
            "Epoch 160/1000\n",
            "89/89 [==============================] - 0s 114us/step - loss: 0.1874 - acc: 0.9326 - val_loss: 0.3266 - val_acc: 0.9231\n",
            "Epoch 161/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1871 - acc: 0.9326 - val_loss: 0.3271 - val_acc: 0.9231\n",
            "Epoch 162/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1868 - acc: 0.9326 - val_loss: 0.3274 - val_acc: 0.9231\n",
            "Epoch 163/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1864 - acc: 0.9326 - val_loss: 0.3274 - val_acc: 0.9231\n",
            "Epoch 164/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1861 - acc: 0.9326 - val_loss: 0.3273 - val_acc: 0.9231\n",
            "Epoch 165/1000\n",
            "89/89 [==============================] - 0s 83us/step - loss: 0.1858 - acc: 0.9326 - val_loss: 0.3272 - val_acc: 0.9231\n",
            "Epoch 166/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1854 - acc: 0.9326 - val_loss: 0.3270 - val_acc: 0.9231\n",
            "Epoch 167/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.1851 - acc: 0.9326 - val_loss: 0.3270 - val_acc: 0.9231\n",
            "Epoch 168/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1848 - acc: 0.9326 - val_loss: 0.3272 - val_acc: 0.9231\n",
            "Epoch 169/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1844 - acc: 0.9326 - val_loss: 0.3275 - val_acc: 0.9231\n",
            "Epoch 170/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1841 - acc: 0.9326 - val_loss: 0.3278 - val_acc: 0.9231\n",
            "Epoch 171/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1838 - acc: 0.9326 - val_loss: 0.3279 - val_acc: 0.9231\n",
            "Epoch 172/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1834 - acc: 0.9326 - val_loss: 0.3279 - val_acc: 0.9231\n",
            "Epoch 173/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1831 - acc: 0.9326 - val_loss: 0.3279 - val_acc: 0.9231\n",
            "Epoch 174/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1828 - acc: 0.9326 - val_loss: 0.3281 - val_acc: 0.9231\n",
            "Epoch 175/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1824 - acc: 0.9326 - val_loss: 0.3283 - val_acc: 0.9231\n",
            "Epoch 176/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1821 - acc: 0.9326 - val_loss: 0.3286 - val_acc: 0.9231\n",
            "Epoch 177/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.1818 - acc: 0.9326 - val_loss: 0.3289 - val_acc: 0.9231\n",
            "Epoch 178/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1814 - acc: 0.9326 - val_loss: 0.3293 - val_acc: 0.9231\n",
            "Epoch 179/1000\n",
            "89/89 [==============================] - 0s 89us/step - loss: 0.1811 - acc: 0.9326 - val_loss: 0.3294 - val_acc: 0.9231\n",
            "Epoch 180/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1808 - acc: 0.9326 - val_loss: 0.3295 - val_acc: 0.9231\n",
            "Epoch 181/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1805 - acc: 0.9438 - val_loss: 0.3295 - val_acc: 0.9231\n",
            "Epoch 182/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1801 - acc: 0.9438 - val_loss: 0.3296 - val_acc: 0.9231\n",
            "Epoch 183/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1798 - acc: 0.9438 - val_loss: 0.3299 - val_acc: 0.9231\n",
            "Epoch 184/1000\n",
            "89/89 [==============================] - 0s 90us/step - loss: 0.1795 - acc: 0.9438 - val_loss: 0.3297 - val_acc: 0.9231\n",
            "Epoch 185/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1792 - acc: 0.9438 - val_loss: 0.3296 - val_acc: 0.9231\n",
            "Epoch 186/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1788 - acc: 0.9438 - val_loss: 0.3298 - val_acc: 0.9231\n",
            "Epoch 187/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1785 - acc: 0.9438 - val_loss: 0.3302 - val_acc: 0.9231\n",
            "Epoch 188/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1782 - acc: 0.9438 - val_loss: 0.3306 - val_acc: 0.9231\n",
            "Epoch 189/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1779 - acc: 0.9438 - val_loss: 0.3304 - val_acc: 0.9231\n",
            "Epoch 190/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1775 - acc: 0.9438 - val_loss: 0.3303 - val_acc: 0.9231\n",
            "Epoch 191/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1772 - acc: 0.9438 - val_loss: 0.3305 - val_acc: 0.9231\n",
            "Epoch 192/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1769 - acc: 0.9438 - val_loss: 0.3308 - val_acc: 0.9231\n",
            "Epoch 193/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.1766 - acc: 0.9438 - val_loss: 0.3312 - val_acc: 0.9231\n",
            "Epoch 194/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1763 - acc: 0.9438 - val_loss: 0.3317 - val_acc: 0.9231\n",
            "Epoch 195/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1760 - acc: 0.9438 - val_loss: 0.3316 - val_acc: 0.9231\n",
            "Epoch 196/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1756 - acc: 0.9438 - val_loss: 0.3314 - val_acc: 0.9231\n",
            "Epoch 197/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1753 - acc: 0.9438 - val_loss: 0.3315 - val_acc: 0.9231\n",
            "Epoch 198/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.1750 - acc: 0.9438 - val_loss: 0.3318 - val_acc: 0.9231\n",
            "Epoch 199/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1747 - acc: 0.9438 - val_loss: 0.3320 - val_acc: 0.9231\n",
            "Epoch 200/1000\n",
            "89/89 [==============================] - 0s 112us/step - loss: 0.1744 - acc: 0.9438 - val_loss: 0.3323 - val_acc: 0.9231\n",
            "Epoch 201/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1741 - acc: 0.9438 - val_loss: 0.3325 - val_acc: 0.9231\n",
            "Epoch 202/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.1738 - acc: 0.9438 - val_loss: 0.3325 - val_acc: 0.9231\n",
            "Epoch 203/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1734 - acc: 0.9438 - val_loss: 0.3324 - val_acc: 0.9231\n",
            "Epoch 204/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1731 - acc: 0.9438 - val_loss: 0.3325 - val_acc: 0.9231\n",
            "Epoch 205/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1728 - acc: 0.9438 - val_loss: 0.3327 - val_acc: 0.9231\n",
            "Epoch 206/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1725 - acc: 0.9438 - val_loss: 0.3327 - val_acc: 0.9231\n",
            "Epoch 207/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1722 - acc: 0.9438 - val_loss: 0.3330 - val_acc: 0.9231\n",
            "Epoch 208/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1719 - acc: 0.9438 - val_loss: 0.3337 - val_acc: 0.9231\n",
            "Epoch 209/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1716 - acc: 0.9438 - val_loss: 0.3342 - val_acc: 0.9231\n",
            "Epoch 210/1000\n",
            "89/89 [==============================] - 0s 88us/step - loss: 0.1713 - acc: 0.9438 - val_loss: 0.3343 - val_acc: 0.9231\n",
            "Epoch 211/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1710 - acc: 0.9438 - val_loss: 0.3345 - val_acc: 0.9231\n",
            "Epoch 212/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1707 - acc: 0.9438 - val_loss: 0.3347 - val_acc: 0.9231\n",
            "Epoch 213/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1704 - acc: 0.9438 - val_loss: 0.3351 - val_acc: 0.9231\n",
            "Epoch 214/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.1700 - acc: 0.9438 - val_loss: 0.3352 - val_acc: 0.9231\n",
            "Epoch 215/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1697 - acc: 0.9438 - val_loss: 0.3351 - val_acc: 0.9231\n",
            "Epoch 216/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1694 - acc: 0.9438 - val_loss: 0.3350 - val_acc: 0.9231\n",
            "Epoch 217/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1691 - acc: 0.9438 - val_loss: 0.3352 - val_acc: 0.9231\n",
            "Epoch 218/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1688 - acc: 0.9438 - val_loss: 0.3357 - val_acc: 0.9231\n",
            "Epoch 219/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1685 - acc: 0.9438 - val_loss: 0.3362 - val_acc: 0.9231\n",
            "Epoch 220/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1682 - acc: 0.9438 - val_loss: 0.3366 - val_acc: 0.9231\n",
            "Epoch 221/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1679 - acc: 0.9438 - val_loss: 0.3366 - val_acc: 0.9231\n",
            "Epoch 222/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1676 - acc: 0.9438 - val_loss: 0.3365 - val_acc: 0.9231\n",
            "Epoch 223/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1673 - acc: 0.9438 - val_loss: 0.3365 - val_acc: 0.9231\n",
            "Epoch 224/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1670 - acc: 0.9438 - val_loss: 0.3370 - val_acc: 0.9231\n",
            "Epoch 225/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1667 - acc: 0.9438 - val_loss: 0.3373 - val_acc: 0.9231\n",
            "Epoch 226/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1664 - acc: 0.9438 - val_loss: 0.3375 - val_acc: 0.9231\n",
            "Epoch 227/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1661 - acc: 0.9438 - val_loss: 0.3378 - val_acc: 0.9231\n",
            "Epoch 228/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1658 - acc: 0.9438 - val_loss: 0.3384 - val_acc: 0.9231\n",
            "Epoch 229/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1655 - acc: 0.9438 - val_loss: 0.3386 - val_acc: 0.9231\n",
            "Epoch 230/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1652 - acc: 0.9438 - val_loss: 0.3385 - val_acc: 0.9231\n",
            "Epoch 231/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1649 - acc: 0.9438 - val_loss: 0.3384 - val_acc: 0.9231\n",
            "Epoch 232/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1646 - acc: 0.9438 - val_loss: 0.3386 - val_acc: 0.9231\n",
            "Epoch 233/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.1643 - acc: 0.9438 - val_loss: 0.3389 - val_acc: 0.9231\n",
            "Epoch 234/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1640 - acc: 0.9438 - val_loss: 0.3391 - val_acc: 0.9231\n",
            "Epoch 235/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1637 - acc: 0.9438 - val_loss: 0.3393 - val_acc: 0.9231\n",
            "Epoch 236/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1634 - acc: 0.9438 - val_loss: 0.3393 - val_acc: 0.9231\n",
            "Epoch 237/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1631 - acc: 0.9438 - val_loss: 0.3394 - val_acc: 0.9231\n",
            "Epoch 238/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1628 - acc: 0.9438 - val_loss: 0.3396 - val_acc: 0.9231\n",
            "Epoch 239/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1625 - acc: 0.9438 - val_loss: 0.3399 - val_acc: 0.9231\n",
            "Epoch 240/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1622 - acc: 0.9438 - val_loss: 0.3402 - val_acc: 0.9231\n",
            "Epoch 241/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1619 - acc: 0.9438 - val_loss: 0.3404 - val_acc: 0.9231\n",
            "Epoch 242/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1617 - acc: 0.9438 - val_loss: 0.3405 - val_acc: 0.9231\n",
            "Epoch 243/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1614 - acc: 0.9438 - val_loss: 0.3408 - val_acc: 0.9231\n",
            "Epoch 244/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1611 - acc: 0.9438 - val_loss: 0.3411 - val_acc: 0.9231\n",
            "Epoch 245/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1608 - acc: 0.9438 - val_loss: 0.3415 - val_acc: 0.9231\n",
            "Epoch 246/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1605 - acc: 0.9551 - val_loss: 0.3418 - val_acc: 0.9231\n",
            "Epoch 247/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1602 - acc: 0.9551 - val_loss: 0.3421 - val_acc: 0.9231\n",
            "Epoch 248/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1599 - acc: 0.9551 - val_loss: 0.3423 - val_acc: 0.9231\n",
            "Epoch 249/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1596 - acc: 0.9551 - val_loss: 0.3421 - val_acc: 0.9231\n",
            "Epoch 250/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1594 - acc: 0.9551 - val_loss: 0.3418 - val_acc: 0.9231\n",
            "Epoch 251/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1591 - acc: 0.9551 - val_loss: 0.3417 - val_acc: 0.9231\n",
            "Epoch 252/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1588 - acc: 0.9551 - val_loss: 0.3423 - val_acc: 0.9231\n",
            "Epoch 253/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.1585 - acc: 0.9663 - val_loss: 0.3428 - val_acc: 0.9231\n",
            "Epoch 254/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1582 - acc: 0.9663 - val_loss: 0.3429 - val_acc: 0.9231\n",
            "Epoch 255/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.1579 - acc: 0.9663 - val_loss: 0.3431 - val_acc: 0.9231\n",
            "Epoch 256/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.1577 - acc: 0.9663 - val_loss: 0.3431 - val_acc: 0.9231\n",
            "Epoch 257/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1574 - acc: 0.9663 - val_loss: 0.3434 - val_acc: 0.9231\n",
            "Epoch 258/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1571 - acc: 0.9663 - val_loss: 0.3441 - val_acc: 0.9231\n",
            "Epoch 259/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1568 - acc: 0.9663 - val_loss: 0.3446 - val_acc: 0.9231\n",
            "Epoch 260/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1566 - acc: 0.9663 - val_loss: 0.3451 - val_acc: 0.9231\n",
            "Epoch 261/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1563 - acc: 0.9663 - val_loss: 0.3452 - val_acc: 0.9231\n",
            "Epoch 262/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1560 - acc: 0.9663 - val_loss: 0.3452 - val_acc: 0.9231\n",
            "Epoch 263/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1557 - acc: 0.9663 - val_loss: 0.3455 - val_acc: 0.9231\n",
            "Epoch 264/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1554 - acc: 0.9663 - val_loss: 0.3461 - val_acc: 0.9231\n",
            "Epoch 265/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1552 - acc: 0.9663 - val_loss: 0.3463 - val_acc: 0.9231\n",
            "Epoch 266/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1549 - acc: 0.9663 - val_loss: 0.3462 - val_acc: 0.9231\n",
            "Epoch 267/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1546 - acc: 0.9663 - val_loss: 0.3464 - val_acc: 0.9231\n",
            "Epoch 268/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1543 - acc: 0.9663 - val_loss: 0.3472 - val_acc: 0.8462\n",
            "Epoch 269/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1541 - acc: 0.9663 - val_loss: 0.3478 - val_acc: 0.8462\n",
            "Epoch 270/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1538 - acc: 0.9663 - val_loss: 0.3480 - val_acc: 0.8462\n",
            "Epoch 271/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1535 - acc: 0.9663 - val_loss: 0.3480 - val_acc: 0.8462\n",
            "Epoch 272/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1532 - acc: 0.9663 - val_loss: 0.3481 - val_acc: 0.8462\n",
            "Epoch 273/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1530 - acc: 0.9663 - val_loss: 0.3486 - val_acc: 0.8462\n",
            "Epoch 274/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1527 - acc: 0.9663 - val_loss: 0.3488 - val_acc: 0.8462\n",
            "Epoch 275/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1524 - acc: 0.9663 - val_loss: 0.3488 - val_acc: 0.8462\n",
            "Epoch 276/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1521 - acc: 0.9663 - val_loss: 0.3488 - val_acc: 0.8462\n",
            "Epoch 277/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1519 - acc: 0.9663 - val_loss: 0.3492 - val_acc: 0.8462\n",
            "Epoch 278/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1516 - acc: 0.9663 - val_loss: 0.3500 - val_acc: 0.8462\n",
            "Epoch 279/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1513 - acc: 0.9663 - val_loss: 0.3507 - val_acc: 0.8462\n",
            "Epoch 280/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1511 - acc: 0.9663 - val_loss: 0.3508 - val_acc: 0.8462\n",
            "Epoch 281/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1508 - acc: 0.9663 - val_loss: 0.3511 - val_acc: 0.8462\n",
            "Epoch 282/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1505 - acc: 0.9663 - val_loss: 0.3514 - val_acc: 0.8462\n",
            "Epoch 283/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1503 - acc: 0.9663 - val_loss: 0.3517 - val_acc: 0.8462\n",
            "Epoch 284/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1500 - acc: 0.9663 - val_loss: 0.3523 - val_acc: 0.8462\n",
            "Epoch 285/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1497 - acc: 0.9663 - val_loss: 0.3526 - val_acc: 0.8462\n",
            "Epoch 286/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1494 - acc: 0.9663 - val_loss: 0.3529 - val_acc: 0.8462\n",
            "Epoch 287/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1492 - acc: 0.9663 - val_loss: 0.3533 - val_acc: 0.8462\n",
            "Epoch 288/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1489 - acc: 0.9663 - val_loss: 0.3535 - val_acc: 0.8462\n",
            "Epoch 289/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1486 - acc: 0.9663 - val_loss: 0.3537 - val_acc: 0.8462\n",
            "Epoch 290/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1484 - acc: 0.9663 - val_loss: 0.3543 - val_acc: 0.8462\n",
            "Epoch 291/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1481 - acc: 0.9663 - val_loss: 0.3550 - val_acc: 0.8462\n",
            "Epoch 292/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1479 - acc: 0.9663 - val_loss: 0.3554 - val_acc: 0.8462\n",
            "Epoch 293/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1476 - acc: 0.9663 - val_loss: 0.3558 - val_acc: 0.8462\n",
            "Epoch 294/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1473 - acc: 0.9663 - val_loss: 0.3565 - val_acc: 0.8462\n",
            "Epoch 295/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1471 - acc: 0.9663 - val_loss: 0.3569 - val_acc: 0.8462\n",
            "Epoch 296/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.1468 - acc: 0.9663 - val_loss: 0.3571 - val_acc: 0.8462\n",
            "Epoch 297/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1465 - acc: 0.9663 - val_loss: 0.3574 - val_acc: 0.8462\n",
            "Epoch 298/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1463 - acc: 0.9663 - val_loss: 0.3579 - val_acc: 0.8462\n",
            "Epoch 299/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.1460 - acc: 0.9663 - val_loss: 0.3580 - val_acc: 0.8462\n",
            "Epoch 300/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1458 - acc: 0.9663 - val_loss: 0.3579 - val_acc: 0.8462\n",
            "Epoch 301/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1455 - acc: 0.9663 - val_loss: 0.3583 - val_acc: 0.8462\n",
            "Epoch 302/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1452 - acc: 0.9663 - val_loss: 0.3589 - val_acc: 0.8462\n",
            "Epoch 303/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1450 - acc: 0.9663 - val_loss: 0.3598 - val_acc: 0.8462\n",
            "Epoch 304/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1447 - acc: 0.9663 - val_loss: 0.3601 - val_acc: 0.8462\n",
            "Epoch 305/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1445 - acc: 0.9663 - val_loss: 0.3600 - val_acc: 0.8462\n",
            "Epoch 306/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.1442 - acc: 0.9663 - val_loss: 0.3602 - val_acc: 0.8462\n",
            "Epoch 307/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1440 - acc: 0.9663 - val_loss: 0.3607 - val_acc: 0.8462\n",
            "Epoch 308/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1437 - acc: 0.9663 - val_loss: 0.3611 - val_acc: 0.8462\n",
            "Epoch 309/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1434 - acc: 0.9663 - val_loss: 0.3612 - val_acc: 0.8462\n",
            "Epoch 310/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1432 - acc: 0.9663 - val_loss: 0.3615 - val_acc: 0.8462\n",
            "Epoch 311/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1429 - acc: 0.9663 - val_loss: 0.3619 - val_acc: 0.8462\n",
            "Epoch 312/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1427 - acc: 0.9663 - val_loss: 0.3622 - val_acc: 0.8462\n",
            "Epoch 313/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.1424 - acc: 0.9663 - val_loss: 0.3628 - val_acc: 0.8462\n",
            "Epoch 314/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1422 - acc: 0.9663 - val_loss: 0.3629 - val_acc: 0.8462\n",
            "Epoch 315/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1419 - acc: 0.9663 - val_loss: 0.3629 - val_acc: 0.8462\n",
            "Epoch 316/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1417 - acc: 0.9663 - val_loss: 0.3633 - val_acc: 0.8462\n",
            "Epoch 317/1000\n",
            "89/89 [==============================] - 0s 97us/step - loss: 0.1414 - acc: 0.9663 - val_loss: 0.3638 - val_acc: 0.8462\n",
            "Epoch 318/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1412 - acc: 0.9663 - val_loss: 0.3640 - val_acc: 0.8462\n",
            "Epoch 319/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1409 - acc: 0.9663 - val_loss: 0.3642 - val_acc: 0.8462\n",
            "Epoch 320/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1407 - acc: 0.9663 - val_loss: 0.3644 - val_acc: 0.8462\n",
            "Epoch 321/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1404 - acc: 0.9663 - val_loss: 0.3649 - val_acc: 0.8462\n",
            "Epoch 322/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1402 - acc: 0.9663 - val_loss: 0.3653 - val_acc: 0.8462\n",
            "Epoch 323/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1399 - acc: 0.9663 - val_loss: 0.3654 - val_acc: 0.8462\n",
            "Epoch 324/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.1396 - acc: 0.9663 - val_loss: 0.3653 - val_acc: 0.8462\n",
            "Epoch 325/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1394 - acc: 0.9663 - val_loss: 0.3656 - val_acc: 0.8462\n",
            "Epoch 326/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1391 - acc: 0.9663 - val_loss: 0.3663 - val_acc: 0.8462\n",
            "Epoch 327/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1389 - acc: 0.9663 - val_loss: 0.3669 - val_acc: 0.8462\n",
            "Epoch 328/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1386 - acc: 0.9775 - val_loss: 0.3670 - val_acc: 0.8462\n",
            "Epoch 329/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1384 - acc: 0.9775 - val_loss: 0.3672 - val_acc: 0.8462\n",
            "Epoch 330/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1381 - acc: 0.9775 - val_loss: 0.3677 - val_acc: 0.8462\n",
            "Epoch 331/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1379 - acc: 0.9775 - val_loss: 0.3682 - val_acc: 0.8462\n",
            "Epoch 332/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1376 - acc: 0.9775 - val_loss: 0.3682 - val_acc: 0.8462\n",
            "Epoch 333/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1374 - acc: 0.9775 - val_loss: 0.3685 - val_acc: 0.8462\n",
            "Epoch 334/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.1371 - acc: 0.9775 - val_loss: 0.3687 - val_acc: 0.8462\n",
            "Epoch 335/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1369 - acc: 0.9775 - val_loss: 0.3687 - val_acc: 0.8462\n",
            "Epoch 336/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1366 - acc: 0.9775 - val_loss: 0.3689 - val_acc: 0.8462\n",
            "Epoch 337/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1364 - acc: 0.9775 - val_loss: 0.3693 - val_acc: 0.8462\n",
            "Epoch 338/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1361 - acc: 0.9775 - val_loss: 0.3698 - val_acc: 0.8462\n",
            "Epoch 339/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1359 - acc: 0.9775 - val_loss: 0.3698 - val_acc: 0.8462\n",
            "Epoch 340/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1356 - acc: 0.9775 - val_loss: 0.3700 - val_acc: 0.8462\n",
            "Epoch 341/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1353 - acc: 0.9775 - val_loss: 0.3704 - val_acc: 0.8462\n",
            "Epoch 342/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1351 - acc: 0.9775 - val_loss: 0.3709 - val_acc: 0.8462\n",
            "Epoch 343/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1348 - acc: 0.9775 - val_loss: 0.3714 - val_acc: 0.8462\n",
            "Epoch 344/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1346 - acc: 0.9775 - val_loss: 0.3720 - val_acc: 0.8462\n",
            "Epoch 345/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.1344 - acc: 0.9775 - val_loss: 0.3724 - val_acc: 0.8462\n",
            "Epoch 346/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.1341 - acc: 0.9775 - val_loss: 0.3724 - val_acc: 0.8462\n",
            "Epoch 347/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1339 - acc: 0.9775 - val_loss: 0.3728 - val_acc: 0.8462\n",
            "Epoch 348/1000\n",
            "89/89 [==============================] - 0s 83us/step - loss: 0.1336 - acc: 0.9775 - val_loss: 0.3733 - val_acc: 0.8462\n",
            "Epoch 349/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1334 - acc: 0.9775 - val_loss: 0.3740 - val_acc: 0.8462\n",
            "Epoch 350/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.1331 - acc: 0.9775 - val_loss: 0.3742 - val_acc: 0.8462\n",
            "Epoch 351/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1329 - acc: 0.9775 - val_loss: 0.3745 - val_acc: 0.8462\n",
            "Epoch 352/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1326 - acc: 0.9775 - val_loss: 0.3746 - val_acc: 0.8462\n",
            "Epoch 353/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1324 - acc: 0.9775 - val_loss: 0.3749 - val_acc: 0.8462\n",
            "Epoch 354/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1321 - acc: 0.9775 - val_loss: 0.3755 - val_acc: 0.8462\n",
            "Epoch 355/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1319 - acc: 0.9775 - val_loss: 0.3761 - val_acc: 0.8462\n",
            "Epoch 356/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1316 - acc: 0.9775 - val_loss: 0.3764 - val_acc: 0.8462\n",
            "Epoch 357/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1314 - acc: 0.9775 - val_loss: 0.3762 - val_acc: 0.8462\n",
            "Epoch 358/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1311 - acc: 0.9775 - val_loss: 0.3765 - val_acc: 0.8462\n",
            "Epoch 359/1000\n",
            "89/89 [==============================] - 0s 85us/step - loss: 0.1309 - acc: 0.9775 - val_loss: 0.3772 - val_acc: 0.8462\n",
            "Epoch 360/1000\n",
            "89/89 [==============================] - 0s 92us/step - loss: 0.1307 - acc: 0.9775 - val_loss: 0.3780 - val_acc: 0.8462\n",
            "Epoch 361/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1304 - acc: 0.9775 - val_loss: 0.3783 - val_acc: 0.8462\n",
            "Epoch 362/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1302 - acc: 0.9775 - val_loss: 0.3785 - val_acc: 0.8462\n",
            "Epoch 363/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1299 - acc: 0.9775 - val_loss: 0.3786 - val_acc: 0.8462\n",
            "Epoch 364/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1297 - acc: 0.9775 - val_loss: 0.3790 - val_acc: 0.8462\n",
            "Epoch 365/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1294 - acc: 0.9775 - val_loss: 0.3796 - val_acc: 0.8462\n",
            "Epoch 366/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1292 - acc: 0.9775 - val_loss: 0.3800 - val_acc: 0.8462\n",
            "Epoch 367/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.1289 - acc: 0.9888 - val_loss: 0.3805 - val_acc: 0.8462\n",
            "Epoch 368/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1287 - acc: 0.9888 - val_loss: 0.3808 - val_acc: 0.8462\n",
            "Epoch 369/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1285 - acc: 0.9888 - val_loss: 0.3812 - val_acc: 0.8462\n",
            "Epoch 370/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1282 - acc: 0.9888 - val_loss: 0.3817 - val_acc: 0.8462\n",
            "Epoch 371/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1280 - acc: 0.9888 - val_loss: 0.3822 - val_acc: 0.8462\n",
            "Epoch 372/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.1277 - acc: 0.9888 - val_loss: 0.3828 - val_acc: 0.8462\n",
            "Epoch 373/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1275 - acc: 0.9888 - val_loss: 0.3831 - val_acc: 0.8462\n",
            "Epoch 374/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1273 - acc: 0.9888 - val_loss: 0.3832 - val_acc: 0.8462\n",
            "Epoch 375/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1270 - acc: 0.9888 - val_loss: 0.3834 - val_acc: 0.8462\n",
            "Epoch 376/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1268 - acc: 0.9888 - val_loss: 0.3837 - val_acc: 0.8462\n",
            "Epoch 377/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1265 - acc: 0.9888 - val_loss: 0.3845 - val_acc: 0.8462\n",
            "Epoch 378/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.1263 - acc: 0.9888 - val_loss: 0.3847 - val_acc: 0.8462\n",
            "Epoch 379/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1260 - acc: 0.9888 - val_loss: 0.3849 - val_acc: 0.8462\n",
            "Epoch 380/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1258 - acc: 0.9888 - val_loss: 0.3851 - val_acc: 0.8462\n",
            "Epoch 381/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1256 - acc: 0.9888 - val_loss: 0.3857 - val_acc: 0.8462\n",
            "Epoch 382/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1253 - acc: 0.9888 - val_loss: 0.3862 - val_acc: 0.8462\n",
            "Epoch 383/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1251 - acc: 0.9888 - val_loss: 0.3864 - val_acc: 0.8462\n",
            "Epoch 384/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1248 - acc: 0.9888 - val_loss: 0.3868 - val_acc: 0.8462\n",
            "Epoch 385/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1246 - acc: 0.9888 - val_loss: 0.3873 - val_acc: 0.8462\n",
            "Epoch 386/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1243 - acc: 0.9888 - val_loss: 0.3877 - val_acc: 0.8462\n",
            "Epoch 387/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1241 - acc: 0.9888 - val_loss: 0.3884 - val_acc: 0.8462\n",
            "Epoch 388/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1239 - acc: 0.9888 - val_loss: 0.3890 - val_acc: 0.8462\n",
            "Epoch 389/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1236 - acc: 0.9888 - val_loss: 0.3894 - val_acc: 0.8462\n",
            "Epoch 390/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1234 - acc: 0.9888 - val_loss: 0.3893 - val_acc: 0.8462\n",
            "Epoch 391/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1231 - acc: 0.9888 - val_loss: 0.3893 - val_acc: 0.8462\n",
            "Epoch 392/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1229 - acc: 0.9888 - val_loss: 0.3897 - val_acc: 0.8462\n",
            "Epoch 393/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1227 - acc: 0.9888 - val_loss: 0.3901 - val_acc: 0.8462\n",
            "Epoch 394/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1224 - acc: 0.9888 - val_loss: 0.3905 - val_acc: 0.8462\n",
            "Epoch 395/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1222 - acc: 0.9888 - val_loss: 0.3910 - val_acc: 0.8462\n",
            "Epoch 396/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1219 - acc: 0.9888 - val_loss: 0.3918 - val_acc: 0.8462\n",
            "Epoch 397/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1217 - acc: 0.9888 - val_loss: 0.3923 - val_acc: 0.8462\n",
            "Epoch 398/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1214 - acc: 0.9888 - val_loss: 0.3927 - val_acc: 0.8462\n",
            "Epoch 399/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1212 - acc: 0.9888 - val_loss: 0.3929 - val_acc: 0.8462\n",
            "Epoch 400/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1210 - acc: 0.9888 - val_loss: 0.3929 - val_acc: 0.8462\n",
            "Epoch 401/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1207 - acc: 0.9888 - val_loss: 0.3932 - val_acc: 0.8462\n",
            "Epoch 402/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1205 - acc: 0.9888 - val_loss: 0.3936 - val_acc: 0.8462\n",
            "Epoch 403/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1202 - acc: 0.9888 - val_loss: 0.3943 - val_acc: 0.8462\n",
            "Epoch 404/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1200 - acc: 0.9888 - val_loss: 0.3947 - val_acc: 0.8462\n",
            "Epoch 405/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1198 - acc: 0.9888 - val_loss: 0.3949 - val_acc: 0.8462\n",
            "Epoch 406/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1195 - acc: 0.9888 - val_loss: 0.3952 - val_acc: 0.8462\n",
            "Epoch 407/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1193 - acc: 0.9888 - val_loss: 0.3957 - val_acc: 0.8462\n",
            "Epoch 408/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1190 - acc: 0.9888 - val_loss: 0.3963 - val_acc: 0.8462\n",
            "Epoch 409/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1188 - acc: 0.9888 - val_loss: 0.3968 - val_acc: 0.8462\n",
            "Epoch 410/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1186 - acc: 0.9888 - val_loss: 0.3971 - val_acc: 0.8462\n",
            "Epoch 411/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1183 - acc: 0.9888 - val_loss: 0.3977 - val_acc: 0.8462\n",
            "Epoch 412/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1181 - acc: 0.9888 - val_loss: 0.3984 - val_acc: 0.8462\n",
            "Epoch 413/1000\n",
            "89/89 [==============================] - 0s 91us/step - loss: 0.1179 - acc: 0.9888 - val_loss: 0.3990 - val_acc: 0.8462\n",
            "Epoch 414/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1176 - acc: 0.9888 - val_loss: 0.3994 - val_acc: 0.8462\n",
            "Epoch 415/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.1174 - acc: 0.9888 - val_loss: 0.3997 - val_acc: 0.8462\n",
            "Epoch 416/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1171 - acc: 0.9888 - val_loss: 0.3999 - val_acc: 0.8462\n",
            "Epoch 417/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1169 - acc: 0.9888 - val_loss: 0.4005 - val_acc: 0.8462\n",
            "Epoch 418/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1167 - acc: 0.9888 - val_loss: 0.4011 - val_acc: 0.8462\n",
            "Epoch 419/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1164 - acc: 0.9888 - val_loss: 0.4016 - val_acc: 0.8462\n",
            "Epoch 420/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1162 - acc: 0.9888 - val_loss: 0.4023 - val_acc: 0.8462\n",
            "Epoch 421/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1160 - acc: 0.9888 - val_loss: 0.4028 - val_acc: 0.8462\n",
            "Epoch 422/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1157 - acc: 0.9888 - val_loss: 0.4031 - val_acc: 0.8462\n",
            "Epoch 423/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1155 - acc: 0.9888 - val_loss: 0.4035 - val_acc: 0.8462\n",
            "Epoch 424/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1152 - acc: 0.9888 - val_loss: 0.4042 - val_acc: 0.8462\n",
            "Epoch 425/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1150 - acc: 0.9888 - val_loss: 0.4049 - val_acc: 0.8462\n",
            "Epoch 426/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1148 - acc: 0.9888 - val_loss: 0.4054 - val_acc: 0.8462\n",
            "Epoch 427/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.1145 - acc: 0.9888 - val_loss: 0.4056 - val_acc: 0.8462\n",
            "Epoch 428/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1143 - acc: 0.9888 - val_loss: 0.4057 - val_acc: 0.8462\n",
            "Epoch 429/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1140 - acc: 0.9888 - val_loss: 0.4058 - val_acc: 0.8462\n",
            "Epoch 430/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1138 - acc: 0.9888 - val_loss: 0.4062 - val_acc: 0.8462\n",
            "Epoch 431/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1136 - acc: 0.9888 - val_loss: 0.4070 - val_acc: 0.8462\n",
            "Epoch 432/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1133 - acc: 0.9888 - val_loss: 0.4079 - val_acc: 0.8462\n",
            "Epoch 433/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1131 - acc: 0.9888 - val_loss: 0.4087 - val_acc: 0.8462\n",
            "Epoch 434/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.1129 - acc: 0.9888 - val_loss: 0.4094 - val_acc: 0.8462\n",
            "Epoch 435/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1126 - acc: 0.9888 - val_loss: 0.4099 - val_acc: 0.8462\n",
            "Epoch 436/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1124 - acc: 0.9888 - val_loss: 0.4101 - val_acc: 0.8462\n",
            "Epoch 437/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1122 - acc: 0.9888 - val_loss: 0.4101 - val_acc: 0.8462\n",
            "Epoch 438/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1119 - acc: 0.9888 - val_loss: 0.4104 - val_acc: 0.8462\n",
            "Epoch 439/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1117 - acc: 0.9888 - val_loss: 0.4111 - val_acc: 0.8462\n",
            "Epoch 440/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.1115 - acc: 0.9888 - val_loss: 0.4119 - val_acc: 0.8462\n",
            "Epoch 441/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1112 - acc: 0.9888 - val_loss: 0.4127 - val_acc: 0.8462\n",
            "Epoch 442/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1110 - acc: 0.9888 - val_loss: 0.4133 - val_acc: 0.8462\n",
            "Epoch 443/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1108 - acc: 0.9888 - val_loss: 0.4140 - val_acc: 0.8462\n",
            "Epoch 444/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.1105 - acc: 0.9888 - val_loss: 0.4147 - val_acc: 0.8462\n",
            "Epoch 445/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1103 - acc: 0.9888 - val_loss: 0.4149 - val_acc: 0.8462\n",
            "Epoch 446/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1101 - acc: 0.9888 - val_loss: 0.4149 - val_acc: 0.8462\n",
            "Epoch 447/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.1098 - acc: 0.9888 - val_loss: 0.4150 - val_acc: 0.8462\n",
            "Epoch 448/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1096 - acc: 0.9888 - val_loss: 0.4156 - val_acc: 0.8462\n",
            "Epoch 449/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1094 - acc: 0.9888 - val_loss: 0.4164 - val_acc: 0.8462\n",
            "Epoch 450/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1091 - acc: 0.9888 - val_loss: 0.4171 - val_acc: 0.8462\n",
            "Epoch 451/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1089 - acc: 0.9888 - val_loss: 0.4174 - val_acc: 0.8462\n",
            "Epoch 452/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.1087 - acc: 0.9888 - val_loss: 0.4176 - val_acc: 0.8462\n",
            "Epoch 453/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1084 - acc: 0.9888 - val_loss: 0.4178 - val_acc: 0.8462\n",
            "Epoch 454/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1082 - acc: 0.9888 - val_loss: 0.4185 - val_acc: 0.8462\n",
            "Epoch 455/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1080 - acc: 0.9888 - val_loss: 0.4196 - val_acc: 0.8462\n",
            "Epoch 456/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1077 - acc: 0.9888 - val_loss: 0.4205 - val_acc: 0.8462\n",
            "Epoch 457/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1075 - acc: 0.9888 - val_loss: 0.4209 - val_acc: 0.8462\n",
            "Epoch 458/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1073 - acc: 0.9888 - val_loss: 0.4210 - val_acc: 0.8462\n",
            "Epoch 459/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1071 - acc: 0.9888 - val_loss: 0.4217 - val_acc: 0.8462\n",
            "Epoch 460/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1068 - acc: 0.9888 - val_loss: 0.4225 - val_acc: 0.8462\n",
            "Epoch 461/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1066 - acc: 0.9888 - val_loss: 0.4231 - val_acc: 0.8462\n",
            "Epoch 462/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1064 - acc: 0.9888 - val_loss: 0.4234 - val_acc: 0.8462\n",
            "Epoch 463/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1061 - acc: 0.9888 - val_loss: 0.4237 - val_acc: 0.8462\n",
            "Epoch 464/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1059 - acc: 0.9888 - val_loss: 0.4241 - val_acc: 0.8462\n",
            "Epoch 465/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1057 - acc: 0.9888 - val_loss: 0.4245 - val_acc: 0.8462\n",
            "Epoch 466/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1055 - acc: 0.9888 - val_loss: 0.4250 - val_acc: 0.8462\n",
            "Epoch 467/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1052 - acc: 0.9888 - val_loss: 0.4256 - val_acc: 0.8462\n",
            "Epoch 468/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1050 - acc: 0.9888 - val_loss: 0.4262 - val_acc: 0.8462\n",
            "Epoch 469/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1048 - acc: 0.9888 - val_loss: 0.4265 - val_acc: 0.8462\n",
            "Epoch 470/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1046 - acc: 0.9888 - val_loss: 0.4267 - val_acc: 0.8462\n",
            "Epoch 471/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1043 - acc: 0.9888 - val_loss: 0.4271 - val_acc: 0.8462\n",
            "Epoch 472/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1041 - acc: 0.9888 - val_loss: 0.4278 - val_acc: 0.8462\n",
            "Epoch 473/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1039 - acc: 0.9888 - val_loss: 0.4288 - val_acc: 0.8462\n",
            "Epoch 474/1000\n",
            "89/89 [==============================] - 0s 147us/step - loss: 0.1037 - acc: 0.9888 - val_loss: 0.4296 - val_acc: 0.8462\n",
            "Epoch 475/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1035 - acc: 0.9888 - val_loss: 0.4302 - val_acc: 0.8462\n",
            "Epoch 476/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1032 - acc: 0.9888 - val_loss: 0.4307 - val_acc: 0.8462\n",
            "Epoch 477/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1030 - acc: 0.9888 - val_loss: 0.4310 - val_acc: 0.8462\n",
            "Epoch 478/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1028 - acc: 0.9888 - val_loss: 0.4314 - val_acc: 0.8462\n",
            "Epoch 479/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1026 - acc: 0.9888 - val_loss: 0.4317 - val_acc: 0.8462\n",
            "Epoch 480/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1024 - acc: 0.9888 - val_loss: 0.4319 - val_acc: 0.8462\n",
            "Epoch 481/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1021 - acc: 0.9888 - val_loss: 0.4322 - val_acc: 0.8462\n",
            "Epoch 482/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1019 - acc: 0.9888 - val_loss: 0.4326 - val_acc: 0.8462\n",
            "Epoch 483/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1017 - acc: 0.9888 - val_loss: 0.4334 - val_acc: 0.8462\n",
            "Epoch 484/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1015 - acc: 0.9888 - val_loss: 0.4344 - val_acc: 0.8462\n",
            "Epoch 485/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1013 - acc: 0.9888 - val_loss: 0.4351 - val_acc: 0.8462\n",
            "Epoch 486/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1011 - acc: 0.9888 - val_loss: 0.4355 - val_acc: 0.8462\n",
            "Epoch 487/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1008 - acc: 0.9888 - val_loss: 0.4357 - val_acc: 0.8462\n",
            "Epoch 488/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1006 - acc: 0.9888 - val_loss: 0.4362 - val_acc: 0.8462\n",
            "Epoch 489/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1004 - acc: 0.9888 - val_loss: 0.4370 - val_acc: 0.8462\n",
            "Epoch 490/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1002 - acc: 0.9888 - val_loss: 0.4377 - val_acc: 0.8462\n",
            "Epoch 491/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1000 - acc: 0.9888 - val_loss: 0.4386 - val_acc: 0.8462\n",
            "Epoch 492/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0998 - acc: 0.9888 - val_loss: 0.4393 - val_acc: 0.8462\n",
            "Epoch 493/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0996 - acc: 0.9888 - val_loss: 0.4397 - val_acc: 0.8462\n",
            "Epoch 494/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0993 - acc: 0.9888 - val_loss: 0.4398 - val_acc: 0.8462\n",
            "Epoch 495/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0991 - acc: 0.9888 - val_loss: 0.4403 - val_acc: 0.8462\n",
            "Epoch 496/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.0989 - acc: 0.9888 - val_loss: 0.4410 - val_acc: 0.8462\n",
            "Epoch 497/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0987 - acc: 0.9888 - val_loss: 0.4417 - val_acc: 0.8462\n",
            "Epoch 498/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0985 - acc: 1.0000 - val_loss: 0.4425 - val_acc: 0.8462\n",
            "Epoch 499/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0983 - acc: 1.0000 - val_loss: 0.4431 - val_acc: 0.8462\n",
            "Epoch 500/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0981 - acc: 1.0000 - val_loss: 0.4435 - val_acc: 0.8462\n",
            "Epoch 501/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0979 - acc: 1.0000 - val_loss: 0.4440 - val_acc: 0.8462\n",
            "Epoch 502/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0976 - acc: 1.0000 - val_loss: 0.4446 - val_acc: 0.8462\n",
            "Epoch 503/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0974 - acc: 1.0000 - val_loss: 0.4452 - val_acc: 0.8462\n",
            "Epoch 504/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0972 - acc: 1.0000 - val_loss: 0.4458 - val_acc: 0.8462\n",
            "Epoch 505/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0970 - acc: 1.0000 - val_loss: 0.4465 - val_acc: 0.8462\n",
            "Epoch 506/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0968 - acc: 1.0000 - val_loss: 0.4471 - val_acc: 0.8462\n",
            "Epoch 507/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.0966 - acc: 1.0000 - val_loss: 0.4479 - val_acc: 0.8462\n",
            "Epoch 508/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0964 - acc: 1.0000 - val_loss: 0.4485 - val_acc: 0.8462\n",
            "Epoch 509/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0962 - acc: 1.0000 - val_loss: 0.4493 - val_acc: 0.8462\n",
            "Epoch 510/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0960 - acc: 1.0000 - val_loss: 0.4498 - val_acc: 0.8462\n",
            "Epoch 511/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0958 - acc: 1.0000 - val_loss: 0.4500 - val_acc: 0.8462\n",
            "Epoch 512/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0956 - acc: 1.0000 - val_loss: 0.4505 - val_acc: 0.8462\n",
            "Epoch 513/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0954 - acc: 1.0000 - val_loss: 0.4511 - val_acc: 0.8462\n",
            "Epoch 514/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0951 - acc: 1.0000 - val_loss: 0.4519 - val_acc: 0.8462\n",
            "Epoch 515/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0949 - acc: 1.0000 - val_loss: 0.4527 - val_acc: 0.8462\n",
            "Epoch 516/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0947 - acc: 1.0000 - val_loss: 0.4533 - val_acc: 0.8462\n",
            "Epoch 517/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0945 - acc: 1.0000 - val_loss: 0.4537 - val_acc: 0.8462\n",
            "Epoch 518/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0943 - acc: 1.0000 - val_loss: 0.4540 - val_acc: 0.8462\n",
            "Epoch 519/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0941 - acc: 1.0000 - val_loss: 0.4547 - val_acc: 0.8462\n",
            "Epoch 520/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0939 - acc: 1.0000 - val_loss: 0.4555 - val_acc: 0.8462\n",
            "Epoch 521/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0937 - acc: 1.0000 - val_loss: 0.4563 - val_acc: 0.8462\n",
            "Epoch 522/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.0935 - acc: 1.0000 - val_loss: 0.4569 - val_acc: 0.8462\n",
            "Epoch 523/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.0933 - acc: 1.0000 - val_loss: 0.4575 - val_acc: 0.8462\n",
            "Epoch 524/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0931 - acc: 1.0000 - val_loss: 0.4579 - val_acc: 0.8462\n",
            "Epoch 525/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.0929 - acc: 1.0000 - val_loss: 0.4585 - val_acc: 0.8462\n",
            "Epoch 526/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0927 - acc: 1.0000 - val_loss: 0.4589 - val_acc: 0.8462\n",
            "Epoch 527/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0925 - acc: 1.0000 - val_loss: 0.4598 - val_acc: 0.8462\n",
            "Epoch 528/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0923 - acc: 1.0000 - val_loss: 0.4606 - val_acc: 0.8462\n",
            "Epoch 529/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0921 - acc: 1.0000 - val_loss: 0.4610 - val_acc: 0.8462\n",
            "Epoch 530/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.0919 - acc: 1.0000 - val_loss: 0.4611 - val_acc: 0.8462\n",
            "Epoch 531/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0917 - acc: 1.0000 - val_loss: 0.4614 - val_acc: 0.8462\n",
            "Epoch 532/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.0915 - acc: 1.0000 - val_loss: 0.4620 - val_acc: 0.8462\n",
            "Epoch 533/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0913 - acc: 1.0000 - val_loss: 0.4632 - val_acc: 0.8462\n",
            "Epoch 534/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0911 - acc: 1.0000 - val_loss: 0.4641 - val_acc: 0.8462\n",
            "Epoch 535/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0909 - acc: 1.0000 - val_loss: 0.4649 - val_acc: 0.8462\n",
            "Epoch 536/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0907 - acc: 1.0000 - val_loss: 0.4656 - val_acc: 0.8462\n",
            "Epoch 537/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0905 - acc: 1.0000 - val_loss: 0.4661 - val_acc: 0.8462\n",
            "Epoch 538/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0903 - acc: 1.0000 - val_loss: 0.4665 - val_acc: 0.8462\n",
            "Epoch 539/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0901 - acc: 1.0000 - val_loss: 0.4670 - val_acc: 0.8462\n",
            "Epoch 540/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0899 - acc: 1.0000 - val_loss: 0.4677 - val_acc: 0.8462\n",
            "Epoch 541/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0897 - acc: 1.0000 - val_loss: 0.4683 - val_acc: 0.8462\n",
            "Epoch 542/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0895 - acc: 1.0000 - val_loss: 0.4689 - val_acc: 0.8462\n",
            "Epoch 543/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0893 - acc: 1.0000 - val_loss: 0.4695 - val_acc: 0.8462\n",
            "Epoch 544/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.0891 - acc: 1.0000 - val_loss: 0.4701 - val_acc: 0.8462\n",
            "Epoch 545/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0889 - acc: 1.0000 - val_loss: 0.4704 - val_acc: 0.8462\n",
            "Epoch 546/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0887 - acc: 1.0000 - val_loss: 0.4710 - val_acc: 0.8462\n",
            "Epoch 547/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0885 - acc: 1.0000 - val_loss: 0.4715 - val_acc: 0.8462\n",
            "Epoch 548/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0883 - acc: 1.0000 - val_loss: 0.4720 - val_acc: 0.8462\n",
            "Epoch 549/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0881 - acc: 1.0000 - val_loss: 0.4727 - val_acc: 0.8462\n",
            "Epoch 550/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0879 - acc: 1.0000 - val_loss: 0.4732 - val_acc: 0.8462\n",
            "Epoch 551/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0877 - acc: 1.0000 - val_loss: 0.4737 - val_acc: 0.8462\n",
            "Epoch 552/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0875 - acc: 1.0000 - val_loss: 0.4742 - val_acc: 0.8462\n",
            "Epoch 553/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0873 - acc: 1.0000 - val_loss: 0.4749 - val_acc: 0.8462\n",
            "Epoch 554/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0871 - acc: 1.0000 - val_loss: 0.4757 - val_acc: 0.8462\n",
            "Epoch 555/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0869 - acc: 1.0000 - val_loss: 0.4761 - val_acc: 0.8462\n",
            "Epoch 556/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0868 - acc: 1.0000 - val_loss: 0.4767 - val_acc: 0.8462\n",
            "Epoch 557/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0866 - acc: 1.0000 - val_loss: 0.4778 - val_acc: 0.8462\n",
            "Epoch 558/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0864 - acc: 1.0000 - val_loss: 0.4785 - val_acc: 0.8462\n",
            "Epoch 559/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0862 - acc: 1.0000 - val_loss: 0.4789 - val_acc: 0.8462\n",
            "Epoch 560/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0860 - acc: 1.0000 - val_loss: 0.4790 - val_acc: 0.8462\n",
            "Epoch 561/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0858 - acc: 1.0000 - val_loss: 0.4790 - val_acc: 0.8462\n",
            "Epoch 562/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0856 - acc: 1.0000 - val_loss: 0.4794 - val_acc: 0.8462\n",
            "Epoch 563/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0854 - acc: 1.0000 - val_loss: 0.4803 - val_acc: 0.8462\n",
            "Epoch 564/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0852 - acc: 1.0000 - val_loss: 0.4813 - val_acc: 0.8462\n",
            "Epoch 565/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0851 - acc: 1.0000 - val_loss: 0.4819 - val_acc: 0.8462\n",
            "Epoch 566/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0849 - acc: 1.0000 - val_loss: 0.4823 - val_acc: 0.8462\n",
            "Epoch 567/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0847 - acc: 1.0000 - val_loss: 0.4828 - val_acc: 0.8462\n",
            "Epoch 568/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0845 - acc: 1.0000 - val_loss: 0.4834 - val_acc: 0.8462\n",
            "Epoch 569/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0843 - acc: 1.0000 - val_loss: 0.4842 - val_acc: 0.8462\n",
            "Epoch 570/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0841 - acc: 1.0000 - val_loss: 0.4848 - val_acc: 0.8462\n",
            "Epoch 571/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0839 - acc: 1.0000 - val_loss: 0.4852 - val_acc: 0.8462\n",
            "Epoch 572/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0838 - acc: 1.0000 - val_loss: 0.4855 - val_acc: 0.8462\n",
            "Epoch 573/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0836 - acc: 1.0000 - val_loss: 0.4861 - val_acc: 0.8462\n",
            "Epoch 574/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0834 - acc: 1.0000 - val_loss: 0.4871 - val_acc: 0.8462\n",
            "Epoch 575/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0832 - acc: 1.0000 - val_loss: 0.4883 - val_acc: 0.8462\n",
            "Epoch 576/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0830 - acc: 1.0000 - val_loss: 0.4893 - val_acc: 0.8462\n",
            "Epoch 577/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0829 - acc: 1.0000 - val_loss: 0.4898 - val_acc: 0.8462\n",
            "Epoch 578/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0827 - acc: 1.0000 - val_loss: 0.4899 - val_acc: 0.8462\n",
            "Epoch 579/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0825 - acc: 1.0000 - val_loss: 0.4898 - val_acc: 0.8462\n",
            "Epoch 580/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0823 - acc: 1.0000 - val_loss: 0.4897 - val_acc: 0.8462\n",
            "Epoch 581/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0821 - acc: 1.0000 - val_loss: 0.4901 - val_acc: 0.8462\n",
            "Epoch 582/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0819 - acc: 1.0000 - val_loss: 0.4908 - val_acc: 0.8462\n",
            "Epoch 583/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0818 - acc: 1.0000 - val_loss: 0.4916 - val_acc: 0.8462\n",
            "Epoch 584/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0816 - acc: 1.0000 - val_loss: 0.4921 - val_acc: 0.8462\n",
            "Epoch 585/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0814 - acc: 1.0000 - val_loss: 0.4927 - val_acc: 0.8462\n",
            "Epoch 586/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0812 - acc: 1.0000 - val_loss: 0.4931 - val_acc: 0.8462\n",
            "Epoch 587/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0810 - acc: 1.0000 - val_loss: 0.4935 - val_acc: 0.8462\n",
            "Epoch 588/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.0809 - acc: 1.0000 - val_loss: 0.4940 - val_acc: 0.8462\n",
            "Epoch 589/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0807 - acc: 1.0000 - val_loss: 0.4944 - val_acc: 0.8462\n",
            "Epoch 590/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0805 - acc: 1.0000 - val_loss: 0.4948 - val_acc: 0.8462\n",
            "Epoch 591/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0803 - acc: 1.0000 - val_loss: 0.4956 - val_acc: 0.8462\n",
            "Epoch 592/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0802 - acc: 1.0000 - val_loss: 0.4963 - val_acc: 0.8462\n",
            "Epoch 593/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0800 - acc: 1.0000 - val_loss: 0.4969 - val_acc: 0.8462\n",
            "Epoch 594/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0798 - acc: 1.0000 - val_loss: 0.4974 - val_acc: 0.8462\n",
            "Epoch 595/1000\n",
            "89/89 [==============================] - 0s 100us/step - loss: 0.0796 - acc: 1.0000 - val_loss: 0.4981 - val_acc: 0.8462\n",
            "Epoch 596/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0794 - acc: 1.0000 - val_loss: 0.4987 - val_acc: 0.8462\n",
            "Epoch 597/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.0793 - acc: 1.0000 - val_loss: 0.4994 - val_acc: 0.8462\n",
            "Epoch 598/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0791 - acc: 1.0000 - val_loss: 0.4999 - val_acc: 0.8462\n",
            "Epoch 599/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.0789 - acc: 1.0000 - val_loss: 0.5001 - val_acc: 0.8462\n",
            "Epoch 600/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0787 - acc: 1.0000 - val_loss: 0.5006 - val_acc: 0.8462\n",
            "Epoch 601/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0786 - acc: 1.0000 - val_loss: 0.5012 - val_acc: 0.8462\n",
            "Epoch 602/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0784 - acc: 1.0000 - val_loss: 0.5018 - val_acc: 0.8462\n",
            "Epoch 603/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0782 - acc: 1.0000 - val_loss: 0.5022 - val_acc: 0.8462\n",
            "Epoch 604/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0780 - acc: 1.0000 - val_loss: 0.5028 - val_acc: 0.8462\n",
            "Epoch 605/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0779 - acc: 1.0000 - val_loss: 0.5036 - val_acc: 0.8462\n",
            "Epoch 606/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0777 - acc: 1.0000 - val_loss: 0.5043 - val_acc: 0.8462\n",
            "Epoch 607/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0775 - acc: 1.0000 - val_loss: 0.5049 - val_acc: 0.8462\n",
            "Epoch 608/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0773 - acc: 1.0000 - val_loss: 0.5053 - val_acc: 0.8462\n",
            "Epoch 609/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0772 - acc: 1.0000 - val_loss: 0.5059 - val_acc: 0.8462\n",
            "Epoch 610/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.0770 - acc: 1.0000 - val_loss: 0.5063 - val_acc: 0.8462\n",
            "Epoch 611/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0768 - acc: 1.0000 - val_loss: 0.5068 - val_acc: 0.8462\n",
            "Epoch 612/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0767 - acc: 1.0000 - val_loss: 0.5076 - val_acc: 0.8462\n",
            "Epoch 613/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0765 - acc: 1.0000 - val_loss: 0.5084 - val_acc: 0.8462\n",
            "Epoch 614/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0763 - acc: 1.0000 - val_loss: 0.5091 - val_acc: 0.8462\n",
            "Epoch 615/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0761 - acc: 1.0000 - val_loss: 0.5096 - val_acc: 0.8462\n",
            "Epoch 616/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0760 - acc: 1.0000 - val_loss: 0.5102 - val_acc: 0.8462\n",
            "Epoch 617/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0758 - acc: 1.0000 - val_loss: 0.5105 - val_acc: 0.8462\n",
            "Epoch 618/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0756 - acc: 1.0000 - val_loss: 0.5107 - val_acc: 0.8462\n",
            "Epoch 619/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0755 - acc: 1.0000 - val_loss: 0.5112 - val_acc: 0.8462\n",
            "Epoch 620/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0753 - acc: 1.0000 - val_loss: 0.5117 - val_acc: 0.8462\n",
            "Epoch 621/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0751 - acc: 1.0000 - val_loss: 0.5122 - val_acc: 0.8462\n",
            "Epoch 622/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0750 - acc: 1.0000 - val_loss: 0.5129 - val_acc: 0.8462\n",
            "Epoch 623/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0748 - acc: 1.0000 - val_loss: 0.5135 - val_acc: 0.8462\n",
            "Epoch 624/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0746 - acc: 1.0000 - val_loss: 0.5141 - val_acc: 0.8462\n",
            "Epoch 625/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0745 - acc: 1.0000 - val_loss: 0.5145 - val_acc: 0.8462\n",
            "Epoch 626/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0743 - acc: 1.0000 - val_loss: 0.5154 - val_acc: 0.8462\n",
            "Epoch 627/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.0741 - acc: 1.0000 - val_loss: 0.5166 - val_acc: 0.8462\n",
            "Epoch 628/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.0740 - acc: 1.0000 - val_loss: 0.5173 - val_acc: 0.8462\n",
            "Epoch 629/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0738 - acc: 1.0000 - val_loss: 0.5177 - val_acc: 0.8462\n",
            "Epoch 630/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0736 - acc: 1.0000 - val_loss: 0.5177 - val_acc: 0.8462\n",
            "Epoch 631/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0735 - acc: 1.0000 - val_loss: 0.5180 - val_acc: 0.8462\n",
            "Epoch 632/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0733 - acc: 1.0000 - val_loss: 0.5186 - val_acc: 0.8462\n",
            "Epoch 633/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0731 - acc: 1.0000 - val_loss: 0.5194 - val_acc: 0.8462\n",
            "Epoch 634/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0730 - acc: 1.0000 - val_loss: 0.5200 - val_acc: 0.8462\n",
            "Epoch 635/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0728 - acc: 1.0000 - val_loss: 0.5205 - val_acc: 0.8462\n",
            "Epoch 636/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0727 - acc: 1.0000 - val_loss: 0.5208 - val_acc: 0.8462\n",
            "Epoch 637/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0725 - acc: 1.0000 - val_loss: 0.5214 - val_acc: 0.8462\n",
            "Epoch 638/1000\n",
            "89/89 [==============================] - 0s 107us/step - loss: 0.0723 - acc: 1.0000 - val_loss: 0.5220 - val_acc: 0.8462\n",
            "Epoch 639/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0722 - acc: 1.0000 - val_loss: 0.5226 - val_acc: 0.8462\n",
            "Epoch 640/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0720 - acc: 1.0000 - val_loss: 0.5233 - val_acc: 0.8462\n",
            "Epoch 641/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.0719 - acc: 1.0000 - val_loss: 0.5238 - val_acc: 0.8462\n",
            "Epoch 642/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0717 - acc: 1.0000 - val_loss: 0.5243 - val_acc: 0.8462\n",
            "Epoch 643/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.0715 - acc: 1.0000 - val_loss: 0.5249 - val_acc: 0.8462\n",
            "Epoch 644/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0714 - acc: 1.0000 - val_loss: 0.5253 - val_acc: 0.8462\n",
            "Epoch 645/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0712 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 0.8462\n",
            "Epoch 646/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0711 - acc: 1.0000 - val_loss: 0.5265 - val_acc: 0.8462\n",
            "Epoch 647/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0709 - acc: 1.0000 - val_loss: 0.5271 - val_acc: 0.8462\n",
            "Epoch 648/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0707 - acc: 1.0000 - val_loss: 0.5277 - val_acc: 0.8462\n",
            "Epoch 649/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0706 - acc: 1.0000 - val_loss: 0.5285 - val_acc: 0.8462\n",
            "Epoch 650/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0704 - acc: 1.0000 - val_loss: 0.5290 - val_acc: 0.8462\n",
            "Epoch 651/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0703 - acc: 1.0000 - val_loss: 0.5295 - val_acc: 0.8462\n",
            "Epoch 652/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0701 - acc: 1.0000 - val_loss: 0.5302 - val_acc: 0.8462\n",
            "Epoch 653/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0700 - acc: 1.0000 - val_loss: 0.5306 - val_acc: 0.8462\n",
            "Epoch 654/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0698 - acc: 1.0000 - val_loss: 0.5311 - val_acc: 0.8462\n",
            "Epoch 655/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0697 - acc: 1.0000 - val_loss: 0.5315 - val_acc: 0.8462\n",
            "Epoch 656/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0695 - acc: 1.0000 - val_loss: 0.5321 - val_acc: 0.8462\n",
            "Epoch 657/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.0693 - acc: 1.0000 - val_loss: 0.5328 - val_acc: 0.8462\n",
            "Epoch 658/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0692 - acc: 1.0000 - val_loss: 0.5335 - val_acc: 0.8462\n",
            "Epoch 659/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.0690 - acc: 1.0000 - val_loss: 0.5340 - val_acc: 0.8462\n",
            "Epoch 660/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.0689 - acc: 1.0000 - val_loss: 0.5343 - val_acc: 0.8462\n",
            "Epoch 661/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0687 - acc: 1.0000 - val_loss: 0.5351 - val_acc: 0.8462\n",
            "Epoch 662/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0686 - acc: 1.0000 - val_loss: 0.5357 - val_acc: 0.8462\n",
            "Epoch 663/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0684 - acc: 1.0000 - val_loss: 0.5360 - val_acc: 0.8462\n",
            "Epoch 664/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0683 - acc: 1.0000 - val_loss: 0.5364 - val_acc: 0.8462\n",
            "Epoch 665/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.0681 - acc: 1.0000 - val_loss: 0.5371 - val_acc: 0.8462\n",
            "Epoch 666/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0680 - acc: 1.0000 - val_loss: 0.5378 - val_acc: 0.8462\n",
            "Epoch 667/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0678 - acc: 1.0000 - val_loss: 0.5384 - val_acc: 0.8462\n",
            "Epoch 668/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0677 - acc: 1.0000 - val_loss: 0.5390 - val_acc: 0.8462\n",
            "Epoch 669/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0675 - acc: 1.0000 - val_loss: 0.5396 - val_acc: 0.8462\n",
            "Epoch 670/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0674 - acc: 1.0000 - val_loss: 0.5400 - val_acc: 0.8462\n",
            "Epoch 671/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0672 - acc: 1.0000 - val_loss: 0.5404 - val_acc: 0.8462\n",
            "Epoch 672/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0671 - acc: 1.0000 - val_loss: 0.5409 - val_acc: 0.8462\n",
            "Epoch 673/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0669 - acc: 1.0000 - val_loss: 0.5415 - val_acc: 0.8462\n",
            "Epoch 674/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0668 - acc: 1.0000 - val_loss: 0.5421 - val_acc: 0.8462\n",
            "Epoch 675/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0666 - acc: 1.0000 - val_loss: 0.5426 - val_acc: 0.8462\n",
            "Epoch 676/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0665 - acc: 1.0000 - val_loss: 0.5432 - val_acc: 0.8462\n",
            "Epoch 677/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0663 - acc: 1.0000 - val_loss: 0.5437 - val_acc: 0.8462\n",
            "Epoch 678/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0662 - acc: 1.0000 - val_loss: 0.5440 - val_acc: 0.8462\n",
            "Epoch 679/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0660 - acc: 1.0000 - val_loss: 0.5446 - val_acc: 0.8462\n",
            "Epoch 680/1000\n",
            "89/89 [==============================] - 0s 85us/step - loss: 0.0659 - acc: 1.0000 - val_loss: 0.5454 - val_acc: 0.8462\n",
            "Epoch 681/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0657 - acc: 1.0000 - val_loss: 0.5461 - val_acc: 0.8462\n",
            "Epoch 682/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0656 - acc: 1.0000 - val_loss: 0.5465 - val_acc: 0.8462\n",
            "Epoch 683/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0655 - acc: 1.0000 - val_loss: 0.5470 - val_acc: 0.8462\n",
            "Epoch 684/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0653 - acc: 1.0000 - val_loss: 0.5478 - val_acc: 0.8462\n",
            "Epoch 685/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0652 - acc: 1.0000 - val_loss: 0.5485 - val_acc: 0.8462\n",
            "Epoch 686/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.0650 - acc: 1.0000 - val_loss: 0.5489 - val_acc: 0.8462\n",
            "Epoch 687/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0649 - acc: 1.0000 - val_loss: 0.5491 - val_acc: 0.8462\n",
            "Epoch 688/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0647 - acc: 1.0000 - val_loss: 0.5496 - val_acc: 0.8462\n",
            "Epoch 689/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0646 - acc: 1.0000 - val_loss: 0.5502 - val_acc: 0.8462\n",
            "Epoch 690/1000\n",
            "89/89 [==============================] - 0s 92us/step - loss: 0.0644 - acc: 1.0000 - val_loss: 0.5512 - val_acc: 0.8462\n",
            "Epoch 691/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0643 - acc: 1.0000 - val_loss: 0.5520 - val_acc: 0.8462\n",
            "Epoch 692/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.0642 - acc: 1.0000 - val_loss: 0.5526 - val_acc: 0.8462\n",
            "Epoch 693/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0640 - acc: 1.0000 - val_loss: 0.5530 - val_acc: 0.8462\n",
            "Epoch 694/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.0639 - acc: 1.0000 - val_loss: 0.5534 - val_acc: 0.8462\n",
            "Epoch 695/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0637 - acc: 1.0000 - val_loss: 0.5541 - val_acc: 0.8462\n",
            "Epoch 696/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0636 - acc: 1.0000 - val_loss: 0.5543 - val_acc: 0.8462\n",
            "Epoch 697/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0635 - acc: 1.0000 - val_loss: 0.5546 - val_acc: 0.8462\n",
            "Epoch 698/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0633 - acc: 1.0000 - val_loss: 0.5548 - val_acc: 0.8462\n",
            "Epoch 699/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0632 - acc: 1.0000 - val_loss: 0.5555 - val_acc: 0.8462\n",
            "Epoch 700/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0630 - acc: 1.0000 - val_loss: 0.5564 - val_acc: 0.8462\n",
            "Epoch 701/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0629 - acc: 1.0000 - val_loss: 0.5572 - val_acc: 0.8462\n",
            "Epoch 702/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0628 - acc: 1.0000 - val_loss: 0.5577 - val_acc: 0.8462\n",
            "Epoch 703/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0626 - acc: 1.0000 - val_loss: 0.5582 - val_acc: 0.8462\n",
            "Epoch 704/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0625 - acc: 1.0000 - val_loss: 0.5585 - val_acc: 0.8462\n",
            "Epoch 705/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0624 - acc: 1.0000 - val_loss: 0.5591 - val_acc: 0.8462\n",
            "Epoch 706/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0622 - acc: 1.0000 - val_loss: 0.5598 - val_acc: 0.8462\n",
            "Epoch 707/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0621 - acc: 1.0000 - val_loss: 0.5605 - val_acc: 0.8462\n",
            "Epoch 708/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0619 - acc: 1.0000 - val_loss: 0.5608 - val_acc: 0.8462\n",
            "Epoch 709/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0618 - acc: 1.0000 - val_loss: 0.5611 - val_acc: 0.8462\n",
            "Epoch 710/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0617 - acc: 1.0000 - val_loss: 0.5614 - val_acc: 0.8462\n",
            "Epoch 711/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0615 - acc: 1.0000 - val_loss: 0.5618 - val_acc: 0.8462\n",
            "Epoch 712/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.0614 - acc: 1.0000 - val_loss: 0.5623 - val_acc: 0.8462\n",
            "Epoch 713/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0613 - acc: 1.0000 - val_loss: 0.5628 - val_acc: 0.8462\n",
            "Epoch 714/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0611 - acc: 1.0000 - val_loss: 0.5635 - val_acc: 0.8462\n",
            "Epoch 715/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0610 - acc: 1.0000 - val_loss: 0.5642 - val_acc: 0.8462\n",
            "Epoch 716/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0609 - acc: 1.0000 - val_loss: 0.5650 - val_acc: 0.8462\n",
            "Epoch 717/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0607 - acc: 1.0000 - val_loss: 0.5659 - val_acc: 0.8462\n",
            "Epoch 718/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0606 - acc: 1.0000 - val_loss: 0.5662 - val_acc: 0.8462\n",
            "Epoch 719/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.0605 - acc: 1.0000 - val_loss: 0.5662 - val_acc: 0.8462\n",
            "Epoch 720/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0603 - acc: 1.0000 - val_loss: 0.5666 - val_acc: 0.8462\n",
            "Epoch 721/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0602 - acc: 1.0000 - val_loss: 0.5671 - val_acc: 0.8462\n",
            "Epoch 722/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0601 - acc: 1.0000 - val_loss: 0.5677 - val_acc: 0.8462\n",
            "Epoch 723/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0599 - acc: 1.0000 - val_loss: 0.5685 - val_acc: 0.8462\n",
            "Epoch 724/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0598 - acc: 1.0000 - val_loss: 0.5691 - val_acc: 0.8462\n",
            "Epoch 725/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0597 - acc: 1.0000 - val_loss: 0.5693 - val_acc: 0.8462\n",
            "Epoch 726/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0595 - acc: 1.0000 - val_loss: 0.5698 - val_acc: 0.8462\n",
            "Epoch 727/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0594 - acc: 1.0000 - val_loss: 0.5702 - val_acc: 0.8462\n",
            "Epoch 728/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0593 - acc: 1.0000 - val_loss: 0.5708 - val_acc: 0.8462\n",
            "Epoch 729/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0592 - acc: 1.0000 - val_loss: 0.5714 - val_acc: 0.8462\n",
            "Epoch 730/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0590 - acc: 1.0000 - val_loss: 0.5721 - val_acc: 0.8462\n",
            "Epoch 731/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0589 - acc: 1.0000 - val_loss: 0.5727 - val_acc: 0.8462\n",
            "Epoch 732/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0588 - acc: 1.0000 - val_loss: 0.5731 - val_acc: 0.8462\n",
            "Epoch 733/1000\n",
            "89/89 [==============================] - 0s 79us/step - loss: 0.0586 - acc: 1.0000 - val_loss: 0.5735 - val_acc: 0.8462\n",
            "Epoch 734/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0585 - acc: 1.0000 - val_loss: 0.5739 - val_acc: 0.8462\n",
            "Epoch 735/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0584 - acc: 1.0000 - val_loss: 0.5743 - val_acc: 0.8462\n",
            "Epoch 736/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.0583 - acc: 1.0000 - val_loss: 0.5750 - val_acc: 0.8462\n",
            "Epoch 737/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0581 - acc: 1.0000 - val_loss: 0.5755 - val_acc: 0.8462\n",
            "Epoch 738/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0580 - acc: 1.0000 - val_loss: 0.5760 - val_acc: 0.8462\n",
            "Epoch 739/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0579 - acc: 1.0000 - val_loss: 0.5765 - val_acc: 0.8462\n",
            "Epoch 740/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0578 - acc: 1.0000 - val_loss: 0.5770 - val_acc: 0.8462\n",
            "Epoch 741/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0576 - acc: 1.0000 - val_loss: 0.5775 - val_acc: 0.8462\n",
            "Epoch 742/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0575 - acc: 1.0000 - val_loss: 0.5780 - val_acc: 0.7692\n",
            "Epoch 743/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0574 - acc: 1.0000 - val_loss: 0.5784 - val_acc: 0.7692\n",
            "Epoch 744/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0573 - acc: 1.0000 - val_loss: 0.5788 - val_acc: 0.7692\n",
            "Epoch 745/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0571 - acc: 1.0000 - val_loss: 0.5792 - val_acc: 0.7692\n",
            "Epoch 746/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0570 - acc: 1.0000 - val_loss: 0.5797 - val_acc: 0.7692\n",
            "Epoch 747/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0569 - acc: 1.0000 - val_loss: 0.5800 - val_acc: 0.7692\n",
            "Epoch 748/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0568 - acc: 1.0000 - val_loss: 0.5803 - val_acc: 0.7692\n",
            "Epoch 749/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0567 - acc: 1.0000 - val_loss: 0.5808 - val_acc: 0.7692\n",
            "Epoch 750/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0565 - acc: 1.0000 - val_loss: 0.5813 - val_acc: 0.7692\n",
            "Epoch 751/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0564 - acc: 1.0000 - val_loss: 0.5819 - val_acc: 0.7692\n",
            "Epoch 752/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0563 - acc: 1.0000 - val_loss: 0.5825 - val_acc: 0.7692\n",
            "Epoch 753/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0562 - acc: 1.0000 - val_loss: 0.5831 - val_acc: 0.7692\n",
            "Epoch 754/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0560 - acc: 1.0000 - val_loss: 0.5837 - val_acc: 0.7692\n",
            "Epoch 755/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0559 - acc: 1.0000 - val_loss: 0.5842 - val_acc: 0.7692\n",
            "Epoch 756/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0558 - acc: 1.0000 - val_loss: 0.5847 - val_acc: 0.7692\n",
            "Epoch 757/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0557 - acc: 1.0000 - val_loss: 0.5853 - val_acc: 0.7692\n",
            "Epoch 758/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0556 - acc: 1.0000 - val_loss: 0.5856 - val_acc: 0.6923\n",
            "Epoch 759/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0554 - acc: 1.0000 - val_loss: 0.5858 - val_acc: 0.6923\n",
            "Epoch 760/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0553 - acc: 1.0000 - val_loss: 0.5861 - val_acc: 0.6923\n",
            "Epoch 761/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0552 - acc: 1.0000 - val_loss: 0.5869 - val_acc: 0.6923\n",
            "Epoch 762/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0551 - acc: 1.0000 - val_loss: 0.5878 - val_acc: 0.6923\n",
            "Epoch 763/1000\n",
            "89/89 [==============================] - 0s 109us/step - loss: 0.0550 - acc: 1.0000 - val_loss: 0.5882 - val_acc: 0.6923\n",
            "Epoch 764/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0548 - acc: 1.0000 - val_loss: 0.5886 - val_acc: 0.6923\n",
            "Epoch 765/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0547 - acc: 1.0000 - val_loss: 0.5890 - val_acc: 0.6923\n",
            "Epoch 766/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0546 - acc: 1.0000 - val_loss: 0.5897 - val_acc: 0.6923\n",
            "Epoch 767/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0545 - acc: 1.0000 - val_loss: 0.5903 - val_acc: 0.6923\n",
            "Epoch 768/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0544 - acc: 1.0000 - val_loss: 0.5907 - val_acc: 0.6923\n",
            "Epoch 769/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0542 - acc: 1.0000 - val_loss: 0.5912 - val_acc: 0.6923\n",
            "Epoch 770/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0541 - acc: 1.0000 - val_loss: 0.5917 - val_acc: 0.6923\n",
            "Epoch 771/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0540 - acc: 1.0000 - val_loss: 0.5923 - val_acc: 0.6923\n",
            "Epoch 772/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0539 - acc: 1.0000 - val_loss: 0.5925 - val_acc: 0.6923\n",
            "Epoch 773/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0538 - acc: 1.0000 - val_loss: 0.5928 - val_acc: 0.6923\n",
            "Epoch 774/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0537 - acc: 1.0000 - val_loss: 0.5933 - val_acc: 0.6923\n",
            "Epoch 775/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0536 - acc: 1.0000 - val_loss: 0.5940 - val_acc: 0.6923\n",
            "Epoch 776/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0534 - acc: 1.0000 - val_loss: 0.5948 - val_acc: 0.6923\n",
            "Epoch 777/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0533 - acc: 1.0000 - val_loss: 0.5954 - val_acc: 0.6923\n",
            "Epoch 778/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0532 - acc: 1.0000 - val_loss: 0.5960 - val_acc: 0.6923\n",
            "Epoch 779/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0531 - acc: 1.0000 - val_loss: 0.5964 - val_acc: 0.6923\n",
            "Epoch 780/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0530 - acc: 1.0000 - val_loss: 0.5966 - val_acc: 0.6923\n",
            "Epoch 781/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0529 - acc: 1.0000 - val_loss: 0.5972 - val_acc: 0.6923\n",
            "Epoch 782/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0527 - acc: 1.0000 - val_loss: 0.5976 - val_acc: 0.6923\n",
            "Epoch 783/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0526 - acc: 1.0000 - val_loss: 0.5979 - val_acc: 0.6923\n",
            "Epoch 784/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0525 - acc: 1.0000 - val_loss: 0.5984 - val_acc: 0.6923\n",
            "Epoch 785/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.0524 - acc: 1.0000 - val_loss: 0.5991 - val_acc: 0.6923\n",
            "Epoch 786/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0523 - acc: 1.0000 - val_loss: 0.5996 - val_acc: 0.6923\n",
            "Epoch 787/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0522 - acc: 1.0000 - val_loss: 0.6000 - val_acc: 0.6923\n",
            "Epoch 788/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0521 - acc: 1.0000 - val_loss: 0.6003 - val_acc: 0.6923\n",
            "Epoch 789/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0519 - acc: 1.0000 - val_loss: 0.6009 - val_acc: 0.6923\n",
            "Epoch 790/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.0518 - acc: 1.0000 - val_loss: 0.6014 - val_acc: 0.6923\n",
            "Epoch 791/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.0517 - acc: 1.0000 - val_loss: 0.6021 - val_acc: 0.6923\n",
            "Epoch 792/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.0516 - acc: 1.0000 - val_loss: 0.6028 - val_acc: 0.6923\n",
            "Epoch 793/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0515 - acc: 1.0000 - val_loss: 0.6032 - val_acc: 0.6923\n",
            "Epoch 794/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0514 - acc: 1.0000 - val_loss: 0.6037 - val_acc: 0.6923\n",
            "Epoch 795/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.0513 - acc: 1.0000 - val_loss: 0.6041 - val_acc: 0.6923\n",
            "Epoch 796/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0512 - acc: 1.0000 - val_loss: 0.6046 - val_acc: 0.6923\n",
            "Epoch 797/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0510 - acc: 1.0000 - val_loss: 0.6052 - val_acc: 0.6923\n",
            "Epoch 798/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0509 - acc: 1.0000 - val_loss: 0.6056 - val_acc: 0.6923\n",
            "Epoch 799/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0508 - acc: 1.0000 - val_loss: 0.6059 - val_acc: 0.6923\n",
            "Epoch 800/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0507 - acc: 1.0000 - val_loss: 0.6062 - val_acc: 0.6923\n",
            "Epoch 801/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0506 - acc: 1.0000 - val_loss: 0.6070 - val_acc: 0.6923\n",
            "Epoch 802/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0505 - acc: 1.0000 - val_loss: 0.6077 - val_acc: 0.6923\n",
            "Epoch 803/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0504 - acc: 1.0000 - val_loss: 0.6081 - val_acc: 0.6923\n",
            "Epoch 804/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0503 - acc: 1.0000 - val_loss: 0.6085 - val_acc: 0.6923\n",
            "Epoch 805/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.0502 - acc: 1.0000 - val_loss: 0.6090 - val_acc: 0.6923\n",
            "Epoch 806/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0501 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.6923\n",
            "Epoch 807/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0499 - acc: 1.0000 - val_loss: 0.6096 - val_acc: 0.6923\n",
            "Epoch 808/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0498 - acc: 1.0000 - val_loss: 0.6102 - val_acc: 0.6923\n",
            "Epoch 809/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0497 - acc: 1.0000 - val_loss: 0.6109 - val_acc: 0.6923\n",
            "Epoch 810/1000\n",
            "89/89 [==============================] - 0s 79us/step - loss: 0.0496 - acc: 1.0000 - val_loss: 0.6116 - val_acc: 0.6923\n",
            "Epoch 811/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0495 - acc: 1.0000 - val_loss: 0.6120 - val_acc: 0.6923\n",
            "Epoch 812/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0494 - acc: 1.0000 - val_loss: 0.6123 - val_acc: 0.6923\n",
            "Epoch 813/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0493 - acc: 1.0000 - val_loss: 0.6128 - val_acc: 0.6923\n",
            "Epoch 814/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0492 - acc: 1.0000 - val_loss: 0.6134 - val_acc: 0.6923\n",
            "Epoch 815/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0491 - acc: 1.0000 - val_loss: 0.6137 - val_acc: 0.6923\n",
            "Epoch 816/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0490 - acc: 1.0000 - val_loss: 0.6140 - val_acc: 0.6923\n",
            "Epoch 817/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0489 - acc: 1.0000 - val_loss: 0.6144 - val_acc: 0.6923\n",
            "Epoch 818/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0488 - acc: 1.0000 - val_loss: 0.6151 - val_acc: 0.6923\n",
            "Epoch 819/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0487 - acc: 1.0000 - val_loss: 0.6158 - val_acc: 0.6923\n",
            "Epoch 820/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0485 - acc: 1.0000 - val_loss: 0.6164 - val_acc: 0.6923\n",
            "Epoch 821/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0484 - acc: 1.0000 - val_loss: 0.6169 - val_acc: 0.6923\n",
            "Epoch 822/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0483 - acc: 1.0000 - val_loss: 0.6175 - val_acc: 0.6923\n",
            "Epoch 823/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0482 - acc: 1.0000 - val_loss: 0.6179 - val_acc: 0.6923\n",
            "Epoch 824/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0481 - acc: 1.0000 - val_loss: 0.6181 - val_acc: 0.6923\n",
            "Epoch 825/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0480 - acc: 1.0000 - val_loss: 0.6184 - val_acc: 0.6923\n",
            "Epoch 826/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0479 - acc: 1.0000 - val_loss: 0.6189 - val_acc: 0.6923\n",
            "Epoch 827/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0478 - acc: 1.0000 - val_loss: 0.6195 - val_acc: 0.6923\n",
            "Epoch 828/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0477 - acc: 1.0000 - val_loss: 0.6198 - val_acc: 0.6923\n",
            "Epoch 829/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0476 - acc: 1.0000 - val_loss: 0.6202 - val_acc: 0.6923\n",
            "Epoch 830/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0475 - acc: 1.0000 - val_loss: 0.6208 - val_acc: 0.6923\n",
            "Epoch 831/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0474 - acc: 1.0000 - val_loss: 0.6213 - val_acc: 0.6923\n",
            "Epoch 832/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0473 - acc: 1.0000 - val_loss: 0.6216 - val_acc: 0.6923\n",
            "Epoch 833/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0472 - acc: 1.0000 - val_loss: 0.6220 - val_acc: 0.6923\n",
            "Epoch 834/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0471 - acc: 1.0000 - val_loss: 0.6227 - val_acc: 0.6923\n",
            "Epoch 835/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0470 - acc: 1.0000 - val_loss: 0.6233 - val_acc: 0.6923\n",
            "Epoch 836/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.0469 - acc: 1.0000 - val_loss: 0.6236 - val_acc: 0.6923\n",
            "Epoch 837/1000\n",
            "89/89 [==============================] - 0s 92us/step - loss: 0.0468 - acc: 1.0000 - val_loss: 0.6239 - val_acc: 0.6923\n",
            "Epoch 838/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0467 - acc: 1.0000 - val_loss: 0.6242 - val_acc: 0.6923\n",
            "Epoch 839/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.0466 - acc: 1.0000 - val_loss: 0.6245 - val_acc: 0.6923\n",
            "Epoch 840/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0465 - acc: 1.0000 - val_loss: 0.6247 - val_acc: 0.6923\n",
            "Epoch 841/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0464 - acc: 1.0000 - val_loss: 0.6252 - val_acc: 0.6923\n",
            "Epoch 842/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0463 - acc: 1.0000 - val_loss: 0.6259 - val_acc: 0.6923\n",
            "Epoch 843/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0462 - acc: 1.0000 - val_loss: 0.6267 - val_acc: 0.6923\n",
            "Epoch 844/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.0461 - acc: 1.0000 - val_loss: 0.6273 - val_acc: 0.6923\n",
            "Epoch 845/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0460 - acc: 1.0000 - val_loss: 0.6278 - val_acc: 0.6923\n",
            "Epoch 846/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0459 - acc: 1.0000 - val_loss: 0.6283 - val_acc: 0.6923\n",
            "Epoch 847/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0458 - acc: 1.0000 - val_loss: 0.6287 - val_acc: 0.6923\n",
            "Epoch 848/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0457 - acc: 1.0000 - val_loss: 0.6292 - val_acc: 0.6923\n",
            "Epoch 849/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0456 - acc: 1.0000 - val_loss: 0.6295 - val_acc: 0.6923\n",
            "Epoch 850/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0455 - acc: 1.0000 - val_loss: 0.6300 - val_acc: 0.6923\n",
            "Epoch 851/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0454 - acc: 1.0000 - val_loss: 0.6303 - val_acc: 0.6923\n",
            "Epoch 852/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0453 - acc: 1.0000 - val_loss: 0.6305 - val_acc: 0.6923\n",
            "Epoch 853/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.0452 - acc: 1.0000 - val_loss: 0.6311 - val_acc: 0.6923\n",
            "Epoch 854/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0451 - acc: 1.0000 - val_loss: 0.6317 - val_acc: 0.6923\n",
            "Epoch 855/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0450 - acc: 1.0000 - val_loss: 0.6321 - val_acc: 0.6923\n",
            "Epoch 856/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0449 - acc: 1.0000 - val_loss: 0.6323 - val_acc: 0.6923\n",
            "Epoch 857/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0448 - acc: 1.0000 - val_loss: 0.6327 - val_acc: 0.6923\n",
            "Epoch 858/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0447 - acc: 1.0000 - val_loss: 0.6332 - val_acc: 0.6923\n",
            "Epoch 859/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0446 - acc: 1.0000 - val_loss: 0.6338 - val_acc: 0.6923\n",
            "Epoch 860/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0445 - acc: 1.0000 - val_loss: 0.6343 - val_acc: 0.6923\n",
            "Epoch 861/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0444 - acc: 1.0000 - val_loss: 0.6345 - val_acc: 0.6923\n",
            "Epoch 862/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0443 - acc: 1.0000 - val_loss: 0.6348 - val_acc: 0.6923\n",
            "Epoch 863/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0442 - acc: 1.0000 - val_loss: 0.6352 - val_acc: 0.6923\n",
            "Epoch 864/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0441 - acc: 1.0000 - val_loss: 0.6361 - val_acc: 0.6923\n",
            "Epoch 865/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.0440 - acc: 1.0000 - val_loss: 0.6368 - val_acc: 0.6923\n",
            "Epoch 866/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0440 - acc: 1.0000 - val_loss: 0.6374 - val_acc: 0.6923\n",
            "Epoch 867/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0439 - acc: 1.0000 - val_loss: 0.6379 - val_acc: 0.6923\n",
            "Epoch 868/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0438 - acc: 1.0000 - val_loss: 0.6383 - val_acc: 0.6923\n",
            "Epoch 869/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0437 - acc: 1.0000 - val_loss: 0.6387 - val_acc: 0.6923\n",
            "Epoch 870/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0436 - acc: 1.0000 - val_loss: 0.6393 - val_acc: 0.6923\n",
            "Epoch 871/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0435 - acc: 1.0000 - val_loss: 0.6398 - val_acc: 0.6923\n",
            "Epoch 872/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0434 - acc: 1.0000 - val_loss: 0.6403 - val_acc: 0.6923\n",
            "Epoch 873/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0433 - acc: 1.0000 - val_loss: 0.6407 - val_acc: 0.6923\n",
            "Epoch 874/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0432 - acc: 1.0000 - val_loss: 0.6409 - val_acc: 0.6923\n",
            "Epoch 875/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0431 - acc: 1.0000 - val_loss: 0.6412 - val_acc: 0.6923\n",
            "Epoch 876/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0430 - acc: 1.0000 - val_loss: 0.6416 - val_acc: 0.6923\n",
            "Epoch 877/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0429 - acc: 1.0000 - val_loss: 0.6420 - val_acc: 0.6923\n",
            "Epoch 878/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.0428 - acc: 1.0000 - val_loss: 0.6424 - val_acc: 0.6923\n",
            "Epoch 879/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.6428 - val_acc: 0.6923\n",
            "Epoch 880/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.6433 - val_acc: 0.6923\n",
            "Epoch 881/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.0426 - acc: 1.0000 - val_loss: 0.6438 - val_acc: 0.6923\n",
            "Epoch 882/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.0425 - acc: 1.0000 - val_loss: 0.6444 - val_acc: 0.6923\n",
            "Epoch 883/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0424 - acc: 1.0000 - val_loss: 0.6450 - val_acc: 0.6923\n",
            "Epoch 884/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0423 - acc: 1.0000 - val_loss: 0.6453 - val_acc: 0.6923\n",
            "Epoch 885/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.6457 - val_acc: 0.6923\n",
            "Epoch 886/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0421 - acc: 1.0000 - val_loss: 0.6460 - val_acc: 0.6923\n",
            "Epoch 887/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0420 - acc: 1.0000 - val_loss: 0.6463 - val_acc: 0.6923\n",
            "Epoch 888/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0419 - acc: 1.0000 - val_loss: 0.6468 - val_acc: 0.6923\n",
            "Epoch 889/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.6474 - val_acc: 0.6923\n",
            "Epoch 890/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0418 - acc: 1.0000 - val_loss: 0.6477 - val_acc: 0.6923\n",
            "Epoch 891/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0417 - acc: 1.0000 - val_loss: 0.6482 - val_acc: 0.6923\n",
            "Epoch 892/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0416 - acc: 1.0000 - val_loss: 0.6489 - val_acc: 0.6923\n",
            "Epoch 893/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0415 - acc: 1.0000 - val_loss: 0.6498 - val_acc: 0.6923\n",
            "Epoch 894/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0414 - acc: 1.0000 - val_loss: 0.6502 - val_acc: 0.6923\n",
            "Epoch 895/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0413 - acc: 1.0000 - val_loss: 0.6505 - val_acc: 0.6923\n",
            "Epoch 896/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0412 - acc: 1.0000 - val_loss: 0.6507 - val_acc: 0.6923\n",
            "Epoch 897/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0411 - acc: 1.0000 - val_loss: 0.6510 - val_acc: 0.6923\n",
            "Epoch 898/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0411 - acc: 1.0000 - val_loss: 0.6516 - val_acc: 0.6923\n",
            "Epoch 899/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0410 - acc: 1.0000 - val_loss: 0.6523 - val_acc: 0.6923\n",
            "Epoch 900/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0409 - acc: 1.0000 - val_loss: 0.6528 - val_acc: 0.6923\n",
            "Epoch 901/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.6529 - val_acc: 0.6923\n",
            "Epoch 902/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0407 - acc: 1.0000 - val_loss: 0.6531 - val_acc: 0.6923\n",
            "Epoch 903/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.6534 - val_acc: 0.6923\n",
            "Epoch 904/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0405 - acc: 1.0000 - val_loss: 0.6539 - val_acc: 0.6923\n",
            "Epoch 905/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0405 - acc: 1.0000 - val_loss: 0.6547 - val_acc: 0.6923\n",
            "Epoch 906/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0404 - acc: 1.0000 - val_loss: 0.6554 - val_acc: 0.6923\n",
            "Epoch 907/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0403 - acc: 1.0000 - val_loss: 0.6559 - val_acc: 0.6923\n",
            "Epoch 908/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.6562 - val_acc: 0.6923\n",
            "Epoch 909/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0401 - acc: 1.0000 - val_loss: 0.6565 - val_acc: 0.6923\n",
            "Epoch 910/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0400 - acc: 1.0000 - val_loss: 0.6569 - val_acc: 0.6923\n",
            "Epoch 911/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.6574 - val_acc: 0.6923\n",
            "Epoch 912/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.6579 - val_acc: 0.6923\n",
            "Epoch 913/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0398 - acc: 1.0000 - val_loss: 0.6585 - val_acc: 0.6923\n",
            "Epoch 914/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0397 - acc: 1.0000 - val_loss: 0.6593 - val_acc: 0.6923\n",
            "Epoch 915/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0396 - acc: 1.0000 - val_loss: 0.6599 - val_acc: 0.6923\n",
            "Epoch 916/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.6604 - val_acc: 0.6923\n",
            "Epoch 917/1000\n",
            "89/89 [==============================] - 0s 80us/step - loss: 0.0394 - acc: 1.0000 - val_loss: 0.6607 - val_acc: 0.6923\n",
            "Epoch 918/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.0394 - acc: 1.0000 - val_loss: 0.6608 - val_acc: 0.6923\n",
            "Epoch 919/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0393 - acc: 1.0000 - val_loss: 0.6614 - val_acc: 0.6923\n",
            "Epoch 920/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0392 - acc: 1.0000 - val_loss: 0.6619 - val_acc: 0.6923\n",
            "Epoch 921/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.0391 - acc: 1.0000 - val_loss: 0.6623 - val_acc: 0.6923\n",
            "Epoch 922/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.6629 - val_acc: 0.6923\n",
            "Epoch 923/1000\n",
            "89/89 [==============================] - 0s 84us/step - loss: 0.0389 - acc: 1.0000 - val_loss: 0.6634 - val_acc: 0.6923\n",
            "Epoch 924/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0389 - acc: 1.0000 - val_loss: 0.6638 - val_acc: 0.6923\n",
            "Epoch 925/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0388 - acc: 1.0000 - val_loss: 0.6643 - val_acc: 0.6923\n",
            "Epoch 926/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.6651 - val_acc: 0.6923\n",
            "Epoch 927/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0386 - acc: 1.0000 - val_loss: 0.6657 - val_acc: 0.6923\n",
            "Epoch 928/1000\n",
            "89/89 [==============================] - 0s 84us/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.6663 - val_acc: 0.6923\n",
            "Epoch 929/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0385 - acc: 1.0000 - val_loss: 0.6669 - val_acc: 0.6923\n",
            "Epoch 930/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0384 - acc: 1.0000 - val_loss: 0.6674 - val_acc: 0.6923\n",
            "Epoch 931/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0383 - acc: 1.0000 - val_loss: 0.6677 - val_acc: 0.6923\n",
            "Epoch 932/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0382 - acc: 1.0000 - val_loss: 0.6682 - val_acc: 0.6923\n",
            "Epoch 933/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.6686 - val_acc: 0.6923\n",
            "Epoch 934/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.0381 - acc: 1.0000 - val_loss: 0.6690 - val_acc: 0.6923\n",
            "Epoch 935/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0380 - acc: 1.0000 - val_loss: 0.6694 - val_acc: 0.6923\n",
            "Epoch 936/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0379 - acc: 1.0000 - val_loss: 0.6699 - val_acc: 0.6923\n",
            "Epoch 937/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0378 - acc: 1.0000 - val_loss: 0.6705 - val_acc: 0.6923\n",
            "Epoch 938/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0377 - acc: 1.0000 - val_loss: 0.6709 - val_acc: 0.6923\n",
            "Epoch 939/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0377 - acc: 1.0000 - val_loss: 0.6713 - val_acc: 0.6923\n",
            "Epoch 940/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.0376 - acc: 1.0000 - val_loss: 0.6718 - val_acc: 0.6923\n",
            "Epoch 941/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0375 - acc: 1.0000 - val_loss: 0.6727 - val_acc: 0.6923\n",
            "Epoch 942/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.6737 - val_acc: 0.6923\n",
            "Epoch 943/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.6741 - val_acc: 0.6923\n",
            "Epoch 944/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0373 - acc: 1.0000 - val_loss: 0.6744 - val_acc: 0.6923\n",
            "Epoch 945/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0372 - acc: 1.0000 - val_loss: 0.6747 - val_acc: 0.6923\n",
            "Epoch 946/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0371 - acc: 1.0000 - val_loss: 0.6751 - val_acc: 0.6923\n",
            "Epoch 947/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.6755 - val_acc: 0.6923\n",
            "Epoch 948/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.6758 - val_acc: 0.6923\n",
            "Epoch 949/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0369 - acc: 1.0000 - val_loss: 0.6763 - val_acc: 0.6923\n",
            "Epoch 950/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.0368 - acc: 1.0000 - val_loss: 0.6766 - val_acc: 0.6923\n",
            "Epoch 951/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.6772 - val_acc: 0.6923\n",
            "Epoch 952/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.6776 - val_acc: 0.6923\n",
            "Epoch 953/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.6782 - val_acc: 0.6923\n",
            "Epoch 954/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0365 - acc: 1.0000 - val_loss: 0.6789 - val_acc: 0.6923\n",
            "Epoch 955/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.6794 - val_acc: 0.6923\n",
            "Epoch 956/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0364 - acc: 1.0000 - val_loss: 0.6799 - val_acc: 0.6923\n",
            "Epoch 957/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0363 - acc: 1.0000 - val_loss: 0.6802 - val_acc: 0.6923\n",
            "Epoch 958/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.6804 - val_acc: 0.6923\n",
            "Epoch 959/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.6808 - val_acc: 0.6923\n",
            "Epoch 960/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0361 - acc: 1.0000 - val_loss: 0.6813 - val_acc: 0.6923\n",
            "Epoch 961/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0360 - acc: 1.0000 - val_loss: 0.6821 - val_acc: 0.6923\n",
            "Epoch 962/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.6828 - val_acc: 0.6923\n",
            "Epoch 963/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0358 - acc: 1.0000 - val_loss: 0.6835 - val_acc: 0.6923\n",
            "Epoch 964/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0358 - acc: 1.0000 - val_loss: 0.6841 - val_acc: 0.6923\n",
            "Epoch 965/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.0357 - acc: 1.0000 - val_loss: 0.6846 - val_acc: 0.6923\n",
            "Epoch 966/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.6849 - val_acc: 0.6923\n",
            "Epoch 967/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0355 - acc: 1.0000 - val_loss: 0.6852 - val_acc: 0.6923\n",
            "Epoch 968/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0355 - acc: 1.0000 - val_loss: 0.6856 - val_acc: 0.6923\n",
            "Epoch 969/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0354 - acc: 1.0000 - val_loss: 0.6863 - val_acc: 0.6923\n",
            "Epoch 970/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0353 - acc: 1.0000 - val_loss: 0.6868 - val_acc: 0.6923\n",
            "Epoch 971/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.6873 - val_acc: 0.6923\n",
            "Epoch 972/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.6879 - val_acc: 0.6923\n",
            "Epoch 973/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0351 - acc: 1.0000 - val_loss: 0.6883 - val_acc: 0.6923\n",
            "Epoch 974/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.6887 - val_acc: 0.6923\n",
            "Epoch 975/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.6891 - val_acc: 0.6923\n",
            "Epoch 976/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0349 - acc: 1.0000 - val_loss: 0.6899 - val_acc: 0.6923\n",
            "Epoch 977/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0348 - acc: 1.0000 - val_loss: 0.6905 - val_acc: 0.6923\n",
            "Epoch 978/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.6909 - val_acc: 0.6923\n",
            "Epoch 979/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0347 - acc: 1.0000 - val_loss: 0.6909 - val_acc: 0.6923\n",
            "Epoch 980/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0346 - acc: 1.0000 - val_loss: 0.6912 - val_acc: 0.6923\n",
            "Epoch 981/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.6919 - val_acc: 0.6923\n",
            "Epoch 982/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.6924 - val_acc: 0.6923\n",
            "Epoch 983/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0344 - acc: 1.0000 - val_loss: 0.6927 - val_acc: 0.6923\n",
            "Epoch 984/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0343 - acc: 1.0000 - val_loss: 0.6932 - val_acc: 0.6923\n",
            "Epoch 985/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.6938 - val_acc: 0.6923\n",
            "Epoch 986/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.6946 - val_acc: 0.6923\n",
            "Epoch 987/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0341 - acc: 1.0000 - val_loss: 0.6954 - val_acc: 0.6923\n",
            "Epoch 988/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.0340 - acc: 1.0000 - val_loss: 0.6959 - val_acc: 0.6923\n",
            "Epoch 989/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0340 - acc: 1.0000 - val_loss: 0.6962 - val_acc: 0.6923\n",
            "Epoch 990/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.0339 - acc: 1.0000 - val_loss: 0.6963 - val_acc: 0.6923\n",
            "Epoch 991/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.6970 - val_acc: 0.6923\n",
            "Epoch 992/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0338 - acc: 1.0000 - val_loss: 0.6976 - val_acc: 0.6923\n",
            "Epoch 993/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.0337 - acc: 1.0000 - val_loss: 0.6980 - val_acc: 0.6923\n",
            "Epoch 994/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0336 - acc: 1.0000 - val_loss: 0.6983 - val_acc: 0.6923\n",
            "Epoch 995/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.0335 - acc: 1.0000 - val_loss: 0.6986 - val_acc: 0.6923\n",
            "Epoch 996/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.0335 - acc: 1.0000 - val_loss: 0.6991 - val_acc: 0.6923\n",
            "Epoch 997/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.0334 - acc: 1.0000 - val_loss: 0.6993 - val_acc: 0.6923\n",
            "Epoch 998/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 0.6998 - val_acc: 0.6923\n",
            "Epoch 999/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 0.7002 - val_acc: 0.6923\n",
            "Epoch 1000/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.0332 - acc: 1.0000 - val_loss: 0.7011 - val_acc: 0.6923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1820cbc9-ceb0-4f1f-9bb8-1d563f348da3",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb674992da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "43c794e0-0c11-4acb-b02d-bb053b535889",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb674a6ed30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6Zv9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6372075c-9cd9-4e98-c09d-03d4ca95fb7c",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_labels, epochs= 200, batch_size=89, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "89/89 [==============================] - 2s 25ms/step - loss: 0.6778 - acc: 0.5955\n",
            "Epoch 2/200\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.6709 - acc: 0.5730\n",
            "Epoch 3/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.6636 - acc: 0.5843\n",
            "Epoch 4/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.6567 - acc: 0.6404\n",
            "Epoch 5/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.6505 - acc: 0.6404\n",
            "Epoch 6/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.6444 - acc: 0.6404\n",
            "Epoch 7/200\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.6388 - acc: 0.6404\n",
            "Epoch 8/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.6336 - acc: 0.6517\n",
            "Epoch 9/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6289 - acc: 0.6517\n",
            "Epoch 10/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6242 - acc: 0.6629\n",
            "Epoch 11/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6195 - acc: 0.6742\n",
            "Epoch 12/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6146 - acc: 0.6854\n",
            "Epoch 13/200\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.6097 - acc: 0.6854\n",
            "Epoch 14/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6047 - acc: 0.6742\n",
            "Epoch 15/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5998 - acc: 0.6742\n",
            "Epoch 16/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5946 - acc: 0.6966\n",
            "Epoch 17/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.5895 - acc: 0.7079\n",
            "Epoch 18/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5844 - acc: 0.7191\n",
            "Epoch 19/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.5791 - acc: 0.7191\n",
            "Epoch 20/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5737 - acc: 0.7191\n",
            "Epoch 21/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5683 - acc: 0.7303\n",
            "Epoch 22/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5631 - acc: 0.7416\n",
            "Epoch 23/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5580 - acc: 0.7416\n",
            "Epoch 24/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5528 - acc: 0.7416\n",
            "Epoch 25/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.5475 - acc: 0.7528\n",
            "Epoch 26/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5422 - acc: 0.7528\n",
            "Epoch 27/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5368 - acc: 0.7640\n",
            "Epoch 28/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5315 - acc: 0.7640\n",
            "Epoch 29/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5263 - acc: 0.7753\n",
            "Epoch 30/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5212 - acc: 0.7640\n",
            "Epoch 31/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5161 - acc: 0.7640\n",
            "Epoch 32/200\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5109 - acc: 0.7753\n",
            "Epoch 33/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5056 - acc: 0.7865\n",
            "Epoch 34/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5005 - acc: 0.7865\n",
            "Epoch 35/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.4953 - acc: 0.7865\n",
            "Epoch 36/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4901 - acc: 0.7978\n",
            "Epoch 37/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.4846 - acc: 0.8090\n",
            "Epoch 38/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.4793 - acc: 0.8202\n",
            "Epoch 39/200\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.4739 - acc: 0.8202\n",
            "Epoch 40/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.4682 - acc: 0.8202\n",
            "Epoch 41/200\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4625 - acc: 0.8202\n",
            "Epoch 42/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.4570 - acc: 0.8202\n",
            "Epoch 43/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.4514 - acc: 0.8202\n",
            "Epoch 44/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.4457 - acc: 0.8202\n",
            "Epoch 45/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.4400 - acc: 0.8427\n",
            "Epoch 46/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.4342 - acc: 0.8427\n",
            "Epoch 47/200\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.4284 - acc: 0.8427\n",
            "Epoch 48/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.4226 - acc: 0.8427\n",
            "Epoch 49/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4170 - acc: 0.8539\n",
            "Epoch 50/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4114 - acc: 0.8652\n",
            "Epoch 51/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.4056 - acc: 0.8652\n",
            "Epoch 52/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3999 - acc: 0.8539\n",
            "Epoch 53/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.3943 - acc: 0.8539\n",
            "Epoch 54/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.3887 - acc: 0.8539\n",
            "Epoch 55/200\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3832 - acc: 0.8539\n",
            "Epoch 56/200\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3776 - acc: 0.8539\n",
            "Epoch 57/200\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3720 - acc: 0.8539\n",
            "Epoch 58/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3665 - acc: 0.8539\n",
            "Epoch 59/200\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.3609 - acc: 0.8539\n",
            "Epoch 60/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3553 - acc: 0.8539\n",
            "Epoch 61/200\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3497 - acc: 0.8539\n",
            "Epoch 62/200\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3442 - acc: 0.8539\n",
            "Epoch 63/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.3386 - acc: 0.8539\n",
            "Epoch 64/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3329 - acc: 0.8539\n",
            "Epoch 65/200\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3273 - acc: 0.8652\n",
            "Epoch 66/200\n",
            "89/89 [==============================] - 0s 21us/step - loss: 0.3217 - acc: 0.8652\n",
            "Epoch 67/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3161 - acc: 0.8764\n",
            "Epoch 68/200\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.3103 - acc: 0.8764\n",
            "Epoch 69/200\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3045 - acc: 0.8876\n",
            "Epoch 70/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2987 - acc: 0.9101\n",
            "Epoch 71/200\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.2931 - acc: 0.9101\n",
            "Epoch 72/200\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2875 - acc: 0.9101\n",
            "Epoch 73/200\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2819 - acc: 0.9213\n",
            "Epoch 74/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2763 - acc: 0.9213\n",
            "Epoch 75/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.2708 - acc: 0.9213\n",
            "Epoch 76/200\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2653 - acc: 0.9213\n",
            "Epoch 77/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2600 - acc: 0.9213\n",
            "Epoch 78/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2545 - acc: 0.9213\n",
            "Epoch 79/200\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2492 - acc: 0.9326\n",
            "Epoch 80/200\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2440 - acc: 0.9326\n",
            "Epoch 81/200\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.2388 - acc: 0.9326\n",
            "Epoch 82/200\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2336 - acc: 0.9326\n",
            "Epoch 83/200\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2285 - acc: 0.9326\n",
            "Epoch 84/200\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2234 - acc: 0.9438\n",
            "Epoch 85/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2184 - acc: 0.9438\n",
            "Epoch 86/200\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2136 - acc: 0.9438\n",
            "Epoch 87/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2088 - acc: 0.9438\n",
            "Epoch 88/200\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2041 - acc: 0.9438\n",
            "Epoch 89/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1995 - acc: 0.9438\n",
            "Epoch 90/200\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1949 - acc: 0.9551\n",
            "Epoch 91/200\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1903 - acc: 0.9775\n",
            "Epoch 92/200\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1858 - acc: 0.9775\n",
            "Epoch 93/200\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1814 - acc: 0.9775\n",
            "Epoch 94/200\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.1770 - acc: 0.9775\n",
            "Epoch 95/200\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1726 - acc: 0.9775\n",
            "Epoch 96/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.1683 - acc: 0.9888\n",
            "Epoch 97/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1642 - acc: 1.0000\n",
            "Epoch 98/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1602 - acc: 1.0000\n",
            "Epoch 99/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1561 - acc: 1.0000\n",
            "Epoch 100/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1521 - acc: 1.0000\n",
            "Epoch 101/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1482 - acc: 1.0000\n",
            "Epoch 102/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1443 - acc: 1.0000\n",
            "Epoch 103/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1405 - acc: 1.0000\n",
            "Epoch 104/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1367 - acc: 1.0000\n",
            "Epoch 105/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1330 - acc: 1.0000\n",
            "Epoch 106/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1295 - acc: 1.0000\n",
            "Epoch 107/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1261 - acc: 1.0000\n",
            "Epoch 108/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1227 - acc: 1.0000\n",
            "Epoch 109/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1194 - acc: 1.0000\n",
            "Epoch 110/200\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1163 - acc: 1.0000\n",
            "Epoch 111/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.1132 - acc: 1.0000\n",
            "Epoch 112/200\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.1101 - acc: 1.0000\n",
            "Epoch 113/200\n",
            "89/89 [==============================] - 0s 95us/step - loss: 0.1070 - acc: 1.0000\n",
            "Epoch 114/200\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.1041 - acc: 1.0000\n",
            "Epoch 115/200\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1013 - acc: 1.0000\n",
            "Epoch 116/200\n",
            "89/89 [==============================] - 0s 119us/step - loss: 0.0984 - acc: 1.0000\n",
            "Epoch 117/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0956 - acc: 1.0000\n",
            "Epoch 118/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0929 - acc: 1.0000\n",
            "Epoch 119/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0904 - acc: 1.0000\n",
            "Epoch 120/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0878 - acc: 1.0000\n",
            "Epoch 121/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0854 - acc: 1.0000\n",
            "Epoch 122/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0830 - acc: 1.0000\n",
            "Epoch 123/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0807 - acc: 1.0000\n",
            "Epoch 124/200\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0785 - acc: 1.0000\n",
            "Epoch 125/200\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.0763 - acc: 1.0000\n",
            "Epoch 126/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0742 - acc: 1.0000\n",
            "Epoch 127/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0721 - acc: 1.0000\n",
            "Epoch 128/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0702 - acc: 1.0000\n",
            "Epoch 129/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0682 - acc: 1.0000\n",
            "Epoch 130/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0664 - acc: 1.0000\n",
            "Epoch 131/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0645 - acc: 1.0000\n",
            "Epoch 132/200\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0628 - acc: 1.0000\n",
            "Epoch 133/200\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0611 - acc: 1.0000\n",
            "Epoch 134/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0595 - acc: 1.0000\n",
            "Epoch 135/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0579 - acc: 1.0000\n",
            "Epoch 136/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0564 - acc: 1.0000\n",
            "Epoch 137/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0549 - acc: 1.0000\n",
            "Epoch 138/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0534 - acc: 1.0000\n",
            "Epoch 139/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0520 - acc: 1.0000\n",
            "Epoch 140/200\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0506 - acc: 1.0000\n",
            "Epoch 141/200\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.0493 - acc: 1.0000\n",
            "Epoch 142/200\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.0480 - acc: 1.0000\n",
            "Epoch 143/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0467 - acc: 1.0000\n",
            "Epoch 144/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0455 - acc: 1.0000\n",
            "Epoch 145/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0444 - acc: 1.0000\n",
            "Epoch 146/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0432 - acc: 1.0000\n",
            "Epoch 147/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0421 - acc: 1.0000\n",
            "Epoch 148/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0411 - acc: 1.0000\n",
            "Epoch 149/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0401 - acc: 1.0000\n",
            "Epoch 150/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0391 - acc: 1.0000\n",
            "Epoch 151/200\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.0381 - acc: 1.0000\n",
            "Epoch 152/200\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0372 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0363 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0354 - acc: 1.0000\n",
            "Epoch 155/200\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0346 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0338 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0330 - acc: 1.0000\n",
            "Epoch 158/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0323 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0315 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0308 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0301 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0294 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.0288 - acc: 1.0000\n",
            "Epoch 164/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0282 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.0275 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0269 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0264 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0258 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0253 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0248 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0242 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0237 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0232 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0228 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0223 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0218 - acc: 1.0000\n",
            "Epoch 177/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0214 - acc: 1.0000\n",
            "Epoch 178/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0210 - acc: 1.0000\n",
            "Epoch 179/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0206 - acc: 1.0000\n",
            "Epoch 180/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0201 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0198 - acc: 1.0000\n",
            "Epoch 182/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0194 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0190 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0186 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0183 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0180 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0176 - acc: 1.0000\n",
            "Epoch 188/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0173 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0170 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0166 - acc: 1.0000\n",
            "Epoch 191/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0163 - acc: 1.0000\n",
            "Epoch 192/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0160 - acc: 1.0000\n",
            "Epoch 193/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0158 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0155 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0152 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0150 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0147 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.0145 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0142 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.0140 - acc: 1.0000\n",
            "13/13 [==============================] - 1s 71ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8be9e41d-1d32-46a0-e4e3-a5fa3ca8b2db",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "454be7e0-dc2f-4d02-8ea8-db3fbbb8acb0",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5384615659713745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoQqtH0O67Et",
        "colab_type": "text"
      },
      "source": [
        "#Prova con k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzvU-X5kHUSy"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dxhYL3vVHUS9",
        "colab": {}
      },
      "source": [
        "word_index={'large cell':0, 'squamous cell carcinoma':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67D9Q0e5HUUA",
        "colab": {}
      },
      "source": [
        "train_big_labels_dec = [word_index[label] for label in y_train_big]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m0YUzrR4HUTu",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvArdJwhHUUL",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rp2OYvp9HUUU",
        "colab": {}
      },
      "source": [
        "one_hot_train_big_labels = to_categorical(train_big_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-UPhhg4KPGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(107,), kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #kernel_regularizer=regularizers.l2(l=0.001)\n",
        "  model.add(layers.Dense(20, activation='relu'))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLLDv0tiJ3VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAdk6a6IUue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7128aee7-7e0f-4292-9dc2-4a78c5255b50"
      },
      "source": [
        "X_train_big.shape"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "#skf.get_n_splits(X_train_big, one_hot_train_big_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "1ad2e8f5-0e58-4fe5-8d76-5d0b999ee3a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "for train_index, test_index in skf.split(X_train_big, train_big_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   8   9  13  14  15  16  17  19  21  22  23  24\n",
            "  25  26  27  28  29  32  33  34  35  36  37  38  39  40  41  43  45  46\n",
            "  47  48  49  51  53  54  58  59  60  61  62  63  64  65  66  67  70  71\n",
            "  72  73  74  77  78  80  82  88  89  94  95  96 100 101] TEST: [ 6  7 10 11 12 18 20 30 31 42 44 50 52 55 56 57 68 69 75 76 79 81 83 84\n",
            " 85 86 87 90 91 92 93 97 98 99]\n",
            "TRAIN: [  0   2   3   5   6   7   8   9  10  11  12  14  18  20  21  25  27  28\n",
            "  30  31  32  34  35  37  38  42  43  44  45  48  50  51  52  55  56  57\n",
            "  58  59  60  61  64  67  68  69  70  75  76  77  78  79  81  83  84  85\n",
            "  86  87  88  90  91  92  93  94  95  97  98  99 100 101] TEST: [ 1  4 13 15 16 17 19 22 23 24 26 29 33 36 39 40 41 46 47 49 53 54 62 63\n",
            " 65 66 71 72 73 74 80 82 89 96]\n",
            "TRAIN: [ 1  4  6  7 10 11 12 13 15 16 17 18 19 20 22 23 24 26 29 30 31 33 36 39\n",
            " 40 41 42 44 46 47 49 50 52 53 54 55 56 57 62 63 65 66 68 69 71 72 73 74\n",
            " 75 76 79 80 81 82 83 84 85 86 87 89 90 91 92 93 96 97 98 99] TEST: [  0   2   3   5   8   9  14  21  25  27  28  32  34  35  37  38  43  45\n",
            "  48  51  58  59  60  61  64  67  70  77  78  88  94  95 100 101]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "9e149101-d471-45ef-be3e-803b7a9be544",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-24105c2ab45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels_dec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels_dec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVSoMnogHVqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br4g2_GG9ePb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train_big)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "e786c244-faac-4a24-edc4-ca6afa10fbae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand, train_big_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_big_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand[i] for i in val_index])\n",
        "  val_targets = np.array([train_big_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=68, callbacks=[red_lr])\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 68 samples, validate on 34 samples\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 9ms/step - loss: 0.7543 - acc: 0.4559 - val_loss: 0.7451 - val_acc: 0.4706\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.7443 - acc: 0.4706 - val_loss: 0.7429 - val_acc: 0.4706\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.7335 - acc: 0.5000 - val_loss: 0.7415 - val_acc: 0.4706\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.7232 - acc: 0.5588 - val_loss: 0.7401 - val_acc: 0.5000\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.7142 - acc: 0.5441 - val_loss: 0.7390 - val_acc: 0.4706\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.7061 - acc: 0.6029 - val_loss: 0.7387 - val_acc: 0.5294\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.6988 - acc: 0.6324 - val_loss: 0.7389 - val_acc: 0.5294\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6917 - acc: 0.6471 - val_loss: 0.7393 - val_acc: 0.5882\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.6847 - acc: 0.6618 - val_loss: 0.7399 - val_acc: 0.6176\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.6776 - acc: 0.6765 - val_loss: 0.7403 - val_acc: 0.6471\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.6702 - acc: 0.6912 - val_loss: 0.7410 - val_acc: 0.6471\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.6627 - acc: 0.7206 - val_loss: 0.7417 - val_acc: 0.6765\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 0s 30us/step - loss: 0.6555 - acc: 0.7353 - val_loss: 0.7427 - val_acc: 0.6765\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.6481 - acc: 0.7353 - val_loss: 0.7433 - val_acc: 0.6765\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.6409 - acc: 0.7500 - val_loss: 0.7438 - val_acc: 0.6471\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.6336 - acc: 0.7500 - val_loss: 0.7446 - val_acc: 0.6176\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6264 - acc: 0.7647 - val_loss: 0.7447 - val_acc: 0.6176\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.6256 - acc: 0.7647 - val_loss: 0.7448 - val_acc: 0.6176\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.6248 - acc: 0.7647 - val_loss: 0.7449 - val_acc: 0.6176\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.6240 - acc: 0.7647 - val_loss: 0.7450 - val_acc: 0.6176\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 0s 72us/step - loss: 0.6232 - acc: 0.7647 - val_loss: 0.7452 - val_acc: 0.6176\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.6224 - acc: 0.7647 - val_loss: 0.7453 - val_acc: 0.6176\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.6216 - acc: 0.7647 - val_loss: 0.7454 - val_acc: 0.6176\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.6207 - acc: 0.7647 - val_loss: 0.7456 - val_acc: 0.6176\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.6199 - acc: 0.7647 - val_loss: 0.7457 - val_acc: 0.6176\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.6191 - acc: 0.7647 - val_loss: 0.7459 - val_acc: 0.6176\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.6182 - acc: 0.7647 - val_loss: 0.7461 - val_acc: 0.6176\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.6173 - acc: 0.7647 - val_loss: 0.7463 - val_acc: 0.6176\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6165 - acc: 0.7647 - val_loss: 0.7465 - val_acc: 0.6176\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.6156 - acc: 0.7647 - val_loss: 0.7467 - val_acc: 0.6176\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.6147 - acc: 0.7647 - val_loss: 0.7469 - val_acc: 0.6176\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6139 - acc: 0.7647 - val_loss: 0.7471 - val_acc: 0.6176\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.6130 - acc: 0.7647 - val_loss: 0.7473 - val_acc: 0.6176\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.6121 - acc: 0.7647 - val_loss: 0.7475 - val_acc: 0.6176\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.6113 - acc: 0.7647 - val_loss: 0.7477 - val_acc: 0.6176\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.6104 - acc: 0.7647 - val_loss: 0.7479 - val_acc: 0.6176\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.6095 - acc: 0.7794 - val_loss: 0.7481 - val_acc: 0.6176\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6086 - acc: 0.7794 - val_loss: 0.7483 - val_acc: 0.6176\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.6078 - acc: 0.7794 - val_loss: 0.7485 - val_acc: 0.6176\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.6069 - acc: 0.7794 - val_loss: 0.7486 - val_acc: 0.6176\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.6061 - acc: 0.7794 - val_loss: 0.7488 - val_acc: 0.6176\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.6052 - acc: 0.7794 - val_loss: 0.7490 - val_acc: 0.6176\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.6043 - acc: 0.7794 - val_loss: 0.7492 - val_acc: 0.6176\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.6035 - acc: 0.7794 - val_loss: 0.7494 - val_acc: 0.6176\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.6026 - acc: 0.7794 - val_loss: 0.7496 - val_acc: 0.6176\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.6017 - acc: 0.7794 - val_loss: 0.7498 - val_acc: 0.6176\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.6009 - acc: 0.7794 - val_loss: 0.7500 - val_acc: 0.6176\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6000 - acc: 0.7794 - val_loss: 0.7502 - val_acc: 0.6176\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5991 - acc: 0.7794 - val_loss: 0.7505 - val_acc: 0.6176\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5982 - acc: 0.7794 - val_loss: 0.7507 - val_acc: 0.6176\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.5973 - acc: 0.7794 - val_loss: 0.7509 - val_acc: 0.6176\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5965 - acc: 0.7794 - val_loss: 0.7511 - val_acc: 0.5882\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5956 - acc: 0.7794 - val_loss: 0.7514 - val_acc: 0.5882\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5947 - acc: 0.7794 - val_loss: 0.7516 - val_acc: 0.5882\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5938 - acc: 0.7794 - val_loss: 0.7519 - val_acc: 0.5882\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5930 - acc: 0.7794 - val_loss: 0.7522 - val_acc: 0.5882\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5921 - acc: 0.7794 - val_loss: 0.7524 - val_acc: 0.5882\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5912 - acc: 0.7794 - val_loss: 0.7527 - val_acc: 0.5882\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.5904 - acc: 0.7794 - val_loss: 0.7529 - val_acc: 0.5882\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5895 - acc: 0.7794 - val_loss: 0.7532 - val_acc: 0.5882\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 0s 40us/step - loss: 0.5887 - acc: 0.7794 - val_loss: 0.7534 - val_acc: 0.5882\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5879 - acc: 0.7794 - val_loss: 0.7537 - val_acc: 0.5882\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.5870 - acc: 0.7794 - val_loss: 0.7540 - val_acc: 0.5882\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5862 - acc: 0.7794 - val_loss: 0.7543 - val_acc: 0.5882\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.5853 - acc: 0.7794 - val_loss: 0.7546 - val_acc: 0.5882\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.5844 - acc: 0.7794 - val_loss: 0.7549 - val_acc: 0.5882\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5836 - acc: 0.7794 - val_loss: 0.7553 - val_acc: 0.5882\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.5827 - acc: 0.7794 - val_loss: 0.7556 - val_acc: 0.5882\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5819 - acc: 0.7794 - val_loss: 0.7559 - val_acc: 0.5882\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5811 - acc: 0.7794 - val_loss: 0.7563 - val_acc: 0.5882\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5802 - acc: 0.7794 - val_loss: 0.7566 - val_acc: 0.5882\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5794 - acc: 0.7794 - val_loss: 0.7569 - val_acc: 0.5882\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5785 - acc: 0.7794 - val_loss: 0.7572 - val_acc: 0.5882\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5777 - acc: 0.7794 - val_loss: 0.7575 - val_acc: 0.5882\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5769 - acc: 0.7794 - val_loss: 0.7578 - val_acc: 0.5882\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5760 - acc: 0.7794 - val_loss: 0.7582 - val_acc: 0.5882\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.5752 - acc: 0.7794 - val_loss: 0.7585 - val_acc: 0.5882\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5744 - acc: 0.7794 - val_loss: 0.7589 - val_acc: 0.5882\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 0s 37us/step - loss: 0.5736 - acc: 0.7794 - val_loss: 0.7592 - val_acc: 0.5882\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.5727 - acc: 0.7794 - val_loss: 0.7596 - val_acc: 0.5882\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.5719 - acc: 0.7794 - val_loss: 0.7599 - val_acc: 0.5882\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.5710 - acc: 0.7794 - val_loss: 0.7603 - val_acc: 0.5882\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 0s 40us/step - loss: 0.5702 - acc: 0.7794 - val_loss: 0.7606 - val_acc: 0.5882\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 0s 31us/step - loss: 0.5694 - acc: 0.7794 - val_loss: 0.7610 - val_acc: 0.5882\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 0s 82us/step - loss: 0.5685 - acc: 0.7794 - val_loss: 0.7614 - val_acc: 0.5882\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.5677 - acc: 0.7941 - val_loss: 0.7617 - val_acc: 0.5882\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.5669 - acc: 0.7941 - val_loss: 0.7621 - val_acc: 0.5882\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 0s 86us/step - loss: 0.5661 - acc: 0.7941 - val_loss: 0.7625 - val_acc: 0.5882\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.5653 - acc: 0.7941 - val_loss: 0.7629 - val_acc: 0.6176\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.5645 - acc: 0.7941 - val_loss: 0.7632 - val_acc: 0.6176\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5636 - acc: 0.7941 - val_loss: 0.7636 - val_acc: 0.6176\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5628 - acc: 0.8088 - val_loss: 0.7639 - val_acc: 0.6176\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 0s 80us/step - loss: 0.5620 - acc: 0.8088 - val_loss: 0.7643 - val_acc: 0.6176\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5612 - acc: 0.8088 - val_loss: 0.7647 - val_acc: 0.6176\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5604 - acc: 0.8088 - val_loss: 0.7650 - val_acc: 0.6176\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5596 - acc: 0.8088 - val_loss: 0.7654 - val_acc: 0.6176\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5588 - acc: 0.8088 - val_loss: 0.7658 - val_acc: 0.6176\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.5580 - acc: 0.8088 - val_loss: 0.7662 - val_acc: 0.6176\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5573 - acc: 0.8088 - val_loss: 0.7666 - val_acc: 0.6176\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.5565 - acc: 0.8088 - val_loss: 0.7670 - val_acc: 0.6176\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 0s 92us/step - loss: 0.5557 - acc: 0.8088 - val_loss: 0.7674 - val_acc: 0.6176\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 0s 86us/step - loss: 0.5549 - acc: 0.8088 - val_loss: 0.7678 - val_acc: 0.6176\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5541 - acc: 0.8088 - val_loss: 0.7682 - val_acc: 0.6176\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.5533 - acc: 0.8088 - val_loss: 0.7686 - val_acc: 0.6176\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 0s 163us/step - loss: 0.5525 - acc: 0.8088 - val_loss: 0.7690 - val_acc: 0.6176\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 0s 80us/step - loss: 0.5517 - acc: 0.8088 - val_loss: 0.7694 - val_acc: 0.6176\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5510 - acc: 0.8088 - val_loss: 0.7699 - val_acc: 0.6176\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.5502 - acc: 0.8088 - val_loss: 0.7703 - val_acc: 0.6176\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.5494 - acc: 0.8088 - val_loss: 0.7707 - val_acc: 0.6176\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.5487 - acc: 0.8088 - val_loss: 0.7712 - val_acc: 0.6176\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5479 - acc: 0.8088 - val_loss: 0.7716 - val_acc: 0.6176\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.5471 - acc: 0.8088 - val_loss: 0.7721 - val_acc: 0.6176\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5464 - acc: 0.8088 - val_loss: 0.7725 - val_acc: 0.6176\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 0s 42us/step - loss: 0.5456 - acc: 0.8088 - val_loss: 0.7730 - val_acc: 0.6176\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.5448 - acc: 0.8088 - val_loss: 0.7735 - val_acc: 0.6176\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5441 - acc: 0.8088 - val_loss: 0.7740 - val_acc: 0.6176\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5433 - acc: 0.8088 - val_loss: 0.7745 - val_acc: 0.6176\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.5426 - acc: 0.8088 - val_loss: 0.7751 - val_acc: 0.6176\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5418 - acc: 0.8235 - val_loss: 0.7756 - val_acc: 0.6176\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5411 - acc: 0.8235 - val_loss: 0.7761 - val_acc: 0.6176\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.5404 - acc: 0.8235 - val_loss: 0.7767 - val_acc: 0.6176\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5396 - acc: 0.8235 - val_loss: 0.7772 - val_acc: 0.6176\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5389 - acc: 0.8235 - val_loss: 0.7777 - val_acc: 0.6176\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5382 - acc: 0.8235 - val_loss: 0.7783 - val_acc: 0.6176\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.5375 - acc: 0.8235 - val_loss: 0.7788 - val_acc: 0.6176\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.5367 - acc: 0.8235 - val_loss: 0.7793 - val_acc: 0.6176\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.5360 - acc: 0.8235 - val_loss: 0.7798 - val_acc: 0.6176\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 0s 69us/step - loss: 0.5353 - acc: 0.8235 - val_loss: 0.7804 - val_acc: 0.6176\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 0s 79us/step - loss: 0.5346 - acc: 0.8235 - val_loss: 0.7809 - val_acc: 0.6176\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 0s 77us/step - loss: 0.5339 - acc: 0.8235 - val_loss: 0.7814 - val_acc: 0.6176\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.5332 - acc: 0.8235 - val_loss: 0.7819 - val_acc: 0.6176\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5325 - acc: 0.8235 - val_loss: 0.7824 - val_acc: 0.6176\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5318 - acc: 0.8088 - val_loss: 0.7830 - val_acc: 0.6176\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5311 - acc: 0.8235 - val_loss: 0.7835 - val_acc: 0.6176\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5304 - acc: 0.8235 - val_loss: 0.7840 - val_acc: 0.6176\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 0s 72us/step - loss: 0.5297 - acc: 0.8235 - val_loss: 0.7846 - val_acc: 0.6176\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.5290 - acc: 0.8235 - val_loss: 0.7852 - val_acc: 0.6176\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 0s 68us/step - loss: 0.5283 - acc: 0.8235 - val_loss: 0.7858 - val_acc: 0.6176\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 0s 78us/step - loss: 0.5276 - acc: 0.8235 - val_loss: 0.7863 - val_acc: 0.6176\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5270 - acc: 0.8235 - val_loss: 0.7869 - val_acc: 0.6176\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.5263 - acc: 0.8235 - val_loss: 0.7874 - val_acc: 0.6176\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5256 - acc: 0.8235 - val_loss: 0.7880 - val_acc: 0.6176\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.5249 - acc: 0.8235 - val_loss: 0.7886 - val_acc: 0.6176\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5242 - acc: 0.8235 - val_loss: 0.7891 - val_acc: 0.6176\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 0s 68us/step - loss: 0.5235 - acc: 0.8235 - val_loss: 0.7897 - val_acc: 0.6176\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.5229 - acc: 0.8235 - val_loss: 0.7903 - val_acc: 0.6176\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 0s 85us/step - loss: 0.5222 - acc: 0.8235 - val_loss: 0.7908 - val_acc: 0.6176\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5215 - acc: 0.8235 - val_loss: 0.7914 - val_acc: 0.6176\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5208 - acc: 0.8235 - val_loss: 0.7919 - val_acc: 0.6176\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.5201 - acc: 0.8235 - val_loss: 0.7925 - val_acc: 0.6176\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5195 - acc: 0.8235 - val_loss: 0.7930 - val_acc: 0.6176\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.5188 - acc: 0.8235 - val_loss: 0.7936 - val_acc: 0.6176\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.5181 - acc: 0.8235 - val_loss: 0.7941 - val_acc: 0.6176\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5175 - acc: 0.8235 - val_loss: 0.7947 - val_acc: 0.6176\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 0s 69us/step - loss: 0.5168 - acc: 0.8235 - val_loss: 0.7952 - val_acc: 0.6176\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.5161 - acc: 0.8235 - val_loss: 0.7958 - val_acc: 0.6176\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.5155 - acc: 0.8235 - val_loss: 0.7964 - val_acc: 0.6176\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.5148 - acc: 0.8235 - val_loss: 0.7969 - val_acc: 0.6176\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 0s 74us/step - loss: 0.5141 - acc: 0.8235 - val_loss: 0.7975 - val_acc: 0.6176\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5135 - acc: 0.8235 - val_loss: 0.7980 - val_acc: 0.6176\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5128 - acc: 0.8235 - val_loss: 0.7986 - val_acc: 0.6176\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5122 - acc: 0.8235 - val_loss: 0.7992 - val_acc: 0.6176\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5115 - acc: 0.8235 - val_loss: 0.7997 - val_acc: 0.6176\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5108 - acc: 0.8235 - val_loss: 0.8003 - val_acc: 0.6176\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5102 - acc: 0.8235 - val_loss: 0.8009 - val_acc: 0.6176\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.5095 - acc: 0.8235 - val_loss: 0.8014 - val_acc: 0.6176\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5088 - acc: 0.8382 - val_loss: 0.8020 - val_acc: 0.6176\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5082 - acc: 0.8382 - val_loss: 0.8026 - val_acc: 0.6176\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.5075 - acc: 0.8382 - val_loss: 0.8031 - val_acc: 0.6176\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 0s 68us/step - loss: 0.5068 - acc: 0.8382 - val_loss: 0.8037 - val_acc: 0.6176\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 0s 77us/step - loss: 0.5061 - acc: 0.8382 - val_loss: 0.8043 - val_acc: 0.6176\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.5055 - acc: 0.8382 - val_loss: 0.8049 - val_acc: 0.6176\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 0s 72us/step - loss: 0.5048 - acc: 0.8382 - val_loss: 0.8055 - val_acc: 0.6176\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 0s 78us/step - loss: 0.5041 - acc: 0.8382 - val_loss: 0.8061 - val_acc: 0.6176\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 0s 69us/step - loss: 0.5035 - acc: 0.8382 - val_loss: 0.8066 - val_acc: 0.6176\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5028 - acc: 0.8382 - val_loss: 0.8072 - val_acc: 0.6176\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 0s 77us/step - loss: 0.5021 - acc: 0.8382 - val_loss: 0.8079 - val_acc: 0.6176\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.5014 - acc: 0.8382 - val_loss: 0.8085 - val_acc: 0.6176\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 0s 174us/step - loss: 0.5007 - acc: 0.8382 - val_loss: 0.8091 - val_acc: 0.6176\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5000 - acc: 0.8382 - val_loss: 0.8098 - val_acc: 0.6176\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.4993 - acc: 0.8382 - val_loss: 0.8104 - val_acc: 0.6176\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4986 - acc: 0.8382 - val_loss: 0.8111 - val_acc: 0.6176\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4979 - acc: 0.8382 - val_loss: 0.8117 - val_acc: 0.6176\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4972 - acc: 0.8382 - val_loss: 0.8123 - val_acc: 0.6176\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.4964 - acc: 0.8382 - val_loss: 0.8130 - val_acc: 0.6176\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4957 - acc: 0.8382 - val_loss: 0.8136 - val_acc: 0.6176\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.4949 - acc: 0.8382 - val_loss: 0.8143 - val_acc: 0.6176\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4942 - acc: 0.8382 - val_loss: 0.8150 - val_acc: 0.6176\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4934 - acc: 0.8382 - val_loss: 0.8157 - val_acc: 0.6176\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4927 - acc: 0.8382 - val_loss: 0.8164 - val_acc: 0.6176\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.4919 - acc: 0.8529 - val_loss: 0.8171 - val_acc: 0.6176\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4911 - acc: 0.8529 - val_loss: 0.8177 - val_acc: 0.6176\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4903 - acc: 0.8529 - val_loss: 0.8184 - val_acc: 0.6176\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4895 - acc: 0.8529 - val_loss: 0.8191 - val_acc: 0.6176\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4888 - acc: 0.8529 - val_loss: 0.8198 - val_acc: 0.5882\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4880 - acc: 0.8529 - val_loss: 0.8206 - val_acc: 0.5882\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4873 - acc: 0.8529 - val_loss: 0.8213 - val_acc: 0.5882\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4865 - acc: 0.8529 - val_loss: 0.8220 - val_acc: 0.5882\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4857 - acc: 0.8529 - val_loss: 0.8228 - val_acc: 0.5882\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4850 - acc: 0.8529 - val_loss: 0.8235 - val_acc: 0.5882\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 0s 79us/step - loss: 0.4842 - acc: 0.8529 - val_loss: 0.8242 - val_acc: 0.5882\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4835 - acc: 0.8529 - val_loss: 0.8249 - val_acc: 0.5882\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4828 - acc: 0.8529 - val_loss: 0.8256 - val_acc: 0.5882\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 0s 31us/step - loss: 0.4820 - acc: 0.8529 - val_loss: 0.8263 - val_acc: 0.5882\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4813 - acc: 0.8529 - val_loss: 0.8270 - val_acc: 0.5882\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.4806 - acc: 0.8529 - val_loss: 0.8277 - val_acc: 0.5882\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.4799 - acc: 0.8529 - val_loss: 0.8283 - val_acc: 0.5882\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.4791 - acc: 0.8529 - val_loss: 0.8290 - val_acc: 0.5882\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 0s 35us/step - loss: 0.4784 - acc: 0.8529 - val_loss: 0.8297 - val_acc: 0.5882\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 0s 30us/step - loss: 0.4777 - acc: 0.8529 - val_loss: 0.8304 - val_acc: 0.5882\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.4770 - acc: 0.8676 - val_loss: 0.8311 - val_acc: 0.5882\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4762 - acc: 0.8676 - val_loss: 0.8318 - val_acc: 0.5882\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4755 - acc: 0.8676 - val_loss: 0.8326 - val_acc: 0.5882\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4748 - acc: 0.8676 - val_loss: 0.8333 - val_acc: 0.5882\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.4741 - acc: 0.8676 - val_loss: 0.8340 - val_acc: 0.5882\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4734 - acc: 0.8676 - val_loss: 0.8347 - val_acc: 0.5882\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 0s 70us/step - loss: 0.4726 - acc: 0.8676 - val_loss: 0.8355 - val_acc: 0.5882\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4719 - acc: 0.8676 - val_loss: 0.8362 - val_acc: 0.5882\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 0s 81us/step - loss: 0.4712 - acc: 0.8676 - val_loss: 0.8369 - val_acc: 0.5882\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4705 - acc: 0.8824 - val_loss: 0.8376 - val_acc: 0.5882\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4698 - acc: 0.8824 - val_loss: 0.8383 - val_acc: 0.5882\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4691 - acc: 0.8824 - val_loss: 0.8391 - val_acc: 0.5882\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4684 - acc: 0.8824 - val_loss: 0.8398 - val_acc: 0.5882\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4677 - acc: 0.8824 - val_loss: 0.8405 - val_acc: 0.5882\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4670 - acc: 0.8824 - val_loss: 0.8412 - val_acc: 0.5882\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4663 - acc: 0.8824 - val_loss: 0.8419 - val_acc: 0.5588\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4656 - acc: 0.8824 - val_loss: 0.8427 - val_acc: 0.5588\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4649 - acc: 0.8824 - val_loss: 0.8434 - val_acc: 0.5588\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4642 - acc: 0.8824 - val_loss: 0.8442 - val_acc: 0.5588\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4635 - acc: 0.8824 - val_loss: 0.8449 - val_acc: 0.5588\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4628 - acc: 0.8824 - val_loss: 0.8456 - val_acc: 0.5588\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4621 - acc: 0.8824 - val_loss: 0.8463 - val_acc: 0.5588\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4614 - acc: 0.8824 - val_loss: 0.8470 - val_acc: 0.5588\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4607 - acc: 0.8824 - val_loss: 0.8478 - val_acc: 0.5588\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4601 - acc: 0.8824 - val_loss: 0.8485 - val_acc: 0.5588\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.4594 - acc: 0.8824 - val_loss: 0.8493 - val_acc: 0.5588\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4587 - acc: 0.8824 - val_loss: 0.8501 - val_acc: 0.5588\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4580 - acc: 0.8824 - val_loss: 0.8508 - val_acc: 0.5588\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4573 - acc: 0.8824 - val_loss: 0.8516 - val_acc: 0.5588\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4566 - acc: 0.8824 - val_loss: 0.8524 - val_acc: 0.5588\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4559 - acc: 0.8824 - val_loss: 0.8532 - val_acc: 0.5588\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4553 - acc: 0.8824 - val_loss: 0.8540 - val_acc: 0.5588\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4546 - acc: 0.8824 - val_loss: 0.8549 - val_acc: 0.5588\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4539 - acc: 0.8824 - val_loss: 0.8557 - val_acc: 0.5588\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4533 - acc: 0.8824 - val_loss: 0.8565 - val_acc: 0.5588\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4526 - acc: 0.8824 - val_loss: 0.8574 - val_acc: 0.5588\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4519 - acc: 0.8824 - val_loss: 0.8582 - val_acc: 0.5588\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4512 - acc: 0.8824 - val_loss: 0.8590 - val_acc: 0.5588\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4506 - acc: 0.8824 - val_loss: 0.8598 - val_acc: 0.5588\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.4499 - acc: 0.8824 - val_loss: 0.8607 - val_acc: 0.5588\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4492 - acc: 0.8824 - val_loss: 0.8615 - val_acc: 0.5588\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4485 - acc: 0.8824 - val_loss: 0.8623 - val_acc: 0.5588\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4479 - acc: 0.8824 - val_loss: 0.8631 - val_acc: 0.5588\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4472 - acc: 0.8824 - val_loss: 0.8640 - val_acc: 0.5588\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 0s 81us/step - loss: 0.4465 - acc: 0.8824 - val_loss: 0.8649 - val_acc: 0.5588\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4458 - acc: 0.8824 - val_loss: 0.8657 - val_acc: 0.5588\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4452 - acc: 0.8824 - val_loss: 0.8664 - val_acc: 0.5588\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 0s 68us/step - loss: 0.4445 - acc: 0.8824 - val_loss: 0.8672 - val_acc: 0.5588\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4438 - acc: 0.8824 - val_loss: 0.8680 - val_acc: 0.5588\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4432 - acc: 0.8824 - val_loss: 0.8688 - val_acc: 0.5588\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4425 - acc: 0.8824 - val_loss: 0.8697 - val_acc: 0.5588\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4419 - acc: 0.8824 - val_loss: 0.8705 - val_acc: 0.5588\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4412 - acc: 0.8824 - val_loss: 0.8714 - val_acc: 0.5588\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4406 - acc: 0.8824 - val_loss: 0.8723 - val_acc: 0.5588\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4399 - acc: 0.8824 - val_loss: 0.8731 - val_acc: 0.5588\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4393 - acc: 0.8824 - val_loss: 0.8740 - val_acc: 0.5588\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4386 - acc: 0.8824 - val_loss: 0.8748 - val_acc: 0.5588\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4380 - acc: 0.8824 - val_loss: 0.8756 - val_acc: 0.5588\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4373 - acc: 0.8824 - val_loss: 0.8764 - val_acc: 0.5588\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4367 - acc: 0.8824 - val_loss: 0.8772 - val_acc: 0.5588\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4360 - acc: 0.8824 - val_loss: 0.8781 - val_acc: 0.5588\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4354 - acc: 0.8824 - val_loss: 0.8790 - val_acc: 0.5588\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4347 - acc: 0.8824 - val_loss: 0.8799 - val_acc: 0.5588\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4341 - acc: 0.8824 - val_loss: 0.8808 - val_acc: 0.5588\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4334 - acc: 0.8824 - val_loss: 0.8816 - val_acc: 0.5588\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4328 - acc: 0.8824 - val_loss: 0.8825 - val_acc: 0.5588\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4321 - acc: 0.8824 - val_loss: 0.8834 - val_acc: 0.5588\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4315 - acc: 0.8824 - val_loss: 0.8843 - val_acc: 0.5588\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4309 - acc: 0.8824 - val_loss: 0.8852 - val_acc: 0.5588\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4302 - acc: 0.8824 - val_loss: 0.8861 - val_acc: 0.5588\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 0s 94us/step - loss: 0.4296 - acc: 0.8824 - val_loss: 0.8870 - val_acc: 0.5588\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4289 - acc: 0.8824 - val_loss: 0.8879 - val_acc: 0.5588\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4283 - acc: 0.8824 - val_loss: 0.8888 - val_acc: 0.5588\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4277 - acc: 0.8824 - val_loss: 0.8897 - val_acc: 0.5588\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 0s 114us/step - loss: 0.4270 - acc: 0.8824 - val_loss: 0.8906 - val_acc: 0.5588\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4264 - acc: 0.8824 - val_loss: 0.8915 - val_acc: 0.5588\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4257 - acc: 0.8824 - val_loss: 0.8925 - val_acc: 0.5588\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4251 - acc: 0.8824 - val_loss: 0.8934 - val_acc: 0.5588\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4244 - acc: 0.8824 - val_loss: 0.8943 - val_acc: 0.5588\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4238 - acc: 0.8824 - val_loss: 0.8953 - val_acc: 0.5588\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4232 - acc: 0.8824 - val_loss: 0.8962 - val_acc: 0.5588\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4225 - acc: 0.8824 - val_loss: 0.8971 - val_acc: 0.5588\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4218 - acc: 0.8824 - val_loss: 0.8981 - val_acc: 0.5588\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4212 - acc: 0.8824 - val_loss: 0.8991 - val_acc: 0.5588\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4205 - acc: 0.8824 - val_loss: 0.9001 - val_acc: 0.5588\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4198 - acc: 0.8824 - val_loss: 0.9011 - val_acc: 0.5588\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 0s 72us/step - loss: 0.4192 - acc: 0.8824 - val_loss: 0.9021 - val_acc: 0.5588\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4185 - acc: 0.8824 - val_loss: 0.9032 - val_acc: 0.5588\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4178 - acc: 0.8824 - val_loss: 0.9042 - val_acc: 0.5588\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4171 - acc: 0.8824 - val_loss: 0.9053 - val_acc: 0.5588\n",
            "Train on 68 samples, validate on 34 samples\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 10ms/step - loss: 0.9398 - acc: 0.3971 - val_loss: 0.8754 - val_acc: 0.3529\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.8904 - acc: 0.4118 - val_loss: 0.8409 - val_acc: 0.2941\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.8468 - acc: 0.3971 - val_loss: 0.8156 - val_acc: 0.3235\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.8084 - acc: 0.4853 - val_loss: 0.7992 - val_acc: 0.3235\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.7771 - acc: 0.5147 - val_loss: 0.7892 - val_acc: 0.4412\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.7538 - acc: 0.5735 - val_loss: 0.7854 - val_acc: 0.4118\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 0s 31us/step - loss: 0.7348 - acc: 0.5735 - val_loss: 0.7856 - val_acc: 0.4412\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.7194 - acc: 0.5882 - val_loss: 0.7863 - val_acc: 0.3529\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.7070 - acc: 0.6471 - val_loss: 0.7874 - val_acc: 0.3529\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.6965 - acc: 0.6765 - val_loss: 0.7887 - val_acc: 0.4118\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.6876 - acc: 0.6765 - val_loss: 0.7902 - val_acc: 0.3824\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.6792 - acc: 0.6912 - val_loss: 0.7914 - val_acc: 0.3824\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.6708 - acc: 0.7206 - val_loss: 0.7923 - val_acc: 0.4118\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.6624 - acc: 0.7500 - val_loss: 0.7933 - val_acc: 0.4118\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6541 - acc: 0.7794 - val_loss: 0.7946 - val_acc: 0.4412\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 0s 34us/step - loss: 0.6459 - acc: 0.7794 - val_loss: 0.7950 - val_acc: 0.4412\n",
            "\n",
            "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.6380 - acc: 0.7794 - val_loss: 0.7950 - val_acc: 0.4412\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6372 - acc: 0.7794 - val_loss: 0.7950 - val_acc: 0.4412\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.6364 - acc: 0.7794 - val_loss: 0.7949 - val_acc: 0.4412\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.6355 - acc: 0.7794 - val_loss: 0.7949 - val_acc: 0.4412\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.6347 - acc: 0.7794 - val_loss: 0.7948 - val_acc: 0.4412\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.6338 - acc: 0.7794 - val_loss: 0.7947 - val_acc: 0.4412\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.6329 - acc: 0.7647 - val_loss: 0.7946 - val_acc: 0.4412\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.6320 - acc: 0.7647 - val_loss: 0.7945 - val_acc: 0.4412\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.6311 - acc: 0.7647 - val_loss: 0.7944 - val_acc: 0.4412\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.6302 - acc: 0.7647 - val_loss: 0.7943 - val_acc: 0.4412\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.6293 - acc: 0.7647 - val_loss: 0.7941 - val_acc: 0.4412\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.6284 - acc: 0.7647 - val_loss: 0.7940 - val_acc: 0.4412\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.6274 - acc: 0.7647 - val_loss: 0.7940 - val_acc: 0.4412\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.6265 - acc: 0.7647 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6255 - acc: 0.7647 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.6245 - acc: 0.7647 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.6235 - acc: 0.7647 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 0s 35us/step - loss: 0.6224 - acc: 0.7647 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.6214 - acc: 0.7794 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 0s 36us/step - loss: 0.6203 - acc: 0.7794 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 0s 34us/step - loss: 0.6193 - acc: 0.7794 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 0s 34us/step - loss: 0.6182 - acc: 0.7794 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6171 - acc: 0.7941 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.6160 - acc: 0.7941 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 0s 35us/step - loss: 0.6148 - acc: 0.7941 - val_loss: 0.7939 - val_acc: 0.4412\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 0s 34us/step - loss: 0.6137 - acc: 0.7941 - val_loss: 0.7940 - val_acc: 0.4706\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.6126 - acc: 0.7941 - val_loss: 0.7940 - val_acc: 0.4706\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6115 - acc: 0.7941 - val_loss: 0.7941 - val_acc: 0.4706\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.6103 - acc: 0.7941 - val_loss: 0.7942 - val_acc: 0.4706\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.6092 - acc: 0.7941 - val_loss: 0.7943 - val_acc: 0.4706\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.6081 - acc: 0.7941 - val_loss: 0.7944 - val_acc: 0.4706\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.6070 - acc: 0.7941 - val_loss: 0.7945 - val_acc: 0.4706\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.6059 - acc: 0.7941 - val_loss: 0.7945 - val_acc: 0.4706\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.6048 - acc: 0.7941 - val_loss: 0.7947 - val_acc: 0.4706\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.6037 - acc: 0.7941 - val_loss: 0.7948 - val_acc: 0.4706\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 0s 70us/step - loss: 0.6026 - acc: 0.7941 - val_loss: 0.7950 - val_acc: 0.4706\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.6016 - acc: 0.8088 - val_loss: 0.7951 - val_acc: 0.4706\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.6005 - acc: 0.8088 - val_loss: 0.7953 - val_acc: 0.4706\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 0s 82us/step - loss: 0.5995 - acc: 0.8088 - val_loss: 0.7954 - val_acc: 0.4706\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.5984 - acc: 0.8088 - val_loss: 0.7955 - val_acc: 0.4706\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5974 - acc: 0.8088 - val_loss: 0.7956 - val_acc: 0.4706\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5964 - acc: 0.8088 - val_loss: 0.7957 - val_acc: 0.4706\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.5953 - acc: 0.8088 - val_loss: 0.7958 - val_acc: 0.4706\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.5943 - acc: 0.8088 - val_loss: 0.7958 - val_acc: 0.4706\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.5933 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.4706\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 0s 74us/step - loss: 0.5923 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.4706\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5913 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.4706\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 0s 215us/step - loss: 0.5902 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.4706\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 0s 35us/step - loss: 0.5892 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.5000\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5881 - acc: 0.8088 - val_loss: 0.7958 - val_acc: 0.5000\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.5871 - acc: 0.8088 - val_loss: 0.7958 - val_acc: 0.5000\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5861 - acc: 0.8088 - val_loss: 0.7958 - val_acc: 0.5000\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5851 - acc: 0.8088 - val_loss: 0.7958 - val_acc: 0.5000\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5840 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.5000\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.5830 - acc: 0.8088 - val_loss: 0.7959 - val_acc: 0.5000\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5819 - acc: 0.8088 - val_loss: 0.7960 - val_acc: 0.5000\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5808 - acc: 0.8088 - val_loss: 0.7962 - val_acc: 0.5000\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5798 - acc: 0.8088 - val_loss: 0.7964 - val_acc: 0.5000\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5787 - acc: 0.8088 - val_loss: 0.7966 - val_acc: 0.5000\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5776 - acc: 0.8088 - val_loss: 0.7968 - val_acc: 0.5000\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5765 - acc: 0.8088 - val_loss: 0.7970 - val_acc: 0.5000\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5755 - acc: 0.8088 - val_loss: 0.7973 - val_acc: 0.5000\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.5744 - acc: 0.8088 - val_loss: 0.7975 - val_acc: 0.5000\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5734 - acc: 0.8088 - val_loss: 0.7978 - val_acc: 0.5000\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5723 - acc: 0.8088 - val_loss: 0.7981 - val_acc: 0.5000\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5713 - acc: 0.8088 - val_loss: 0.7983 - val_acc: 0.5000\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.5703 - acc: 0.8088 - val_loss: 0.7985 - val_acc: 0.5000\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.5693 - acc: 0.8088 - val_loss: 0.7987 - val_acc: 0.5000\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5683 - acc: 0.8088 - val_loss: 0.7989 - val_acc: 0.5000\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5673 - acc: 0.8088 - val_loss: 0.7991 - val_acc: 0.5000\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5663 - acc: 0.8088 - val_loss: 0.7992 - val_acc: 0.5000\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5653 - acc: 0.8088 - val_loss: 0.7994 - val_acc: 0.5294\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 0s 85us/step - loss: 0.5642 - acc: 0.8088 - val_loss: 0.7996 - val_acc: 0.5294\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 0s 83us/step - loss: 0.5632 - acc: 0.8088 - val_loss: 0.7998 - val_acc: 0.5294\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.5622 - acc: 0.8088 - val_loss: 0.8000 - val_acc: 0.5294\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5613 - acc: 0.8088 - val_loss: 0.8003 - val_acc: 0.5294\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 0s 83us/step - loss: 0.5603 - acc: 0.8088 - val_loss: 0.8006 - val_acc: 0.5000\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5592 - acc: 0.8088 - val_loss: 0.8009 - val_acc: 0.5000\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.5583 - acc: 0.8088 - val_loss: 0.8012 - val_acc: 0.5000\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5573 - acc: 0.8088 - val_loss: 0.8015 - val_acc: 0.5000\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.5562 - acc: 0.8088 - val_loss: 0.8019 - val_acc: 0.5000\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5552 - acc: 0.8088 - val_loss: 0.8022 - val_acc: 0.5000\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.5542 - acc: 0.8088 - val_loss: 0.8025 - val_acc: 0.5000\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5532 - acc: 0.8088 - val_loss: 0.8028 - val_acc: 0.5000\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5522 - acc: 0.8088 - val_loss: 0.8032 - val_acc: 0.5000\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5512 - acc: 0.8088 - val_loss: 0.8035 - val_acc: 0.5000\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.5501 - acc: 0.8088 - val_loss: 0.8038 - val_acc: 0.5000\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 0s 31us/step - loss: 0.5491 - acc: 0.8088 - val_loss: 0.8041 - val_acc: 0.5000\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5481 - acc: 0.8235 - val_loss: 0.8044 - val_acc: 0.5000\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5471 - acc: 0.8382 - val_loss: 0.8047 - val_acc: 0.5000\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.5461 - acc: 0.8382 - val_loss: 0.8049 - val_acc: 0.5000\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5452 - acc: 0.8382 - val_loss: 0.8052 - val_acc: 0.5000\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5442 - acc: 0.8382 - val_loss: 0.8054 - val_acc: 0.5000\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5432 - acc: 0.8382 - val_loss: 0.8056 - val_acc: 0.5000\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5422 - acc: 0.8382 - val_loss: 0.8057 - val_acc: 0.5000\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5412 - acc: 0.8382 - val_loss: 0.8059 - val_acc: 0.5000\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 0s 80us/step - loss: 0.5402 - acc: 0.8382 - val_loss: 0.8061 - val_acc: 0.5000\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.5392 - acc: 0.8382 - val_loss: 0.8062 - val_acc: 0.5000\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5382 - acc: 0.8382 - val_loss: 0.8063 - val_acc: 0.5000\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.5372 - acc: 0.8382 - val_loss: 0.8064 - val_acc: 0.5000\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5362 - acc: 0.8382 - val_loss: 0.8066 - val_acc: 0.5000\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5352 - acc: 0.8382 - val_loss: 0.8067 - val_acc: 0.5000\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5342 - acc: 0.8382 - val_loss: 0.8068 - val_acc: 0.5000\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.5332 - acc: 0.8382 - val_loss: 0.8070 - val_acc: 0.5000\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5322 - acc: 0.8382 - val_loss: 0.8072 - val_acc: 0.5000\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5312 - acc: 0.8382 - val_loss: 0.8074 - val_acc: 0.5000\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5303 - acc: 0.8382 - val_loss: 0.8076 - val_acc: 0.5000\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5293 - acc: 0.8382 - val_loss: 0.8078 - val_acc: 0.5000\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5283 - acc: 0.8382 - val_loss: 0.8080 - val_acc: 0.5000\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5273 - acc: 0.8382 - val_loss: 0.8081 - val_acc: 0.5000\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5264 - acc: 0.8382 - val_loss: 0.8083 - val_acc: 0.5000\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.5254 - acc: 0.8382 - val_loss: 0.8085 - val_acc: 0.5000\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.5244 - acc: 0.8382 - val_loss: 0.8087 - val_acc: 0.5000\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5235 - acc: 0.8382 - val_loss: 0.8089 - val_acc: 0.5000\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5225 - acc: 0.8382 - val_loss: 0.8091 - val_acc: 0.5000\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5215 - acc: 0.8382 - val_loss: 0.8092 - val_acc: 0.5000\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5205 - acc: 0.8382 - val_loss: 0.8094 - val_acc: 0.5000\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.5195 - acc: 0.8382 - val_loss: 0.8096 - val_acc: 0.5000\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5185 - acc: 0.8382 - val_loss: 0.8098 - val_acc: 0.5000\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5175 - acc: 0.8382 - val_loss: 0.8100 - val_acc: 0.5000\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5165 - acc: 0.8382 - val_loss: 0.8102 - val_acc: 0.5000\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.5155 - acc: 0.8382 - val_loss: 0.8104 - val_acc: 0.5294\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.5145 - acc: 0.8382 - val_loss: 0.8106 - val_acc: 0.5294\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5135 - acc: 0.8382 - val_loss: 0.8108 - val_acc: 0.5294\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5125 - acc: 0.8382 - val_loss: 0.8110 - val_acc: 0.5294\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5114 - acc: 0.8382 - val_loss: 0.8111 - val_acc: 0.5294\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5104 - acc: 0.8382 - val_loss: 0.8113 - val_acc: 0.5000\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5094 - acc: 0.8382 - val_loss: 0.8114 - val_acc: 0.5000\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.5083 - acc: 0.8382 - val_loss: 0.8116 - val_acc: 0.5000\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.5072 - acc: 0.8382 - val_loss: 0.8117 - val_acc: 0.5000\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5062 - acc: 0.8382 - val_loss: 0.8119 - val_acc: 0.5000\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.5051 - acc: 0.8529 - val_loss: 0.8121 - val_acc: 0.5000\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5041 - acc: 0.8529 - val_loss: 0.8123 - val_acc: 0.5000\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.5030 - acc: 0.8676 - val_loss: 0.8125 - val_acc: 0.5000\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.5020 - acc: 0.8676 - val_loss: 0.8127 - val_acc: 0.5000\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5009 - acc: 0.8824 - val_loss: 0.8129 - val_acc: 0.5000\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.4999 - acc: 0.8824 - val_loss: 0.8131 - val_acc: 0.5000\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4989 - acc: 0.8824 - val_loss: 0.8134 - val_acc: 0.5000\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4978 - acc: 0.8824 - val_loss: 0.8136 - val_acc: 0.5000\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4968 - acc: 0.8824 - val_loss: 0.8138 - val_acc: 0.5000\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4957 - acc: 0.8824 - val_loss: 0.8141 - val_acc: 0.5000\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4947 - acc: 0.8824 - val_loss: 0.8144 - val_acc: 0.5000\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4936 - acc: 0.8824 - val_loss: 0.8146 - val_acc: 0.5000\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4926 - acc: 0.8824 - val_loss: 0.8148 - val_acc: 0.5000\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4915 - acc: 0.8824 - val_loss: 0.8151 - val_acc: 0.5000\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4905 - acc: 0.8824 - val_loss: 0.8153 - val_acc: 0.5000\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4894 - acc: 0.8824 - val_loss: 0.8155 - val_acc: 0.5000\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4884 - acc: 0.8824 - val_loss: 0.8158 - val_acc: 0.5000\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4874 - acc: 0.8824 - val_loss: 0.8160 - val_acc: 0.5000\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4864 - acc: 0.8824 - val_loss: 0.8163 - val_acc: 0.5000\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.4853 - acc: 0.8824 - val_loss: 0.8165 - val_acc: 0.5294\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4843 - acc: 0.8824 - val_loss: 0.8168 - val_acc: 0.5294\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.4833 - acc: 0.8824 - val_loss: 0.8170 - val_acc: 0.5294\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.4823 - acc: 0.8824 - val_loss: 0.8173 - val_acc: 0.5294\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4813 - acc: 0.8824 - val_loss: 0.8176 - val_acc: 0.5294\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4803 - acc: 0.8824 - val_loss: 0.8180 - val_acc: 0.5294\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.4793 - acc: 0.8824 - val_loss: 0.8183 - val_acc: 0.5294\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 0s 80us/step - loss: 0.4783 - acc: 0.8824 - val_loss: 0.8187 - val_acc: 0.5294\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4773 - acc: 0.8824 - val_loss: 0.8190 - val_acc: 0.5294\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4763 - acc: 0.8824 - val_loss: 0.8195 - val_acc: 0.5294\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4753 - acc: 0.8824 - val_loss: 0.8199 - val_acc: 0.5294\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4743 - acc: 0.8971 - val_loss: 0.8203 - val_acc: 0.5294\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4733 - acc: 0.8971 - val_loss: 0.8207 - val_acc: 0.5000\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4723 - acc: 0.8971 - val_loss: 0.8210 - val_acc: 0.5000\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4713 - acc: 0.8971 - val_loss: 0.8214 - val_acc: 0.5000\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4704 - acc: 0.8971 - val_loss: 0.8219 - val_acc: 0.5000\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4694 - acc: 0.8971 - val_loss: 0.8223 - val_acc: 0.5000\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.4685 - acc: 0.8971 - val_loss: 0.8227 - val_acc: 0.5000\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4675 - acc: 0.8971 - val_loss: 0.8232 - val_acc: 0.5000\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 0s 200us/step - loss: 0.4666 - acc: 0.8971 - val_loss: 0.8237 - val_acc: 0.5000\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4656 - acc: 0.8971 - val_loss: 0.8242 - val_acc: 0.5000\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4647 - acc: 0.8971 - val_loss: 0.8246 - val_acc: 0.5000\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 0s 124us/step - loss: 0.4637 - acc: 0.8971 - val_loss: 0.8250 - val_acc: 0.5000\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4628 - acc: 0.8971 - val_loss: 0.8254 - val_acc: 0.5000\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 0s 70us/step - loss: 0.4619 - acc: 0.8971 - val_loss: 0.8257 - val_acc: 0.5000\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.4610 - acc: 0.8971 - val_loss: 0.8261 - val_acc: 0.5000\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 0s 80us/step - loss: 0.4600 - acc: 0.9118 - val_loss: 0.8264 - val_acc: 0.5000\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.4591 - acc: 0.9118 - val_loss: 0.8268 - val_acc: 0.5000\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 0s 76us/step - loss: 0.4582 - acc: 0.9118 - val_loss: 0.8272 - val_acc: 0.5000\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4573 - acc: 0.9118 - val_loss: 0.8275 - val_acc: 0.5000\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4564 - acc: 0.9118 - val_loss: 0.8279 - val_acc: 0.5000\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.4555 - acc: 0.9118 - val_loss: 0.8283 - val_acc: 0.5000\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 0s 83us/step - loss: 0.4546 - acc: 0.9118 - val_loss: 0.8287 - val_acc: 0.5000\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.4538 - acc: 0.9118 - val_loss: 0.8290 - val_acc: 0.5000\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4529 - acc: 0.9118 - val_loss: 0.8293 - val_acc: 0.5000\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.4520 - acc: 0.9118 - val_loss: 0.8296 - val_acc: 0.5000\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4511 - acc: 0.9118 - val_loss: 0.8299 - val_acc: 0.5000\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4502 - acc: 0.9118 - val_loss: 0.8303 - val_acc: 0.5000\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4493 - acc: 0.9118 - val_loss: 0.8307 - val_acc: 0.5000\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4485 - acc: 0.9118 - val_loss: 0.8310 - val_acc: 0.5000\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 0s 37us/step - loss: 0.4476 - acc: 0.9118 - val_loss: 0.8314 - val_acc: 0.5000\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4467 - acc: 0.9118 - val_loss: 0.8317 - val_acc: 0.5000\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4459 - acc: 0.9118 - val_loss: 0.8321 - val_acc: 0.5000\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4450 - acc: 0.9118 - val_loss: 0.8325 - val_acc: 0.5294\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4442 - acc: 0.9118 - val_loss: 0.8330 - val_acc: 0.5294\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.4433 - acc: 0.9118 - val_loss: 0.8335 - val_acc: 0.5000\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4425 - acc: 0.9118 - val_loss: 0.8340 - val_acc: 0.5000\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4417 - acc: 0.9118 - val_loss: 0.8345 - val_acc: 0.5000\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4408 - acc: 0.9118 - val_loss: 0.8349 - val_acc: 0.5000\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4400 - acc: 0.9118 - val_loss: 0.8354 - val_acc: 0.5000\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4392 - acc: 0.9118 - val_loss: 0.8359 - val_acc: 0.5000\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4383 - acc: 0.9118 - val_loss: 0.8365 - val_acc: 0.5000\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4375 - acc: 0.9118 - val_loss: 0.8371 - val_acc: 0.5000\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4367 - acc: 0.9118 - val_loss: 0.8376 - val_acc: 0.5000\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4359 - acc: 0.9118 - val_loss: 0.8382 - val_acc: 0.5000\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4350 - acc: 0.9118 - val_loss: 0.8387 - val_acc: 0.5000\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4342 - acc: 0.9118 - val_loss: 0.8393 - val_acc: 0.5000\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4334 - acc: 0.9118 - val_loss: 0.8399 - val_acc: 0.5000\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4326 - acc: 0.9118 - val_loss: 0.8405 - val_acc: 0.5000\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4318 - acc: 0.9118 - val_loss: 0.8411 - val_acc: 0.4706\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4310 - acc: 0.9118 - val_loss: 0.8418 - val_acc: 0.4706\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4302 - acc: 0.9118 - val_loss: 0.8424 - val_acc: 0.4706\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.4294 - acc: 0.9118 - val_loss: 0.8429 - val_acc: 0.4706\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4286 - acc: 0.9118 - val_loss: 0.8434 - val_acc: 0.4706\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4278 - acc: 0.9118 - val_loss: 0.8439 - val_acc: 0.4706\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4270 - acc: 0.9118 - val_loss: 0.8446 - val_acc: 0.4706\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4262 - acc: 0.9118 - val_loss: 0.8452 - val_acc: 0.4706\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4254 - acc: 0.9118 - val_loss: 0.8458 - val_acc: 0.4706\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4246 - acc: 0.9118 - val_loss: 0.8464 - val_acc: 0.4706\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4238 - acc: 0.9118 - val_loss: 0.8469 - val_acc: 0.4706\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4230 - acc: 0.9118 - val_loss: 0.8475 - val_acc: 0.4706\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4222 - acc: 0.9118 - val_loss: 0.8481 - val_acc: 0.4706\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4214 - acc: 0.9118 - val_loss: 0.8487 - val_acc: 0.4706\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4206 - acc: 0.9118 - val_loss: 0.8493 - val_acc: 0.4706\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4198 - acc: 0.9118 - val_loss: 0.8499 - val_acc: 0.4706\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4190 - acc: 0.9118 - val_loss: 0.8505 - val_acc: 0.4706\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4182 - acc: 0.9118 - val_loss: 0.8510 - val_acc: 0.4706\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4175 - acc: 0.9118 - val_loss: 0.8515 - val_acc: 0.4706\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4167 - acc: 0.9118 - val_loss: 0.8521 - val_acc: 0.4706\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.4160 - acc: 0.9118 - val_loss: 0.8527 - val_acc: 0.4706\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4152 - acc: 0.9118 - val_loss: 0.8533 - val_acc: 0.4706\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4144 - acc: 0.9118 - val_loss: 0.8539 - val_acc: 0.4706\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4137 - acc: 0.9118 - val_loss: 0.8544 - val_acc: 0.4706\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4129 - acc: 0.9118 - val_loss: 0.8550 - val_acc: 0.4706\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4122 - acc: 0.9118 - val_loss: 0.8555 - val_acc: 0.4706\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4114 - acc: 0.9118 - val_loss: 0.8562 - val_acc: 0.4706\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4107 - acc: 0.9118 - val_loss: 0.8568 - val_acc: 0.4706\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4099 - acc: 0.9118 - val_loss: 0.8574 - val_acc: 0.5000\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4092 - acc: 0.9118 - val_loss: 0.8579 - val_acc: 0.5000\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4084 - acc: 0.9118 - val_loss: 0.8585 - val_acc: 0.5000\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4077 - acc: 0.9118 - val_loss: 0.8591 - val_acc: 0.5000\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4070 - acc: 0.9118 - val_loss: 0.8598 - val_acc: 0.5000\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4062 - acc: 0.9118 - val_loss: 0.8604 - val_acc: 0.5000\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4055 - acc: 0.9118 - val_loss: 0.8610 - val_acc: 0.5000\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4047 - acc: 0.9118 - val_loss: 0.8616 - val_acc: 0.5000\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4040 - acc: 0.9118 - val_loss: 0.8622 - val_acc: 0.5000\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.4033 - acc: 0.9118 - val_loss: 0.8628 - val_acc: 0.5000\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 0s 97us/step - loss: 0.4026 - acc: 0.9118 - val_loss: 0.8634 - val_acc: 0.5000\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 0s 103us/step - loss: 0.4018 - acc: 0.9118 - val_loss: 0.8640 - val_acc: 0.5000\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4011 - acc: 0.9118 - val_loss: 0.8647 - val_acc: 0.5000\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 0s 80us/step - loss: 0.4004 - acc: 0.9118 - val_loss: 0.8653 - val_acc: 0.5000\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.3997 - acc: 0.9118 - val_loss: 0.8660 - val_acc: 0.5000\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3990 - acc: 0.9118 - val_loss: 0.8666 - val_acc: 0.5000\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3982 - acc: 0.9118 - val_loss: 0.8673 - val_acc: 0.5000\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3975 - acc: 0.9118 - val_loss: 0.8680 - val_acc: 0.5000\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.3968 - acc: 0.9118 - val_loss: 0.8687 - val_acc: 0.5000\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3961 - acc: 0.9118 - val_loss: 0.8694 - val_acc: 0.5000\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.3954 - acc: 0.9118 - val_loss: 0.8700 - val_acc: 0.5000\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3946 - acc: 0.9118 - val_loss: 0.8706 - val_acc: 0.5000\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.3939 - acc: 0.9118 - val_loss: 0.8712 - val_acc: 0.5000\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3932 - acc: 0.9118 - val_loss: 0.8718 - val_acc: 0.5000\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3925 - acc: 0.9118 - val_loss: 0.8725 - val_acc: 0.5000\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3918 - acc: 0.9118 - val_loss: 0.8731 - val_acc: 0.5000\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3910 - acc: 0.9118 - val_loss: 0.8738 - val_acc: 0.5000\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.3903 - acc: 0.9265 - val_loss: 0.8745 - val_acc: 0.5000\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3896 - acc: 0.9265 - val_loss: 0.8751 - val_acc: 0.5000\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.3889 - acc: 0.9265 - val_loss: 0.8757 - val_acc: 0.5000\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3882 - acc: 0.9265 - val_loss: 0.8763 - val_acc: 0.5000\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.3876 - acc: 0.9265 - val_loss: 0.8769 - val_acc: 0.5000\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.3868 - acc: 0.9265 - val_loss: 0.8775 - val_acc: 0.5000\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3862 - acc: 0.9265 - val_loss: 0.8782 - val_acc: 0.5000\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3855 - acc: 0.9265 - val_loss: 0.8788 - val_acc: 0.5000\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3848 - acc: 0.9265 - val_loss: 0.8795 - val_acc: 0.5000\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3841 - acc: 0.9265 - val_loss: 0.8801 - val_acc: 0.5000\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3834 - acc: 0.9265 - val_loss: 0.8808 - val_acc: 0.5000\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3827 - acc: 0.9265 - val_loss: 0.8814 - val_acc: 0.5000\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3820 - acc: 0.9265 - val_loss: 0.8819 - val_acc: 0.5000\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3813 - acc: 0.9265 - val_loss: 0.8824 - val_acc: 0.5000\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3807 - acc: 0.9265 - val_loss: 0.8831 - val_acc: 0.5000\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.3800 - acc: 0.9265 - val_loss: 0.8837 - val_acc: 0.5000\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.3793 - acc: 0.9265 - val_loss: 0.8844 - val_acc: 0.5000\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3786 - acc: 0.9265 - val_loss: 0.8851 - val_acc: 0.5000\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3780 - acc: 0.9265 - val_loss: 0.8858 - val_acc: 0.5000\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3773 - acc: 0.9265 - val_loss: 0.8865 - val_acc: 0.5000\n",
            "Train on 68 samples, validate on 34 samples\n",
            "Epoch 1/300\n",
            "68/68 [==============================] - 1s 11ms/step - loss: 0.8790 - acc: 0.5735 - val_loss: 0.7891 - val_acc: 0.6176\n",
            "Epoch 2/300\n",
            "68/68 [==============================] - 0s 42us/step - loss: 0.8390 - acc: 0.5588 - val_loss: 0.7782 - val_acc: 0.6176\n",
            "Epoch 3/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.8060 - acc: 0.5882 - val_loss: 0.7703 - val_acc: 0.6176\n",
            "Epoch 4/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.7782 - acc: 0.6176 - val_loss: 0.7639 - val_acc: 0.6176\n",
            "Epoch 5/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.7536 - acc: 0.6029 - val_loss: 0.7571 - val_acc: 0.6176\n",
            "Epoch 6/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.7317 - acc: 0.6176 - val_loss: 0.7517 - val_acc: 0.6176\n",
            "Epoch 7/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.7119 - acc: 0.6176 - val_loss: 0.7480 - val_acc: 0.6471\n",
            "Epoch 8/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.6955 - acc: 0.6324 - val_loss: 0.7459 - val_acc: 0.5882\n",
            "Epoch 9/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6831 - acc: 0.6324 - val_loss: 0.7440 - val_acc: 0.5882\n",
            "Epoch 10/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.6740 - acc: 0.6618 - val_loss: 0.7420 - val_acc: 0.6176\n",
            "Epoch 11/300\n",
            "68/68 [==============================] - 0s 35us/step - loss: 0.6670 - acc: 0.6912 - val_loss: 0.7404 - val_acc: 0.6176\n",
            "Epoch 12/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.6609 - acc: 0.6912 - val_loss: 0.7382 - val_acc: 0.5882\n",
            "Epoch 13/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.6557 - acc: 0.6765 - val_loss: 0.7360 - val_acc: 0.5882\n",
            "Epoch 14/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6507 - acc: 0.7059 - val_loss: 0.7340 - val_acc: 0.5882\n",
            "Epoch 15/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.6455 - acc: 0.6765 - val_loss: 0.7327 - val_acc: 0.6176\n",
            "Epoch 16/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6404 - acc: 0.6912 - val_loss: 0.7315 - val_acc: 0.6176\n",
            "Epoch 17/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6354 - acc: 0.7059 - val_loss: 0.7302 - val_acc: 0.6176\n",
            "Epoch 18/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.6306 - acc: 0.7059 - val_loss: 0.7291 - val_acc: 0.5882\n",
            "Epoch 19/300\n",
            "68/68 [==============================] - 0s 40us/step - loss: 0.6259 - acc: 0.7206 - val_loss: 0.7283 - val_acc: 0.5882\n",
            "Epoch 20/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.6211 - acc: 0.7206 - val_loss: 0.7276 - val_acc: 0.5882\n",
            "Epoch 21/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.6163 - acc: 0.7059 - val_loss: 0.7270 - val_acc: 0.5882\n",
            "Epoch 22/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.6114 - acc: 0.7206 - val_loss: 0.7262 - val_acc: 0.5882\n",
            "Epoch 23/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.6063 - acc: 0.7353 - val_loss: 0.7253 - val_acc: 0.5882\n",
            "Epoch 24/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.6012 - acc: 0.7353 - val_loss: 0.7247 - val_acc: 0.5882\n",
            "Epoch 25/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.5960 - acc: 0.7353 - val_loss: 0.7249 - val_acc: 0.5882\n",
            "Epoch 26/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.5907 - acc: 0.7353 - val_loss: 0.7252 - val_acc: 0.5882\n",
            "Epoch 27/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5855 - acc: 0.7353 - val_loss: 0.7257 - val_acc: 0.5882\n",
            "Epoch 28/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.5803 - acc: 0.7647 - val_loss: 0.7253 - val_acc: 0.5882\n",
            "Epoch 29/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.5752 - acc: 0.7647 - val_loss: 0.7248 - val_acc: 0.5882\n",
            "Epoch 30/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.5699 - acc: 0.7647 - val_loss: 0.7245 - val_acc: 0.5882\n",
            "Epoch 31/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.5645 - acc: 0.7647 - val_loss: 0.7245 - val_acc: 0.5882\n",
            "Epoch 32/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5591 - acc: 0.7794 - val_loss: 0.7248 - val_acc: 0.5882\n",
            "Epoch 33/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5536 - acc: 0.7794 - val_loss: 0.7254 - val_acc: 0.5882\n",
            "Epoch 34/300\n",
            "68/68 [==============================] - 0s 34us/step - loss: 0.5481 - acc: 0.7941 - val_loss: 0.7262 - val_acc: 0.6176\n",
            "Epoch 35/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.5425 - acc: 0.8088 - val_loss: 0.7270 - val_acc: 0.6176\n",
            "Epoch 36/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5369 - acc: 0.8088 - val_loss: 0.7280 - val_acc: 0.6176\n",
            "Epoch 37/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5313 - acc: 0.8088 - val_loss: 0.7302 - val_acc: 0.6176\n",
            "Epoch 38/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.5258 - acc: 0.8088 - val_loss: 0.7330 - val_acc: 0.6176\n",
            "Epoch 39/300\n",
            "68/68 [==============================] - 0s 40us/step - loss: 0.5203 - acc: 0.8088 - val_loss: 0.7360 - val_acc: 0.6176\n",
            "Epoch 40/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.5148 - acc: 0.8088 - val_loss: 0.7392 - val_acc: 0.6176\n",
            "\n",
            "Epoch 00040: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 41/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.5095 - acc: 0.8235 - val_loss: 0.7395 - val_acc: 0.6176\n",
            "Epoch 42/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.5090 - acc: 0.8235 - val_loss: 0.7398 - val_acc: 0.6176\n",
            "Epoch 43/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5084 - acc: 0.8235 - val_loss: 0.7401 - val_acc: 0.6176\n",
            "Epoch 44/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5078 - acc: 0.8235 - val_loss: 0.7404 - val_acc: 0.6176\n",
            "Epoch 45/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.5073 - acc: 0.8235 - val_loss: 0.7407 - val_acc: 0.6176\n",
            "Epoch 46/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5067 - acc: 0.8235 - val_loss: 0.7411 - val_acc: 0.6176\n",
            "Epoch 47/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.5061 - acc: 0.8088 - val_loss: 0.7414 - val_acc: 0.6176\n",
            "Epoch 48/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.5055 - acc: 0.8088 - val_loss: 0.7417 - val_acc: 0.6176\n",
            "Epoch 49/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.5049 - acc: 0.8088 - val_loss: 0.7420 - val_acc: 0.6176\n",
            "Epoch 50/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.5044 - acc: 0.8088 - val_loss: 0.7423 - val_acc: 0.6176\n",
            "\n",
            "Epoch 00050: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 51/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.5038 - acc: 0.8088 - val_loss: 0.7426 - val_acc: 0.6176\n",
            "Epoch 52/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.5032 - acc: 0.8088 - val_loss: 0.7430 - val_acc: 0.6176\n",
            "Epoch 53/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5026 - acc: 0.8088 - val_loss: 0.7433 - val_acc: 0.6176\n",
            "Epoch 54/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5020 - acc: 0.8088 - val_loss: 0.7436 - val_acc: 0.6176\n",
            "Epoch 55/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.5013 - acc: 0.8088 - val_loss: 0.7439 - val_acc: 0.6176\n",
            "Epoch 56/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5007 - acc: 0.8088 - val_loss: 0.7442 - val_acc: 0.6176\n",
            "Epoch 57/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.5001 - acc: 0.8088 - val_loss: 0.7445 - val_acc: 0.6176\n",
            "Epoch 58/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4995 - acc: 0.8088 - val_loss: 0.7448 - val_acc: 0.6176\n",
            "Epoch 59/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.4989 - acc: 0.8088 - val_loss: 0.7451 - val_acc: 0.6176\n",
            "Epoch 60/300\n",
            "68/68 [==============================] - 0s 38us/step - loss: 0.4983 - acc: 0.8088 - val_loss: 0.7454 - val_acc: 0.6176\n",
            "Epoch 61/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.4977 - acc: 0.8088 - val_loss: 0.7457 - val_acc: 0.6176\n",
            "Epoch 62/300\n",
            "68/68 [==============================] - 0s 32us/step - loss: 0.4971 - acc: 0.8088 - val_loss: 0.7460 - val_acc: 0.6176\n",
            "Epoch 63/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4965 - acc: 0.8088 - val_loss: 0.7463 - val_acc: 0.6176\n",
            "Epoch 64/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4959 - acc: 0.8088 - val_loss: 0.7466 - val_acc: 0.6176\n",
            "Epoch 65/300\n",
            "68/68 [==============================] - 0s 31us/step - loss: 0.4953 - acc: 0.8088 - val_loss: 0.7469 - val_acc: 0.6176\n",
            "Epoch 66/300\n",
            "68/68 [==============================] - 0s 31us/step - loss: 0.4947 - acc: 0.8088 - val_loss: 0.7472 - val_acc: 0.6176\n",
            "Epoch 67/300\n",
            "68/68 [==============================] - 0s 33us/step - loss: 0.4940 - acc: 0.8088 - val_loss: 0.7475 - val_acc: 0.6176\n",
            "Epoch 68/300\n",
            "68/68 [==============================] - 0s 42us/step - loss: 0.4934 - acc: 0.8088 - val_loss: 0.7478 - val_acc: 0.6176\n",
            "Epoch 69/300\n",
            "68/68 [==============================] - 0s 42us/step - loss: 0.4929 - acc: 0.8088 - val_loss: 0.7482 - val_acc: 0.6176\n",
            "Epoch 70/300\n",
            "68/68 [==============================] - 0s 30us/step - loss: 0.4923 - acc: 0.8088 - val_loss: 0.7485 - val_acc: 0.6176\n",
            "Epoch 71/300\n",
            "68/68 [==============================] - 0s 34us/step - loss: 0.4917 - acc: 0.8088 - val_loss: 0.7489 - val_acc: 0.6176\n",
            "Epoch 72/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.4911 - acc: 0.8088 - val_loss: 0.7493 - val_acc: 0.6176\n",
            "Epoch 73/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.4905 - acc: 0.8088 - val_loss: 0.7497 - val_acc: 0.6176\n",
            "Epoch 74/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.4899 - acc: 0.8088 - val_loss: 0.7501 - val_acc: 0.6176\n",
            "Epoch 75/300\n",
            "68/68 [==============================] - 0s 37us/step - loss: 0.4893 - acc: 0.8088 - val_loss: 0.7505 - val_acc: 0.6176\n",
            "Epoch 76/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4887 - acc: 0.8088 - val_loss: 0.7509 - val_acc: 0.6176\n",
            "Epoch 77/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.4881 - acc: 0.8088 - val_loss: 0.7513 - val_acc: 0.6176\n",
            "Epoch 78/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.4875 - acc: 0.8088 - val_loss: 0.7517 - val_acc: 0.6176\n",
            "Epoch 79/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4870 - acc: 0.8088 - val_loss: 0.7521 - val_acc: 0.6176\n",
            "Epoch 80/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.4864 - acc: 0.8088 - val_loss: 0.7525 - val_acc: 0.6176\n",
            "Epoch 81/300\n",
            "68/68 [==============================] - 0s 30us/step - loss: 0.4858 - acc: 0.8088 - val_loss: 0.7529 - val_acc: 0.6176\n",
            "Epoch 82/300\n",
            "68/68 [==============================] - 0s 77us/step - loss: 0.4852 - acc: 0.8088 - val_loss: 0.7533 - val_acc: 0.6176\n",
            "Epoch 83/300\n",
            "68/68 [==============================] - 0s 77us/step - loss: 0.4846 - acc: 0.8088 - val_loss: 0.7538 - val_acc: 0.6176\n",
            "Epoch 84/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.4841 - acc: 0.8088 - val_loss: 0.7542 - val_acc: 0.6176\n",
            "Epoch 85/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.4835 - acc: 0.8088 - val_loss: 0.7546 - val_acc: 0.6176\n",
            "Epoch 86/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4829 - acc: 0.8088 - val_loss: 0.7551 - val_acc: 0.6176\n",
            "Epoch 87/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4823 - acc: 0.8088 - val_loss: 0.7555 - val_acc: 0.6176\n",
            "Epoch 88/300\n",
            "68/68 [==============================] - 0s 71us/step - loss: 0.4817 - acc: 0.8088 - val_loss: 0.7559 - val_acc: 0.6176\n",
            "Epoch 89/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4812 - acc: 0.8088 - val_loss: 0.7564 - val_acc: 0.6176\n",
            "Epoch 90/300\n",
            "68/68 [==============================] - 0s 74us/step - loss: 0.4806 - acc: 0.8088 - val_loss: 0.7567 - val_acc: 0.6176\n",
            "Epoch 91/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4800 - acc: 0.8088 - val_loss: 0.7571 - val_acc: 0.6176\n",
            "Epoch 92/300\n",
            "68/68 [==============================] - 0s 37us/step - loss: 0.4794 - acc: 0.8088 - val_loss: 0.7575 - val_acc: 0.6176\n",
            "Epoch 93/300\n",
            "68/68 [==============================] - 0s 68us/step - loss: 0.4788 - acc: 0.8088 - val_loss: 0.7579 - val_acc: 0.6176\n",
            "Epoch 94/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4782 - acc: 0.8088 - val_loss: 0.7583 - val_acc: 0.6176\n",
            "Epoch 95/300\n",
            "68/68 [==============================] - 0s 41us/step - loss: 0.4777 - acc: 0.8088 - val_loss: 0.7587 - val_acc: 0.6176\n",
            "Epoch 96/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4771 - acc: 0.8088 - val_loss: 0.7591 - val_acc: 0.6176\n",
            "Epoch 97/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4765 - acc: 0.8088 - val_loss: 0.7595 - val_acc: 0.6176\n",
            "Epoch 98/300\n",
            "68/68 [==============================] - 0s 72us/step - loss: 0.4759 - acc: 0.8088 - val_loss: 0.7599 - val_acc: 0.6176\n",
            "Epoch 99/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4753 - acc: 0.8088 - val_loss: 0.7603 - val_acc: 0.6176\n",
            "Epoch 100/300\n",
            "68/68 [==============================] - 0s 77us/step - loss: 0.4748 - acc: 0.8088 - val_loss: 0.7607 - val_acc: 0.6176\n",
            "Epoch 101/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4742 - acc: 0.8088 - val_loss: 0.7610 - val_acc: 0.6176\n",
            "Epoch 102/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4736 - acc: 0.8088 - val_loss: 0.7614 - val_acc: 0.6176\n",
            "Epoch 103/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4731 - acc: 0.8088 - val_loss: 0.7618 - val_acc: 0.6176\n",
            "Epoch 104/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4725 - acc: 0.8088 - val_loss: 0.7623 - val_acc: 0.6176\n",
            "Epoch 105/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4719 - acc: 0.8088 - val_loss: 0.7627 - val_acc: 0.6176\n",
            "Epoch 106/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4713 - acc: 0.8088 - val_loss: 0.7631 - val_acc: 0.6176\n",
            "Epoch 107/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4707 - acc: 0.8088 - val_loss: 0.7635 - val_acc: 0.6176\n",
            "Epoch 108/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4702 - acc: 0.8088 - val_loss: 0.7639 - val_acc: 0.6176\n",
            "Epoch 109/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4696 - acc: 0.8088 - val_loss: 0.7643 - val_acc: 0.6176\n",
            "Epoch 110/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4690 - acc: 0.8088 - val_loss: 0.7647 - val_acc: 0.6176\n",
            "Epoch 111/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4684 - acc: 0.8088 - val_loss: 0.7651 - val_acc: 0.6176\n",
            "Epoch 112/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4678 - acc: 0.8088 - val_loss: 0.7655 - val_acc: 0.6176\n",
            "Epoch 113/300\n",
            "68/68 [==============================] - 0s 65us/step - loss: 0.4672 - acc: 0.8088 - val_loss: 0.7659 - val_acc: 0.6176\n",
            "Epoch 114/300\n",
            "68/68 [==============================] - 0s 82us/step - loss: 0.4666 - acc: 0.8088 - val_loss: 0.7662 - val_acc: 0.6176\n",
            "Epoch 115/300\n",
            "68/68 [==============================] - 0s 73us/step - loss: 0.4660 - acc: 0.8088 - val_loss: 0.7666 - val_acc: 0.6176\n",
            "Epoch 116/300\n",
            "68/68 [==============================] - 0s 68us/step - loss: 0.4654 - acc: 0.8088 - val_loss: 0.7670 - val_acc: 0.6176\n",
            "Epoch 117/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4648 - acc: 0.8088 - val_loss: 0.7674 - val_acc: 0.6176\n",
            "Epoch 118/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4642 - acc: 0.8088 - val_loss: 0.7678 - val_acc: 0.6176\n",
            "Epoch 119/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4636 - acc: 0.8088 - val_loss: 0.7682 - val_acc: 0.6176\n",
            "Epoch 120/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.4631 - acc: 0.8088 - val_loss: 0.7686 - val_acc: 0.6176\n",
            "Epoch 121/300\n",
            "68/68 [==============================] - 0s 75us/step - loss: 0.4625 - acc: 0.8088 - val_loss: 0.7690 - val_acc: 0.6176\n",
            "Epoch 122/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4619 - acc: 0.8088 - val_loss: 0.7693 - val_acc: 0.6176\n",
            "Epoch 123/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4613 - acc: 0.8088 - val_loss: 0.7697 - val_acc: 0.6176\n",
            "Epoch 124/300\n",
            "68/68 [==============================] - 0s 74us/step - loss: 0.4607 - acc: 0.8088 - val_loss: 0.7701 - val_acc: 0.6176\n",
            "Epoch 125/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4601 - acc: 0.8088 - val_loss: 0.7705 - val_acc: 0.6176\n",
            "Epoch 126/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4595 - acc: 0.8088 - val_loss: 0.7710 - val_acc: 0.6176\n",
            "Epoch 127/300\n",
            "68/68 [==============================] - 0s 72us/step - loss: 0.4589 - acc: 0.8088 - val_loss: 0.7714 - val_acc: 0.6176\n",
            "Epoch 128/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4583 - acc: 0.8088 - val_loss: 0.7718 - val_acc: 0.6176\n",
            "Epoch 129/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4577 - acc: 0.8088 - val_loss: 0.7722 - val_acc: 0.6176\n",
            "Epoch 130/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4571 - acc: 0.8088 - val_loss: 0.7726 - val_acc: 0.6176\n",
            "Epoch 131/300\n",
            "68/68 [==============================] - 0s 44us/step - loss: 0.4565 - acc: 0.8088 - val_loss: 0.7730 - val_acc: 0.6176\n",
            "Epoch 132/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4559 - acc: 0.8088 - val_loss: 0.7734 - val_acc: 0.6176\n",
            "Epoch 133/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.4553 - acc: 0.8088 - val_loss: 0.7739 - val_acc: 0.6176\n",
            "Epoch 134/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4546 - acc: 0.8088 - val_loss: 0.7743 - val_acc: 0.6176\n",
            "Epoch 135/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4540 - acc: 0.8088 - val_loss: 0.7747 - val_acc: 0.6176\n",
            "Epoch 136/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4534 - acc: 0.8088 - val_loss: 0.7751 - val_acc: 0.6176\n",
            "Epoch 137/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4528 - acc: 0.8088 - val_loss: 0.7755 - val_acc: 0.6176\n",
            "Epoch 138/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4522 - acc: 0.8088 - val_loss: 0.7760 - val_acc: 0.6176\n",
            "Epoch 139/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4516 - acc: 0.8088 - val_loss: 0.7764 - val_acc: 0.6176\n",
            "Epoch 140/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4510 - acc: 0.8088 - val_loss: 0.7769 - val_acc: 0.6176\n",
            "Epoch 141/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4504 - acc: 0.8088 - val_loss: 0.7774 - val_acc: 0.6176\n",
            "Epoch 142/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4498 - acc: 0.8088 - val_loss: 0.7779 - val_acc: 0.6176\n",
            "Epoch 143/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4491 - acc: 0.8088 - val_loss: 0.7783 - val_acc: 0.6176\n",
            "Epoch 144/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4485 - acc: 0.8088 - val_loss: 0.7788 - val_acc: 0.6176\n",
            "Epoch 145/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4479 - acc: 0.8088 - val_loss: 0.7794 - val_acc: 0.6176\n",
            "Epoch 146/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4473 - acc: 0.8088 - val_loss: 0.7798 - val_acc: 0.6176\n",
            "Epoch 147/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4466 - acc: 0.8088 - val_loss: 0.7803 - val_acc: 0.6176\n",
            "Epoch 148/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4460 - acc: 0.8088 - val_loss: 0.7808 - val_acc: 0.6176\n",
            "Epoch 149/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4454 - acc: 0.8088 - val_loss: 0.7813 - val_acc: 0.6176\n",
            "Epoch 150/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4448 - acc: 0.8088 - val_loss: 0.7818 - val_acc: 0.6176\n",
            "Epoch 151/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4442 - acc: 0.8088 - val_loss: 0.7823 - val_acc: 0.6176\n",
            "Epoch 152/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4435 - acc: 0.8088 - val_loss: 0.7828 - val_acc: 0.6176\n",
            "Epoch 153/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4429 - acc: 0.8088 - val_loss: 0.7832 - val_acc: 0.6176\n",
            "Epoch 154/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4423 - acc: 0.8088 - val_loss: 0.7837 - val_acc: 0.6176\n",
            "Epoch 155/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4416 - acc: 0.8088 - val_loss: 0.7842 - val_acc: 0.6176\n",
            "Epoch 156/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4410 - acc: 0.8088 - val_loss: 0.7846 - val_acc: 0.6176\n",
            "Epoch 157/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4403 - acc: 0.8088 - val_loss: 0.7851 - val_acc: 0.6176\n",
            "Epoch 158/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4396 - acc: 0.8235 - val_loss: 0.7857 - val_acc: 0.6176\n",
            "Epoch 159/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4389 - acc: 0.8235 - val_loss: 0.7863 - val_acc: 0.6176\n",
            "Epoch 160/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4382 - acc: 0.8235 - val_loss: 0.7868 - val_acc: 0.6176\n",
            "Epoch 161/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4375 - acc: 0.8235 - val_loss: 0.7874 - val_acc: 0.6176\n",
            "Epoch 162/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4367 - acc: 0.8235 - val_loss: 0.7879 - val_acc: 0.6176\n",
            "Epoch 163/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4359 - acc: 0.8235 - val_loss: 0.7883 - val_acc: 0.6176\n",
            "Epoch 164/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4351 - acc: 0.8235 - val_loss: 0.7888 - val_acc: 0.6176\n",
            "Epoch 165/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4343 - acc: 0.8235 - val_loss: 0.7892 - val_acc: 0.6176\n",
            "Epoch 166/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4335 - acc: 0.8235 - val_loss: 0.7897 - val_acc: 0.6176\n",
            "Epoch 167/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.4327 - acc: 0.8235 - val_loss: 0.7902 - val_acc: 0.5882\n",
            "Epoch 168/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.4319 - acc: 0.8235 - val_loss: 0.7906 - val_acc: 0.5882\n",
            "Epoch 169/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4311 - acc: 0.8235 - val_loss: 0.7910 - val_acc: 0.5882\n",
            "Epoch 170/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4303 - acc: 0.8235 - val_loss: 0.7914 - val_acc: 0.5882\n",
            "Epoch 171/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4294 - acc: 0.8235 - val_loss: 0.7918 - val_acc: 0.5882\n",
            "Epoch 172/300\n",
            "68/68 [==============================] - 0s 64us/step - loss: 0.4286 - acc: 0.8235 - val_loss: 0.7922 - val_acc: 0.5882\n",
            "Epoch 173/300\n",
            "68/68 [==============================] - 0s 74us/step - loss: 0.4278 - acc: 0.8235 - val_loss: 0.7926 - val_acc: 0.5882\n",
            "Epoch 174/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4271 - acc: 0.8235 - val_loss: 0.7930 - val_acc: 0.5882\n",
            "Epoch 175/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4263 - acc: 0.8235 - val_loss: 0.7934 - val_acc: 0.5882\n",
            "Epoch 176/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4256 - acc: 0.8235 - val_loss: 0.7938 - val_acc: 0.5882\n",
            "Epoch 177/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4249 - acc: 0.8235 - val_loss: 0.7942 - val_acc: 0.5882\n",
            "Epoch 178/300\n",
            "68/68 [==============================] - 0s 67us/step - loss: 0.4242 - acc: 0.8235 - val_loss: 0.7947 - val_acc: 0.5882\n",
            "Epoch 179/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4235 - acc: 0.8235 - val_loss: 0.7951 - val_acc: 0.5882\n",
            "Epoch 180/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.4227 - acc: 0.8235 - val_loss: 0.7955 - val_acc: 0.5882\n",
            "Epoch 181/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4220 - acc: 0.8235 - val_loss: 0.7960 - val_acc: 0.5882\n",
            "Epoch 182/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.4212 - acc: 0.8235 - val_loss: 0.7963 - val_acc: 0.5882\n",
            "Epoch 183/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.4204 - acc: 0.8235 - val_loss: 0.7967 - val_acc: 0.5882\n",
            "Epoch 184/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4197 - acc: 0.8235 - val_loss: 0.7971 - val_acc: 0.5882\n",
            "Epoch 185/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4189 - acc: 0.8235 - val_loss: 0.7975 - val_acc: 0.5882\n",
            "Epoch 186/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4182 - acc: 0.8235 - val_loss: 0.7979 - val_acc: 0.5882\n",
            "Epoch 187/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4174 - acc: 0.8235 - val_loss: 0.7984 - val_acc: 0.5882\n",
            "Epoch 188/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4167 - acc: 0.8235 - val_loss: 0.7989 - val_acc: 0.5882\n",
            "Epoch 189/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4160 - acc: 0.8235 - val_loss: 0.7994 - val_acc: 0.5882\n",
            "Epoch 190/300\n",
            "68/68 [==============================] - 0s 62us/step - loss: 0.4153 - acc: 0.8235 - val_loss: 0.7998 - val_acc: 0.5882\n",
            "Epoch 191/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4146 - acc: 0.8235 - val_loss: 0.8002 - val_acc: 0.5882\n",
            "Epoch 192/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.4139 - acc: 0.8235 - val_loss: 0.8006 - val_acc: 0.5882\n",
            "Epoch 193/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4132 - acc: 0.8235 - val_loss: 0.8010 - val_acc: 0.5882\n",
            "Epoch 194/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4125 - acc: 0.8235 - val_loss: 0.8014 - val_acc: 0.5882\n",
            "Epoch 195/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4118 - acc: 0.8235 - val_loss: 0.8018 - val_acc: 0.5882\n",
            "Epoch 196/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4111 - acc: 0.8235 - val_loss: 0.8022 - val_acc: 0.5882\n",
            "Epoch 197/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4105 - acc: 0.8235 - val_loss: 0.8026 - val_acc: 0.5882\n",
            "Epoch 198/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4098 - acc: 0.8235 - val_loss: 0.8031 - val_acc: 0.5882\n",
            "Epoch 199/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4091 - acc: 0.8235 - val_loss: 0.8035 - val_acc: 0.5882\n",
            "Epoch 200/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4084 - acc: 0.8382 - val_loss: 0.8040 - val_acc: 0.5882\n",
            "Epoch 201/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.4078 - acc: 0.8382 - val_loss: 0.8045 - val_acc: 0.5882\n",
            "Epoch 202/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.4071 - acc: 0.8382 - val_loss: 0.8050 - val_acc: 0.5882\n",
            "Epoch 203/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.4064 - acc: 0.8382 - val_loss: 0.8055 - val_acc: 0.5882\n",
            "Epoch 204/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4058 - acc: 0.8382 - val_loss: 0.8060 - val_acc: 0.5882\n",
            "Epoch 205/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.4051 - acc: 0.8382 - val_loss: 0.8065 - val_acc: 0.5882\n",
            "Epoch 206/300\n",
            "68/68 [==============================] - 0s 83us/step - loss: 0.4044 - acc: 0.8382 - val_loss: 0.8071 - val_acc: 0.5882\n",
            "Epoch 207/300\n",
            "68/68 [==============================] - 0s 63us/step - loss: 0.4038 - acc: 0.8382 - val_loss: 0.8077 - val_acc: 0.5882\n",
            "Epoch 208/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.4031 - acc: 0.8382 - val_loss: 0.8082 - val_acc: 0.5882\n",
            "Epoch 209/300\n",
            "68/68 [==============================] - 0s 66us/step - loss: 0.4024 - acc: 0.8382 - val_loss: 0.8087 - val_acc: 0.5882\n",
            "Epoch 210/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.4018 - acc: 0.8382 - val_loss: 0.8093 - val_acc: 0.5882\n",
            "Epoch 211/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.4011 - acc: 0.8382 - val_loss: 0.8098 - val_acc: 0.5882\n",
            "Epoch 212/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.4005 - acc: 0.8382 - val_loss: 0.8103 - val_acc: 0.5882\n",
            "Epoch 213/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3998 - acc: 0.8382 - val_loss: 0.8108 - val_acc: 0.5882\n",
            "Epoch 214/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3991 - acc: 0.8382 - val_loss: 0.8114 - val_acc: 0.5882\n",
            "Epoch 215/300\n",
            "68/68 [==============================] - 0s 54us/step - loss: 0.3984 - acc: 0.8529 - val_loss: 0.8120 - val_acc: 0.5882\n",
            "Epoch 216/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3978 - acc: 0.8529 - val_loss: 0.8125 - val_acc: 0.5882\n",
            "Epoch 217/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3971 - acc: 0.8529 - val_loss: 0.8131 - val_acc: 0.5882\n",
            "Epoch 218/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3964 - acc: 0.8529 - val_loss: 0.8136 - val_acc: 0.5882\n",
            "Epoch 219/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3958 - acc: 0.8529 - val_loss: 0.8142 - val_acc: 0.5882\n",
            "Epoch 220/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.3951 - acc: 0.8529 - val_loss: 0.8147 - val_acc: 0.5882\n",
            "Epoch 221/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3944 - acc: 0.8529 - val_loss: 0.8153 - val_acc: 0.5882\n",
            "Epoch 222/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3938 - acc: 0.8529 - val_loss: 0.8159 - val_acc: 0.5882\n",
            "Epoch 223/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3931 - acc: 0.8529 - val_loss: 0.8165 - val_acc: 0.5882\n",
            "Epoch 224/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3924 - acc: 0.8529 - val_loss: 0.8171 - val_acc: 0.5882\n",
            "Epoch 225/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3918 - acc: 0.8529 - val_loss: 0.8176 - val_acc: 0.5882\n",
            "Epoch 226/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3911 - acc: 0.8529 - val_loss: 0.8180 - val_acc: 0.5882\n",
            "Epoch 227/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3905 - acc: 0.8529 - val_loss: 0.8184 - val_acc: 0.5882\n",
            "Epoch 228/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3898 - acc: 0.8529 - val_loss: 0.8189 - val_acc: 0.5882\n",
            "Epoch 229/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3892 - acc: 0.8529 - val_loss: 0.8193 - val_acc: 0.5882\n",
            "Epoch 230/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3885 - acc: 0.8529 - val_loss: 0.8197 - val_acc: 0.5882\n",
            "Epoch 231/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3879 - acc: 0.8529 - val_loss: 0.8202 - val_acc: 0.5882\n",
            "Epoch 232/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3872 - acc: 0.8529 - val_loss: 0.8206 - val_acc: 0.5882\n",
            "Epoch 233/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3865 - acc: 0.8529 - val_loss: 0.8211 - val_acc: 0.5882\n",
            "Epoch 234/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3859 - acc: 0.8529 - val_loss: 0.8216 - val_acc: 0.5882\n",
            "Epoch 235/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3852 - acc: 0.8529 - val_loss: 0.8220 - val_acc: 0.5882\n",
            "Epoch 236/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.3846 - acc: 0.8529 - val_loss: 0.8225 - val_acc: 0.5882\n",
            "Epoch 237/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3839 - acc: 0.8529 - val_loss: 0.8229 - val_acc: 0.5882\n",
            "Epoch 238/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3833 - acc: 0.8529 - val_loss: 0.8234 - val_acc: 0.5882\n",
            "Epoch 239/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3826 - acc: 0.8676 - val_loss: 0.8238 - val_acc: 0.5882\n",
            "Epoch 240/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.3820 - acc: 0.8676 - val_loss: 0.8243 - val_acc: 0.5882\n",
            "Epoch 241/300\n",
            "68/68 [==============================] - 0s 46us/step - loss: 0.3813 - acc: 0.8676 - val_loss: 0.8248 - val_acc: 0.5882\n",
            "Epoch 242/300\n",
            "68/68 [==============================] - 0s 52us/step - loss: 0.3807 - acc: 0.8676 - val_loss: 0.8254 - val_acc: 0.5882\n",
            "Epoch 243/300\n",
            "68/68 [==============================] - 0s 47us/step - loss: 0.3800 - acc: 0.8824 - val_loss: 0.8259 - val_acc: 0.5882\n",
            "Epoch 244/300\n",
            "68/68 [==============================] - 0s 39us/step - loss: 0.3794 - acc: 0.8824 - val_loss: 0.8265 - val_acc: 0.5882\n",
            "Epoch 245/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.3787 - acc: 0.8824 - val_loss: 0.8270 - val_acc: 0.5882\n",
            "Epoch 246/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3781 - acc: 0.8824 - val_loss: 0.8276 - val_acc: 0.5882\n",
            "Epoch 247/300\n",
            "68/68 [==============================] - 0s 160us/step - loss: 0.3774 - acc: 0.8824 - val_loss: 0.8281 - val_acc: 0.5882\n",
            "Epoch 248/300\n",
            "68/68 [==============================] - 0s 43us/step - loss: 0.3768 - acc: 0.8824 - val_loss: 0.8285 - val_acc: 0.5882\n",
            "Epoch 249/300\n",
            "68/68 [==============================] - 0s 45us/step - loss: 0.3761 - acc: 0.8824 - val_loss: 0.8290 - val_acc: 0.5882\n",
            "Epoch 250/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3754 - acc: 0.8824 - val_loss: 0.8294 - val_acc: 0.5882\n",
            "Epoch 251/300\n",
            "68/68 [==============================] - 0s 61us/step - loss: 0.3748 - acc: 0.8824 - val_loss: 0.8299 - val_acc: 0.5882\n",
            "Epoch 252/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3741 - acc: 0.8824 - val_loss: 0.8304 - val_acc: 0.5882\n",
            "Epoch 253/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3735 - acc: 0.8824 - val_loss: 0.8308 - val_acc: 0.5882\n",
            "Epoch 254/300\n",
            "68/68 [==============================] - 0s 55us/step - loss: 0.3728 - acc: 0.8824 - val_loss: 0.8312 - val_acc: 0.5882\n",
            "Epoch 255/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3721 - acc: 0.8824 - val_loss: 0.8316 - val_acc: 0.5882\n",
            "Epoch 256/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3715 - acc: 0.8824 - val_loss: 0.8321 - val_acc: 0.5882\n",
            "Epoch 257/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3708 - acc: 0.8824 - val_loss: 0.8325 - val_acc: 0.5882\n",
            "Epoch 258/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3702 - acc: 0.8824 - val_loss: 0.8330 - val_acc: 0.5882\n",
            "Epoch 259/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3695 - acc: 0.8824 - val_loss: 0.8336 - val_acc: 0.5882\n",
            "Epoch 260/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3688 - acc: 0.8824 - val_loss: 0.8341 - val_acc: 0.5882\n",
            "Epoch 261/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3682 - acc: 0.8824 - val_loss: 0.8347 - val_acc: 0.5882\n",
            "Epoch 262/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3675 - acc: 0.8824 - val_loss: 0.8353 - val_acc: 0.5882\n",
            "Epoch 263/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3668 - acc: 0.8824 - val_loss: 0.8358 - val_acc: 0.5882\n",
            "Epoch 264/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3661 - acc: 0.8824 - val_loss: 0.8363 - val_acc: 0.5882\n",
            "Epoch 265/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3655 - acc: 0.8824 - val_loss: 0.8367 - val_acc: 0.5588\n",
            "Epoch 266/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3648 - acc: 0.8824 - val_loss: 0.8371 - val_acc: 0.5588\n",
            "Epoch 267/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3641 - acc: 0.8824 - val_loss: 0.8375 - val_acc: 0.5588\n",
            "Epoch 268/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.3634 - acc: 0.8824 - val_loss: 0.8380 - val_acc: 0.5588\n",
            "Epoch 269/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3627 - acc: 0.8824 - val_loss: 0.8385 - val_acc: 0.5588\n",
            "Epoch 270/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3620 - acc: 0.8824 - val_loss: 0.8391 - val_acc: 0.5588\n",
            "Epoch 271/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3613 - acc: 0.8824 - val_loss: 0.8396 - val_acc: 0.5588\n",
            "Epoch 272/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3607 - acc: 0.8824 - val_loss: 0.8402 - val_acc: 0.5588\n",
            "Epoch 273/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3600 - acc: 0.8824 - val_loss: 0.8407 - val_acc: 0.5588\n",
            "Epoch 274/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3593 - acc: 0.8824 - val_loss: 0.8411 - val_acc: 0.5588\n",
            "Epoch 275/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3586 - acc: 0.8824 - val_loss: 0.8416 - val_acc: 0.5588\n",
            "Epoch 276/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3579 - acc: 0.8824 - val_loss: 0.8422 - val_acc: 0.5588\n",
            "Epoch 277/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3572 - acc: 0.8824 - val_loss: 0.8429 - val_acc: 0.5588\n",
            "Epoch 278/300\n",
            "68/68 [==============================] - 0s 42us/step - loss: 0.3565 - acc: 0.8824 - val_loss: 0.8434 - val_acc: 0.5588\n",
            "Epoch 279/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3558 - acc: 0.8824 - val_loss: 0.8440 - val_acc: 0.5588\n",
            "Epoch 280/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3551 - acc: 0.8824 - val_loss: 0.8445 - val_acc: 0.5588\n",
            "Epoch 281/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3544 - acc: 0.8971 - val_loss: 0.8451 - val_acc: 0.5588\n",
            "Epoch 282/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3537 - acc: 0.8971 - val_loss: 0.8457 - val_acc: 0.5588\n",
            "Epoch 283/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3530 - acc: 0.8971 - val_loss: 0.8463 - val_acc: 0.5588\n",
            "Epoch 284/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3523 - acc: 0.8971 - val_loss: 0.8468 - val_acc: 0.5588\n",
            "Epoch 285/300\n",
            "68/68 [==============================] - 0s 57us/step - loss: 0.3517 - acc: 0.8971 - val_loss: 0.8474 - val_acc: 0.5588\n",
            "Epoch 286/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3510 - acc: 0.8971 - val_loss: 0.8479 - val_acc: 0.5588\n",
            "Epoch 287/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3503 - acc: 0.8971 - val_loss: 0.8485 - val_acc: 0.5588\n",
            "Epoch 288/300\n",
            "68/68 [==============================] - 0s 56us/step - loss: 0.3496 - acc: 0.8971 - val_loss: 0.8491 - val_acc: 0.5588\n",
            "Epoch 289/300\n",
            "68/68 [==============================] - 0s 58us/step - loss: 0.3489 - acc: 0.8971 - val_loss: 0.8497 - val_acc: 0.5588\n",
            "Epoch 290/300\n",
            "68/68 [==============================] - 0s 59us/step - loss: 0.3482 - acc: 0.9118 - val_loss: 0.8504 - val_acc: 0.5588\n",
            "Epoch 291/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.3475 - acc: 0.9118 - val_loss: 0.8511 - val_acc: 0.5588\n",
            "Epoch 292/300\n",
            "68/68 [==============================] - 0s 60us/step - loss: 0.3468 - acc: 0.9118 - val_loss: 0.8518 - val_acc: 0.5588\n",
            "Epoch 293/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3462 - acc: 0.9118 - val_loss: 0.8524 - val_acc: 0.5588\n",
            "Epoch 294/300\n",
            "68/68 [==============================] - 0s 53us/step - loss: 0.3455 - acc: 0.9118 - val_loss: 0.8530 - val_acc: 0.5588\n",
            "Epoch 295/300\n",
            "68/68 [==============================] - 0s 50us/step - loss: 0.3448 - acc: 0.9118 - val_loss: 0.8536 - val_acc: 0.5588\n",
            "Epoch 296/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3441 - acc: 0.9118 - val_loss: 0.8543 - val_acc: 0.5588\n",
            "Epoch 297/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3435 - acc: 0.9118 - val_loss: 0.8549 - val_acc: 0.5588\n",
            "Epoch 298/300\n",
            "68/68 [==============================] - 0s 49us/step - loss: 0.3428 - acc: 0.9118 - val_loss: 0.8556 - val_acc: 0.5588\n",
            "Epoch 299/300\n",
            "68/68 [==============================] - 0s 48us/step - loss: 0.3421 - acc: 0.9118 - val_loss: 0.8563 - val_acc: 0.5588\n",
            "Epoch 300/300\n",
            "68/68 [==============================] - 0s 51us/step - loss: 0.3415 - acc: 0.9118 - val_loss: 0.8570 - val_acc: 0.5588\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYxTUsmLOQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data, train_labels_dec):\n",
        "  \n",
        "  train_data_np = \n",
        "\n",
        "  partial_train_data = np.array([train_data[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "  #Z-score\n",
        "  scaler = StandardScaler()\n",
        "  partial_train_data_stand = scaler.fit_transform(partial_train_data)\n",
        "  val_data_stand = scaler.transform(val_data)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=7, svd_solver='full')\n",
        "  pca.fit(partial_train_data_stand)\n",
        "  partial_train_data_stand_pca = pca.transform(partial_train_data_stand)\n",
        "  val_data_stand_pca = pca.transform(val_data_stand)\n",
        "\n",
        "  #Z-score after PCA\n",
        "  scaler_2 = StandardScaler()\n",
        "  partial_train_data_stand_pca_stand = scaler_2.fit_transform(partial_train_data_stand_pca)\n",
        "  val_data_stand_pca_stand = scaler_2.transform(val_data_stand_pca)\n",
        "\n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data_stand_pca_stand, one_hot_partial_train_targets, validation_data=(val_data_stand_pca_stand, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=88)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1xS2-BcJraQ",
        "colab_type": "code",
        "outputId": "be1256fc-76bf-49dd-ce81-61483874b366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "train_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   2,   4,   5,   8,  10,  11,  12,  13,  14,  17,  20,  21,\n",
              "        22,  23,  24,  25,  26,  27,  29,  30,  31,  33,  34,  37,  38,\n",
              "        39,  40,  41,  42,  43,  46,  47,  48,  49,  50,  55,  58,  60,\n",
              "        61,  62,  63,  64,  65,  67,  69,  70,  71,  73,  75,  76,  77,\n",
              "        79,  81,  82,  83,  84,  85,  87,  88,  89,  91,  92,  94,  96,\n",
              "        97,  98,  99, 100, 101, 103, 106, 107, 108, 110, 115, 116, 117,\n",
              "       118, 119, 121, 122, 124, 126, 127, 129, 130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "2b9b3c0e-3544-4a4c-e16f-4062ec1bcce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "51edaaba-4158-460c-8bcf-b0f4feb889f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "acaf3151-8673-4743-dadb-ee89b96dd7a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "428e8451-8b2c-42d8-9481-702011bd126c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f84408fe358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "9f06d793-f545-48f4-e752-fbc5ff08af15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f84408e16a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWRMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "f4633679-2157-479c-bac5-67e4e64fb748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_big_labels, epochs= num_epochs, batch_size=68, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "102/102 [==============================] - 1s 7ms/step - loss: 0.7492 - acc: 0.5588\n",
            "Epoch 2/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.7194 - acc: 0.5980\n",
            "Epoch 3/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.6990 - acc: 0.6373\n",
            "Epoch 4/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.6818 - acc: 0.6275\n",
            "Epoch 5/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.6707 - acc: 0.6471\n",
            "Epoch 6/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.6568 - acc: 0.6373\n",
            "Epoch 7/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.6461 - acc: 0.6569\n",
            "Epoch 8/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.6338 - acc: 0.6863\n",
            "Epoch 9/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.6231 - acc: 0.7059\n",
            "Epoch 10/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.6143 - acc: 0.7353\n",
            "Epoch 11/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.6083 - acc: 0.7451\n",
            "Epoch 12/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.6002 - acc: 0.7549\n",
            "Epoch 13/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.5917 - acc: 0.7549\n",
            "Epoch 14/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.5848 - acc: 0.7451\n",
            "Epoch 15/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.5766 - acc: 0.7549\n",
            "Epoch 16/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.5697 - acc: 0.7549\n",
            "Epoch 17/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.5630 - acc: 0.7549\n",
            "Epoch 18/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.5570 - acc: 0.7745\n",
            "Epoch 19/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.5501 - acc: 0.7745\n",
            "Epoch 20/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.5439 - acc: 0.7745\n",
            "Epoch 21/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.5380 - acc: 0.7843\n",
            "Epoch 22/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.5318 - acc: 0.7745\n",
            "Epoch 23/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.5255 - acc: 0.7745\n",
            "Epoch 24/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.5201 - acc: 0.7745\n",
            "Epoch 25/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.5142 - acc: 0.7745\n",
            "Epoch 26/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.5081 - acc: 0.7745\n",
            "Epoch 27/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.5019 - acc: 0.7745\n",
            "Epoch 28/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.4962 - acc: 0.7745\n",
            "Epoch 29/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.4905 - acc: 0.7745\n",
            "Epoch 30/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.4840 - acc: 0.7941\n",
            "Epoch 31/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.4775 - acc: 0.7941\n",
            "Epoch 32/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.4713 - acc: 0.7941\n",
            "Epoch 33/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.4651 - acc: 0.8137\n",
            "Epoch 34/300\n",
            "102/102 [==============================] - 0s 76us/step - loss: 0.4587 - acc: 0.8235\n",
            "Epoch 35/300\n",
            "102/102 [==============================] - 0s 82us/step - loss: 0.4517 - acc: 0.8333\n",
            "Epoch 36/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.4450 - acc: 0.8333\n",
            "Epoch 37/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.4383 - acc: 0.8333\n",
            "Epoch 38/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.4319 - acc: 0.8431\n",
            "Epoch 39/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.4239 - acc: 0.8431\n",
            "Epoch 40/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.4173 - acc: 0.8333\n",
            "Epoch 41/300\n",
            "102/102 [==============================] - 0s 87us/step - loss: 0.4104 - acc: 0.8431\n",
            "Epoch 42/300\n",
            "102/102 [==============================] - 0s 86us/step - loss: 0.4034 - acc: 0.8431\n",
            "Epoch 43/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.3970 - acc: 0.8431\n",
            "Epoch 44/300\n",
            "102/102 [==============================] - 0s 38us/step - loss: 0.3895 - acc: 0.8529\n",
            "Epoch 45/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.3838 - acc: 0.8431\n",
            "Epoch 46/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.3765 - acc: 0.8431\n",
            "Epoch 47/300\n",
            "102/102 [==============================] - 0s 76us/step - loss: 0.3697 - acc: 0.8431\n",
            "Epoch 48/300\n",
            "102/102 [==============================] - 0s 81us/step - loss: 0.3624 - acc: 0.8627\n",
            "Epoch 49/300\n",
            "102/102 [==============================] - 0s 92us/step - loss: 0.3551 - acc: 0.8824\n",
            "Epoch 50/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.3488 - acc: 0.8824\n",
            "Epoch 51/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.3425 - acc: 0.8922\n",
            "Epoch 52/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.3362 - acc: 0.9118\n",
            "Epoch 53/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.3292 - acc: 0.9216\n",
            "Epoch 54/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.3231 - acc: 0.9216\n",
            "Epoch 55/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.3160 - acc: 0.9216\n",
            "Epoch 56/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.3114 - acc: 0.9216\n",
            "Epoch 57/300\n",
            "102/102 [==============================] - 0s 72us/step - loss: 0.3048 - acc: 0.9216\n",
            "Epoch 58/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.2985 - acc: 0.9216\n",
            "Epoch 59/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.2910 - acc: 0.9216\n",
            "Epoch 60/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.2859 - acc: 0.9314\n",
            "Epoch 61/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.2788 - acc: 0.9412\n",
            "Epoch 62/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.2733 - acc: 0.9510\n",
            "Epoch 63/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.2667 - acc: 0.9608\n",
            "Epoch 64/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.2650 - acc: 0.9510\n",
            "Epoch 65/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.2565 - acc: 0.9608\n",
            "Epoch 66/300\n",
            "102/102 [==============================] - 0s 72us/step - loss: 0.2496 - acc: 0.9706\n",
            "Epoch 67/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.2447 - acc: 0.9902\n",
            "Epoch 68/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.2403 - acc: 0.9902\n",
            "Epoch 69/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.2332 - acc: 0.9804\n",
            "Epoch 70/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.2283 - acc: 0.9804\n",
            "Epoch 71/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.2254 - acc: 0.9804\n",
            "Epoch 72/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.2212 - acc: 0.9902\n",
            "Epoch 73/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.2161 - acc: 0.9902\n",
            "Epoch 74/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.2099 - acc: 0.9902\n",
            "Epoch 75/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.2060 - acc: 0.9902\n",
            "Epoch 76/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.2020 - acc: 0.9902\n",
            "Epoch 77/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.1969 - acc: 0.9902\n",
            "Epoch 78/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.1933 - acc: 0.9902\n",
            "Epoch 79/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.1883 - acc: 0.9902\n",
            "Epoch 80/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.1849 - acc: 0.9902\n",
            "Epoch 81/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.1812 - acc: 0.9902\n",
            "Epoch 82/300\n",
            "102/102 [==============================] - 0s 76us/step - loss: 0.1772 - acc: 0.9902\n",
            "Epoch 83/300\n",
            "102/102 [==============================] - 0s 89us/step - loss: 0.1737 - acc: 0.9902\n",
            "Epoch 84/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.1698 - acc: 0.9902\n",
            "Epoch 85/300\n",
            "102/102 [==============================] - 0s 76us/step - loss: 0.1669 - acc: 0.9902\n",
            "Epoch 86/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.1633 - acc: 0.9902\n",
            "Epoch 87/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.1592 - acc: 0.9902\n",
            "Epoch 88/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.1562 - acc: 0.9902\n",
            "Epoch 89/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.1535 - acc: 1.0000\n",
            "Epoch 90/300\n",
            "102/102 [==============================] - 0s 79us/step - loss: 0.1512 - acc: 1.0000\n",
            "Epoch 91/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.1474 - acc: 1.0000\n",
            "Epoch 92/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.1453 - acc: 1.0000\n",
            "Epoch 93/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.1435 - acc: 0.9902\n",
            "Epoch 94/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.1395 - acc: 1.0000\n",
            "Epoch 95/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.1373 - acc: 1.0000\n",
            "Epoch 96/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.1356 - acc: 1.0000\n",
            "Epoch 97/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.1329 - acc: 1.0000\n",
            "Epoch 98/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.1307 - acc: 1.0000\n",
            "Epoch 99/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.1281 - acc: 1.0000\n",
            "Epoch 100/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.1259 - acc: 1.0000\n",
            "Epoch 101/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.1238 - acc: 1.0000\n",
            "Epoch 102/300\n",
            "102/102 [==============================] - 0s 80us/step - loss: 0.1217 - acc: 1.0000\n",
            "Epoch 103/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.1197 - acc: 1.0000\n",
            "Epoch 104/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.1182 - acc: 1.0000\n",
            "Epoch 105/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.1161 - acc: 1.0000\n",
            "Epoch 106/300\n",
            "102/102 [==============================] - 0s 81us/step - loss: 0.1142 - acc: 1.0000\n",
            "Epoch 107/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.1128 - acc: 1.0000\n",
            "Epoch 108/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.1115 - acc: 1.0000\n",
            "Epoch 109/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.1103 - acc: 1.0000\n",
            "Epoch 110/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.1081 - acc: 1.0000\n",
            "Epoch 111/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.1064 - acc: 1.0000\n",
            "Epoch 112/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.1056 - acc: 1.0000\n",
            "Epoch 113/300\n",
            "102/102 [==============================] - 0s 72us/step - loss: 0.1049 - acc: 1.0000\n",
            "Epoch 114/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.1028 - acc: 1.0000\n",
            "Epoch 115/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.1015 - acc: 1.0000\n",
            "Epoch 116/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.1003 - acc: 1.0000\n",
            "Epoch 117/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0991 - acc: 1.0000\n",
            "Epoch 118/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0979 - acc: 1.0000\n",
            "Epoch 119/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0971 - acc: 1.0000\n",
            "Epoch 120/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.0962 - acc: 1.0000\n",
            "Epoch 121/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0950 - acc: 1.0000\n",
            "Epoch 122/300\n",
            "102/102 [==============================] - 0s 87us/step - loss: 0.0935 - acc: 1.0000\n",
            "Epoch 123/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0927 - acc: 1.0000\n",
            "Epoch 124/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.0918 - acc: 1.0000\n",
            "Epoch 125/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0904 - acc: 1.0000\n",
            "Epoch 126/300\n",
            "102/102 [==============================] - 0s 87us/step - loss: 0.0900 - acc: 1.0000\n",
            "Epoch 127/300\n",
            "102/102 [==============================] - 0s 81us/step - loss: 0.0889 - acc: 1.0000\n",
            "Epoch 128/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0879 - acc: 1.0000\n",
            "Epoch 129/300\n",
            "102/102 [==============================] - 0s 80us/step - loss: 0.0872 - acc: 1.0000\n",
            "Epoch 130/300\n",
            "102/102 [==============================] - 0s 93us/step - loss: 0.0862 - acc: 1.0000\n",
            "Epoch 131/300\n",
            "102/102 [==============================] - 0s 83us/step - loss: 0.0856 - acc: 1.0000\n",
            "Epoch 132/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.0851 - acc: 1.0000\n",
            "Epoch 133/300\n",
            "102/102 [==============================] - 0s 86us/step - loss: 0.0843 - acc: 1.0000\n",
            "Epoch 134/300\n",
            "102/102 [==============================] - 0s 83us/step - loss: 0.0832 - acc: 1.0000\n",
            "Epoch 135/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0826 - acc: 1.0000\n",
            "Epoch 136/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0818 - acc: 1.0000\n",
            "Epoch 137/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.0814 - acc: 1.0000\n",
            "Epoch 138/300\n",
            "102/102 [==============================] - 0s 76us/step - loss: 0.0805 - acc: 1.0000\n",
            "Epoch 139/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.0799 - acc: 1.0000\n",
            "Epoch 140/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0795 - acc: 1.0000\n",
            "Epoch 141/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.0783 - acc: 1.0000\n",
            "Epoch 142/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0778 - acc: 1.0000\n",
            "Epoch 143/300\n",
            "102/102 [==============================] - 0s 82us/step - loss: 0.0773 - acc: 1.0000\n",
            "Epoch 144/300\n",
            "102/102 [==============================] - 0s 94us/step - loss: 0.0767 - acc: 1.0000\n",
            "Epoch 145/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.0761 - acc: 1.0000\n",
            "Epoch 146/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0755 - acc: 1.0000\n",
            "Epoch 147/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0750 - acc: 1.0000\n",
            "Epoch 148/300\n",
            "102/102 [==============================] - 0s 79us/step - loss: 0.0744 - acc: 1.0000\n",
            "Epoch 149/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0740 - acc: 1.0000\n",
            "Epoch 150/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0736 - acc: 1.0000\n",
            "Epoch 151/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0730 - acc: 1.0000\n",
            "Epoch 152/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0728 - acc: 1.0000\n",
            "Epoch 153/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0724 - acc: 1.0000\n",
            "Epoch 154/300\n",
            "102/102 [==============================] - 0s 79us/step - loss: 0.0719 - acc: 1.0000\n",
            "Epoch 155/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0716 - acc: 1.0000\n",
            "Epoch 156/300\n",
            "102/102 [==============================] - 0s 89us/step - loss: 0.0711 - acc: 1.0000\n",
            "Epoch 157/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.0707 - acc: 1.0000\n",
            "Epoch 158/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0703 - acc: 1.0000\n",
            "Epoch 159/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0699 - acc: 1.0000\n",
            "Epoch 160/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.0697 - acc: 1.0000\n",
            "Epoch 161/300\n",
            "102/102 [==============================] - 0s 72us/step - loss: 0.0693 - acc: 1.0000\n",
            "Epoch 162/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0689 - acc: 1.0000\n",
            "Epoch 163/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0685 - acc: 1.0000\n",
            "Epoch 164/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0682 - acc: 1.0000\n",
            "Epoch 165/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0679 - acc: 1.0000\n",
            "Epoch 166/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0676 - acc: 1.0000\n",
            "Epoch 167/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0672 - acc: 1.0000\n",
            "Epoch 168/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0670 - acc: 1.0000\n",
            "Epoch 169/300\n",
            "102/102 [==============================] - 0s 196us/step - loss: 0.0666 - acc: 1.0000\n",
            "Epoch 170/300\n",
            "102/102 [==============================] - 0s 103us/step - loss: 0.0665 - acc: 1.0000\n",
            "Epoch 171/300\n",
            "102/102 [==============================] - 0s 97us/step - loss: 0.0661 - acc: 1.0000\n",
            "Epoch 172/300\n",
            "102/102 [==============================] - 0s 103us/step - loss: 0.0658 - acc: 1.0000\n",
            "Epoch 173/300\n",
            "102/102 [==============================] - 0s 101us/step - loss: 0.0655 - acc: 1.0000\n",
            "Epoch 174/300\n",
            "102/102 [==============================] - 0s 127us/step - loss: 0.0653 - acc: 1.0000\n",
            "Epoch 175/300\n",
            "102/102 [==============================] - 0s 122us/step - loss: 0.0650 - acc: 1.0000\n",
            "Epoch 176/300\n",
            "102/102 [==============================] - 0s 100us/step - loss: 0.0647 - acc: 1.0000\n",
            "Epoch 177/300\n",
            "102/102 [==============================] - 0s 102us/step - loss: 0.0644 - acc: 1.0000\n",
            "Epoch 178/300\n",
            "102/102 [==============================] - 0s 95us/step - loss: 0.0642 - acc: 1.0000\n",
            "Epoch 179/300\n",
            "102/102 [==============================] - 0s 96us/step - loss: 0.0639 - acc: 1.0000\n",
            "Epoch 180/300\n",
            "102/102 [==============================] - 0s 79us/step - loss: 0.0637 - acc: 1.0000\n",
            "Epoch 181/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.0634 - acc: 1.0000\n",
            "Epoch 182/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0632 - acc: 1.0000\n",
            "Epoch 183/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0629 - acc: 1.0000\n",
            "Epoch 184/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0627 - acc: 1.0000\n",
            "Epoch 185/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0624 - acc: 1.0000\n",
            "Epoch 186/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0623 - acc: 1.0000\n",
            "Epoch 187/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.0621 - acc: 1.0000\n",
            "Epoch 188/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0619 - acc: 1.0000\n",
            "Epoch 189/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0615 - acc: 1.0000\n",
            "Epoch 190/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0613 - acc: 1.0000\n",
            "Epoch 191/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0611 - acc: 1.0000\n",
            "Epoch 192/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0610 - acc: 1.0000\n",
            "Epoch 193/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0608 - acc: 1.0000\n",
            "Epoch 194/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0605 - acc: 1.0000\n",
            "Epoch 195/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0604 - acc: 1.0000\n",
            "Epoch 196/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.0601 - acc: 1.0000\n",
            "Epoch 197/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0599 - acc: 1.0000\n",
            "Epoch 198/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0597 - acc: 1.0000\n",
            "Epoch 199/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0595 - acc: 1.0000\n",
            "Epoch 200/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0593 - acc: 1.0000\n",
            "Epoch 201/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0591 - acc: 1.0000\n",
            "Epoch 202/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0589 - acc: 1.0000\n",
            "Epoch 203/300\n",
            "102/102 [==============================] - 0s 79us/step - loss: 0.0587 - acc: 1.0000\n",
            "Epoch 204/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0585 - acc: 1.0000\n",
            "Epoch 205/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0584 - acc: 1.0000\n",
            "Epoch 206/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.0582 - acc: 1.0000\n",
            "Epoch 207/300\n",
            "102/102 [==============================] - 0s 72us/step - loss: 0.0581 - acc: 1.0000\n",
            "Epoch 208/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0578 - acc: 1.0000\n",
            "Epoch 209/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0576 - acc: 1.0000\n",
            "Epoch 210/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0575 - acc: 1.0000\n",
            "Epoch 211/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0573 - acc: 1.0000\n",
            "Epoch 212/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0571 - acc: 1.0000\n",
            "Epoch 213/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0570 - acc: 1.0000\n",
            "Epoch 214/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0568 - acc: 1.0000\n",
            "Epoch 215/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0566 - acc: 1.0000\n",
            "Epoch 216/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0565 - acc: 1.0000\n",
            "Epoch 217/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0563 - acc: 1.0000\n",
            "Epoch 218/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0561 - acc: 1.0000\n",
            "Epoch 219/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0559 - acc: 1.0000\n",
            "Epoch 220/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0559 - acc: 1.0000\n",
            "Epoch 221/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0557 - acc: 1.0000\n",
            "Epoch 222/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.0555 - acc: 1.0000\n",
            "Epoch 223/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.0553 - acc: 1.0000\n",
            "Epoch 224/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0552 - acc: 1.0000\n",
            "Epoch 225/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.0551 - acc: 1.0000\n",
            "Epoch 226/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0549 - acc: 1.0000\n",
            "Epoch 227/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0548 - acc: 1.0000\n",
            "Epoch 228/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0546 - acc: 1.0000\n",
            "Epoch 229/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0545 - acc: 1.0000\n",
            "Epoch 230/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0543 - acc: 1.0000\n",
            "Epoch 231/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0542 - acc: 1.0000\n",
            "Epoch 232/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0540 - acc: 1.0000\n",
            "Epoch 233/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0539 - acc: 1.0000\n",
            "Epoch 234/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0538 - acc: 1.0000\n",
            "Epoch 235/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0536 - acc: 1.0000\n",
            "Epoch 236/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0535 - acc: 1.0000\n",
            "Epoch 237/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0533 - acc: 1.0000\n",
            "Epoch 238/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0532 - acc: 1.0000\n",
            "Epoch 239/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0531 - acc: 1.0000\n",
            "Epoch 240/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0530 - acc: 1.0000\n",
            "Epoch 241/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0528 - acc: 1.0000\n",
            "Epoch 242/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.0527 - acc: 1.0000\n",
            "Epoch 243/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.0526 - acc: 1.0000\n",
            "Epoch 244/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.0524 - acc: 1.0000\n",
            "Epoch 245/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.0523 - acc: 1.0000\n",
            "Epoch 246/300\n",
            "102/102 [==============================] - 0s 93us/step - loss: 0.0521 - acc: 1.0000\n",
            "Epoch 247/300\n",
            "102/102 [==============================] - 0s 83us/step - loss: 0.0520 - acc: 1.0000\n",
            "Epoch 248/300\n",
            "102/102 [==============================] - 0s 106us/step - loss: 0.0519 - acc: 1.0000\n",
            "Epoch 249/300\n",
            "102/102 [==============================] - 0s 83us/step - loss: 0.0518 - acc: 1.0000\n",
            "Epoch 250/300\n",
            "102/102 [==============================] - 0s 113us/step - loss: 0.0516 - acc: 1.0000\n",
            "Epoch 251/300\n",
            "102/102 [==============================] - 0s 108us/step - loss: 0.0515 - acc: 1.0000\n",
            "Epoch 252/300\n",
            "102/102 [==============================] - 0s 103us/step - loss: 0.0514 - acc: 1.0000\n",
            "Epoch 253/300\n",
            "102/102 [==============================] - 0s 84us/step - loss: 0.0513 - acc: 1.0000\n",
            "Epoch 254/300\n",
            "102/102 [==============================] - 0s 93us/step - loss: 0.0512 - acc: 1.0000\n",
            "Epoch 255/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0510 - acc: 1.0000\n",
            "Epoch 256/300\n",
            "102/102 [==============================] - 0s 81us/step - loss: 0.0509 - acc: 1.0000\n",
            "Epoch 257/300\n",
            "102/102 [==============================] - 0s 85us/step - loss: 0.0508 - acc: 1.0000\n",
            "Epoch 258/300\n",
            "102/102 [==============================] - 0s 85us/step - loss: 0.0507 - acc: 1.0000\n",
            "Epoch 259/300\n",
            "102/102 [==============================] - 0s 82us/step - loss: 0.0505 - acc: 1.0000\n",
            "Epoch 260/300\n",
            "102/102 [==============================] - 0s 80us/step - loss: 0.0504 - acc: 1.0000\n",
            "Epoch 261/300\n",
            "102/102 [==============================] - 0s 85us/step - loss: 0.0503 - acc: 1.0000\n",
            "Epoch 262/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.0502 - acc: 1.0000\n",
            "Epoch 263/300\n",
            "102/102 [==============================] - 0s 84us/step - loss: 0.0500 - acc: 1.0000\n",
            "Epoch 264/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0499 - acc: 1.0000\n",
            "Epoch 265/300\n",
            "102/102 [==============================] - 0s 72us/step - loss: 0.0498 - acc: 1.0000\n",
            "Epoch 266/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0497 - acc: 1.0000\n",
            "Epoch 267/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0496 - acc: 1.0000\n",
            "Epoch 268/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.0495 - acc: 1.0000\n",
            "Epoch 269/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0493 - acc: 1.0000\n",
            "Epoch 270/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0492 - acc: 1.0000\n",
            "Epoch 271/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0491 - acc: 1.0000\n",
            "Epoch 272/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0490 - acc: 1.0000\n",
            "Epoch 273/300\n",
            "102/102 [==============================] - 0s 97us/step - loss: 0.0489 - acc: 1.0000\n",
            "Epoch 274/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.0488 - acc: 1.0000\n",
            "Epoch 275/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0487 - acc: 1.0000\n",
            "Epoch 276/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0486 - acc: 1.0000\n",
            "Epoch 277/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.0484 - acc: 1.0000\n",
            "Epoch 278/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.0483 - acc: 1.0000\n",
            "Epoch 279/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0482 - acc: 1.0000\n",
            "Epoch 280/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0481 - acc: 1.0000\n",
            "Epoch 281/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0480 - acc: 1.0000\n",
            "Epoch 282/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.0479 - acc: 1.0000\n",
            "Epoch 283/300\n",
            "102/102 [==============================] - 0s 83us/step - loss: 0.0478 - acc: 1.0000\n",
            "Epoch 284/300\n",
            "102/102 [==============================] - 0s 155us/step - loss: 0.0477 - acc: 1.0000\n",
            "Epoch 285/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.0476 - acc: 1.0000\n",
            "Epoch 286/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.0475 - acc: 1.0000\n",
            "Epoch 287/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0474 - acc: 1.0000\n",
            "Epoch 288/300\n",
            "102/102 [==============================] - 0s 85us/step - loss: 0.0472 - acc: 1.0000\n",
            "Epoch 289/300\n",
            "102/102 [==============================] - 0s 73us/step - loss: 0.0471 - acc: 1.0000\n",
            "Epoch 290/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0471 - acc: 1.0000\n",
            "Epoch 291/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0469 - acc: 1.0000\n",
            "Epoch 292/300\n",
            "102/102 [==============================] - 0s 78us/step - loss: 0.0468 - acc: 1.0000\n",
            "Epoch 293/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0467 - acc: 1.0000\n",
            "Epoch 294/300\n",
            "102/102 [==============================] - 0s 76us/step - loss: 0.0466 - acc: 1.0000\n",
            "Epoch 295/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0465 - acc: 1.0000\n",
            "Epoch 296/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0464 - acc: 1.0000\n",
            "Epoch 297/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0463 - acc: 1.0000\n",
            "Epoch 298/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0462 - acc: 1.0000\n",
            "Epoch 299/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0461 - acc: 1.0000\n",
            "Epoch 300/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0460 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 21ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "bfcdc956-e7d5-4402-ae3d-eaa3c774f1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "51015ebf-95aa-4756-cf44-b72fe154aeac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    }
  ]
}
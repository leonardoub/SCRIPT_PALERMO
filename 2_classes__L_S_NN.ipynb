{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_classes _L_S_NN.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMsg6mk891zCvLV0O2kcHyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/2_classes__L_S_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ucro1fblcxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGJk36soJp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "97bb121b-b454-4a1a-df99-13ce01e13196"
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "1c778aa1-044a-45fb-a77f-619e09034cb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qPpCYboRP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_LS = df_train[df_train['Histology'] != 'adenocarcinoma']\n",
        "df_test_LS = df_test[df_test['Histology'] != 'adenocarcinoma']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train_LS.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test_LS.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train_LS.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test_LS.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "3187e552-ecb3-4ff6-e443-86744d15f4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(89, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'large cell':0, 'squamous cell carcinoma':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYt5NQPG8fX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "train_big_labels_dec = [word_index[label] for label in y_train_big]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)\n",
        "one_hot_train_big_labels = to_categorical(train_big_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(8,)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #kernel_regularizer=regularizers.l2(l=0.001)\n",
        "  model.add(layers.Dense(20, activation='relu'))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "07f0d2f6-6c01-4d55-fb08-9f6f308a3a70",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "92499dd1-eb71-4187-e468-95d1a03f2d93",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=89, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 89 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "89/89 [==============================] - 0s 4ms/step - loss: 0.7427 - acc: 0.5281 - val_loss: 0.7345 - val_acc: 0.5385\n",
            "Epoch 2/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.7399 - acc: 0.5281 - val_loss: 0.7330 - val_acc: 0.5385\n",
            "Epoch 3/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.7367 - acc: 0.5281 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 4/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.7334 - acc: 0.5281 - val_loss: 0.7301 - val_acc: 0.5385\n",
            "Epoch 5/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.7302 - acc: 0.5281 - val_loss: 0.7288 - val_acc: 0.5385\n",
            "Epoch 6/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.7270 - acc: 0.5281 - val_loss: 0.7276 - val_acc: 0.5385\n",
            "Epoch 7/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.7239 - acc: 0.5281 - val_loss: 0.7265 - val_acc: 0.5385\n",
            "Epoch 8/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.7209 - acc: 0.5281 - val_loss: 0.7256 - val_acc: 0.5385\n",
            "Epoch 9/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.7180 - acc: 0.5281 - val_loss: 0.7249 - val_acc: 0.5385\n",
            "Epoch 10/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.7152 - acc: 0.5281 - val_loss: 0.7243 - val_acc: 0.5385\n",
            "Epoch 11/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.7125 - acc: 0.5281 - val_loss: 0.7237 - val_acc: 0.5385\n",
            "Epoch 12/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.7099 - acc: 0.5281 - val_loss: 0.7233 - val_acc: 0.5385\n",
            "Epoch 13/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.7075 - acc: 0.5281 - val_loss: 0.7230 - val_acc: 0.5385\n",
            "Epoch 14/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.7052 - acc: 0.5281 - val_loss: 0.7228 - val_acc: 0.5385\n",
            "Epoch 15/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.7030 - acc: 0.5281 - val_loss: 0.7227 - val_acc: 0.5385\n",
            "Epoch 16/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.7009 - acc: 0.5281 - val_loss: 0.7226 - val_acc: 0.5385\n",
            "Epoch 17/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6989 - acc: 0.5281 - val_loss: 0.7227 - val_acc: 0.5385\n",
            "Epoch 18/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6969 - acc: 0.5281 - val_loss: 0.7227 - val_acc: 0.5385\n",
            "Epoch 19/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6951 - acc: 0.5281 - val_loss: 0.7227 - val_acc: 0.5385\n",
            "Epoch 20/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6932 - acc: 0.5393 - val_loss: 0.7228 - val_acc: 0.5385\n",
            "Epoch 21/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6915 - acc: 0.5506 - val_loss: 0.7230 - val_acc: 0.5385\n",
            "Epoch 22/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.6898 - acc: 0.5618 - val_loss: 0.7232 - val_acc: 0.5385\n",
            "Epoch 23/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6882 - acc: 0.5506 - val_loss: 0.7235 - val_acc: 0.5385\n",
            "Epoch 24/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6865 - acc: 0.5506 - val_loss: 0.7238 - val_acc: 0.5385\n",
            "Epoch 25/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6850 - acc: 0.5506 - val_loss: 0.7243 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 26/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6835 - acc: 0.5506 - val_loss: 0.7243 - val_acc: 0.5385\n",
            "Epoch 27/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6833 - acc: 0.5506 - val_loss: 0.7243 - val_acc: 0.5385\n",
            "Epoch 28/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6832 - acc: 0.5506 - val_loss: 0.7244 - val_acc: 0.5385\n",
            "Epoch 29/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6830 - acc: 0.5393 - val_loss: 0.7245 - val_acc: 0.5385\n",
            "Epoch 30/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6829 - acc: 0.5506 - val_loss: 0.7245 - val_acc: 0.5385\n",
            "Epoch 31/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6827 - acc: 0.5506 - val_loss: 0.7246 - val_acc: 0.5385\n",
            "Epoch 32/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6826 - acc: 0.5506 - val_loss: 0.7246 - val_acc: 0.5385\n",
            "Epoch 33/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6824 - acc: 0.5506 - val_loss: 0.7247 - val_acc: 0.5385\n",
            "Epoch 34/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6823 - acc: 0.5506 - val_loss: 0.7248 - val_acc: 0.5385\n",
            "Epoch 35/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6821 - acc: 0.5506 - val_loss: 0.7248 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 36/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6820 - acc: 0.5506 - val_loss: 0.7249 - val_acc: 0.5385\n",
            "Epoch 37/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6818 - acc: 0.5506 - val_loss: 0.7249 - val_acc: 0.5385\n",
            "Epoch 38/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6817 - acc: 0.5506 - val_loss: 0.7250 - val_acc: 0.5385\n",
            "Epoch 39/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6816 - acc: 0.5506 - val_loss: 0.7251 - val_acc: 0.5385\n",
            "Epoch 40/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6814 - acc: 0.5506 - val_loss: 0.7251 - val_acc: 0.5385\n",
            "Epoch 41/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6813 - acc: 0.5506 - val_loss: 0.7252 - val_acc: 0.5385\n",
            "Epoch 42/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6811 - acc: 0.5506 - val_loss: 0.7253 - val_acc: 0.5385\n",
            "Epoch 43/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6810 - acc: 0.5618 - val_loss: 0.7254 - val_acc: 0.5385\n",
            "Epoch 44/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6808 - acc: 0.5618 - val_loss: 0.7254 - val_acc: 0.5385\n",
            "Epoch 45/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6807 - acc: 0.5618 - val_loss: 0.7255 - val_acc: 0.5385\n",
            "Epoch 46/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6805 - acc: 0.5618 - val_loss: 0.7256 - val_acc: 0.5385\n",
            "Epoch 47/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6804 - acc: 0.5618 - val_loss: 0.7257 - val_acc: 0.5385\n",
            "Epoch 48/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6803 - acc: 0.5618 - val_loss: 0.7257 - val_acc: 0.5385\n",
            "Epoch 49/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6801 - acc: 0.5618 - val_loss: 0.7258 - val_acc: 0.5385\n",
            "Epoch 50/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6800 - acc: 0.5618 - val_loss: 0.7259 - val_acc: 0.5385\n",
            "Epoch 51/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6798 - acc: 0.5618 - val_loss: 0.7260 - val_acc: 0.5385\n",
            "Epoch 52/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6797 - acc: 0.5618 - val_loss: 0.7261 - val_acc: 0.5385\n",
            "Epoch 53/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6795 - acc: 0.5618 - val_loss: 0.7261 - val_acc: 0.5385\n",
            "Epoch 54/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6794 - acc: 0.5618 - val_loss: 0.7262 - val_acc: 0.5385\n",
            "Epoch 55/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6792 - acc: 0.5618 - val_loss: 0.7263 - val_acc: 0.5385\n",
            "Epoch 56/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6791 - acc: 0.5618 - val_loss: 0.7264 - val_acc: 0.5385\n",
            "Epoch 57/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6790 - acc: 0.5618 - val_loss: 0.7265 - val_acc: 0.5385\n",
            "Epoch 58/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6788 - acc: 0.5618 - val_loss: 0.7265 - val_acc: 0.5385\n",
            "Epoch 59/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6787 - acc: 0.5618 - val_loss: 0.7266 - val_acc: 0.5385\n",
            "Epoch 60/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6785 - acc: 0.5618 - val_loss: 0.7267 - val_acc: 0.5385\n",
            "Epoch 61/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6784 - acc: 0.5618 - val_loss: 0.7268 - val_acc: 0.5385\n",
            "Epoch 62/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6782 - acc: 0.5618 - val_loss: 0.7269 - val_acc: 0.5385\n",
            "Epoch 63/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6781 - acc: 0.5618 - val_loss: 0.7270 - val_acc: 0.5385\n",
            "Epoch 64/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6779 - acc: 0.5618 - val_loss: 0.7270 - val_acc: 0.5385\n",
            "Epoch 65/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6778 - acc: 0.5618 - val_loss: 0.7271 - val_acc: 0.5385\n",
            "Epoch 66/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6777 - acc: 0.5618 - val_loss: 0.7272 - val_acc: 0.5385\n",
            "Epoch 67/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6775 - acc: 0.5618 - val_loss: 0.7273 - val_acc: 0.5385\n",
            "Epoch 68/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6774 - acc: 0.5618 - val_loss: 0.7273 - val_acc: 0.5385\n",
            "Epoch 69/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6772 - acc: 0.5618 - val_loss: 0.7274 - val_acc: 0.5385\n",
            "Epoch 70/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6771 - acc: 0.5618 - val_loss: 0.7275 - val_acc: 0.5385\n",
            "Epoch 71/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6769 - acc: 0.5618 - val_loss: 0.7275 - val_acc: 0.5385\n",
            "Epoch 72/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6768 - acc: 0.5393 - val_loss: 0.7276 - val_acc: 0.5385\n",
            "Epoch 73/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6767 - acc: 0.5393 - val_loss: 0.7277 - val_acc: 0.5385\n",
            "Epoch 74/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6765 - acc: 0.5393 - val_loss: 0.7277 - val_acc: 0.5385\n",
            "Epoch 75/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6764 - acc: 0.5281 - val_loss: 0.7278 - val_acc: 0.5385\n",
            "Epoch 76/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6763 - acc: 0.5281 - val_loss: 0.7279 - val_acc: 0.5385\n",
            "Epoch 77/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6761 - acc: 0.5281 - val_loss: 0.7279 - val_acc: 0.5385\n",
            "Epoch 78/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6760 - acc: 0.5281 - val_loss: 0.7280 - val_acc: 0.5385\n",
            "Epoch 79/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6758 - acc: 0.5281 - val_loss: 0.7280 - val_acc: 0.5385\n",
            "Epoch 80/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6757 - acc: 0.5281 - val_loss: 0.7281 - val_acc: 0.5385\n",
            "Epoch 81/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6756 - acc: 0.5281 - val_loss: 0.7282 - val_acc: 0.5385\n",
            "Epoch 82/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6754 - acc: 0.5169 - val_loss: 0.7282 - val_acc: 0.5385\n",
            "Epoch 83/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6753 - acc: 0.5169 - val_loss: 0.7283 - val_acc: 0.5385\n",
            "Epoch 84/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6752 - acc: 0.5169 - val_loss: 0.7283 - val_acc: 0.5385\n",
            "Epoch 85/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6750 - acc: 0.5169 - val_loss: 0.7284 - val_acc: 0.5385\n",
            "Epoch 86/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6749 - acc: 0.5169 - val_loss: 0.7284 - val_acc: 0.5385\n",
            "Epoch 87/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6747 - acc: 0.5169 - val_loss: 0.7285 - val_acc: 0.5385\n",
            "Epoch 88/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.6746 - acc: 0.5169 - val_loss: 0.7285 - val_acc: 0.5385\n",
            "Epoch 89/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.6745 - acc: 0.5169 - val_loss: 0.7286 - val_acc: 0.5385\n",
            "Epoch 90/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.6743 - acc: 0.5169 - val_loss: 0.7286 - val_acc: 0.5385\n",
            "Epoch 91/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.6742 - acc: 0.5169 - val_loss: 0.7287 - val_acc: 0.5385\n",
            "Epoch 92/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.6740 - acc: 0.5169 - val_loss: 0.7287 - val_acc: 0.5385\n",
            "Epoch 93/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.6739 - acc: 0.5169 - val_loss: 0.7288 - val_acc: 0.5385\n",
            "Epoch 94/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6738 - acc: 0.5169 - val_loss: 0.7288 - val_acc: 0.5385\n",
            "Epoch 95/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.6736 - acc: 0.5169 - val_loss: 0.7289 - val_acc: 0.5385\n",
            "Epoch 96/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.6735 - acc: 0.5169 - val_loss: 0.7289 - val_acc: 0.5385\n",
            "Epoch 97/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.6734 - acc: 0.5169 - val_loss: 0.7290 - val_acc: 0.5385\n",
            "Epoch 98/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.6732 - acc: 0.5169 - val_loss: 0.7290 - val_acc: 0.5385\n",
            "Epoch 99/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6731 - acc: 0.5169 - val_loss: 0.7291 - val_acc: 0.5385\n",
            "Epoch 100/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6729 - acc: 0.5169 - val_loss: 0.7291 - val_acc: 0.5385\n",
            "Epoch 101/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6728 - acc: 0.5169 - val_loss: 0.7292 - val_acc: 0.5385\n",
            "Epoch 102/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.5169 - val_loss: 0.7292 - val_acc: 0.5385\n",
            "Epoch 103/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6725 - acc: 0.5169 - val_loss: 0.7293 - val_acc: 0.5385\n",
            "Epoch 104/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6724 - acc: 0.5169 - val_loss: 0.7293 - val_acc: 0.5385\n",
            "Epoch 105/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6723 - acc: 0.5169 - val_loss: 0.7294 - val_acc: 0.5385\n",
            "Epoch 106/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6721 - acc: 0.5169 - val_loss: 0.7294 - val_acc: 0.5385\n",
            "Epoch 107/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6720 - acc: 0.5169 - val_loss: 0.7295 - val_acc: 0.5385\n",
            "Epoch 108/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6719 - acc: 0.5169 - val_loss: 0.7295 - val_acc: 0.5385\n",
            "Epoch 109/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6717 - acc: 0.5169 - val_loss: 0.7296 - val_acc: 0.5385\n",
            "Epoch 110/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6716 - acc: 0.5169 - val_loss: 0.7296 - val_acc: 0.5385\n",
            "Epoch 111/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6715 - acc: 0.5169 - val_loss: 0.7296 - val_acc: 0.5385\n",
            "Epoch 112/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6713 - acc: 0.5169 - val_loss: 0.7297 - val_acc: 0.5385\n",
            "Epoch 113/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6712 - acc: 0.5169 - val_loss: 0.7297 - val_acc: 0.5385\n",
            "Epoch 114/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6711 - acc: 0.5169 - val_loss: 0.7297 - val_acc: 0.5385\n",
            "Epoch 115/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6709 - acc: 0.5169 - val_loss: 0.7298 - val_acc: 0.5385\n",
            "Epoch 116/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6708 - acc: 0.5169 - val_loss: 0.7298 - val_acc: 0.5385\n",
            "Epoch 117/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6707 - acc: 0.5169 - val_loss: 0.7299 - val_acc: 0.5385\n",
            "Epoch 118/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6705 - acc: 0.5169 - val_loss: 0.7299 - val_acc: 0.5385\n",
            "Epoch 119/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6704 - acc: 0.5169 - val_loss: 0.7299 - val_acc: 0.5385\n",
            "Epoch 120/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6703 - acc: 0.5169 - val_loss: 0.7300 - val_acc: 0.5385\n",
            "Epoch 121/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6701 - acc: 0.5169 - val_loss: 0.7300 - val_acc: 0.5385\n",
            "Epoch 122/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6700 - acc: 0.5169 - val_loss: 0.7300 - val_acc: 0.5385\n",
            "Epoch 123/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6699 - acc: 0.5169 - val_loss: 0.7301 - val_acc: 0.5385\n",
            "Epoch 124/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6697 - acc: 0.5169 - val_loss: 0.7301 - val_acc: 0.5385\n",
            "Epoch 125/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6696 - acc: 0.5169 - val_loss: 0.7301 - val_acc: 0.5385\n",
            "Epoch 126/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6695 - acc: 0.5169 - val_loss: 0.7302 - val_acc: 0.5385\n",
            "Epoch 127/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6693 - acc: 0.5169 - val_loss: 0.7302 - val_acc: 0.5385\n",
            "Epoch 128/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6692 - acc: 0.5169 - val_loss: 0.7302 - val_acc: 0.5385\n",
            "Epoch 129/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6691 - acc: 0.5169 - val_loss: 0.7303 - val_acc: 0.5385\n",
            "Epoch 130/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6690 - acc: 0.5169 - val_loss: 0.7303 - val_acc: 0.5385\n",
            "Epoch 131/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6688 - acc: 0.5169 - val_loss: 0.7304 - val_acc: 0.5385\n",
            "Epoch 132/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.6687 - acc: 0.5169 - val_loss: 0.7304 - val_acc: 0.5385\n",
            "Epoch 133/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6686 - acc: 0.5169 - val_loss: 0.7304 - val_acc: 0.5385\n",
            "Epoch 134/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6684 - acc: 0.5169 - val_loss: 0.7305 - val_acc: 0.5385\n",
            "Epoch 135/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6683 - acc: 0.5169 - val_loss: 0.7305 - val_acc: 0.5385\n",
            "Epoch 136/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6682 - acc: 0.5169 - val_loss: 0.7305 - val_acc: 0.5385\n",
            "Epoch 137/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6680 - acc: 0.5169 - val_loss: 0.7306 - val_acc: 0.5385\n",
            "Epoch 138/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6679 - acc: 0.5169 - val_loss: 0.7306 - val_acc: 0.5385\n",
            "Epoch 139/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6678 - acc: 0.5169 - val_loss: 0.7306 - val_acc: 0.5385\n",
            "Epoch 140/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6677 - acc: 0.5169 - val_loss: 0.7307 - val_acc: 0.5385\n",
            "Epoch 141/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6675 - acc: 0.5169 - val_loss: 0.7307 - val_acc: 0.5385\n",
            "Epoch 142/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6674 - acc: 0.5169 - val_loss: 0.7307 - val_acc: 0.5385\n",
            "Epoch 143/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6673 - acc: 0.5169 - val_loss: 0.7308 - val_acc: 0.5385\n",
            "Epoch 144/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6671 - acc: 0.5169 - val_loss: 0.7308 - val_acc: 0.5385\n",
            "Epoch 145/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6670 - acc: 0.5169 - val_loss: 0.7308 - val_acc: 0.5385\n",
            "Epoch 146/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6669 - acc: 0.5169 - val_loss: 0.7309 - val_acc: 0.5385\n",
            "Epoch 147/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6667 - acc: 0.5169 - val_loss: 0.7309 - val_acc: 0.5385\n",
            "Epoch 148/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6666 - acc: 0.5169 - val_loss: 0.7309 - val_acc: 0.5385\n",
            "Epoch 149/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6665 - acc: 0.5169 - val_loss: 0.7310 - val_acc: 0.5385\n",
            "Epoch 150/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6664 - acc: 0.5169 - val_loss: 0.7310 - val_acc: 0.5385\n",
            "Epoch 151/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6662 - acc: 0.5169 - val_loss: 0.7311 - val_acc: 0.5385\n",
            "Epoch 152/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6661 - acc: 0.5169 - val_loss: 0.7311 - val_acc: 0.5385\n",
            "Epoch 153/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6660 - acc: 0.5169 - val_loss: 0.7311 - val_acc: 0.5385\n",
            "Epoch 154/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6658 - acc: 0.5169 - val_loss: 0.7312 - val_acc: 0.5385\n",
            "Epoch 155/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6657 - acc: 0.5169 - val_loss: 0.7312 - val_acc: 0.5385\n",
            "Epoch 156/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6656 - acc: 0.5169 - val_loss: 0.7313 - val_acc: 0.5385\n",
            "Epoch 157/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6654 - acc: 0.5169 - val_loss: 0.7313 - val_acc: 0.5385\n",
            "Epoch 158/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.6653 - acc: 0.5169 - val_loss: 0.7313 - val_acc: 0.5385\n",
            "Epoch 159/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6652 - acc: 0.5169 - val_loss: 0.7314 - val_acc: 0.5385\n",
            "Epoch 160/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6651 - acc: 0.5169 - val_loss: 0.7314 - val_acc: 0.5385\n",
            "Epoch 161/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6649 - acc: 0.5169 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 162/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6648 - acc: 0.5169 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 163/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6647 - acc: 0.5169 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 164/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6645 - acc: 0.5169 - val_loss: 0.7316 - val_acc: 0.5385\n",
            "Epoch 165/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6644 - acc: 0.5169 - val_loss: 0.7316 - val_acc: 0.5385\n",
            "Epoch 166/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6643 - acc: 0.5169 - val_loss: 0.7316 - val_acc: 0.5385\n",
            "Epoch 167/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6642 - acc: 0.5169 - val_loss: 0.7317 - val_acc: 0.5385\n",
            "Epoch 168/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6640 - acc: 0.5169 - val_loss: 0.7317 - val_acc: 0.5385\n",
            "Epoch 169/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6639 - acc: 0.5169 - val_loss: 0.7318 - val_acc: 0.5385\n",
            "Epoch 170/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6638 - acc: 0.5281 - val_loss: 0.7318 - val_acc: 0.5385\n",
            "Epoch 171/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6636 - acc: 0.5281 - val_loss: 0.7319 - val_acc: 0.5385\n",
            "Epoch 172/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6635 - acc: 0.5281 - val_loss: 0.7319 - val_acc: 0.5385\n",
            "Epoch 173/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6634 - acc: 0.5281 - val_loss: 0.7319 - val_acc: 0.5385\n",
            "Epoch 174/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6633 - acc: 0.5281 - val_loss: 0.7320 - val_acc: 0.5385\n",
            "Epoch 175/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6631 - acc: 0.5281 - val_loss: 0.7320 - val_acc: 0.5385\n",
            "Epoch 176/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6630 - acc: 0.5281 - val_loss: 0.7321 - val_acc: 0.5385\n",
            "Epoch 177/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6629 - acc: 0.5281 - val_loss: 0.7321 - val_acc: 0.5385\n",
            "Epoch 178/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6627 - acc: 0.5281 - val_loss: 0.7322 - val_acc: 0.5385\n",
            "Epoch 179/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6626 - acc: 0.5281 - val_loss: 0.7322 - val_acc: 0.5385\n",
            "Epoch 180/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6625 - acc: 0.5169 - val_loss: 0.7322 - val_acc: 0.5385\n",
            "Epoch 181/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6624 - acc: 0.5169 - val_loss: 0.7323 - val_acc: 0.5385\n",
            "Epoch 182/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6622 - acc: 0.5169 - val_loss: 0.7323 - val_acc: 0.5385\n",
            "Epoch 183/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6621 - acc: 0.5169 - val_loss: 0.7324 - val_acc: 0.5385\n",
            "Epoch 184/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6620 - acc: 0.5169 - val_loss: 0.7324 - val_acc: 0.5385\n",
            "Epoch 185/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6618 - acc: 0.5169 - val_loss: 0.7325 - val_acc: 0.5385\n",
            "Epoch 186/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6617 - acc: 0.5169 - val_loss: 0.7325 - val_acc: 0.5385\n",
            "Epoch 187/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6616 - acc: 0.5169 - val_loss: 0.7326 - val_acc: 0.5385\n",
            "Epoch 188/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6615 - acc: 0.5169 - val_loss: 0.7326 - val_acc: 0.5385\n",
            "Epoch 189/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6613 - acc: 0.5169 - val_loss: 0.7327 - val_acc: 0.5385\n",
            "Epoch 190/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6612 - acc: 0.5169 - val_loss: 0.7327 - val_acc: 0.5385\n",
            "Epoch 191/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6611 - acc: 0.5169 - val_loss: 0.7327 - val_acc: 0.5385\n",
            "Epoch 192/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6609 - acc: 0.5281 - val_loss: 0.7328 - val_acc: 0.5385\n",
            "Epoch 193/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6608 - acc: 0.5281 - val_loss: 0.7328 - val_acc: 0.5385\n",
            "Epoch 194/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6606 - acc: 0.5281 - val_loss: 0.7329 - val_acc: 0.5385\n",
            "Epoch 195/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6605 - acc: 0.5281 - val_loss: 0.7329 - val_acc: 0.5385\n",
            "Epoch 196/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6604 - acc: 0.5281 - val_loss: 0.7330 - val_acc: 0.5385\n",
            "Epoch 197/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6602 - acc: 0.5281 - val_loss: 0.7330 - val_acc: 0.5385\n",
            "Epoch 198/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6601 - acc: 0.5281 - val_loss: 0.7331 - val_acc: 0.5385\n",
            "Epoch 199/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6600 - acc: 0.5281 - val_loss: 0.7331 - val_acc: 0.5385\n",
            "Epoch 200/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6598 - acc: 0.5281 - val_loss: 0.7332 - val_acc: 0.5385\n",
            "Epoch 201/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6597 - acc: 0.5281 - val_loss: 0.7332 - val_acc: 0.5385\n",
            "Epoch 202/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6596 - acc: 0.5281 - val_loss: 0.7333 - val_acc: 0.5385\n",
            "Epoch 203/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6594 - acc: 0.5281 - val_loss: 0.7333 - val_acc: 0.5385\n",
            "Epoch 204/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6593 - acc: 0.5281 - val_loss: 0.7334 - val_acc: 0.5385\n",
            "Epoch 205/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6592 - acc: 0.5281 - val_loss: 0.7334 - val_acc: 0.5385\n",
            "Epoch 206/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6590 - acc: 0.5281 - val_loss: 0.7335 - val_acc: 0.5385\n",
            "Epoch 207/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6589 - acc: 0.5281 - val_loss: 0.7335 - val_acc: 0.5385\n",
            "Epoch 208/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6587 - acc: 0.5281 - val_loss: 0.7336 - val_acc: 0.5385\n",
            "Epoch 209/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6586 - acc: 0.5393 - val_loss: 0.7336 - val_acc: 0.5385\n",
            "Epoch 210/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6585 - acc: 0.5393 - val_loss: 0.7337 - val_acc: 0.5385\n",
            "Epoch 211/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.6583 - acc: 0.5393 - val_loss: 0.7337 - val_acc: 0.5385\n",
            "Epoch 212/1000\n",
            "89/89 [==============================] - 0s 152us/step - loss: 0.6582 - acc: 0.5393 - val_loss: 0.7338 - val_acc: 0.5385\n",
            "Epoch 213/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6581 - acc: 0.5393 - val_loss: 0.7338 - val_acc: 0.5385\n",
            "Epoch 214/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6579 - acc: 0.5506 - val_loss: 0.7339 - val_acc: 0.5385\n",
            "Epoch 215/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6578 - acc: 0.5506 - val_loss: 0.7339 - val_acc: 0.5385\n",
            "Epoch 216/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6577 - acc: 0.5506 - val_loss: 0.7340 - val_acc: 0.5385\n",
            "Epoch 217/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.6575 - acc: 0.5506 - val_loss: 0.7340 - val_acc: 0.5385\n",
            "Epoch 218/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6574 - acc: 0.5506 - val_loss: 0.7340 - val_acc: 0.5385\n",
            "Epoch 219/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6573 - acc: 0.5506 - val_loss: 0.7341 - val_acc: 0.5385\n",
            "Epoch 220/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6571 - acc: 0.5506 - val_loss: 0.7341 - val_acc: 0.5385\n",
            "Epoch 221/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6570 - acc: 0.5506 - val_loss: 0.7342 - val_acc: 0.5385\n",
            "Epoch 222/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6568 - acc: 0.5506 - val_loss: 0.7342 - val_acc: 0.5385\n",
            "Epoch 223/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6567 - acc: 0.5506 - val_loss: 0.7343 - val_acc: 0.5385\n",
            "Epoch 224/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6566 - acc: 0.5506 - val_loss: 0.7343 - val_acc: 0.5385\n",
            "Epoch 225/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6564 - acc: 0.5506 - val_loss: 0.7344 - val_acc: 0.5385\n",
            "Epoch 226/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6563 - acc: 0.5506 - val_loss: 0.7344 - val_acc: 0.5385\n",
            "Epoch 227/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6562 - acc: 0.5506 - val_loss: 0.7345 - val_acc: 0.5385\n",
            "Epoch 228/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.6560 - acc: 0.5506 - val_loss: 0.7345 - val_acc: 0.5385\n",
            "Epoch 229/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6559 - acc: 0.5506 - val_loss: 0.7346 - val_acc: 0.5385\n",
            "Epoch 230/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6558 - acc: 0.5506 - val_loss: 0.7346 - val_acc: 0.5385\n",
            "Epoch 231/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6556 - acc: 0.5506 - val_loss: 0.7347 - val_acc: 0.5385\n",
            "Epoch 232/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6555 - acc: 0.5506 - val_loss: 0.7347 - val_acc: 0.5385\n",
            "Epoch 233/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6554 - acc: 0.5506 - val_loss: 0.7348 - val_acc: 0.4615\n",
            "Epoch 234/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6552 - acc: 0.5506 - val_loss: 0.7348 - val_acc: 0.4615\n",
            "Epoch 235/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6551 - acc: 0.5506 - val_loss: 0.7348 - val_acc: 0.4615\n",
            "Epoch 236/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6549 - acc: 0.5506 - val_loss: 0.7349 - val_acc: 0.4615\n",
            "Epoch 237/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6548 - acc: 0.5506 - val_loss: 0.7349 - val_acc: 0.4615\n",
            "Epoch 238/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.6547 - acc: 0.5506 - val_loss: 0.7350 - val_acc: 0.4615\n",
            "Epoch 239/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.6545 - acc: 0.5506 - val_loss: 0.7350 - val_acc: 0.4615\n",
            "Epoch 240/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.6544 - acc: 0.5506 - val_loss: 0.7350 - val_acc: 0.4615\n",
            "Epoch 241/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.6543 - acc: 0.5506 - val_loss: 0.7351 - val_acc: 0.4615\n",
            "Epoch 242/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.6541 - acc: 0.5506 - val_loss: 0.7351 - val_acc: 0.4615\n",
            "Epoch 243/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6540 - acc: 0.5506 - val_loss: 0.7351 - val_acc: 0.4615\n",
            "Epoch 244/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6538 - acc: 0.5506 - val_loss: 0.7352 - val_acc: 0.4615\n",
            "Epoch 245/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6537 - acc: 0.5506 - val_loss: 0.7352 - val_acc: 0.4615\n",
            "Epoch 246/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6535 - acc: 0.5506 - val_loss: 0.7352 - val_acc: 0.4615\n",
            "Epoch 247/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6534 - acc: 0.5506 - val_loss: 0.7353 - val_acc: 0.4615\n",
            "Epoch 248/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6532 - acc: 0.5618 - val_loss: 0.7353 - val_acc: 0.4615\n",
            "Epoch 249/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.6531 - acc: 0.5618 - val_loss: 0.7354 - val_acc: 0.4615\n",
            "Epoch 250/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.6530 - acc: 0.5618 - val_loss: 0.7354 - val_acc: 0.4615\n",
            "Epoch 251/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.6528 - acc: 0.5618 - val_loss: 0.7354 - val_acc: 0.4615\n",
            "Epoch 252/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.6527 - acc: 0.5618 - val_loss: 0.7354 - val_acc: 0.4615\n",
            "Epoch 253/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.6525 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.4615\n",
            "Epoch 254/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.6524 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.4615\n",
            "Epoch 255/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.6522 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.4615\n",
            "Epoch 256/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.6521 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.4615\n",
            "Epoch 257/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.6519 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.4615\n",
            "Epoch 258/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.6518 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.4615\n",
            "Epoch 259/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.6516 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 260/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6515 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 261/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6513 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 262/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6512 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 263/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.6510 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 264/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6509 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 265/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6507 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 266/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.6506 - acc: 0.5618 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 267/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6504 - acc: 0.5506 - val_loss: 0.7355 - val_acc: 0.5385\n",
            "Epoch 268/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.6503 - acc: 0.5506 - val_loss: 0.7354 - val_acc: 0.5385\n",
            "Epoch 269/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.6501 - acc: 0.5506 - val_loss: 0.7354 - val_acc: 0.5385\n",
            "Epoch 270/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6500 - acc: 0.5506 - val_loss: 0.7354 - val_acc: 0.5385\n",
            "Epoch 271/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6499 - acc: 0.5506 - val_loss: 0.7354 - val_acc: 0.5385\n",
            "Epoch 272/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.6497 - acc: 0.5506 - val_loss: 0.7354 - val_acc: 0.5385\n",
            "Epoch 273/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6496 - acc: 0.5506 - val_loss: 0.7354 - val_acc: 0.5385\n",
            "Epoch 274/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6494 - acc: 0.5618 - val_loss: 0.7353 - val_acc: 0.5385\n",
            "Epoch 275/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6493 - acc: 0.5618 - val_loss: 0.7353 - val_acc: 0.5385\n",
            "Epoch 276/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6491 - acc: 0.5730 - val_loss: 0.7353 - val_acc: 0.5385\n",
            "Epoch 277/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6490 - acc: 0.5730 - val_loss: 0.7353 - val_acc: 0.5385\n",
            "Epoch 278/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6488 - acc: 0.5730 - val_loss: 0.7352 - val_acc: 0.5385\n",
            "Epoch 279/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6487 - acc: 0.5730 - val_loss: 0.7352 - val_acc: 0.5385\n",
            "Epoch 280/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6485 - acc: 0.5730 - val_loss: 0.7352 - val_acc: 0.5385\n",
            "Epoch 281/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6484 - acc: 0.5730 - val_loss: 0.7352 - val_acc: 0.5385\n",
            "Epoch 282/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6482 - acc: 0.5730 - val_loss: 0.7351 - val_acc: 0.5385\n",
            "Epoch 283/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6481 - acc: 0.5843 - val_loss: 0.7351 - val_acc: 0.5385\n",
            "Epoch 284/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6479 - acc: 0.5843 - val_loss: 0.7351 - val_acc: 0.5385\n",
            "Epoch 285/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.6478 - acc: 0.5843 - val_loss: 0.7350 - val_acc: 0.5385\n",
            "Epoch 286/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.6476 - acc: 0.5843 - val_loss: 0.7350 - val_acc: 0.5385\n",
            "Epoch 287/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6475 - acc: 0.5955 - val_loss: 0.7349 - val_acc: 0.5385\n",
            "Epoch 288/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6473 - acc: 0.5955 - val_loss: 0.7349 - val_acc: 0.5385\n",
            "Epoch 289/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6472 - acc: 0.5955 - val_loss: 0.7349 - val_acc: 0.5385\n",
            "Epoch 290/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6470 - acc: 0.5955 - val_loss: 0.7348 - val_acc: 0.5385\n",
            "Epoch 291/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.6469 - acc: 0.5955 - val_loss: 0.7348 - val_acc: 0.5385\n",
            "Epoch 292/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6467 - acc: 0.5955 - val_loss: 0.7347 - val_acc: 0.5385\n",
            "Epoch 293/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6466 - acc: 0.5955 - val_loss: 0.7347 - val_acc: 0.5385\n",
            "Epoch 294/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.6464 - acc: 0.5955 - val_loss: 0.7347 - val_acc: 0.5385\n",
            "Epoch 295/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6463 - acc: 0.5955 - val_loss: 0.7346 - val_acc: 0.5385\n",
            "Epoch 296/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6461 - acc: 0.5955 - val_loss: 0.7346 - val_acc: 0.5385\n",
            "Epoch 297/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.6460 - acc: 0.5955 - val_loss: 0.7345 - val_acc: 0.5385\n",
            "Epoch 298/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.6458 - acc: 0.5955 - val_loss: 0.7345 - val_acc: 0.5385\n",
            "Epoch 299/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6457 - acc: 0.5955 - val_loss: 0.7345 - val_acc: 0.5385\n",
            "Epoch 300/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.6455 - acc: 0.5955 - val_loss: 0.7344 - val_acc: 0.5385\n",
            "Epoch 301/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6454 - acc: 0.5955 - val_loss: 0.7344 - val_acc: 0.5385\n",
            "Epoch 302/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6452 - acc: 0.5955 - val_loss: 0.7343 - val_acc: 0.5385\n",
            "Epoch 303/1000\n",
            "89/89 [==============================] - 0s 87us/step - loss: 0.6451 - acc: 0.5955 - val_loss: 0.7343 - val_acc: 0.5385\n",
            "Epoch 304/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.6449 - acc: 0.5955 - val_loss: 0.7342 - val_acc: 0.5385\n",
            "Epoch 305/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.6448 - acc: 0.5955 - val_loss: 0.7342 - val_acc: 0.5385\n",
            "Epoch 306/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.6446 - acc: 0.5955 - val_loss: 0.7342 - val_acc: 0.5385\n",
            "Epoch 307/1000\n",
            "89/89 [==============================] - 0s 92us/step - loss: 0.6445 - acc: 0.5955 - val_loss: 0.7341 - val_acc: 0.5385\n",
            "Epoch 308/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6443 - acc: 0.5955 - val_loss: 0.7341 - val_acc: 0.5385\n",
            "Epoch 309/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6442 - acc: 0.5955 - val_loss: 0.7340 - val_acc: 0.5385\n",
            "Epoch 310/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6440 - acc: 0.5955 - val_loss: 0.7340 - val_acc: 0.5385\n",
            "Epoch 311/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6439 - acc: 0.5955 - val_loss: 0.7339 - val_acc: 0.5385\n",
            "Epoch 312/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6437 - acc: 0.5955 - val_loss: 0.7339 - val_acc: 0.5385\n",
            "Epoch 313/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6436 - acc: 0.5955 - val_loss: 0.7339 - val_acc: 0.5385\n",
            "Epoch 314/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6434 - acc: 0.5955 - val_loss: 0.7338 - val_acc: 0.5385\n",
            "Epoch 315/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6433 - acc: 0.5955 - val_loss: 0.7338 - val_acc: 0.5385\n",
            "Epoch 316/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6431 - acc: 0.5955 - val_loss: 0.7337 - val_acc: 0.5385\n",
            "Epoch 317/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6429 - acc: 0.5955 - val_loss: 0.7337 - val_acc: 0.5385\n",
            "Epoch 318/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6428 - acc: 0.5955 - val_loss: 0.7336 - val_acc: 0.5385\n",
            "Epoch 319/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6426 - acc: 0.5955 - val_loss: 0.7336 - val_acc: 0.5385\n",
            "Epoch 320/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6425 - acc: 0.5955 - val_loss: 0.7335 - val_acc: 0.5385\n",
            "Epoch 321/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6423 - acc: 0.5955 - val_loss: 0.7335 - val_acc: 0.5385\n",
            "Epoch 322/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6422 - acc: 0.5955 - val_loss: 0.7334 - val_acc: 0.5385\n",
            "Epoch 323/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6420 - acc: 0.5955 - val_loss: 0.7333 - val_acc: 0.5385\n",
            "Epoch 324/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6419 - acc: 0.5955 - val_loss: 0.7333 - val_acc: 0.5385\n",
            "Epoch 325/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6417 - acc: 0.5955 - val_loss: 0.7332 - val_acc: 0.5385\n",
            "Epoch 326/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6415 - acc: 0.5955 - val_loss: 0.7332 - val_acc: 0.5385\n",
            "Epoch 327/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6414 - acc: 0.5955 - val_loss: 0.7331 - val_acc: 0.5385\n",
            "Epoch 328/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6412 - acc: 0.5955 - val_loss: 0.7331 - val_acc: 0.5385\n",
            "Epoch 329/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6411 - acc: 0.5955 - val_loss: 0.7330 - val_acc: 0.5385\n",
            "Epoch 330/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6409 - acc: 0.5955 - val_loss: 0.7330 - val_acc: 0.5385\n",
            "Epoch 331/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6408 - acc: 0.5955 - val_loss: 0.7329 - val_acc: 0.5385\n",
            "Epoch 332/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6406 - acc: 0.5955 - val_loss: 0.7328 - val_acc: 0.5385\n",
            "Epoch 333/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6404 - acc: 0.5955 - val_loss: 0.7328 - val_acc: 0.5385\n",
            "Epoch 334/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6403 - acc: 0.5955 - val_loss: 0.7327 - val_acc: 0.5385\n",
            "Epoch 335/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6401 - acc: 0.5955 - val_loss: 0.7327 - val_acc: 0.5385\n",
            "Epoch 336/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6400 - acc: 0.5955 - val_loss: 0.7326 - val_acc: 0.5385\n",
            "Epoch 337/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6398 - acc: 0.5955 - val_loss: 0.7326 - val_acc: 0.5385\n",
            "Epoch 338/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6397 - acc: 0.5955 - val_loss: 0.7325 - val_acc: 0.5385\n",
            "Epoch 339/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6395 - acc: 0.5955 - val_loss: 0.7324 - val_acc: 0.5385\n",
            "Epoch 340/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6393 - acc: 0.6067 - val_loss: 0.7324 - val_acc: 0.5385\n",
            "Epoch 341/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6392 - acc: 0.6067 - val_loss: 0.7323 - val_acc: 0.5385\n",
            "Epoch 342/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6390 - acc: 0.6067 - val_loss: 0.7322 - val_acc: 0.5385\n",
            "Epoch 343/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6389 - acc: 0.6067 - val_loss: 0.7322 - val_acc: 0.5385\n",
            "Epoch 344/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6387 - acc: 0.6067 - val_loss: 0.7321 - val_acc: 0.5385\n",
            "Epoch 345/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6385 - acc: 0.6067 - val_loss: 0.7321 - val_acc: 0.5385\n",
            "Epoch 346/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6384 - acc: 0.6067 - val_loss: 0.7320 - val_acc: 0.5385\n",
            "Epoch 347/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6382 - acc: 0.6067 - val_loss: 0.7319 - val_acc: 0.5385\n",
            "Epoch 348/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6381 - acc: 0.6067 - val_loss: 0.7319 - val_acc: 0.5385\n",
            "Epoch 349/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6379 - acc: 0.6067 - val_loss: 0.7318 - val_acc: 0.5385\n",
            "Epoch 350/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6378 - acc: 0.6067 - val_loss: 0.7318 - val_acc: 0.5385\n",
            "Epoch 351/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6376 - acc: 0.6067 - val_loss: 0.7317 - val_acc: 0.5385\n",
            "Epoch 352/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6374 - acc: 0.6067 - val_loss: 0.7316 - val_acc: 0.5385\n",
            "Epoch 353/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6373 - acc: 0.6067 - val_loss: 0.7316 - val_acc: 0.5385\n",
            "Epoch 354/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6371 - acc: 0.6067 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 355/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6370 - acc: 0.6067 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 356/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6368 - acc: 0.6067 - val_loss: 0.7314 - val_acc: 0.5385\n",
            "Epoch 357/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6366 - acc: 0.6067 - val_loss: 0.7314 - val_acc: 0.5385\n",
            "Epoch 358/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6365 - acc: 0.6067 - val_loss: 0.7313 - val_acc: 0.5385\n",
            "Epoch 359/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.6363 - acc: 0.6067 - val_loss: 0.7313 - val_acc: 0.5385\n",
            "Epoch 360/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6362 - acc: 0.6067 - val_loss: 0.7312 - val_acc: 0.5385\n",
            "Epoch 361/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6360 - acc: 0.6067 - val_loss: 0.7311 - val_acc: 0.5385\n",
            "Epoch 362/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.6359 - acc: 0.6067 - val_loss: 0.7311 - val_acc: 0.5385\n",
            "Epoch 363/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6357 - acc: 0.6067 - val_loss: 0.7310 - val_acc: 0.5385\n",
            "Epoch 364/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6355 - acc: 0.6067 - val_loss: 0.7310 - val_acc: 0.5385\n",
            "Epoch 365/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.6354 - acc: 0.6067 - val_loss: 0.7309 - val_acc: 0.5385\n",
            "Epoch 366/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6352 - acc: 0.6067 - val_loss: 0.7309 - val_acc: 0.5385\n",
            "Epoch 367/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6351 - acc: 0.6067 - val_loss: 0.7308 - val_acc: 0.5385\n",
            "Epoch 368/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6349 - acc: 0.6067 - val_loss: 0.7307 - val_acc: 0.5385\n",
            "Epoch 369/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6347 - acc: 0.6067 - val_loss: 0.7307 - val_acc: 0.5385\n",
            "Epoch 370/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6346 - acc: 0.6067 - val_loss: 0.7306 - val_acc: 0.5385\n",
            "Epoch 371/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6344 - acc: 0.6067 - val_loss: 0.7305 - val_acc: 0.5385\n",
            "Epoch 372/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6343 - acc: 0.6067 - val_loss: 0.7305 - val_acc: 0.5385\n",
            "Epoch 373/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6341 - acc: 0.6067 - val_loss: 0.7304 - val_acc: 0.5385\n",
            "Epoch 374/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.6339 - acc: 0.6067 - val_loss: 0.7304 - val_acc: 0.5385\n",
            "Epoch 375/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.6338 - acc: 0.6067 - val_loss: 0.7303 - val_acc: 0.5385\n",
            "Epoch 376/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.6336 - acc: 0.6180 - val_loss: 0.7302 - val_acc: 0.5385\n",
            "Epoch 377/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6335 - acc: 0.6180 - val_loss: 0.7302 - val_acc: 0.5385\n",
            "Epoch 378/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6333 - acc: 0.6180 - val_loss: 0.7301 - val_acc: 0.5385\n",
            "Epoch 379/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6332 - acc: 0.6180 - val_loss: 0.7301 - val_acc: 0.5385\n",
            "Epoch 380/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6330 - acc: 0.6180 - val_loss: 0.7300 - val_acc: 0.5385\n",
            "Epoch 381/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6328 - acc: 0.6180 - val_loss: 0.7299 - val_acc: 0.5385\n",
            "Epoch 382/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6327 - acc: 0.6180 - val_loss: 0.7299 - val_acc: 0.5385\n",
            "Epoch 383/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6325 - acc: 0.6180 - val_loss: 0.7298 - val_acc: 0.5385\n",
            "Epoch 384/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6324 - acc: 0.6180 - val_loss: 0.7297 - val_acc: 0.5385\n",
            "Epoch 385/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6322 - acc: 0.6180 - val_loss: 0.7297 - val_acc: 0.5385\n",
            "Epoch 386/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6321 - acc: 0.6180 - val_loss: 0.7296 - val_acc: 0.5385\n",
            "Epoch 387/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6319 - acc: 0.6180 - val_loss: 0.7295 - val_acc: 0.5385\n",
            "Epoch 388/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6317 - acc: 0.6180 - val_loss: 0.7295 - val_acc: 0.5385\n",
            "Epoch 389/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6316 - acc: 0.6180 - val_loss: 0.7294 - val_acc: 0.5385\n",
            "Epoch 390/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6314 - acc: 0.6180 - val_loss: 0.7293 - val_acc: 0.5385\n",
            "Epoch 391/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6313 - acc: 0.6180 - val_loss: 0.7293 - val_acc: 0.5385\n",
            "Epoch 392/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6311 - acc: 0.6180 - val_loss: 0.7292 - val_acc: 0.5385\n",
            "Epoch 393/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6310 - acc: 0.6180 - val_loss: 0.7291 - val_acc: 0.5385\n",
            "Epoch 394/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6308 - acc: 0.6180 - val_loss: 0.7291 - val_acc: 0.5385\n",
            "Epoch 395/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6306 - acc: 0.6180 - val_loss: 0.7290 - val_acc: 0.5385\n",
            "Epoch 396/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6305 - acc: 0.6180 - val_loss: 0.7289 - val_acc: 0.5385\n",
            "Epoch 397/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6303 - acc: 0.6180 - val_loss: 0.7288 - val_acc: 0.5385\n",
            "Epoch 398/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6302 - acc: 0.6180 - val_loss: 0.7288 - val_acc: 0.5385\n",
            "Epoch 399/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6300 - acc: 0.6180 - val_loss: 0.7287 - val_acc: 0.5385\n",
            "Epoch 400/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6298 - acc: 0.6180 - val_loss: 0.7286 - val_acc: 0.5385\n",
            "Epoch 401/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6297 - acc: 0.6180 - val_loss: 0.7286 - val_acc: 0.5385\n",
            "Epoch 402/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6295 - acc: 0.6180 - val_loss: 0.7285 - val_acc: 0.5385\n",
            "Epoch 403/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6294 - acc: 0.6180 - val_loss: 0.7284 - val_acc: 0.5385\n",
            "Epoch 404/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6292 - acc: 0.6292 - val_loss: 0.7283 - val_acc: 0.5385\n",
            "Epoch 405/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6290 - acc: 0.6292 - val_loss: 0.7283 - val_acc: 0.5385\n",
            "Epoch 406/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6289 - acc: 0.6292 - val_loss: 0.7282 - val_acc: 0.5385\n",
            "Epoch 407/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6287 - acc: 0.6292 - val_loss: 0.7281 - val_acc: 0.5385\n",
            "Epoch 408/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6286 - acc: 0.6292 - val_loss: 0.7280 - val_acc: 0.5385\n",
            "Epoch 409/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6284 - acc: 0.6292 - val_loss: 0.7280 - val_acc: 0.5385\n",
            "Epoch 410/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6283 - acc: 0.6292 - val_loss: 0.7279 - val_acc: 0.5385\n",
            "Epoch 411/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.6281 - acc: 0.6292 - val_loss: 0.7278 - val_acc: 0.5385\n",
            "Epoch 412/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6279 - acc: 0.6292 - val_loss: 0.7277 - val_acc: 0.5385\n",
            "Epoch 413/1000\n",
            "89/89 [==============================] - 0s 107us/step - loss: 0.6278 - acc: 0.6292 - val_loss: 0.7277 - val_acc: 0.5385\n",
            "Epoch 414/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6276 - acc: 0.6292 - val_loss: 0.7276 - val_acc: 0.5385\n",
            "Epoch 415/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6275 - acc: 0.6404 - val_loss: 0.7275 - val_acc: 0.5385\n",
            "Epoch 416/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6273 - acc: 0.6404 - val_loss: 0.7274 - val_acc: 0.5385\n",
            "Epoch 417/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6271 - acc: 0.6404 - val_loss: 0.7274 - val_acc: 0.5385\n",
            "Epoch 418/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6270 - acc: 0.6404 - val_loss: 0.7273 - val_acc: 0.5385\n",
            "Epoch 419/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6268 - acc: 0.6404 - val_loss: 0.7272 - val_acc: 0.5385\n",
            "Epoch 420/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6267 - acc: 0.6404 - val_loss: 0.7271 - val_acc: 0.5385\n",
            "Epoch 421/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6265 - acc: 0.6404 - val_loss: 0.7270 - val_acc: 0.5385\n",
            "Epoch 422/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6264 - acc: 0.6404 - val_loss: 0.7270 - val_acc: 0.5385\n",
            "Epoch 423/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6262 - acc: 0.6404 - val_loss: 0.7269 - val_acc: 0.5385\n",
            "Epoch 424/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6260 - acc: 0.6404 - val_loss: 0.7268 - val_acc: 0.5385\n",
            "Epoch 425/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6259 - acc: 0.6404 - val_loss: 0.7267 - val_acc: 0.5385\n",
            "Epoch 426/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6257 - acc: 0.6404 - val_loss: 0.7266 - val_acc: 0.5385\n",
            "Epoch 427/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6256 - acc: 0.6404 - val_loss: 0.7265 - val_acc: 0.5385\n",
            "Epoch 428/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6254 - acc: 0.6404 - val_loss: 0.7264 - val_acc: 0.5385\n",
            "Epoch 429/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6252 - acc: 0.6404 - val_loss: 0.7263 - val_acc: 0.5385\n",
            "Epoch 430/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6251 - acc: 0.6404 - val_loss: 0.7263 - val_acc: 0.5385\n",
            "Epoch 431/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6249 - acc: 0.6404 - val_loss: 0.7262 - val_acc: 0.5385\n",
            "Epoch 432/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6248 - acc: 0.6404 - val_loss: 0.7261 - val_acc: 0.5385\n",
            "Epoch 433/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6246 - acc: 0.6404 - val_loss: 0.7260 - val_acc: 0.5385\n",
            "Epoch 434/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6245 - acc: 0.6404 - val_loss: 0.7259 - val_acc: 0.5385\n",
            "Epoch 435/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6243 - acc: 0.6404 - val_loss: 0.7258 - val_acc: 0.5385\n",
            "Epoch 436/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6241 - acc: 0.6404 - val_loss: 0.7258 - val_acc: 0.5385\n",
            "Epoch 437/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6240 - acc: 0.6404 - val_loss: 0.7257 - val_acc: 0.5385\n",
            "Epoch 438/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.6238 - acc: 0.6404 - val_loss: 0.7256 - val_acc: 0.5385\n",
            "Epoch 439/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6237 - acc: 0.6404 - val_loss: 0.7255 - val_acc: 0.5385\n",
            "Epoch 440/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6235 - acc: 0.6404 - val_loss: 0.7254 - val_acc: 0.5385\n",
            "Epoch 441/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6234 - acc: 0.6404 - val_loss: 0.7253 - val_acc: 0.5385\n",
            "Epoch 442/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6232 - acc: 0.6404 - val_loss: 0.7253 - val_acc: 0.5385\n",
            "Epoch 443/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.6230 - acc: 0.6404 - val_loss: 0.7252 - val_acc: 0.5385\n",
            "Epoch 444/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6229 - acc: 0.6517 - val_loss: 0.7251 - val_acc: 0.5385\n",
            "Epoch 445/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.6227 - acc: 0.6517 - val_loss: 0.7250 - val_acc: 0.5385\n",
            "Epoch 446/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6226 - acc: 0.6517 - val_loss: 0.7249 - val_acc: 0.5385\n",
            "Epoch 447/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6224 - acc: 0.6517 - val_loss: 0.7249 - val_acc: 0.5385\n",
            "Epoch 448/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6223 - acc: 0.6517 - val_loss: 0.7248 - val_acc: 0.5385\n",
            "Epoch 449/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6221 - acc: 0.6517 - val_loss: 0.7247 - val_acc: 0.5385\n",
            "Epoch 450/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6219 - acc: 0.6517 - val_loss: 0.7246 - val_acc: 0.5385\n",
            "Epoch 451/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.6218 - acc: 0.6517 - val_loss: 0.7245 - val_acc: 0.5385\n",
            "Epoch 452/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6216 - acc: 0.6517 - val_loss: 0.7245 - val_acc: 0.5385\n",
            "Epoch 453/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6215 - acc: 0.6517 - val_loss: 0.7244 - val_acc: 0.5385\n",
            "Epoch 454/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6213 - acc: 0.6517 - val_loss: 0.7243 - val_acc: 0.5385\n",
            "Epoch 455/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6211 - acc: 0.6517 - val_loss: 0.7243 - val_acc: 0.5385\n",
            "Epoch 456/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6210 - acc: 0.6517 - val_loss: 0.7242 - val_acc: 0.5385\n",
            "Epoch 457/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6208 - acc: 0.6517 - val_loss: 0.7241 - val_acc: 0.5385\n",
            "Epoch 458/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6207 - acc: 0.6517 - val_loss: 0.7240 - val_acc: 0.5385\n",
            "Epoch 459/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6205 - acc: 0.6517 - val_loss: 0.7239 - val_acc: 0.5385\n",
            "Epoch 460/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6203 - acc: 0.6517 - val_loss: 0.7238 - val_acc: 0.5385\n",
            "Epoch 461/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6202 - acc: 0.6517 - val_loss: 0.7238 - val_acc: 0.5385\n",
            "Epoch 462/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6200 - acc: 0.6404 - val_loss: 0.7237 - val_acc: 0.6154\n",
            "Epoch 463/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6199 - acc: 0.6404 - val_loss: 0.7236 - val_acc: 0.6154\n",
            "Epoch 464/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6197 - acc: 0.6404 - val_loss: 0.7235 - val_acc: 0.6154\n",
            "Epoch 465/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6195 - acc: 0.6404 - val_loss: 0.7234 - val_acc: 0.6154\n",
            "Epoch 466/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6194 - acc: 0.6404 - val_loss: 0.7233 - val_acc: 0.6154\n",
            "Epoch 467/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6192 - acc: 0.6517 - val_loss: 0.7232 - val_acc: 0.6154\n",
            "Epoch 468/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6190 - acc: 0.6517 - val_loss: 0.7231 - val_acc: 0.6154\n",
            "Epoch 469/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6189 - acc: 0.6517 - val_loss: 0.7230 - val_acc: 0.6154\n",
            "Epoch 470/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.6187 - acc: 0.6517 - val_loss: 0.7230 - val_acc: 0.6154\n",
            "Epoch 471/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.6186 - acc: 0.6517 - val_loss: 0.7229 - val_acc: 0.6154\n",
            "Epoch 472/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6184 - acc: 0.6517 - val_loss: 0.7228 - val_acc: 0.6154\n",
            "Epoch 473/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6182 - acc: 0.6629 - val_loss: 0.7227 - val_acc: 0.6154\n",
            "Epoch 474/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6181 - acc: 0.6629 - val_loss: 0.7226 - val_acc: 0.6154\n",
            "Epoch 475/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6179 - acc: 0.6629 - val_loss: 0.7225 - val_acc: 0.6154\n",
            "Epoch 476/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6177 - acc: 0.6629 - val_loss: 0.7224 - val_acc: 0.6154\n",
            "Epoch 477/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6176 - acc: 0.6629 - val_loss: 0.7223 - val_acc: 0.6154\n",
            "Epoch 478/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6174 - acc: 0.6629 - val_loss: 0.7222 - val_acc: 0.6154\n",
            "Epoch 479/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6173 - acc: 0.6629 - val_loss: 0.7221 - val_acc: 0.6154\n",
            "Epoch 480/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6171 - acc: 0.6629 - val_loss: 0.7220 - val_acc: 0.6154\n",
            "Epoch 481/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6169 - acc: 0.6629 - val_loss: 0.7219 - val_acc: 0.6154\n",
            "Epoch 482/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6168 - acc: 0.6629 - val_loss: 0.7218 - val_acc: 0.6154\n",
            "Epoch 483/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6166 - acc: 0.6629 - val_loss: 0.7218 - val_acc: 0.6154\n",
            "Epoch 484/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6165 - acc: 0.6629 - val_loss: 0.7217 - val_acc: 0.6154\n",
            "Epoch 485/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6163 - acc: 0.6629 - val_loss: 0.7216 - val_acc: 0.6154\n",
            "Epoch 486/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6162 - acc: 0.6629 - val_loss: 0.7215 - val_acc: 0.6154\n",
            "Epoch 487/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6160 - acc: 0.6629 - val_loss: 0.7214 - val_acc: 0.6154\n",
            "Epoch 488/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6158 - acc: 0.6629 - val_loss: 0.7213 - val_acc: 0.6154\n",
            "Epoch 489/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6157 - acc: 0.6629 - val_loss: 0.7213 - val_acc: 0.6154\n",
            "Epoch 490/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6155 - acc: 0.6629 - val_loss: 0.7212 - val_acc: 0.6154\n",
            "Epoch 491/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6154 - acc: 0.6742 - val_loss: 0.7211 - val_acc: 0.6154\n",
            "Epoch 492/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6152 - acc: 0.6742 - val_loss: 0.7210 - val_acc: 0.6154\n",
            "Epoch 493/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6151 - acc: 0.6742 - val_loss: 0.7210 - val_acc: 0.6154\n",
            "Epoch 494/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.6149 - acc: 0.6742 - val_loss: 0.7209 - val_acc: 0.6154\n",
            "Epoch 495/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6148 - acc: 0.6742 - val_loss: 0.7208 - val_acc: 0.6154\n",
            "Epoch 496/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6146 - acc: 0.6629 - val_loss: 0.7207 - val_acc: 0.6154\n",
            "Epoch 497/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6144 - acc: 0.6629 - val_loss: 0.7207 - val_acc: 0.6154\n",
            "Epoch 498/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6143 - acc: 0.6629 - val_loss: 0.7206 - val_acc: 0.6154\n",
            "Epoch 499/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6141 - acc: 0.6629 - val_loss: 0.7205 - val_acc: 0.6154\n",
            "Epoch 500/1000\n",
            "89/89 [==============================] - 0s 149us/step - loss: 0.6140 - acc: 0.6629 - val_loss: 0.7205 - val_acc: 0.6154\n",
            "Epoch 501/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6138 - acc: 0.6629 - val_loss: 0.7204 - val_acc: 0.6154\n",
            "Epoch 502/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6137 - acc: 0.6629 - val_loss: 0.7203 - val_acc: 0.6154\n",
            "Epoch 503/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6135 - acc: 0.6629 - val_loss: 0.7203 - val_acc: 0.6154\n",
            "Epoch 504/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6133 - acc: 0.6629 - val_loss: 0.7202 - val_acc: 0.6154\n",
            "Epoch 505/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6132 - acc: 0.6629 - val_loss: 0.7201 - val_acc: 0.6154\n",
            "Epoch 506/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6130 - acc: 0.6629 - val_loss: 0.7200 - val_acc: 0.6154\n",
            "Epoch 507/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6129 - acc: 0.6629 - val_loss: 0.7200 - val_acc: 0.6154\n",
            "Epoch 508/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6127 - acc: 0.6629 - val_loss: 0.7199 - val_acc: 0.6154\n",
            "Epoch 509/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6126 - acc: 0.6629 - val_loss: 0.7198 - val_acc: 0.6154\n",
            "Epoch 510/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6124 - acc: 0.6742 - val_loss: 0.7197 - val_acc: 0.6154\n",
            "Epoch 511/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6122 - acc: 0.6742 - val_loss: 0.7197 - val_acc: 0.6154\n",
            "Epoch 512/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6121 - acc: 0.6742 - val_loss: 0.7196 - val_acc: 0.6154\n",
            "Epoch 513/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6119 - acc: 0.6742 - val_loss: 0.7195 - val_acc: 0.6154\n",
            "Epoch 514/1000\n",
            "89/89 [==============================] - 0s 113us/step - loss: 0.6118 - acc: 0.6742 - val_loss: 0.7194 - val_acc: 0.6154\n",
            "Epoch 515/1000\n",
            "89/89 [==============================] - 0s 145us/step - loss: 0.6116 - acc: 0.6742 - val_loss: 0.7194 - val_acc: 0.6154\n",
            "Epoch 516/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6114 - acc: 0.6742 - val_loss: 0.7193 - val_acc: 0.6154\n",
            "Epoch 517/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6113 - acc: 0.6742 - val_loss: 0.7192 - val_acc: 0.6154\n",
            "Epoch 518/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6111 - acc: 0.6742 - val_loss: 0.7191 - val_acc: 0.6154\n",
            "Epoch 519/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6110 - acc: 0.6742 - val_loss: 0.7190 - val_acc: 0.6154\n",
            "Epoch 520/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6108 - acc: 0.6742 - val_loss: 0.7190 - val_acc: 0.6154\n",
            "Epoch 521/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6107 - acc: 0.6854 - val_loss: 0.7189 - val_acc: 0.6154\n",
            "Epoch 522/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6105 - acc: 0.6854 - val_loss: 0.7188 - val_acc: 0.6154\n",
            "Epoch 523/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6103 - acc: 0.6854 - val_loss: 0.7187 - val_acc: 0.6154\n",
            "Epoch 524/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6102 - acc: 0.6742 - val_loss: 0.7187 - val_acc: 0.6154\n",
            "Epoch 525/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6100 - acc: 0.6742 - val_loss: 0.7186 - val_acc: 0.6154\n",
            "Epoch 526/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6099 - acc: 0.6742 - val_loss: 0.7185 - val_acc: 0.6154\n",
            "Epoch 527/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.6097 - acc: 0.6742 - val_loss: 0.7184 - val_acc: 0.6154\n",
            "Epoch 528/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6095 - acc: 0.6742 - val_loss: 0.7184 - val_acc: 0.6154\n",
            "Epoch 529/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6094 - acc: 0.6742 - val_loss: 0.7183 - val_acc: 0.6154\n",
            "Epoch 530/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6092 - acc: 0.6742 - val_loss: 0.7182 - val_acc: 0.6154\n",
            "Epoch 531/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6091 - acc: 0.6742 - val_loss: 0.7181 - val_acc: 0.6154\n",
            "Epoch 532/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6089 - acc: 0.6742 - val_loss: 0.7181 - val_acc: 0.6154\n",
            "Epoch 533/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6088 - acc: 0.6742 - val_loss: 0.7180 - val_acc: 0.6154\n",
            "Epoch 534/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.6086 - acc: 0.6742 - val_loss: 0.7179 - val_acc: 0.6154\n",
            "Epoch 535/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6084 - acc: 0.6742 - val_loss: 0.7178 - val_acc: 0.6154\n",
            "Epoch 536/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6083 - acc: 0.6742 - val_loss: 0.7178 - val_acc: 0.6154\n",
            "Epoch 537/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6081 - acc: 0.6742 - val_loss: 0.7177 - val_acc: 0.6154\n",
            "Epoch 538/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6080 - acc: 0.6742 - val_loss: 0.7176 - val_acc: 0.6154\n",
            "Epoch 539/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6078 - acc: 0.6742 - val_loss: 0.7175 - val_acc: 0.6154\n",
            "Epoch 540/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6076 - acc: 0.6742 - val_loss: 0.7175 - val_acc: 0.6154\n",
            "Epoch 541/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6075 - acc: 0.6742 - val_loss: 0.7174 - val_acc: 0.6154\n",
            "Epoch 542/1000\n",
            "89/89 [==============================] - 0s 110us/step - loss: 0.6073 - acc: 0.6742 - val_loss: 0.7173 - val_acc: 0.6923\n",
            "Epoch 543/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6072 - acc: 0.6742 - val_loss: 0.7172 - val_acc: 0.6923\n",
            "Epoch 544/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.6070 - acc: 0.6742 - val_loss: 0.7172 - val_acc: 0.6923\n",
            "Epoch 545/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6069 - acc: 0.6742 - val_loss: 0.7171 - val_acc: 0.6923\n",
            "Epoch 546/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6067 - acc: 0.6742 - val_loss: 0.7170 - val_acc: 0.6923\n",
            "Epoch 547/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6065 - acc: 0.6742 - val_loss: 0.7170 - val_acc: 0.6923\n",
            "Epoch 548/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6064 - acc: 0.6742 - val_loss: 0.7169 - val_acc: 0.6923\n",
            "Epoch 549/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6062 - acc: 0.6629 - val_loss: 0.7169 - val_acc: 0.6923\n",
            "Epoch 550/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6061 - acc: 0.6629 - val_loss: 0.7168 - val_acc: 0.6923\n",
            "Epoch 551/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6059 - acc: 0.6629 - val_loss: 0.7167 - val_acc: 0.6923\n",
            "Epoch 552/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6057 - acc: 0.6629 - val_loss: 0.7167 - val_acc: 0.6923\n",
            "Epoch 553/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6056 - acc: 0.6629 - val_loss: 0.7166 - val_acc: 0.6923\n",
            "Epoch 554/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6054 - acc: 0.6629 - val_loss: 0.7166 - val_acc: 0.6923\n",
            "Epoch 555/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6052 - acc: 0.6629 - val_loss: 0.7165 - val_acc: 0.6923\n",
            "Epoch 556/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6051 - acc: 0.6629 - val_loss: 0.7164 - val_acc: 0.6923\n",
            "Epoch 557/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6049 - acc: 0.6629 - val_loss: 0.7164 - val_acc: 0.6923\n",
            "Epoch 558/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6048 - acc: 0.6629 - val_loss: 0.7163 - val_acc: 0.6923\n",
            "Epoch 559/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6046 - acc: 0.6629 - val_loss: 0.7163 - val_acc: 0.6923\n",
            "Epoch 560/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6044 - acc: 0.6629 - val_loss: 0.7162 - val_acc: 0.6923\n",
            "Epoch 561/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6043 - acc: 0.6629 - val_loss: 0.7161 - val_acc: 0.6923\n",
            "Epoch 562/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6041 - acc: 0.6629 - val_loss: 0.7161 - val_acc: 0.6923\n",
            "Epoch 563/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6039 - acc: 0.6629 - val_loss: 0.7160 - val_acc: 0.6923\n",
            "Epoch 564/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6038 - acc: 0.6629 - val_loss: 0.7160 - val_acc: 0.6923\n",
            "Epoch 565/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6036 - acc: 0.6629 - val_loss: 0.7159 - val_acc: 0.6923\n",
            "Epoch 566/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6034 - acc: 0.6629 - val_loss: 0.7159 - val_acc: 0.6923\n",
            "Epoch 567/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6033 - acc: 0.6629 - val_loss: 0.7158 - val_acc: 0.6923\n",
            "Epoch 568/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6031 - acc: 0.6629 - val_loss: 0.7158 - val_acc: 0.6923\n",
            "Epoch 569/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.6030 - acc: 0.6629 - val_loss: 0.7157 - val_acc: 0.6923\n",
            "Epoch 570/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6028 - acc: 0.6629 - val_loss: 0.7156 - val_acc: 0.6923\n",
            "Epoch 571/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6026 - acc: 0.6629 - val_loss: 0.7156 - val_acc: 0.6923\n",
            "Epoch 572/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6025 - acc: 0.6629 - val_loss: 0.7155 - val_acc: 0.6923\n",
            "Epoch 573/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6023 - acc: 0.6629 - val_loss: 0.7155 - val_acc: 0.6923\n",
            "Epoch 574/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.6021 - acc: 0.6629 - val_loss: 0.7154 - val_acc: 0.6923\n",
            "Epoch 575/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6020 - acc: 0.6629 - val_loss: 0.7154 - val_acc: 0.6923\n",
            "Epoch 576/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6018 - acc: 0.6629 - val_loss: 0.7153 - val_acc: 0.6923\n",
            "Epoch 577/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6016 - acc: 0.6629 - val_loss: 0.7153 - val_acc: 0.6923\n",
            "Epoch 578/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6015 - acc: 0.6629 - val_loss: 0.7152 - val_acc: 0.6923\n",
            "Epoch 579/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6013 - acc: 0.6629 - val_loss: 0.7152 - val_acc: 0.6923\n",
            "Epoch 580/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.6011 - acc: 0.6629 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 581/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6010 - acc: 0.6629 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 582/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.6008 - acc: 0.6629 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 583/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.6006 - acc: 0.6629 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 584/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6005 - acc: 0.6629 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 585/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.6003 - acc: 0.6629 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 586/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.6001 - acc: 0.6629 - val_loss: 0.7148 - val_acc: 0.6923\n",
            "Epoch 587/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.6000 - acc: 0.6629 - val_loss: 0.7148 - val_acc: 0.6923\n",
            "Epoch 588/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5998 - acc: 0.6629 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 589/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.5997 - acc: 0.6629 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 590/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5995 - acc: 0.6629 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 591/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5993 - acc: 0.6629 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 592/1000\n",
            "89/89 [==============================] - 0s 101us/step - loss: 0.5992 - acc: 0.6629 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 593/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5990 - acc: 0.6629 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 594/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5988 - acc: 0.6629 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 595/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5987 - acc: 0.6629 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 596/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5985 - acc: 0.6629 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 597/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5983 - acc: 0.6629 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 598/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5982 - acc: 0.6629 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 599/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5980 - acc: 0.6629 - val_loss: 0.7143 - val_acc: 0.6923\n",
            "Epoch 600/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5978 - acc: 0.6629 - val_loss: 0.7143 - val_acc: 0.6923\n",
            "Epoch 601/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5977 - acc: 0.6629 - val_loss: 0.7143 - val_acc: 0.6923\n",
            "Epoch 602/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5975 - acc: 0.6629 - val_loss: 0.7142 - val_acc: 0.6923\n",
            "Epoch 603/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5974 - acc: 0.6629 - val_loss: 0.7142 - val_acc: 0.6923\n",
            "Epoch 604/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5972 - acc: 0.6629 - val_loss: 0.7142 - val_acc: 0.6923\n",
            "Epoch 605/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5970 - acc: 0.6629 - val_loss: 0.7141 - val_acc: 0.6923\n",
            "Epoch 606/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5969 - acc: 0.6629 - val_loss: 0.7141 - val_acc: 0.6923\n",
            "Epoch 607/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5967 - acc: 0.6629 - val_loss: 0.7140 - val_acc: 0.6923\n",
            "Epoch 608/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5965 - acc: 0.6629 - val_loss: 0.7140 - val_acc: 0.6923\n",
            "Epoch 609/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5964 - acc: 0.6629 - val_loss: 0.7140 - val_acc: 0.6923\n",
            "Epoch 610/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5962 - acc: 0.6629 - val_loss: 0.7139 - val_acc: 0.6923\n",
            "Epoch 611/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5960 - acc: 0.6629 - val_loss: 0.7139 - val_acc: 0.6923\n",
            "Epoch 612/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5959 - acc: 0.6629 - val_loss: 0.7139 - val_acc: 0.6923\n",
            "Epoch 613/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5957 - acc: 0.6629 - val_loss: 0.7138 - val_acc: 0.6923\n",
            "Epoch 614/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5955 - acc: 0.6629 - val_loss: 0.7138 - val_acc: 0.6923\n",
            "Epoch 615/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5954 - acc: 0.6629 - val_loss: 0.7138 - val_acc: 0.6923\n",
            "Epoch 616/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5952 - acc: 0.6629 - val_loss: 0.7137 - val_acc: 0.6923\n",
            "Epoch 617/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5950 - acc: 0.6629 - val_loss: 0.7137 - val_acc: 0.6923\n",
            "Epoch 618/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5949 - acc: 0.6629 - val_loss: 0.7137 - val_acc: 0.6923\n",
            "Epoch 619/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5947 - acc: 0.6629 - val_loss: 0.7137 - val_acc: 0.6923\n",
            "Epoch 620/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5945 - acc: 0.6629 - val_loss: 0.7136 - val_acc: 0.6923\n",
            "Epoch 621/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5944 - acc: 0.6629 - val_loss: 0.7136 - val_acc: 0.6923\n",
            "Epoch 622/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5942 - acc: 0.6629 - val_loss: 0.7136 - val_acc: 0.6923\n",
            "Epoch 623/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5940 - acc: 0.6629 - val_loss: 0.7136 - val_acc: 0.6923\n",
            "Epoch 624/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5939 - acc: 0.6629 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 625/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5937 - acc: 0.6629 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 626/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5935 - acc: 0.6629 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 627/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5934 - acc: 0.6629 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 628/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5932 - acc: 0.6629 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 629/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5930 - acc: 0.6629 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 630/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5929 - acc: 0.6629 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 631/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5927 - acc: 0.6629 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 632/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5925 - acc: 0.6629 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 633/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5924 - acc: 0.6629 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 634/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5922 - acc: 0.6629 - val_loss: 0.7133 - val_acc: 0.6923\n",
            "Epoch 635/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5920 - acc: 0.6629 - val_loss: 0.7133 - val_acc: 0.6923\n",
            "Epoch 636/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5919 - acc: 0.6629 - val_loss: 0.7133 - val_acc: 0.6923\n",
            "Epoch 637/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5917 - acc: 0.6629 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 638/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5915 - acc: 0.6629 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 639/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5914 - acc: 0.6629 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 640/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5912 - acc: 0.6629 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 641/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5910 - acc: 0.6629 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 642/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5908 - acc: 0.6629 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 643/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5907 - acc: 0.6629 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 644/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5905 - acc: 0.6629 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 645/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5903 - acc: 0.6629 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 646/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5902 - acc: 0.6629 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 647/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5900 - acc: 0.6629 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 648/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.5898 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 649/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.5896 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 650/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.5895 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 651/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5893 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 652/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5891 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 653/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.5889 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 654/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5888 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 655/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5886 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 656/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.5884 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 657/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5883 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 658/1000\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5881 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 659/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5879 - acc: 0.6629 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 660/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5877 - acc: 0.6629 - val_loss: 0.7128 - val_acc: 0.6923\n",
            "Epoch 661/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5876 - acc: 0.6629 - val_loss: 0.7128 - val_acc: 0.6923\n",
            "Epoch 662/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5874 - acc: 0.6629 - val_loss: 0.7128 - val_acc: 0.6923\n",
            "Epoch 663/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5872 - acc: 0.6629 - val_loss: 0.7128 - val_acc: 0.6923\n",
            "Epoch 664/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5870 - acc: 0.6629 - val_loss: 0.7128 - val_acc: 0.6923\n",
            "Epoch 665/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5869 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 666/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5867 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 667/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5865 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 668/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.5864 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 669/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5862 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 670/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.5860 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 671/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5858 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 672/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5857 - acc: 0.6742 - val_loss: 0.7129 - val_acc: 0.6923\n",
            "Epoch 673/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5855 - acc: 0.6742 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 674/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5853 - acc: 0.6742 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 675/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5851 - acc: 0.6742 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 676/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5850 - acc: 0.6742 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 677/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5848 - acc: 0.6742 - val_loss: 0.7130 - val_acc: 0.6923\n",
            "Epoch 678/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5846 - acc: 0.6742 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 679/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5845 - acc: 0.6742 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 680/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5843 - acc: 0.6742 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 681/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5841 - acc: 0.6742 - val_loss: 0.7131 - val_acc: 0.6923\n",
            "Epoch 682/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5839 - acc: 0.6742 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 683/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5838 - acc: 0.6854 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 684/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5836 - acc: 0.6854 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 685/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5834 - acc: 0.6854 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 686/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5832 - acc: 0.6854 - val_loss: 0.7132 - val_acc: 0.6923\n",
            "Epoch 687/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5831 - acc: 0.6854 - val_loss: 0.7133 - val_acc: 0.6923\n",
            "Epoch 688/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5829 - acc: 0.6854 - val_loss: 0.7133 - val_acc: 0.6923\n",
            "Epoch 689/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5827 - acc: 0.6854 - val_loss: 0.7133 - val_acc: 0.6923\n",
            "Epoch 690/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5825 - acc: 0.6854 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 691/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5824 - acc: 0.6854 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 692/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5822 - acc: 0.6854 - val_loss: 0.7134 - val_acc: 0.6923\n",
            "Epoch 693/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5820 - acc: 0.6854 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 694/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5818 - acc: 0.6854 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 695/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5817 - acc: 0.6854 - val_loss: 0.7135 - val_acc: 0.6923\n",
            "Epoch 696/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5815 - acc: 0.6854 - val_loss: 0.7136 - val_acc: 0.6923\n",
            "Epoch 697/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5813 - acc: 0.6966 - val_loss: 0.7136 - val_acc: 0.6923\n",
            "Epoch 698/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5811 - acc: 0.6966 - val_loss: 0.7137 - val_acc: 0.6923\n",
            "Epoch 699/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5810 - acc: 0.6966 - val_loss: 0.7137 - val_acc: 0.6923\n",
            "Epoch 700/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5808 - acc: 0.6966 - val_loss: 0.7138 - val_acc: 0.6923\n",
            "Epoch 701/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5806 - acc: 0.6966 - val_loss: 0.7138 - val_acc: 0.6923\n",
            "Epoch 702/1000\n",
            "89/89 [==============================] - 0s 135us/step - loss: 0.5804 - acc: 0.6966 - val_loss: 0.7139 - val_acc: 0.6923\n",
            "Epoch 703/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5803 - acc: 0.6966 - val_loss: 0.7139 - val_acc: 0.6923\n",
            "Epoch 704/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5801 - acc: 0.6966 - val_loss: 0.7140 - val_acc: 0.6923\n",
            "Epoch 705/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5799 - acc: 0.6966 - val_loss: 0.7141 - val_acc: 0.6923\n",
            "Epoch 706/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5798 - acc: 0.6966 - val_loss: 0.7141 - val_acc: 0.6923\n",
            "Epoch 707/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5796 - acc: 0.6966 - val_loss: 0.7142 - val_acc: 0.6923\n",
            "Epoch 708/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5794 - acc: 0.6966 - val_loss: 0.7142 - val_acc: 0.6923\n",
            "Epoch 709/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5792 - acc: 0.6966 - val_loss: 0.7143 - val_acc: 0.6923\n",
            "Epoch 710/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5791 - acc: 0.6966 - val_loss: 0.7143 - val_acc: 0.6923\n",
            "Epoch 711/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5789 - acc: 0.6966 - val_loss: 0.7143 - val_acc: 0.6923\n",
            "Epoch 712/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5787 - acc: 0.6966 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 713/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5785 - acc: 0.6966 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 714/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5784 - acc: 0.6966 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 715/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5782 - acc: 0.6966 - val_loss: 0.7144 - val_acc: 0.6923\n",
            "Epoch 716/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5780 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 717/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5779 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 718/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.5777 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 719/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5775 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 720/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5773 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 721/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5772 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 722/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5770 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 723/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5768 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 724/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5767 - acc: 0.6966 - val_loss: 0.7145 - val_acc: 0.6923\n",
            "Epoch 725/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5765 - acc: 0.6966 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 726/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5763 - acc: 0.6966 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 727/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5761 - acc: 0.6966 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 728/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5760 - acc: 0.6966 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 729/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5758 - acc: 0.6966 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 730/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5756 - acc: 0.6966 - val_loss: 0.7146 - val_acc: 0.6923\n",
            "Epoch 731/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5755 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 732/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5753 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 733/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5751 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 734/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5750 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 735/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5748 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 736/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5746 - acc: 0.7079 - val_loss: 0.7147 - val_acc: 0.6923\n",
            "Epoch 737/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5745 - acc: 0.7079 - val_loss: 0.7148 - val_acc: 0.6923\n",
            "Epoch 738/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5743 - acc: 0.7079 - val_loss: 0.7148 - val_acc: 0.6923\n",
            "Epoch 739/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5741 - acc: 0.7079 - val_loss: 0.7148 - val_acc: 0.6923\n",
            "Epoch 740/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5739 - acc: 0.7079 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 741/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5738 - acc: 0.7079 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 742/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5736 - acc: 0.7079 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 743/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5734 - acc: 0.7079 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 744/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5733 - acc: 0.7079 - val_loss: 0.7149 - val_acc: 0.6923\n",
            "Epoch 745/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5731 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 746/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5729 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 747/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5727 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 748/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5726 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 749/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5724 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 750/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5722 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 751/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5721 - acc: 0.7079 - val_loss: 0.7150 - val_acc: 0.6923\n",
            "Epoch 752/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5719 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 753/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5717 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 754/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5716 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 755/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5714 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 756/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5712 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 757/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5710 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 758/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5709 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 759/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5707 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 760/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5705 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6923\n",
            "Epoch 761/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5704 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 762/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5702 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 763/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5700 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 764/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5699 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 765/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5697 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 766/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5695 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 767/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5693 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 768/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5692 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 769/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5690 - acc: 0.7079 - val_loss: 0.7151 - val_acc: 0.6154\n",
            "Epoch 770/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.5688 - acc: 0.7079 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 771/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5687 - acc: 0.7079 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 772/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5685 - acc: 0.7079 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 773/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5683 - acc: 0.7079 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 774/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5682 - acc: 0.7079 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 775/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5680 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 776/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5678 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 777/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5676 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 778/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5675 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 779/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5673 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 780/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5671 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 781/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5670 - acc: 0.7079 - val_loss: 0.7153 - val_acc: 0.6154\n",
            "Epoch 782/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5668 - acc: 0.7079 - val_loss: 0.7154 - val_acc: 0.6154\n",
            "Epoch 783/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5666 - acc: 0.7079 - val_loss: 0.7154 - val_acc: 0.6154\n",
            "Epoch 784/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5665 - acc: 0.7079 - val_loss: 0.7154 - val_acc: 0.6154\n",
            "Epoch 785/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5663 - acc: 0.7079 - val_loss: 0.7154 - val_acc: 0.6154\n",
            "Epoch 786/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5661 - acc: 0.7191 - val_loss: 0.7155 - val_acc: 0.6154\n",
            "Epoch 787/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5659 - acc: 0.7191 - val_loss: 0.7155 - val_acc: 0.6154\n",
            "Epoch 788/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5658 - acc: 0.7191 - val_loss: 0.7155 - val_acc: 0.6154\n",
            "Epoch 789/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5656 - acc: 0.7191 - val_loss: 0.7155 - val_acc: 0.6154\n",
            "Epoch 790/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5654 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 791/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5653 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 792/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5651 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 793/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5649 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 794/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5648 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 795/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5646 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 796/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5644 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 797/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5642 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 798/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5641 - acc: 0.7191 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 799/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5639 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 800/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5637 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 801/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5636 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 802/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5634 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 803/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5632 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 804/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5630 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 805/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5629 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 806/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5627 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 807/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5625 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 808/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5623 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 809/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5622 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 810/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5620 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 811/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5618 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 812/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5616 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 813/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5615 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 814/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5613 - acc: 0.7191 - val_loss: 0.7157 - val_acc: 0.6154\n",
            "Epoch 815/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5611 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 816/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5609 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 817/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5608 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 818/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5606 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 819/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.5604 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 820/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5602 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 821/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.5600 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 822/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5599 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 823/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5597 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 824/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5595 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 825/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.5593 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 826/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5592 - acc: 0.7191 - val_loss: 0.7158 - val_acc: 0.6154\n",
            "Epoch 827/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5590 - acc: 0.7191 - val_loss: 0.7159 - val_acc: 0.6154\n",
            "Epoch 828/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5588 - acc: 0.7191 - val_loss: 0.7159 - val_acc: 0.6154\n",
            "Epoch 829/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5586 - acc: 0.7191 - val_loss: 0.7159 - val_acc: 0.6154\n",
            "Epoch 830/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5584 - acc: 0.7191 - val_loss: 0.7159 - val_acc: 0.6154\n",
            "Epoch 831/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5583 - acc: 0.7191 - val_loss: 0.7160 - val_acc: 0.6154\n",
            "Epoch 832/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5581 - acc: 0.7191 - val_loss: 0.7160 - val_acc: 0.6154\n",
            "Epoch 833/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5579 - acc: 0.7191 - val_loss: 0.7160 - val_acc: 0.6154\n",
            "Epoch 834/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5577 - acc: 0.7191 - val_loss: 0.7161 - val_acc: 0.6154\n",
            "Epoch 835/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5576 - acc: 0.7191 - val_loss: 0.7161 - val_acc: 0.6154\n",
            "Epoch 836/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5574 - acc: 0.7191 - val_loss: 0.7161 - val_acc: 0.6154\n",
            "Epoch 837/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5572 - acc: 0.7191 - val_loss: 0.7162 - val_acc: 0.6154\n",
            "Epoch 838/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5570 - acc: 0.7191 - val_loss: 0.7162 - val_acc: 0.6154\n",
            "Epoch 839/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5569 - acc: 0.7191 - val_loss: 0.7162 - val_acc: 0.6154\n",
            "Epoch 840/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5567 - acc: 0.7191 - val_loss: 0.7163 - val_acc: 0.6154\n",
            "Epoch 841/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5565 - acc: 0.7191 - val_loss: 0.7163 - val_acc: 0.6154\n",
            "Epoch 842/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5563 - acc: 0.7191 - val_loss: 0.7163 - val_acc: 0.6154\n",
            "Epoch 843/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5561 - acc: 0.7191 - val_loss: 0.7163 - val_acc: 0.6154\n",
            "Epoch 844/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5560 - acc: 0.7303 - val_loss: 0.7163 - val_acc: 0.6154\n",
            "Epoch 845/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5558 - acc: 0.7303 - val_loss: 0.7164 - val_acc: 0.6154\n",
            "Epoch 846/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5556 - acc: 0.7303 - val_loss: 0.7164 - val_acc: 0.6154\n",
            "Epoch 847/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5554 - acc: 0.7303 - val_loss: 0.7164 - val_acc: 0.6154\n",
            "Epoch 848/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5553 - acc: 0.7303 - val_loss: 0.7164 - val_acc: 0.6154\n",
            "Epoch 849/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5551 - acc: 0.7303 - val_loss: 0.7165 - val_acc: 0.6154\n",
            "Epoch 850/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5549 - acc: 0.7303 - val_loss: 0.7165 - val_acc: 0.6154\n",
            "Epoch 851/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5547 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 852/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5545 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 853/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5544 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 854/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5542 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 855/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5540 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 856/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5538 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 857/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5536 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 858/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5534 - acc: 0.7303 - val_loss: 0.7166 - val_acc: 0.6154\n",
            "Epoch 859/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5532 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 860/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5531 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 861/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5529 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 862/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5527 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 863/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5525 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 864/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5523 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 865/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5521 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 866/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5519 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 867/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5518 - acc: 0.7303 - val_loss: 0.7167 - val_acc: 0.6154\n",
            "Epoch 868/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5516 - acc: 0.7303 - val_loss: 0.7168 - val_acc: 0.6154\n",
            "Epoch 869/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5514 - acc: 0.7303 - val_loss: 0.7168 - val_acc: 0.6154\n",
            "Epoch 870/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5512 - acc: 0.7303 - val_loss: 0.7168 - val_acc: 0.6154\n",
            "Epoch 871/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5510 - acc: 0.7303 - val_loss: 0.7168 - val_acc: 0.6154\n",
            "Epoch 872/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.5508 - acc: 0.7303 - val_loss: 0.7168 - val_acc: 0.6154\n",
            "Epoch 873/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5506 - acc: 0.7303 - val_loss: 0.7168 - val_acc: 0.6154\n",
            "Epoch 874/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5504 - acc: 0.7303 - val_loss: 0.7169 - val_acc: 0.6154\n",
            "Epoch 875/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5502 - acc: 0.7303 - val_loss: 0.7169 - val_acc: 0.6154\n",
            "Epoch 876/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5501 - acc: 0.7303 - val_loss: 0.7169 - val_acc: 0.6154\n",
            "Epoch 877/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5499 - acc: 0.7303 - val_loss: 0.7169 - val_acc: 0.6154\n",
            "Epoch 878/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5497 - acc: 0.7303 - val_loss: 0.7169 - val_acc: 0.6154\n",
            "Epoch 879/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5495 - acc: 0.7303 - val_loss: 0.7169 - val_acc: 0.6154\n",
            "Epoch 880/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5493 - acc: 0.7303 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 881/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5491 - acc: 0.7303 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 882/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5489 - acc: 0.7303 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 883/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.5487 - acc: 0.7303 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 884/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5486 - acc: 0.7303 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 885/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5484 - acc: 0.7303 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 886/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5482 - acc: 0.7416 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 887/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5480 - acc: 0.7416 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 888/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5478 - acc: 0.7416 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 889/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5476 - acc: 0.7416 - val_loss: 0.7170 - val_acc: 0.6154\n",
            "Epoch 890/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5474 - acc: 0.7416 - val_loss: 0.7171 - val_acc: 0.6154\n",
            "Epoch 891/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5473 - acc: 0.7416 - val_loss: 0.7171 - val_acc: 0.6154\n",
            "Epoch 892/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5471 - acc: 0.7416 - val_loss: 0.7171 - val_acc: 0.6154\n",
            "Epoch 893/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5469 - acc: 0.7416 - val_loss: 0.7172 - val_acc: 0.6154\n",
            "Epoch 894/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5467 - acc: 0.7416 - val_loss: 0.7172 - val_acc: 0.6923\n",
            "Epoch 895/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5465 - acc: 0.7416 - val_loss: 0.7172 - val_acc: 0.6923\n",
            "Epoch 896/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5463 - acc: 0.7416 - val_loss: 0.7173 - val_acc: 0.6923\n",
            "Epoch 897/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5461 - acc: 0.7416 - val_loss: 0.7173 - val_acc: 0.6923\n",
            "Epoch 898/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5459 - acc: 0.7416 - val_loss: 0.7173 - val_acc: 0.6923\n",
            "Epoch 899/1000\n",
            "89/89 [==============================] - 0s 87us/step - loss: 0.5458 - acc: 0.7416 - val_loss: 0.7173 - val_acc: 0.6923\n",
            "Epoch 900/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5456 - acc: 0.7416 - val_loss: 0.7173 - val_acc: 0.6923\n",
            "Epoch 901/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5454 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 902/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5452 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 903/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5450 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 904/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5448 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 905/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5446 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 906/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5444 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 907/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5443 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 908/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5441 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 909/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.5439 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 910/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5437 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 911/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5435 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 912/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5433 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 913/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5431 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 914/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5429 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 915/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5428 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 916/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5426 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 917/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5424 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 918/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.5422 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 919/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5420 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 920/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5418 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 921/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5416 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 922/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5414 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 923/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5412 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 924/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5411 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 925/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5409 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 926/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5407 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 927/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5405 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 928/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5403 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 929/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.5401 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 930/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.5399 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 931/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.5397 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 932/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.5395 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 933/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.5393 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 934/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5391 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 935/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5390 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 936/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.5388 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 937/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5386 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 938/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5384 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 939/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.5382 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 940/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.5380 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 941/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5378 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 942/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.5376 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 943/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.5374 - acc: 0.7416 - val_loss: 0.7174 - val_acc: 0.6923\n",
            "Epoch 944/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.5372 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 945/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5370 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 946/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.5368 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 947/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5366 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 948/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.5364 - acc: 0.7416 - val_loss: 0.7175 - val_acc: 0.6923\n",
            "Epoch 949/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5363 - acc: 0.7416 - val_loss: 0.7176 - val_acc: 0.6923\n",
            "Epoch 950/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5361 - acc: 0.7416 - val_loss: 0.7176 - val_acc: 0.6923\n",
            "Epoch 951/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5359 - acc: 0.7416 - val_loss: 0.7176 - val_acc: 0.6923\n",
            "Epoch 952/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5357 - acc: 0.7416 - val_loss: 0.7176 - val_acc: 0.6923\n",
            "Epoch 953/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.5355 - acc: 0.7416 - val_loss: 0.7176 - val_acc: 0.6923\n",
            "Epoch 954/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5353 - acc: 0.7416 - val_loss: 0.7176 - val_acc: 0.6923\n",
            "Epoch 955/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.5351 - acc: 0.7416 - val_loss: 0.7177 - val_acc: 0.6923\n",
            "Epoch 956/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5349 - acc: 0.7416 - val_loss: 0.7177 - val_acc: 0.6923\n",
            "Epoch 957/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5347 - acc: 0.7416 - val_loss: 0.7177 - val_acc: 0.6923\n",
            "Epoch 958/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5345 - acc: 0.7416 - val_loss: 0.7177 - val_acc: 0.6923\n",
            "Epoch 959/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5343 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 960/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.5341 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 961/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5339 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 962/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5338 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 963/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5336 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 964/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.5334 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 965/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5332 - acc: 0.7416 - val_loss: 0.7178 - val_acc: 0.6923\n",
            "Epoch 966/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.5330 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 967/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5328 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 968/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5326 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 969/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5324 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 970/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5322 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 971/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5320 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 972/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5318 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 973/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5316 - acc: 0.7528 - val_loss: 0.7179 - val_acc: 0.6923\n",
            "Epoch 974/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5315 - acc: 0.7528 - val_loss: 0.7180 - val_acc: 0.6923\n",
            "Epoch 975/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5313 - acc: 0.7528 - val_loss: 0.7180 - val_acc: 0.6923\n",
            "Epoch 976/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5311 - acc: 0.7528 - val_loss: 0.7180 - val_acc: 0.6923\n",
            "Epoch 977/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5309 - acc: 0.7528 - val_loss: 0.7181 - val_acc: 0.6923\n",
            "Epoch 978/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5307 - acc: 0.7528 - val_loss: 0.7181 - val_acc: 0.6923\n",
            "Epoch 979/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5305 - acc: 0.7528 - val_loss: 0.7181 - val_acc: 0.6923\n",
            "Epoch 980/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5303 - acc: 0.7528 - val_loss: 0.7181 - val_acc: 0.6923\n",
            "Epoch 981/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5301 - acc: 0.7528 - val_loss: 0.7181 - val_acc: 0.6923\n",
            "Epoch 982/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5299 - acc: 0.7528 - val_loss: 0.7182 - val_acc: 0.6923\n",
            "Epoch 983/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5297 - acc: 0.7528 - val_loss: 0.7182 - val_acc: 0.6923\n",
            "Epoch 984/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5295 - acc: 0.7528 - val_loss: 0.7182 - val_acc: 0.6923\n",
            "Epoch 985/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5294 - acc: 0.7528 - val_loss: 0.7182 - val_acc: 0.6923\n",
            "Epoch 986/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5292 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 987/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5290 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 988/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.5288 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 989/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5286 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 990/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5284 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 991/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.5282 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 992/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.5280 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 993/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5278 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 994/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5277 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 995/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5275 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 996/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5273 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 997/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5271 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 998/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5269 - acc: 0.7528 - val_loss: 0.7183 - val_acc: 0.6923\n",
            "Epoch 999/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5267 - acc: 0.7640 - val_loss: 0.7182 - val_acc: 0.6923\n",
            "Epoch 1000/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.5265 - acc: 0.7640 - val_loss: 0.7182 - val_acc: 0.6923\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "86121b25-bda9-4d76-c2fb-271d9e690466",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7faba71b2d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCcI551ymPEE455zL1P8DMjLcS2zXf9YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "580f93d5-c404-41b7-e9ec-ca2954d37ea2",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7faba7121588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3deZwU1dXA/d9h2BfZUWDAgQjigmyj\nuIviQpSAiAtolHEBJUGExAVNggaTvCavT1wS1GgEl6gQNx5U1Chq9BFfBRWNICggxkFEZGeGbWbO\n+8etnqlpumd6Zrq6p7vO9/PpT3etfauru0+de6tuiapijDHGRGuQ7gIYY4ypnyxAGGOMickChDHG\nmJgsQBhjjInJAoQxxpiYLEAYY4yJyQJEyInIyyIyLtnzppOIrBWR0wNYr4rIId7rB0TkN4nMW4v3\nuURE/lXbcoadiOR5n3/DONNvE5F/pLpcmSjmB2jqNxHZ6RtsDuwBSr3hq1X1iUTXpao/DmLebKeq\n1yRjPSKSB3wFNFLVEm/dTwAJ70NjgmIBIgOpasvIaxFZC1ylqq9HzyciDSN/Osakm30fM49VMWUR\nERkiIoUicpOIfAfMFpG2IvKiiGwUkS3e61zfMm+JyFXe6wIR+T8RudOb9ysR+XEt5+0hIm+LyA4R\neV1EZsZL6xMs4+0i8q63vn+JSAff9EtF5GsR2SQiv6ri8xksIt+JSI5v3CgR+dR7fYyIvCciW0Vk\nvYj8VUQax1nXIyLyO9/wDd4y34rIFVHzniMiH4vIdhH5RkRu801+23veKiI7ReS4yGfrW/54EVks\nItu85+MT/Wxq+Dm3E5HZ3jZsEZF5vmkjRWSptw2rRWSYN75SdZ6/+sZX1XOliPwXeMMb/7S3H7Z5\n35EjfMs3E5H/8fbnNu871kxEXhKRa6O251MRGRVrW6Pm6yEi//Y+n9eADlHT45Yn7CxAZJ+DgHbA\nwcAE3D6e7Q13B3YBf61i+cHAStyP6E/AwyIitZj3SeADoD1wG3BpFe+ZSBkvBi4HOgGNgesBRORw\n4H5v/V2898slBlV9HygCTota75Pe61Jgqrc9xwFDgZ9VUW68MgzzynMG0AuIbv8oAi4D2gDnABNF\n5Fxv2snecxtVbamq70Wtux3wEnCvt21/Bl4SkfZR27DfZxNDdZ/z47gqyyO8dd3lleEY4DHgBm8b\nTgbWxvs8YjgFOAw4yxt+Gfc5dQI+onJ12p3AIOB43Pf4RqAMeBT4aWQmEekHdMV9NtV5EvgQt19v\nB6Lb0aoqT7ipqj0y+IH7oZ7uvR4C7AWaVjF/f2CLb/gtXBUVQAGwyjetOaDAQTWZF/fnUwI0903/\nB/CPBLcpVhl/7Rv+GfCK93o6MMc3rYX3GZweZ92/A2Z5r1vh/rwPjjPvFOB537ACh3ivHwF+572e\nBdzhm6+3f94Y670buMt7nefN29A3vQD4P+/1pcAHUcu/BxRU99nU5HMGOuP+iNvGmO9vkfJW9f3z\nhm+L7GfftvWsogxtvHla4wLYLqBfjPmaAluAXt7wncB9cdZZ/pn6vostfNOfjPdd9Jenrr/NbHhY\nBpF9Nqrq7siAiDQXkb95Kft2XJVGG381S5TvIi9Utdh72bKG83YBNvvGAXwTr8AJlvE73+tiX5m6\n+NetqkXApnjvhftzOE9EmgDnAR+p6tdeOXp71S7feeX4A1HVEXFUKgPwddT2DRaRN72qnW3ANQmu\nN7Lur6PGfY07eo6I99lUUs3n3A23z7bEWLQbsDrB8sZS/tmISI6I3OFVU22nIhPp4D2axnov7zs9\nF/ipiDQAxuIynup0wQXBIt+48s+zmvKEngWI7BPdPe8vgUOBwap6ABVVGvGqjZJhPdBORJr7xnWr\nYv66lHG9f93ee7aPN7OqLsf9QfyYytVL4KqqVuCOUg8AbqlNGXBHrX5PAvOBbqraGnjAt97qulP+\nFlcl5NcdWJdAuaJV9Tl/g9tnbWIs9w3wozjrLMJljxEHxZjHv40XAyNx1XCtcUf7kTL8AOyu4r0e\nBS7BVf0Va1R1XBzrgbYi0sI3zr9/qipP6FmAyH6tcGn7Vq8++9ag39A7Il8C3CYijUXkOOAnAZXx\nGWC4iJworkF5BtV/r58ErsP9QT4dVY7twE4R6QNMTLAM/wQKRORwL0BFl78V7uh8t1eff7Fv2kZc\n1U7POOteAPQWkYtFpKGIXAQcDryYYNmiyxHzc1bV9bi6+Pu8xuxGIhIJIA8Dl4vIUBFpICJdvc8H\nYCkwxps/Hzg/gTLswWV5zXFZWqQMZbjquj+LSBfv6P44L9vDCwhlwP+QWPbg/y7+1vsunkjl72Lc\n8hgLEGFwN9AMd3T2/wGvpOh9L8E19G7C1fvPxf0QY6l1GVV1GfBz3J/+elw9dWE1iz2Fazh9Q1V/\n8I2/HvfnvQN4yCtzImV42duGN4BV3rPfz4AZIrID12byT9+yxcDvgXfFnT11bNS6NwHDcUf/m3CN\ntsOjyp2o6j7nS4F9uCzqe1wbDKr6Aa4R/C5gG/BvKrKa3+CO+LcAv6VyRhbLY7gMbh2w3CuH3/XA\nf4DFwGbgj1T+n3oM6Itr00rUxbgTKjbjguJjNShPqInXMGNMoERkLrBCVQPPYEz2EpHLgAmqemK6\nyxIGlkGYQIjI0SLyI69KYhiunndedcsZE49Xffcz4MF0lyUsLECYoByEOwVzJ+4c/omq+nFaS2Qy\nloichWuv2UD11VgmSayKyRhjTEyWQRhjjIkpazrr69Chg+bl5aW7GMYYk1E+/PDDH1S1Y6xpWRMg\n8vLyWLJkSbqLYYwxGUVEoq/UL2dVTMYYY2KyAGGMMSYmCxDGGGNiypo2iFj27dtHYWEhu3fvrn5m\nEwpNmzYlNzeXRo0apbsoxtR7WR0gCgsLadWqFXl5ecS/540JC1Vl06ZNFBYW0qNHj3QXx5h6L6ur\nmHbv3k379u0tOBgARIT27dtbRmlMgrI6QAAWHEwl9n0wJnFZHyCMMSabPfooPBhQ94UWIAK0adMm\n+vfvT//+/TnooIPo2rVr+fDevXurXHbJkiVMnjy52vc4/vjjk1VcY0yG2bULCgrg6quDWX9WN1Kn\nW/v27Vm6dCkAt912Gy1btuT6668vn15SUkLDhrF3QX5+Pvn5+dW+x6JFi5JT2BQqLS0lJyfeLbGN\nMYkq8u60fe+9wazfMogUKygo4JprrmHw4MHceOONfPDBBxx33HEMGDCA448/npUrVwLw1ltvMXz4\ncMAFlyuuuIIhQ4bQs2dP7vV9G1q2bFk+/5AhQzj//PPp06cPl1xyCZGeehcsWECfPn0YNGgQkydP\nLl+v39q1aznppJMYOHAgAwcOrBR4/vjHP9K3b1/69evHtGnTAFi1ahWnn346/fr1Y+DAgaxevbpS\nmQEmTZrEI488AriuUG666SYGDhzI008/zUMPPcTRRx9Nv379GD16NMXFxQBs2LCBUaNG0a9fP/r1\n68eiRYuYPn06d999d/l6f/WrX3HPPffUeV8Yk+kiAaJFi6rnq63QZBBTpoB3MJ80/fuD738rYYWF\nhSxatIicnBy2b9/OO++8Q8OGDXn99de55ZZbePbZZ/dbZsWKFbz55pvs2LGDQw89lIkTJ+53Lv/H\nH3/MsmXL6NKlCyeccALvvvsu+fn5XH311bz99tv06NGDsWPHxixTp06deO2112jatClffvklY8eO\nZcmSJbz88sv87//+L++//z7Nmzdn8+bNAFxyySVMmzaNUaNGsXv3bsrKyvjmm2+q3O727dvz0Ucf\nAa76bfz48QD8+te/5uGHH+baa69l8uTJnHLKKTz//POUlpayc+dOunTpwnnnnceUKVMoKytjzpw5\nfPDBBzX+3I3JNpEA0bx5MOsPTYCoTy644ILyKpZt27Yxbtw4vvzyS0SEffv2xVzmnHPOoUmTJjRp\n0oROnTqxYcMGcnNzK81zzDHHlI/r378/a9eupWXLlvTs2bP8vP+xY8fyYIwWrX379jFp0iSWLl1K\nTk4OX3zxBQCvv/46l19+Oc29b2C7du3YsWMH69atY9SoUYC7+CwRF110Ufnrzz77jF//+tds3bqV\nnTt3ctZZZwHwxhtv8Nhj7pbBOTk5tG7dmtatW9O+fXs+/vhjNmzYwIABA2jfvn1C72lMNvMSb8sg\n6qo2R/pBaeHbm7/5zW849dRTef7551m7di1DhgyJuUyTJk3KX+fk5FBSUlKreeK56667OPDAA/nk\nk08oKytL+E/fr2HDhpSVlZUPR19v4N/ugoIC5s2bR79+/XjkkUd46623qlz3VVddxSOPPMJ3333H\nFVdcUeOyGZONgq5iCrQNQkSGichKEVklItNiTL9LRJZ6jy9EZKtvWqlv2vwgy5lO27Zto2vXrgDl\n9fXJdOihh7JmzRrWrl0LwNy5c+OWo3PnzjRo0IDHH3+c0tJSAM444wxmz55d3kawefNmWrVqRW5u\nLvPmuVtM79mzh+LiYg4++GCWL1/Onj172Lp1KwsXLoxbrh07dtC5c2f27dvHE088UT5+6NCh3H//\n/YBrzN62bRsAo0aN4pVXXmHx4sXl2YYx2WzDBli+vOrHihVu3oyrYhKRHGAmcAZQCCwWkfmqujwy\nj6pO9c1/LTDAt4pdqto/qPLVFzfeeCPjxo3jd7/7Heecc07S19+sWTPuu+8+hg0bRosWLTj66KNj\nzvezn/2M0aNH89hjj5XPCzBs2DCWLl1Kfn4+jRs35uyzz+YPf/gDjz/+OFdffTXTp0+nUaNGPP30\n0/Ts2ZMLL7yQI488kh49ejBgwICY7wVw++23M3jwYDp27MjgwYPZsWMHAPfccw8TJkzg4YcfJicn\nh/vvv5/jjjuOxo0bc+qpp9KmTRs7A8pkvZ074eCDYc+exOZv1y6YcgR2T2oROQ64TVXP8oZvBlDV\n/yfO/IuAW1X1NW94p6q2TPT98vPzNfqGQZ9//jmHHXZYLbcge+zcuZOWLVuiqvz85z+nV69eTJ06\ntfoF65GysrLyM6B69epVp3XZ98LUd19/DXl5cM01cOqpVc/bti2ccUbt30tEPlTVmOfUB9kG0RXw\nn9ZSCAyONaOIHAz0AN7wjW4qIkuAEuAOVZ0XY7kJwASA7t27J6nY2eehhx7i0UcfZe/evQwYMICr\ng7qqJiDLly9n+PDhjBo1qs7BwZhMEGl8PuUUuPDC9JWjvjRSjwGeUdVS37iDVXWdiPQE3hCR/6jq\nav9Cqvog8CC4DCJ1xc0sU6dOzbiMwe/www9nzZo16S6GMSkT9OmriQqykXod0M03nOuNi2UM8JR/\nhKqu857XAG9RuX3CGGOyVtCnryYqyACxGOglIj1EpDEuCOx3NpKI9AHaAu/5xrUVkSbe6w7ACcDy\n6GWNMSYbBX36aqICq2JS1RIRmQS8CuQAs1R1mYjMAJaoaiRYjAHmaOXW8sOAv4lIGS6I3eE/+8kY\nY7JZfaliCrQNQlUXAAuixk2PGr4txnKLgL5Bls0YY2rr3Xfh2muhBtei1siWLe45azMIA6eeeirT\npk2rdGHX3XffzcqVK8svBos2ZMgQ7rzzTvLz8zn77LN58sknadOmTaV5YvUMG23evHn07t2bww8/\nHIDp06dz8sknc/rppydhy4wJt3//Gz7+GM49F4K6B9Xw4e5U13SyABGgsWPHMmfOnEoBYs6cOfzp\nT39KaPkFCxZUP1Mc8+bNY/jw4eUBYsaMGbVeV7pYt+CmvioqgpwceO654AJEfWDdfQfo/PPP56WX\nXiq/OdDatWv59ttvOemkk5g4cSL5+fkcccQR3HrrrTGXz8vL44cffgDg97//Pb179+bEE08s7xIc\niNlt9qJFi5g/fz433HAD/fv3Z/Xq1RQUFPDMM88AsHDhQgYMGEDfvn254oor2ONdrpmXl8ett97K\nwIED6du3Lysi1/H7WLfgxrizjJo3z+7gAGHKINLQ33e7du045phjePnllxk5ciRz5szhwgsvRET4\n/e9/T7t27SgtLWXo0KF8+umnHHXUUTHX8+GHHzJnzhyWLl1KSUkJAwcOZNCgQQCcd955MbvNHjFi\nBMOHD+f888+vtK7du3dTUFDAwoUL6d27N5dddhn3338/U6ZMAaBDhw589NFH3Hfffdx55538/e9/\nr7S8dQtujMsg0t0+kAqWQQQsUs0Ernopcj+Gf/7znwwcOJABAwawbNkyli+Pf5LWO++8w6hRo2je\nvDkHHHAAI0aMKJ/22WefcdJJJ9G3b1+eeOIJli1bVmV5Vq5cSY8ePejduzcA48aN4+233y6fft55\n5wEwaNCg8g7+/Pbt28f48ePp27cvF1xwQXm5E+0WvHkCp2VEdwsea/veeOMNJk6cCFR0C56Xl1fe\nLfi//vUv6xbcBKa4OBwBIjwZRJr6+x45ciRTp07lo48+ori4mEGDBvHVV19x5513snjxYtq2bUtB\nQcF+XWMnqqbdZlcn0mV4vO7CrVtwY1wGke5TUFMhPAEiTVq2bMmpp57KFVdcUZ49bN++nRYtWtC6\ndWs2bNjAyy+/HPc+EAAnn3wyBQUF3HzzzZSUlPDCCy+U96cU3W12pOvwVq1alfeQ6nfooYeydu1a\nVq1axSGHHMLjjz/OKaeckvD2bNu2jdzcXBo0aMCjjz5aqVvwGTNmcMkll5RXMbVr1668W/Bzzz2X\nPXv2UFpaWqlb8F27drFw4UJOPPHEmO8Xb/si3YJPmTKlvIqpdevWjBo1iunTp7Nv3z6efPLJhLfL\nhNfevfDAAxDj5xLXsmUQhuTUAkQKjB07llGjRpVXNfXr148BAwbQp08funXrxgknnFDl8gMHDuSi\niy6iX79+dOrUqVKX3fG6zR4zZgzjx4/n3nvvLW+cBlfNM3v2bC644AJKSko4+uijueaaaxLeFusW\n3GSbRYvguutqvlwVx3RZI7DuvlPNuvs2kFi34Pa9MH4vvAAjRsB774F37kdCGjbMjrOYquru2xqp\nTdZYvnw5hxxyCEOHDrVuwU3CIt1atG4NjRol/siG4FAdq2IyWcO6BTe1Eek5NQyNzjWV9RlEtlSh\nmeSw74OJVl96Tq2PsjpANG3alE2bNtmfggFccNi0aVOtTs012au+3HuhPsrqKqbc3FwKCwvZuHFj\nuoti6ommTZuSm5ub7mKYJNm5EzZsqNs61q1z7Ql23LC/rA4QjRo1okePHukuhjEmIPn54OuarNba\ntg1Ho3NNZXWAMMZkt2++gbPPhjFj6rYer+cZE8UChDEmI5WVufaD/Hy49NJ0lyY7ZXUjtTEme+3a\n5Z6tcTk4FiCMMRnJrl8IngUIY0xGsusXgmcBwhiTkez6heBZI7UxJi1mz4aZM2u/vFUxBc8ChDEm\nLZ57Dlatgji3AklI374weHDyymQqswBhjEmLoiL3B//ii+kuiYkn0DYIERkmIitFZJWITIsx/S4R\nWeo9vhCRrb5p40TkS+8xLshyGmNSr7jYqofqu8AyCBHJAWYCZwCFwGIRma+qyyPzqOpU3/zXAgO8\n1+2AW4F8QIEPvWW3BFVeY0xqFRVBly7pLoWpSpAZxDHAKlVdo6p7gTnAyCrmHws85b0+C3hNVTd7\nQeE1YFiAZTXGpFhRkZ2BVN8FGSC6At/4hgu9cfsRkYOBHsAbNVlWRCaIyBIRWWI9thqTWSxA1H/1\npZF6DPCMqpbWZCFVfRB4ENw9qYMomDGmZp59Fr76qvr5tm2zNoj6LsgAsQ7o5hvO9cbFMgb4edSy\nQ6KWfSuJZTPGBKC4GC64ABK9R9dhhwVbHlM3QQaIxUAvEemB+8MfA1wcPZOI9AHaAu/5Rr8K/EFE\n2nrDZwI3B1hWY0wS7NzpgsP//A9MmFD1vA0aWAZR3wUWIFS1REQm4f7sc4BZqrpMRGYAS1R1vjfr\nGGCO+u4LqqqbReR2XJABmKGqm4MqqzH1yo4dMG2a+7fNMM12wiPACc9Cy0+StNJx4+C005K0sgD8\n4Q/JuWtRXfzoRzB9etJXK9lyv+b8/HxdsmRJuothTN0tXAinnw4HHZRx98Hcu8/dwrNjR2iZjAbo\nwkI47zyYOzcJKwtASQk0agRt2rhHuvTvD88/X6tFReRDVc2PNa2+NFIbYyLKytzz00/XrR+KNFj6\ngev64sXZcM45SVjh4YdXfB71UaRs118Pv/pVessSAOvN1Zj6JpLVN8i8n2fSu+Bu0CDxFu90yOB9\nlQjLIIxJk7Iy+P57V5O03wQAkZSXqTZ++KHi7m5ff+2ekxYgRDIjg8iQfVVT2Rn2jMkAkydD587w\n+ONREzLoqPT99117Q/fu7nH55W5869ZJeoMGDTIjQGTAvqoNyyCMSZO1ays/l8ugo9JIxnD77S7Y\nAbRrB716JekNRDKjiikD9lVtWIAwJk1273bPkXr7chl0VBop+09/Cnl5AbyBZRBplZ1bZUwGiPy5\n7hcgMqiKKfDbflojdVpl51YZkwHiBogMqmKKlD2wK6KtkTqtLEAYkyaRo+/Ic7kMOiqNlL1Zs4De\nwDKItMrOrTImA2RLBtGsWYD/j5ZBpJU1UhsTgDvvdD0fHHus67gO3MHmVVfBihVuOHILk3fegRNO\nqFh2yA/K74GfXtaAkj7wxBOQk5PS4sd0113wzDOVx61ZE/A9HSyDSKvs3Cpj0uyxx2DRInjooYpx\ne/bArFnu4rjmzWHoUCgogKOPdsORR9Mm7qh023Zh7tyKQJJujz0GX3xRuaxHHgmTJgX4ppZBpJVl\nEMYEwN++oOr+PyLjJk2C666rYuG5CmPgyvENePGWGG0UaVJc7ILanDkpfFM7zTWtsnOrjEmzSLtC\naSns3Vt5XLVVMt6fTrPmUmm5dEvLLULtQrm0sgBhTACKiioOKqMbo6v9k/X+dJo2b1BpuXQrLk7D\nDX4sg0ir7NwqY9JI1f2pd+zohmscICIZRIv6FSDSkkFYI3VaZedWGZNGe/e6//hIgIi+3qHao3Dv\nTydSxVQf2iBKStx2paWKKRMyiCytYrJGamPqYO9e+Mc/Kh/lR7q+7tTJPc+aBbm5Fae31jSDeO65\nGB36pdiePe45LVVMlkGkjQUIY+rg3/+GK6+MPe300930P/2pYlzjxtCtWzUr9f50Oh0otGgBjzzi\nHvXBIYek+A0tg0grCxDG1MHWre75nXfgsMMqxjds6O6JcO21FUff4G4xnWgGcUCbBmzcWD+qmKBi\nm1LKMoi0sgBhTB1E/ry7dIH27fef3rKle9SI76i0WbMA+znKBJZBpFV2hj1jUiTp92CGrD8qrRE7\nzTWtsnOrjEmRhM9MqoksPyqtEbtQLq0sQBhTB4HcD8EyiAqWQaRVoFslIsNEZKWIrBKRaXHmuVBE\nlovIMhF50je+VESWeo/5QZbTmHiKilznev7Hxo0VF8P98INreE5qb6tZ/qdTI9ZInVaBNVKLSA4w\nEzgDKAQWi8h8VV3um6cXcDNwgqpuEZFOvlXsUtX+QZXPmOps3equX4h1JfMtt8C998LOnXDggUl+\n4yyvtqgRa6ROqyDPYjoGWKWqawBEZA4wEljum2c8MFNVtwCo6vcBlseYGtmwwQWHyy+H/PyK8bfc\nAu+954JDQYGbnlSWQVSwDCKtggwQXYFvfMOFwOCoeXoDiMi7QA5wm6q+4k1rKiJLgBLgDlWdF/0G\nIjIBmADQvXv35JbehF4kcxg50j0i/vxnV9UEMGIEnHxykt/YMogKlkGkVbqvg2gI9AKGALnA2yLS\nV1W3Ager6joR6Qm8ISL/UdXV/oVV9UHgQYD8/Px6fJhhMlG8M5SaN3fZRaxpSWEZRAXLINIqyK1a\nB/g7Fcj1xvkVAvNVdZ+qfgV8gQsYqOo673kN8BYwIMCyGrOfeNc4tGhRcZe3QDqvswyigmUQaRVk\ngFgM9BKRHiLSGBgDRJ+NNA+XPSAiHXBVTmtEpK2INPGNP4HKbRfGBK6qDCLyH24ZRMDsNNe0qnar\nROQnIlLjrVfVEmAS8CrwOfBPVV0mIjNEZIQ326vAJhFZDrwJ3KCqm4DDgCUi8ok3/g7/2U/GpEJV\nGUSs10ljGUQFu1AurRJpg7gIuFtEngVmqeqKRFeuqguABVHjpvteK/AL7+GfZxHQN9H3MaY2rr8e\nPvoo/vRvv3XPKQ8QWX5UWiOWQaRVtQFCVX8qIgcAY4FHRESB2cBTqroj6AIaE5S//MVdw5CXF3t6\np04wcOD+1zlccAGsWwddu8JBBwVQsCz/06kRa6ROq4TOYlLV7SLyDNAMmAKMAm4QkXtV9S9BFtCY\nIETukDZ+PPzmNzVb9rzz3CMwWV5tUSPWSJ1WibRBjBCR53FnEjUCjlHVHwP9gF8GWzxjghFpgE75\nLTQTYRlEBcsg0iqRDGI0cJeqvu0fqarFIhLnXlrG1G+BdLKXLJZBVLAMIq0SCRC3AesjAyLSDDhQ\nVdeq6sKgCmZMkAK5j0OyWAZRwTKItEpkq54G/CG81BtnTMYK5D4OyWIZRAXLINIqkQyioarujQyo\n6l7vwjdjMsrevTB3rgsOX33lxtXrDCJL/3RqxE5zTatEAsRGERmhqvMBRGQk8EOwxTIm+V57DS67\nrGK4QQOol308qlpwiLAL5dIqkQBxDfCEiPwVEFwPrZdVvYgx9c+2be75nXfgRz+CZs2gTZv0limm\nsrKsPSKtMcsg0iqRC+VWA8eKSEtveGfgpTImAJGG6bw86Nw5rUWpmmrW/uHUmDVSp1VCF8qJyDnA\nEbh7NACgqjMCLJcxSVevr33wKyvL2iqLGrNG6rRK5EK5B3D9MV2Lq2K6ADg44HIZk3T1+toHP8sg\nKlgGkVaJbNXxqnoZsEVVfwsch3cnOGMySVER5ORA4/p+Dp5lEBUsg0irRKqYdnvPxSLSBdgE1Oca\nXBNSpaXuPtHxbNnisod6/1u2DKKCZRBplUiAeEFE2gD/L/ARoMBDgZbKmFoYPhxeeaXqebp2TU1Z\n6sQyiAqWQaRVlQHCu1HQQu8e0c+KyItAU1XdlpLSGVMDK1ZAfj5cfHH8efr3T8IbffstLAywl5ll\ny7L2iLTGGjRwVzg+/ni6SxLbhx+65yzdX1UGCFUtE5GZePeDVtU9wJ5UFMyYmiouhkGDYOrUgN/o\nttvgoYCT6EMOCXb9meLAAwrVepwAABYJSURBVGHfvspXONY3DRpA+/bpLkUgEqliWigio4HnvDvA\nGVMvFRWl6BTWXbugWzd4883g3qNTp+DWnUl++UsYPbp+VzMdcAB07JjuUgQikQBxNe6WoCUisht3\nqquq6gGBlsyYGlB1GURKAkRZmTsV6kc/SsGbhZwI9OiR7lKEViJXUrdKRUGMqYvdu12QSMk1DtYV\nhgmJagOEiJwca3z0DYSMSaeUXiVtp6GakEikiukG3+umwDHAh8BpgZTIGJ/77oMXXqh+vt3e1Top\nyyCy9LRGY/wSqWL6iX9YRLoBdwdWImN8HngACguhV6/q5z3pJDjhhODLZBmECYuEOuuLUggclsiM\nIjIMuAfIAf6uqnfEmOdC3G1NFfhEVS/2xo8Dfu3N9jtVfbQWZTUZrqgIzj4b/vGPdJfExzIIExKJ\ntEH8BffnDa7vpv64K6qrWy4HmAmcgQsqi0Vkvqou983TC7gZOEFVt4hIJ298O+BWIN977w+9ZbfU\nZONM5kvZqas1YRmECYlEMoglvtclwFOq+m4Cyx0DrFLVNQAiMgcYCSz3zTMemBn541fV773xZwGv\nqepmb9nXgGHAUwm8r8kixcX1sPdVyyBMSCQSIJ4BdqtqKbjMQESaq2pxNct1xd19LqIQGBw1T29v\nne/iqqFuU9VX4iy7Xy86IjIBmADQvV7eO9LUhaplEMakUyLf8oVAM99wM+D1JL1/Q6AXMAQYCzzk\ndQyYEFV9UFXzVTW/Y5ZeyRhme/a4g/V6FyAsgzAhkUgG0dR/m1FV3SkiiST964BuvuFcb5xfIfC+\nqu4DvhKRL3ABYx0uaPiXfSuB9zT1zHvvwcqVtVs2cm1DvaxisgzChEAiAaJIRAaq6kcAIjII2JXA\ncouBXiLSA/eHPwaI7mdzHi5zmC0iHXBVTmuA1cAfRKStN9+ZuMZsk2HOOcfdh6EuDq5v9y+0KiYT\nEokEiCnA0yLyLa4fpoNwtyCtkqqWiMgk4FVc+8IsVV0mIjOAJao635t2pogsB0qBG1R1E4CI3I4L\nMgAzIg3WJnOowtatMHly7XtYbdwYunRJbrnqzKqYTEgkcqHcYhHpAxzqjVrpVQlVS1UXAAuixk33\nvVZcR4C/iLHsLGBWIu9j6qddu1yQ6NIF8vLSXZoksgzChES133IR+TnQQlU/U9XPgJYi8rPgi2Yy\nXUr7R0olyyBMSCRyGDTeu6McAN41C+ODK5LJFkVF7rneNTLXlWUQJiQS+ZbniFQcLnlXSDcOrkgm\nW0QChGUQxmSmRBqpXwHmisjfvOGrgZeDK5LJFtu3u+esCxCWQZiQSCRA3IS7Wvkab/hT3JlMxsT1\n/fdw3HHudatsu+WUZRAmJKo9DFLVMuB9YC2uf6XTgM+DLZbJdP/9r3s+66yKQJE17EI5ExJxMwgR\n6Y27iG0s8AMwF0BVT01N0Uwmi5zBdMMN7lqGrKJqGYQJhaqqmFYA7wDDVXUVgIjU8nInEzZZewYT\nWAZhQqOqb/l5wHrgTRF5SESG4q6kNqZaWXsGE1gjtQmNuN9yVZ2nqmOAPsCbuC43OonI/SJyZqoK\naDJT1l4kB9ZIbUIjkUbqIlV90rs3dS7wMe7MJmPiyuoqJssgTEjU6J7U3lXUD3oPY/bz/fdw3XXw\nn/+4YcsgjMlcdhhkkur//g/mzIGSEhg9OguvgQDLIExo1CiDMKY6kbaHF1+EQw5Jb1kCYxmECQk7\nDDJJldVtDxF2mqsJCfuWm6TK6tNbI+xCORMSFiBMUtXb+0gnk2UQJiTsW26SqqgIGjVyj6xljdQm\nJKyRugrr17vbZvbsWXl8SQm89FLF0XI8xx4LPXoEV750+PxzWLp0//Ht28OZZ8LixdCkSerLlVLW\nSG1CwgJEFbp0cc+qlce/9Race271y591FrzyStKLlVYXXxw7QACsXg3vvQe7d6e2TClnGYQJCQsQ\ntbB5s3t+4QXo1Sv2PJdfDjt2pK5MqbJ5swuOd9xRMe7NN2HiRDettBSuvDJ95UsJyyBMSFiAqIVI\n1dIRR8SvQmrdGjZtSl2ZUqW4GDp3hkMPrRhXWOiei4pgzx44KNtvJ2UZhAkJ+5bXQiKncjZuDPv2\npaY8qVRUtP92R4YjATGrT3EFyyBMaFiASEB0G0QiPZU2agR79wZXpnQoK3ON9vECxMaN7jmrT3EF\nO83VhEag33IRGSYiK0VklYhMizG9QEQ2ishS73GVb1qpb/z8IMtZneg/+kgG0axZ/GUaNcq+DGLX\nLvccHQAiw5EAkfUZhF0oZ0IisDYIEckBZgJnAIXAYhGZr6rLo2adq6qTYqxil6r2D6p8NbFjR+Xz\n+ouKXHCo6iAy06qYVPfPlKLt3Ome42UQGzbEnp51LIMwIRFkI/UxwCpVXQMgInOAkUB0gKj3OnZM\nbJxfJlUxrV8PffrA9u2Jzd+yZezhv/419vSsY43UJiSCDBBdgW98w4XA4BjzjRaRk4EvgKmqGlmm\nqYgsAUqAO1R1XvSCIjIBmADQvXv3ZJa9/M+9VauK01XHj4fcXPd6wICql8+kKqa1a11wuPTS6ntg\nbdIERo6sPK5lS3jsMfjqK5c9nHZaYEWtH6yR2oREuk9zfQF4SlX3iMjVwKNA5O/lYFVdJyI9gTdE\n5D+qutq/sKqW37woPz+/mgqSmom0M8yYAVOnuteTJsFRRyW2fCYFiMi2jh8PJ51Uu3VcemnyylPv\nWQZhQiLIb/k6oJtvONcbV05VN6nqHm/w78Ag37R13vMa4C2gmmP25IrV6VxNzs5p3DhzqphC0cFe\nMlkGYUIiyACxGOglIj1EpDEwBqh0NpKIdPYNjgA+98a3FZEm3usOwAmkuO0i1rUONWl8zcQMIusb\nl5PFMggTEoFVMalqiYhMAl4FcoBZqrpMRGYAS1R1PjBZREbg2hk2AwXe4ocBfxORMlwQuyPG2U+B\ninVUXZsAkQlnRCZyXYfxsQzChESgbRCqugBYEDVuuu/1zcDNMZZbBPQNsmzViXVUXdMqJnA9v9b3\nrq9DcRe4ZLLTXE1IpLuROv1KS+H77/cbve+/cBDQZpd7Bmi4MfHVRpZ77TE4++xkFDQYL78M8+53\nZW2xHYhuN+nQoeoI9/337jMMk9JSyyBMKFiA2Ly5ol9vnyHAeoBzvWeA/WeLa5L34KpqZkyzH3sP\nAHrGmOEnP4H5cS5knz0brrgimILVd02bprsExgTOAkTLlvDAA/uNfv99mDUbZvwWDjywdqu+4w5X\nbTN5ch3LGKA//gmaNIYpU2JMnDkT1q2LMcETmTZzJuTkBFK+eknEBU5jspwFiGbN4Oqr9xv9aQN4\ncDb85grcCbq18Ooc1wYxef/V1xtP3ue6LJ8Sq4wvvQTffBNjgqeszD1ffXW4AoQxIWEtbXEk49TP\nFi2qvy1puhUXV9E43aBB1R00RaZZg60xWcl+2XEkK0BE1lNfxbq/QzmRiiwhlsg0a7A1JitZgIij\nuNjVmtTlFNXmzUOQQVhwMCZrhb4NYs8eV9Ue7dNP3ZF1Xf7/WrSArVvhueegf3/oGessoSTbtAne\nfrvif/3EE6FTJ/d6+XJYsaLy/HXOICxAGJO1Qh8gtm+H0aNjT+vTp27r7tLF9QQ7erT7o37nnbqt\nLxG//S385S8VwwUF7mxUgOHDXY+rscoZUyIZhLU/GJO1Qh8g2raFTz6JPS23lmcvRdx0E4wYAb/4\nBRQW1m1didq82ZX7pZfgoovcsH/aJZfAjTdWjMvJgcMOi7MyyyCMCbXQB4iGDRPvwrumcnLgyCOh\nc2f44otg3iNaUZELekcdBe3aVW4kLyqC7t1rsL2WQRgTavbrToFUns3kb1Pwv+/eve6ajBqdlWUZ\nhDGhZgEiBVIZIPxnJfnPoqrVPR8aNKg+QFgGYUzWsl93CjRvDrt2Vf1fmyzxMohaXddhVUzGhJr9\nulMg8qe8a1fw75XUDMKqmIwJtdA3UqdCq1buuXXr4P9PS0rg5JMr3nf9enexXyQRiJQlIZZBGBNq\nFiBS4Pzz3W0TUnGPahEYO9a9njjRZS+RJKB5cxg6tIYrswzCmNCyAJECHTvCrbem/n179YLbb6/D\nCiyDMCbU7Ndt4rMMwphQswBh4rMMwphQs1+3ic8yCGNCzQKEic8ulDMm1OzXbeKzKiZjQs1+3SY+\nq2IyJtQCDRAiMkxEVorIKhGZFmN6gYhsFJGl3uMq37RxIvKl9xgXZDlNHJZBGBNqgV0HISI5wEzg\nDKAQWCwi81V1edSsc1V1UtSy7YBbgXxAgQ+9ZbcEVV4Tg2UQxoRakId/xwCrVHWNqu4F5gAjE1z2\nLOA1Vd3sBYXXgGEBldPEYxmEMaEW5K+7K/CNb7jQGxdttIh8KiLPiEi3miwrIhNEZImILNm4cWOy\nym0iLIMwJtTSffj3ApCnqkfhsoRHa7Kwqj6oqvmqmt+xY8dAChhqlkEYE2pB/rrXAd18w7neuHKq\nuklV93iDfwcGJbqsSQHLIIwJtSADxGKgl4j0EJHGwBhgvn8GEensGxwBfO69fhU4U0Taikhb4Exv\nnEklu1DOmFAL7CwmVS0RkUm4P/YcYJaqLhORGcASVZ0PTBaREUAJsBko8JbdLCK344IMwAxV3RxU\nWU0cVsVkTKgF2t23qi4AFkSNm+57fTNwc5xlZwGzgiyfqYZVMRkTanb4Z+KzDMKYULNft4nPMghj\nQs0ChIkvkh3EyyIsgzAmq9mv28QXyQ7iBQjLIIzJahYgTHyWQRgTavbrNvFFsoN47RCWQRiT1SxA\nmPgi2UFVAcIyCGOylv26TXxWxWRMqNmv28RnVUzGhJoFCBOfZRDGhJr9uk18lkEYE2oWIEx8lkEY\nE2r26zbxWQZhTKhZgDDxWQZhTKjZr9vEZxmEMaFmAcLEZxfKGRNq9us28VkVkzGhZr9uE59VMRkT\naoHectRkuEh2cPLJ0KjR/tPXrHHTjDFZyQKEie/MM+Hii2Hv3tjTDz/cTTfGZCULECa+Hj3giSfS\nXQpjTJpYG4QxxpiYLEAYY4yJKdAAISLDRGSliKwSkWlVzDdaRFRE8r3hPBHZJSJLvccDQZbTGGPM\n/gJrgxCRHGAmcAZQCCwWkfmqujxqvlbAdcD7UatYrar9gyqfMcaYqgWZQRwDrFLVNaq6F5gDjIwx\n3+3AH4HdAZbFGGNMDQUZILoC3/iGC71x5URkINBNVV+KsXwPEflYRP4tIifFegMRmSAiS0RkycaN\nG5NWcGOMMWlspBaRBsCfgV/GmLwe6K6qA4BfAE+KyAHRM6nqg6qar6r5HTt2DLbAxhgTMkEGiHVA\nN99wrjcuohVwJPCWiKwFjgXmi0i+qu5R1U0AqvohsBroHWBZjTHGRBGN1xFbXVcs0hD4AhiKCwyL\ngYtVdVmc+d8CrlfVJSLSEdisqqUi0hN4B+irqpureL+NwNe1LG4H4IdaLpupbJvDwbY5HOqyzQer\naswqmMDOYlLVEhGZBLwK5ACzVHWZiMwAlqjq/CoWPxmYISL7gDLgmqqCg/d+ta5jEpElqppf2+Uz\nkW1zONg2h0NQ2xxoVxuqugBYEDVuepx5h/hePws8G2TZjDHGVM2upDbGGBOTBQjnwXQXIA1sm8PB\ntjkcAtnmwBqpjTHGZDbLIIwxxsRkAcIYY0xMoQ8QifY4m2lEpJuIvCkiy0VkmYhc541vJyKviciX\n3nNbb7yIyL3e5/Cp1w1KxhGRHK+Llhe94R4i8r63XXNFpLE3vok3vMqbnpfOcteWiLQRkWdEZIWI\nfC4ix4VgH0/1vtOfichTItI0G/eziMwSke9F5DPfuBrvWxEZ583/pYiMq0kZQh0gfD3O/hg4HBgr\nIoent1RJUwL8UlUPx12l/nNv26YBC1W1F7DQGwb3GfTyHhOA+1Nf5KS4DvjcN/xH4C5VPQTYAlzp\njb8S2OKNv8ubLxPdA7yiqn2Afrhtz9p9LCJdgclAvqoeibvGagzZuZ8fAYZFjavRvhWRdsCtwGBc\nB6q3RoJKQlQ1tA/gOOBV3/DNwM3pLldA2/q/uK7XVwKdvXGdgZXe678BY33zl8+XKQ9cdy4LgdOA\nFwHBXV3aMHp/4y7gPM573dCbT9K9DTXc3tbAV9HlzvJ9HOkEtJ23314EzsrW/QzkAZ/Vdt8CY4G/\n+cZXmq+6R6gzCBLocTYbeGn1ANw9Nw5U1fXepO+AA73X2fBZ3A3ciLv6HqA9sFVVS7xh/zaVb683\nfZs3fybpAWwEZnvVan8XkRZk8T5W1XXAncB/cZ16bgM+JLv3s19N922d9nnYA0TWE5GWuKvSp6jq\ndv80dYcUWXGes4gMB75X17ljWDQEBgL3q+v5uIiKKgcgu/YxgFc9MhIXHLsALdi/GiYUUrFvwx4g\nqutxNqOJSCNccHhCVZ/zRm8Qkc7e9M7A9974TP8sTgBGeD0Dz8FVM90DtPE6joTK21S+vd701sCm\nVBY4CQqBQlWN3I3xGVzAyNZ9DHA68JWqblTVfcBzuH2fzfvZr6b7tk77POwBYjHQyzsDojGusauq\nTgQzhogI8DDwuar+2TdpPhA5k2Ecrm0iMv4y72yIY4FtvlS23lPVm1U1V1XzcPvxDVW9BHgTON+b\nLXp7I5/D+d78GXWkrarfAd+IyKHeqKHAcrJ0H3v+CxwrIs2973hkm7N2P0ep6b59FThTRNp62deZ\n3rjEpLsRJt0P4Gxct+SrgV+luzxJ3K4Tcennp8BS73E2rv51IfAl8DrQzptfcGd0rQb+gztLJO3b\nUcttHwK86L3uCXwArAKeBpp445t6w6u86T3TXe5abmt/YIm3n+cBbbN9HwO/BVYAnwGPA02ycT8D\nT+HaWfbhssUra7NvgSu87V8FXF6TMlhXG8YYY2IKexWTMcaYOCxAGGOMickChDHGmJgsQBhjjInJ\nAoQxxpiYLEAYUw0RKRWRpb5H0nr9FZE8f2+dxtQnDaufxZjQ26Wq/dNdCGNSzTIIY2pJRNaKyJ9E\n5D8i8oGIHOKNzxORN7x++ReKSHdv/IEi8ryIfOI9jvdWlSMiD3n3OPiXiDTz5p8s7n4en4rInDRt\npgkxCxDGVK9ZVBXTRb5p21S1L/BXXG+yAH8BHlXVo4AngHu98fcC/1bVfrg+k5Z543sBM1X1CGAr\nMNobPw0Y4K3nmqA2zph47EpqY6ohIjtVtWWM8WuB01R1jdcx4neq2l5EfsD12b/PG79eVTuIyEYg\nV1X3+NaRB7ym7gYwiMhNQCNV/Z2IvALsxHWhMU9Vdwa8qcZUYhmEMXWjcV7XxB7f61Iq2gbPwfWv\nMxBY7Out1JiUsABhTN1c5Ht+z3u9CNejLMAlwDve64XARCi/d3breCsVkQZAN1V9E7gJ1031flmM\nMUGyIxJjqtdMRJb6hl9R1ciprm1F5FNcFjDWG3ct7i5vN+Du+Ha5N/464EERuRKXKUzE9dYZSw7w\nDy+ICHCvqm5N2hYZkwBrgzCmlrw2iHxV/SHdZTEmCFbFZIwxJibLIIwxxsRkGYQxxpiYLEAYY4yJ\nyQKEMcaYmCxAGGOMickChDHGmJj+fzpnnMY9VZv0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "701ca901-d9a2-48e2-9a42-64224a1dc95e",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_labels, epochs= 200, batch_size=89, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "89/89 [==============================] - 0s 3ms/step - loss: 0.7704 - acc: 0.4944\n",
            "Epoch 2/200\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.7533 - acc: 0.5056\n",
            "Epoch 3/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.7378 - acc: 0.5393\n",
            "Epoch 4/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.7233 - acc: 0.5393\n",
            "Epoch 5/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.7105 - acc: 0.5169\n",
            "Epoch 6/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.6992 - acc: 0.5506\n",
            "Epoch 7/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.6894 - acc: 0.5506\n",
            "Epoch 8/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.6810 - acc: 0.5618\n",
            "Epoch 9/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.6730 - acc: 0.5843\n",
            "Epoch 10/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.6655 - acc: 0.6067\n",
            "Epoch 11/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.6590 - acc: 0.6404\n",
            "Epoch 12/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.6534 - acc: 0.6629\n",
            "Epoch 13/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.6487 - acc: 0.6629\n",
            "Epoch 14/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.6439 - acc: 0.6517\n",
            "Epoch 15/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.6391 - acc: 0.6517\n",
            "Epoch 16/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.6342 - acc: 0.6629\n",
            "Epoch 17/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.6290 - acc: 0.6742\n",
            "Epoch 18/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.6239 - acc: 0.6854\n",
            "Epoch 19/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.6187 - acc: 0.6854\n",
            "Epoch 20/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.6137 - acc: 0.7079\n",
            "Epoch 21/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.6088 - acc: 0.7191\n",
            "Epoch 22/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.6038 - acc: 0.7303\n",
            "Epoch 23/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.5988 - acc: 0.7303\n",
            "Epoch 24/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5938 - acc: 0.7416\n",
            "Epoch 25/200\n",
            "89/89 [==============================] - 0s 21us/step - loss: 0.5887 - acc: 0.7528\n",
            "Epoch 26/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.5837 - acc: 0.7640\n",
            "Epoch 27/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5791 - acc: 0.7640\n",
            "Epoch 28/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5746 - acc: 0.7640\n",
            "Epoch 29/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5699 - acc: 0.7640\n",
            "Epoch 30/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5652 - acc: 0.7528\n",
            "Epoch 31/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5606 - acc: 0.7528\n",
            "Epoch 32/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.5561 - acc: 0.7528\n",
            "Epoch 33/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5515 - acc: 0.7528\n",
            "Epoch 34/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5470 - acc: 0.7528\n",
            "Epoch 35/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5426 - acc: 0.7528\n",
            "Epoch 36/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5381 - acc: 0.7528\n",
            "Epoch 37/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.5334 - acc: 0.7528\n",
            "Epoch 38/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5288 - acc: 0.7528\n",
            "Epoch 39/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5244 - acc: 0.7528\n",
            "Epoch 40/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.5199 - acc: 0.7640\n",
            "Epoch 41/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5153 - acc: 0.7865\n",
            "Epoch 42/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.5107 - acc: 0.7865\n",
            "Epoch 43/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.5060 - acc: 0.7865\n",
            "Epoch 44/200\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.5014 - acc: 0.7865\n",
            "Epoch 45/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4970 - acc: 0.7865\n",
            "Epoch 46/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4926 - acc: 0.7865\n",
            "Epoch 47/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.4884 - acc: 0.7865\n",
            "Epoch 48/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4842 - acc: 0.7865\n",
            "Epoch 49/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4800 - acc: 0.7865\n",
            "Epoch 50/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4758 - acc: 0.7978\n",
            "Epoch 51/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4717 - acc: 0.7978\n",
            "Epoch 52/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.4676 - acc: 0.7978\n",
            "Epoch 53/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.4633 - acc: 0.7978\n",
            "Epoch 54/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.4591 - acc: 0.8090\n",
            "Epoch 55/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.4548 - acc: 0.8090\n",
            "Epoch 56/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.4506 - acc: 0.8090\n",
            "Epoch 57/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.4463 - acc: 0.8202\n",
            "Epoch 58/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4422 - acc: 0.8202\n",
            "Epoch 59/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.4380 - acc: 0.8202\n",
            "Epoch 60/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4338 - acc: 0.8315\n",
            "Epoch 61/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.4296 - acc: 0.8315\n",
            "Epoch 62/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.4253 - acc: 0.8315\n",
            "Epoch 63/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.4210 - acc: 0.8315\n",
            "Epoch 64/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.4167 - acc: 0.8315\n",
            "Epoch 65/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.4124 - acc: 0.8315\n",
            "Epoch 66/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.4083 - acc: 0.8315\n",
            "Epoch 67/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.4041 - acc: 0.8315\n",
            "Epoch 68/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.3999 - acc: 0.8427\n",
            "Epoch 69/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.3956 - acc: 0.8539\n",
            "Epoch 70/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.3914 - acc: 0.8652\n",
            "Epoch 71/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.3871 - acc: 0.8652\n",
            "Epoch 72/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.3827 - acc: 0.8652\n",
            "Epoch 73/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3784 - acc: 0.8652\n",
            "Epoch 74/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3740 - acc: 0.8652\n",
            "Epoch 75/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.3696 - acc: 0.8652\n",
            "Epoch 76/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.3651 - acc: 0.8652\n",
            "Epoch 77/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.3607 - acc: 0.8652\n",
            "Epoch 78/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3561 - acc: 0.8764\n",
            "Epoch 79/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.3516 - acc: 0.8764\n",
            "Epoch 80/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.3472 - acc: 0.8764\n",
            "Epoch 81/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.3429 - acc: 0.8764\n",
            "Epoch 82/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.3385 - acc: 0.8764\n",
            "Epoch 83/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.3342 - acc: 0.8764\n",
            "Epoch 84/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.3298 - acc: 0.8764\n",
            "Epoch 85/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.3254 - acc: 0.8764\n",
            "Epoch 86/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3210 - acc: 0.8764\n",
            "Epoch 87/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.3166 - acc: 0.8764\n",
            "Epoch 88/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.3123 - acc: 0.8764\n",
            "Epoch 89/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.3079 - acc: 0.8764\n",
            "Epoch 90/200\n",
            "89/89 [==============================] - 0s 21us/step - loss: 0.3037 - acc: 0.8764\n",
            "Epoch 91/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.2994 - acc: 0.8764\n",
            "Epoch 92/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2951 - acc: 0.8876\n",
            "Epoch 93/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.2909 - acc: 0.8989\n",
            "Epoch 94/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.2867 - acc: 0.8989\n",
            "Epoch 95/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.2824 - acc: 0.8989\n",
            "Epoch 96/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2780 - acc: 0.8989\n",
            "Epoch 97/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2736 - acc: 0.8989\n",
            "Epoch 98/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.2691 - acc: 0.8989\n",
            "Epoch 99/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.2647 - acc: 0.8989\n",
            "Epoch 100/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2604 - acc: 0.8989\n",
            "Epoch 101/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.2561 - acc: 0.8989\n",
            "Epoch 102/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2519 - acc: 0.8989\n",
            "Epoch 103/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.2478 - acc: 0.8989\n",
            "Epoch 104/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2438 - acc: 0.8989\n",
            "Epoch 105/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2398 - acc: 0.8989\n",
            "Epoch 106/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2359 - acc: 0.8989\n",
            "Epoch 107/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2321 - acc: 0.8989\n",
            "Epoch 108/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2282 - acc: 0.9101\n",
            "Epoch 109/200\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2244 - acc: 0.9101\n",
            "Epoch 110/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2207 - acc: 0.9101\n",
            "Epoch 111/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2169 - acc: 0.9213\n",
            "Epoch 112/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2133 - acc: 0.9213\n",
            "Epoch 113/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2096 - acc: 0.9213\n",
            "Epoch 114/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.2060 - acc: 0.9213\n",
            "Epoch 115/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.2024 - acc: 0.9326\n",
            "Epoch 116/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.1989 - acc: 0.9438\n",
            "Epoch 117/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1952 - acc: 0.9438\n",
            "Epoch 118/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.1918 - acc: 0.9438\n",
            "Epoch 119/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1883 - acc: 0.9438\n",
            "Epoch 120/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.1849 - acc: 0.9438\n",
            "Epoch 121/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1816 - acc: 0.9551\n",
            "Epoch 122/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.1781 - acc: 0.9551\n",
            "Epoch 123/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1750 - acc: 0.9551\n",
            "Epoch 124/200\n",
            "89/89 [==============================] - 0s 19us/step - loss: 0.1717 - acc: 0.9551\n",
            "Epoch 125/200\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1683 - acc: 0.9551\n",
            "Epoch 126/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.1651 - acc: 0.9551\n",
            "Epoch 127/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.1619 - acc: 0.9663\n",
            "Epoch 128/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1588 - acc: 0.9663\n",
            "Epoch 129/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1557 - acc: 0.9775\n",
            "Epoch 130/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1525 - acc: 0.9775\n",
            "Epoch 131/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1495 - acc: 0.9888\n",
            "Epoch 132/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1464 - acc: 0.9888\n",
            "Epoch 133/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1433 - acc: 0.9888\n",
            "Epoch 134/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1402 - acc: 0.9888\n",
            "Epoch 135/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1371 - acc: 0.9888\n",
            "Epoch 136/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.1341 - acc: 0.9888\n",
            "Epoch 137/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.1311 - acc: 0.9888\n",
            "Epoch 138/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.1282 - acc: 0.9888\n",
            "Epoch 139/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.1253 - acc: 0.9888\n",
            "Epoch 140/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1225 - acc: 0.9888\n",
            "Epoch 141/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1197 - acc: 0.9888\n",
            "Epoch 142/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1170 - acc: 0.9888\n",
            "Epoch 143/200\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.1143 - acc: 0.9888\n",
            "Epoch 144/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1117 - acc: 0.9888\n",
            "Epoch 145/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1091 - acc: 0.9888\n",
            "Epoch 146/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1066 - acc: 0.9888\n",
            "Epoch 147/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1040 - acc: 0.9888\n",
            "Epoch 148/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1016 - acc: 0.9888\n",
            "Epoch 149/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0993 - acc: 0.9888\n",
            "Epoch 150/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0968 - acc: 0.9888\n",
            "Epoch 151/200\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.0945 - acc: 0.9888\n",
            "Epoch 152/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0922 - acc: 1.0000\n",
            "Epoch 153/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0899 - acc: 1.0000\n",
            "Epoch 154/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0877 - acc: 1.0000\n",
            "Epoch 155/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0856 - acc: 1.0000\n",
            "Epoch 156/200\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0835 - acc: 1.0000\n",
            "Epoch 157/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0815 - acc: 1.0000\n",
            "Epoch 158/200\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0796 - acc: 1.0000\n",
            "Epoch 159/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0776 - acc: 1.0000\n",
            "Epoch 160/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0758 - acc: 1.0000\n",
            "Epoch 161/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0740 - acc: 1.0000\n",
            "Epoch 162/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0721 - acc: 1.0000\n",
            "Epoch 163/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0705 - acc: 1.0000\n",
            "Epoch 164/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0688 - acc: 1.0000\n",
            "Epoch 165/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0671 - acc: 1.0000\n",
            "Epoch 166/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0656 - acc: 1.0000\n",
            "Epoch 167/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0641 - acc: 1.0000\n",
            "Epoch 168/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0625 - acc: 1.0000\n",
            "Epoch 169/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0610 - acc: 1.0000\n",
            "Epoch 170/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0596 - acc: 1.0000\n",
            "Epoch 171/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0582 - acc: 1.0000\n",
            "Epoch 172/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0568 - acc: 1.0000\n",
            "Epoch 173/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0554 - acc: 1.0000\n",
            "Epoch 174/200\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0540 - acc: 1.0000\n",
            "Epoch 175/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0528 - acc: 1.0000\n",
            "Epoch 176/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.0515 - acc: 1.0000\n",
            "Epoch 177/200\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0503 - acc: 1.0000\n",
            "Epoch 178/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0491 - acc: 1.0000\n",
            "Epoch 179/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0480 - acc: 1.0000\n",
            "Epoch 180/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0469 - acc: 1.0000\n",
            "Epoch 181/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0458 - acc: 1.0000\n",
            "Epoch 182/200\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0448 - acc: 1.0000\n",
            "Epoch 183/200\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0437 - acc: 1.0000\n",
            "Epoch 184/200\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0428 - acc: 1.0000\n",
            "Epoch 185/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0418 - acc: 1.0000\n",
            "Epoch 186/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0409 - acc: 1.0000\n",
            "Epoch 187/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0400 - acc: 1.0000\n",
            "Epoch 188/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.0392 - acc: 1.0000\n",
            "Epoch 189/200\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0383 - acc: 1.0000\n",
            "Epoch 190/200\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0375 - acc: 1.0000\n",
            "Epoch 191/200\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.0367 - acc: 1.0000\n",
            "Epoch 192/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0360 - acc: 1.0000\n",
            "Epoch 193/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0352 - acc: 1.0000\n",
            "Epoch 194/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0345 - acc: 1.0000\n",
            "Epoch 195/200\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0338 - acc: 1.0000\n",
            "Epoch 196/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0331 - acc: 1.0000\n",
            "Epoch 197/200\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0325 - acc: 1.0000\n",
            "Epoch 198/200\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0318 - acc: 1.0000\n",
            "Epoch 199/200\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0312 - acc: 1.0000\n",
            "Epoch 200/200\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0306 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 8ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8be9e41d-1d32-46a0-e4e3-a5fa3ca8b2db",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ca654c72-89ec-463e-d599-505fd2bb292a",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6153846383094788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoQqtH0O67Et",
        "colab_type": "text"
      },
      "source": [
        "#Prova con k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NzvU-X5kHUSy"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dxhYL3vVHUS9",
        "colab": {}
      },
      "source": [
        "word_index={'large cell':0, 'squamous cell carcinoma':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "67D9Q0e5HUUA",
        "colab": {}
      },
      "source": [
        "train_big_labels_dec = [word_index[label] for label in y_train_big]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m0YUzrR4HUTu",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvArdJwhHUUL",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rp2OYvp9HUUU",
        "colab": {}
      },
      "source": [
        "one_hot_train_big_labels = to_categorical(train_big_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-UPhhg4KPGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(107,), kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #kernel_regularizer=regularizers.l2(l=0.001)\n",
        "  model.add(layers.Dense(20, activation='relu'))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLLDv0tiJ3VJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDAdk6a6IUue",
        "colab_type": "code",
        "outputId": "eccdd389-782f-4a00-9e5f-0767a1159d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_big.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(102, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "#skf.get_n_splits(X_train_big, one_hot_train_big_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "dec08f86-cff1-4368-f082-00b7aba2f201",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "for train_index, test_index in skf.split(X_train_big, train_big_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   8   9  13  14  15  16  17  19  21  22  23  24\n",
            "  25  26  27  28  29  32  33  34  35  36  37  38  39  40  41  43  45  46\n",
            "  47  48  49  51  53  54  58  59  60  61  62  63  64  65  66  67  70  71\n",
            "  72  73  74  77  78  80  82  88  89  94  95  96 100 101] TEST: [ 6  7 10 11 12 18 20 30 31 42 44 50 52 55 56 57 68 69 75 76 79 81 83 84\n",
            " 85 86 87 90 91 92 93 97 98 99]\n",
            "TRAIN: [  0   2   3   5   6   7   8   9  10  11  12  14  18  20  21  25  27  28\n",
            "  30  31  32  34  35  37  38  42  43  44  45  48  50  51  52  55  56  57\n",
            "  58  59  60  61  64  67  68  69  70  75  76  77  78  79  81  83  84  85\n",
            "  86  87  88  90  91  92  93  94  95  97  98  99 100 101] TEST: [ 1  4 13 15 16 17 19 22 23 24 26 29 33 36 39 40 41 46 47 49 53 54 62 63\n",
            " 65 66 71 72 73 74 80 82 89 96]\n",
            "TRAIN: [ 1  4  6  7 10 11 12 13 15 16 17 18 19 20 22 23 24 26 29 30 31 33 36 39\n",
            " 40 41 42 44 46 47 49 50 52 53 54 55 56 57 62 63 65 66 68 69 71 72 73 74\n",
            " 75 76 79 80 81 82 83 84 85 86 87 89 90 91 92 93 96 97 98 99] TEST: [  0   2   3   5   8   9  14  21  25  27  28  32  34  35  37  38  43  45\n",
            "  48  51  58  59  60  61  64  67  70  77  78  88  94  95 100 101]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "1dfa6744-395d-4e59-87ee-df72f8ab26e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-24105c2ab45b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_labels_dec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m125\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_labels_dec' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C' un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVSoMnogHVqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br4g2_GG9ePb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train_big)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "ada6e337-a06b-4f7e-8a53-478e51ec06ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand, train_big_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_big_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand[i] for i in val_index])\n",
        "  val_targets = np.array([train_big_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=68, callbacks=[red_lr])\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali  una lista lunga num_epochs,\n",
        "#ogni elemento pu essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-b84c56770d5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_val_loss_histories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_stand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_big_labels_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mpartial_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data_stand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mtesting\u001b[0m \u001b[0mset\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthat\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \"\"\"\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m    247\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 212\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [89, 102]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYxTUsmLOQX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "4c9d1c1f-ea89-4ab4-b79d-735df5cf841f"
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(X_train_big, train_big_labels_dec):\n",
        "  \n",
        "  X_train_big.to_numpy() \n",
        "\n",
        "  partial_train_data = np.array([X_train_big[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_big_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([X_train_big[i] for i in val_index])\n",
        "  val_targets = np.array([train_big_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "  #Z-score\n",
        "  scaler = StandardScaler()\n",
        "  partial_train_data_stand = scaler.fit_transform(partial_train_data)\n",
        "  val_data_stand = scaler.transform(val_data)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=7, svd_solver='full')\n",
        "  pca.fit(partial_train_data_stand)\n",
        "  partial_train_data_stand_pca = pca.transform(partial_train_data_stand)\n",
        "  val_data_stand_pca = pca.transform(val_data_stand)\n",
        "\n",
        "  #Z-score after PCA\n",
        "  scaler_2 = StandardScaler()\n",
        "  partial_train_data_stand_pca_stand = scaler_2.fit_transform(partial_train_data_stand_pca)\n",
        "  val_data_stand_pca_stand = scaler_2.transform(val_data_stand_pca)\n",
        "\n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data_stand_pca_stand, one_hot_partial_train_targets, validation_data=(val_data_stand_pca_stand, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=68)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali  una lista lunga num_epochs,\n",
        "#ogni elemento pu essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-ba59bcadbcec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mX_train_big\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mpartial_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_big\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mpartial_train_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_big_labels_dec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-ba59bcadbcec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mX_train_big\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mpartial_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_big\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mpartial_train_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_big_labels_dec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1xS2-BcJraQ",
        "colab_type": "code",
        "outputId": "be1256fc-76bf-49dd-ce81-61483874b366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "train_index"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   2,   4,   5,   8,  10,  11,  12,  13,  14,  17,  20,  21,\n",
              "        22,  23,  24,  25,  26,  27,  29,  30,  31,  33,  34,  37,  38,\n",
              "        39,  40,  41,  42,  43,  46,  47,  48,  49,  50,  55,  58,  60,\n",
              "        61,  62,  63,  64,  65,  67,  69,  70,  71,  73,  75,  76,  77,\n",
              "        79,  81,  82,  83,  84,  85,  87,  88,  89,  91,  92,  94,  96,\n",
              "        97,  98,  99, 100, 101, 103, 106, 107, 108, 110, 115, 116, 117,\n",
              "       118, 119, 121, 122, 124, 126, 127, 129, 130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "38274655-053c-4133-a849-d2874bbf383f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'lr'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBgjbHkBr4Ot",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "80684a3f-f7d6-4cbd-93e9-238f83d26c42"
      },
      "source": [
        "train_big_labels_dec"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "614a53f1-49f0-4e81-f49a-10ec1c5ba9e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "76544e35-39db-4515-c224-c5b9c678f8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbc303b74e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3deZwU9Z3/8deHYWTkkFOUU0BRbgGH\nS25QFxQRTzCiksSYaFyTTXQlycZr466/XdcYIuqi0bCuBo0JrgceURFQQTlEBCSKHHKJHHLJIcfn\n98e3mu4ZZoYBpqd6pt/Px6Mf3V1dU/2pbqh317e+9S1zd0REJHtVibsAERGJl4JARCTLKQhERLKc\ngkBEJMspCEREspyCQEQkyykIpEyZ2Stmdm1ZzxsnM1thZuekYbluZqdFjx8xs1+XZt6jeJ+rzOz1\no62zhOUOMLPVZb1cKX9V4y5A4mdmO1KeVgf2APuj5z9096dKuyx3H5qOeSs7d/9RWSzHzFoAy4Fc\nd98XLfspoNTfoWQfBYHg7jUTj81sBXCdu79ReD4zq5rYuIhI5aGmISlWYtffzG4zsy+BJ8ysrpm9\nZGYbzOzr6HHTlL9528yuix6PMbN3zOy+aN7lZjb0KOdtaWbTzWy7mb1hZuPN7H+Lqbs0Nf6rmb0b\nLe91M2uQ8vrVZrbSzDaZ2a9K+Hx6mNmXZpaTMu1iM1sQPe5uZjPNbIuZrTOzB83suGKW9Ucz+03K\n81ujv1lrZt8rNO8FZvahmW0zs1VmdmfKy9Oj+y1mtsPMeiU+25S/P9vMZpvZ1uj+7NJ+NiUxs7bR\n328xs0VmNjzltfPNbHG0zDVmdks0vUH0/Wwxs81mNsPMtF0qZ/rA5XBOBuoBpwDXE/7NPBE9bw7s\nAh4s4e97AH8HGgD/AfzBzOwo5n0a+ACoD9wJXF3Ce5amxu8A3wUaAscBiQ1TO+DhaPmNo/drShHc\n/X3gG2BQoeU+HT3eD/xTtD69gMHAjSXUTVTDkKiec4HWQOHjE98A1wB1gAuAG8xsRPRav+i+jrvX\ndPeZhZZdD3gZGBet2/3Ay2ZWv9A6HPLZHKbmXOBF4PXo7/4ReMrMzohm+QOhmbEW0AF4K5r+c2A1\ncCJwEvBLQOPelDMFgRzOAeAOd9/j7rvcfZO7/8Xdd7r7duAeoH8Jf7/S3R919/3ARKAR4T98qec1\ns+ZAN+B2d//W3d8BXijuDUtZ4xPu/qm77wKeBTpH0y8DXnL36e6+B/h19BkU50/AlQBmVgs4P5qG\nu89191nuvs/dVwD/XUQdRbkiqm+hu39DCL7U9Xvb3T929wPuviB6v9IsF0JwfObuT0Z1/QlYAlyY\nMk9xn01JegI1gXuj7+gt4CWizwbYC7QzsxPc/Wt3n5cyvRFwirvvdfcZrgHQyp2CQA5ng7vvTjwx\ns+pm9t9R08k2QlNEndTmkUK+TDxw953Rw5pHOG9jYHPKNIBVxRVcyhq/THm8M6WmxqnLjjbEm4p7\nL8Kv/0vMrBpwCTDP3VdGdZweNXt8GdXxb4S9g8MpUAOwstD69TCzqVHT11bgR6VcbmLZKwtNWwk0\nSXle3Gdz2JrdPTU0U5d7KSEkV5rZNDPrFU3/T2Ap8LqZLTOzsaVbDSlLCgI5nMK/zn4OnAH0cPcT\nSDZFFNfcUxbWAfXMrHrKtGYlzH8sNa5LXXb0nvWLm9ndFxM2eEMp2CwEoYlpCdA6quOXR1MDoXkr\n1dOEPaJm7l4beCRluYf7Nb2W0GSWqjmwphR1HW65zQq17x9crrvPdveLCM1GzxP2NHD37e7+c3dv\nBQwHfmZmg4+xFjlCCgI5UrUIbe5bovbmO9L9htEv7DnAnWZ2XPRr8sIS/uRYanwOGGZmfaIDu3dz\n+P8nTwM/IQTOnwvVsQ3YYWZtgBtKWcOzwBgzaxcFUeH6axH2kHabWXdCACVsIDRltSpm2VOA083s\nO2ZW1cxGAu0IzTjH4n3C3sM/m1mumQ0gfEeTou/sKjOr7e57CZ/JAQAzG2Zmp0XHgrYSjquU1BQn\naaAgkCP1AHA8sBGYBbxaTu97FeGA6ybgN8AzhPMdinLUNbr7IuDHhI37OuBrwsHMkiTa6N9y940p\n028hbKS3A49GNZemhleidXiL0GzyVqFZbgTuNrPtwO1Ev66jv91JOCbybtQTp2ehZW8ChhH2mjYB\n/wwMK1T3EXP3bwkb/qGEz/0h4Bp3XxLNcjWwImoi+xHh+4RwMPwNYAcwE3jI3aceSy1y5EzHZaQi\nMrNngCXunvY9EpHKTnsEUiGYWTczO9XMqkTdKy8itDWLyDHSmcVSUZwM/JVw4HY1cIO7fxhvSSKV\ng5qGRESynJqGRESyXIVrGmrQoIG3aNEi7jJERCqUuXPnbnT3E4t6rcIFQYsWLZgzZ07cZYiIVChm\nVviM8oPUNCQikuUUBCIiWU5BICKS5SrcMYKi7N27l9WrV7N79+7DzyyxysvLo2nTpuTm5sZdiohE\nKkUQrF69mlq1atGiRQuKv+aJxM3d2bRpE6tXr6Zly5ZxlyMikUrRNLR7927q16+vEMhwZkb9+vW1\n5yaSYSpFEAAKgQpC35NI5qkUTUMiIpXWl1/C3LnhduGF0KVLmb9FpdkjiNOWLVt46KGHjupvzz//\nfLZs2VLiPLfffjtvvPHGUS2/sBYtWrBx4zENPS8i6eAOa9bAiy/CnXeGjX7jxtCoEQwbFqbNmpWW\nt07rHkE0XPDvgBzgMXe/t9DrpwCPAycCm4HR7n64i4BknEQQ3HjjjYe8tm/fPqpWLf5jnjJlymGX\nf/fddx9TfSKSYdxh7VqYMyf5a3/uXFi/PrxepQq0aQPnnANnnRVuZ54JtWqlpZy07RFEFwofT7hi\nUTvgSjNrV2i2+4D/cfdOhEsC/nu66kmnsWPH8vnnn9O5c2duvfVW3n77bfr27cvw4cNp1y6s8ogR\nIzjrrLNo3749EyZMOPi3iV/oK1asoG3btvzgBz+gffv2nHfeeezatQuAMWPG8Nxzzx2c/4477qBr\n16507NiRJUvCBaA2bNjAueeeS/v27bnuuus45ZRTDvvL//7776dDhw506NCBBx54AIBvvvmGCy64\ngDPPPJMOHTrwzDPPHFzHdu3a0alTJ2655Zay/QBFKrv16+Hll+Guu5K/9Js2hREj4J574IsvYMgQ\nGDcO3n0Xtm2DRYvgf/4HfvIT6NMnbSEA6d0j6A4sdfdlAGY2iXAxkcUp87QDfhY9nkoZXGjkpz+F\n+fOPdSkFde4M0XaySPfeey8LFy5kfvTGb7/9NvPmzWPhwoUHu0k+/vjj1KtXj127dtGtWzcuvfRS\n6tcveE30zz77jD/96U88+uijXHHFFfzlL39h9OjRh7xfgwYNmDdvHg899BD33Xcfjz32GHfddReD\nBg3iF7/4Ba+++ip/+MMfSlynuXPn8sQTT/D+++/j7vTo0YP+/fuzbNkyGjduzMsvvwzA1q1b2bRp\nE5MnT2bJkiWY2WGbskSy2pYt4Zf+nDkwe3a4rVoVXjODtm3hvPOgW7fkL/3q1WMtOZ1B0ARYlfJ8\nNdCj0DwfAZcQmo8uBmqZWf3ouqoHmdn1wPUAzZs3T1vBZal79+4F+sqPGzeOyZMnA7Bq1So+++yz\nQ4KgZcuWdO7cGYCzzjqLFStWFLnsSy655OA8f/3rXwF45513Di5/yJAh1K1bt8T63nnnHS6++GJq\n1KhxcJkzZsxgyJAh/PznP+e2225j2LBh9O3bl3379pGXl8f3v/99hg0bxrBhw47w0xCppL75Jvzy\nTGzwZ8+Gzz5Lvn7qqXD22WGj360bdO0KNWvGV28x4u41dAvwoJmNAaYDa4D9hWdy9wnABID8/PwS\nr6RT0i/38pTYwELYQ3jjjTeYOXMm1atXZ8CAAUX2pa9WrdrBxzk5OQebhoqbLycnh3379pVp3aef\nfjrz5s1jypQp/Mu//AuDBw/m9ttv54MPPuDNN9/kueee48EHH+SttwpfT12kkvv2W/j444Ib/UWL\n4MCB8HqTJpCfD9deGzb6+flQr168NZdSOoNgDdAs5XnTaNpB7r6WsEeAmdUELnX3CtfuUKtWLbZv\n317s61u3bqVu3bpUr16dJUuWMCsNR/579+7Ns88+y2233cbrr7/O119/XeL8ffv2ZcyYMYwdOxZ3\nZ/LkyTz55JOsXbuWevXqMXr0aOrUqcNjjz3Gjh072LlzJ+effz69e/emVatWZV6/SEbZvx+WLCm4\n0f/ooxAGEDbw3brBRRclN/qNG8db8zFIZxDMBlqbWUtCAIwCvpM6g5k1ADa7+wHgF4QeRBVO/fr1\n6d27Nx06dGDo0KFccMEFBV4fMmQIjzzyCG3btuWMM86gZ8+eZV7DHXfcwZVXXsmTTz5Jr169OPnk\nk6lVwsGlrl27MmbMGLp37w7AddddR5cuXXjttde49dZbqVKlCrm5uTz88MNs376diy66iN27d+Pu\n3H///WVev0isvvoKpk8P3TNnz4Z582DHjvBazZqhLf/mm5Mb/ZYtQ3t/JZHWaxab2fnAA4Tuo4+7\n+z1mdjcwx91fMLPLCD2FnNA09GN331PSMvPz873whWk++eQT2rZtm5Z1qCj27NlDTk4OVatWZebM\nmdxwww0HD15nGn1fEruvvoJp0+Dtt8P9okVherVqoXdIok0/Px/OOANycmIttyyY2Vx3zy/qtbQe\nI3D3KcCUQtNuT3n8HPBcOmvIFl988QVXXHEFBw4c4LjjjuPRRx+NuySRzOAOy5aFX/ozZoSN/+Ko\n82KNGqFr5ujRMGBAOJh73HFxVhuLuA8WSxlp3bo1H374YdxliMRv50744AN4551w++ADSBwzq1kz\nbPivuSa54deQ6AoCEanA3EMf/VmzYObMcJs7F/btC2347dvDZZeFJp5u3aBjRyjhTP9spU9ERCqO\nXbvCgdzERn/WrDBUA0BeXtjg//zn0Ldv6L9/mPNpJFAQiEhmcoeVK5Mb/Jkzw8lbe/eG11u2DM07\nPXtCr17hDF018xwVBYGIZIaNG8OGPvGLf9asMAQzhCEYunWDn/0sbPR79oSTToq33kpEw1DHpGZ0\nmvnatWu57LLLipxnwIABFO4qW9gDDzzAzp07Dz4vzbDWpXHnnXdy3333HfNyRA5x4AB8/jn85S/w\n61+HQdiaNYMTT4Rzz4XbboOFC8PIm+PHh2DYujX09rn33nASl0KgTGmPIGaNGzc+OLLo0XjggQcY\nPXo01aNBq0ozrLVIudm9O/TRnz8/efvoI0iciZ+TEwZhGzAg9N/v3Dk08TRoEGvZ2UZ7BGVg7Nix\njB8//uDzxK/pHTt2MHjw4INDRv/f//3fIX+7YsUKOnToAMCuXbsYNWoUbdu25eKLLy4w1tANN9xA\nfn4+7du354477gDCQHZr165l4MCBDBw4ECh44Zmihpkuabjr4syfP5+ePXvSqVMnLr744oPDV4wb\nN+7g0NSjRo0CYNq0aXTu3JnOnTvTpUuXEofekEpm0yZ46y24//7QPbNTpzB0cn4+XHcd/PGPod3/\nmmvg0UfD6Jw7doTxe558MhzkHTxYIRCDyrdHEMM41CNHjuSnP/0pP/7xjwF49tlnee2118jLy2Py\n5MmccMIJbNy4kZ49ezJ8+PBir9v78MMPU716dT755BMWLFhA165dD752zz33UK9ePfbv38/gwYNZ\nsGABN998M/fffz9Tp06lQaH/PMUNM123bt1SD3edcM011/D73/+e/v37c/vtt3PXXXfxwAMPcO+9\n97J8+XKqVat2sDnqvvvuY/z48fTu3ZsdO3aQl5dX6o9ZKgh3WLEi/D/78MPkL/1VKYMNN2kS/t8M\nH578pd+qVbjgimScyhcEMejSpQtfffUVa9euZcOGDdStW5dmzZqxd+9efvnLXzJ9+nSqVKnCmjVr\nWL9+PSeffHKRy5k+fTo333wzAJ06daJTp04HX3v22WeZMGEC+/btY926dSxevLjA64UVN8z08OHD\nSz3cNYQB87Zs2UL//v0BuPbaa7n88ssP1njVVVcxYsQIRowYAYTB7372s59x1VVXcckll9C0adNS\nfoqSkfbsCWfhpjbtzJ8fLpwCyStp9e1bsGmnYcN465YjUvmCIKZxqC+//HKee+45vvzyS0aOHAnA\nU089xYYNG5g7dy65ubm0aNGiyOGnD2f58uXcd999zJ49m7p16zJmzJijWk5CaYe7PpyXX36Z6dOn\n8+KLL3LPPffw8ccfM3bsWC644AKmTJlC7969ee2112jTps1R1yrl6OuvQ/t96i/9xYvDyVkQeu6c\neSZcdVW4gHrnztChAxx/fLx1yzGrfEEQk5EjR/KDH/yAjRs3Mm3aNCD8mm7YsCG5ublMnTqVlStX\nlriMfv368fTTTzNo0CAWLlzIggULANi2bRs1atSgdu3arF+/nldeeYUBAwYAySGwCzcNFTfM9JGq\nXbs2devWZcaMGfTt25cnn3yS/v37c+DAAVatWsXAgQPp06cPkyZNYseOHWzatImOHTvSsWNHZs+e\nzZIlSxQEmWbPHvjkk9AzZ+HC0Ea/cGG4XGJCo0ZhQ3/BBclf+qeeWikGX5NDKQjKSPv27dm+fTtN\nmjShUaNGAFx11VVceOGFdOzYkfz8/MNuEG+44Qa++93v0rZtW9q2bctZZ50FwJlnnkmXLl1o06YN\nzZo1o3fv3gf/5vrrr2fIkCE0btyYqVOnHpxe3DDTJTUDFWfixIn86Ec/YufOnbRq1YonnniC/fv3\nM3r0aLZu3Yq7c/PNN1OnTh1+/etfM3XqVKpUqUL79u0ZOnToEb+flJH9+0M3zcIb/M8+C69BOAGr\nbVvo3RtuvDG50Vf3zKyS1mGo00HDUFd8+r7KmDusWXPoBn/x4tB9E8K4O6eeGppyOnQIY+506ACt\nW+ts3CwR2zDUIlLGNm8+dIO/cGG4YHpC48ZhI3/jjckNftu2YchlkSIoCEQy0c6d4Rd94Q1+YoA1\ngNq1w4Z+1KjkBr99e6hfP766pUKqNEHg7sX2z5fMUdGaItNq//5wgPbTTw+9rVwZmnwgjKrZrl0Y\nciGxwe/QIfTV1795KQOVIgjy8vLYtGkT9evXVxhkMHdn06ZN2XWSmTts2FD0xn7p0tCDJ6FWrXBZ\nxLPPhu9+N7nBV28dSbNKEQRNmzZl9erVbNiwofiZvvkmnM7esKF+RcUoLy+vcp5ktmNH6I1T1AY/\ntf0+NxdOOw1OPx3OPz/cn3FGuNe/TYlJpQiC3NxcWrZsWfJMzz4LI0fC66+HEQ5FjtQ334Rr3372\nWfg1n7j/9NOCbfcAzZuHjft3vlNwY9+8ua6QJRkne/5FDh8OderAxIkKAinagQNh/PvPPw8b/MK3\nxNj4CQ0bhl/355yT3NCfcUZoyolGgxWpCLInCPLyQu+KiRPDOCknnBB3RRKHnTth+fLkxj11o798\nebLfPYRxdJo1C4OlXXBBuD/ttHA79dTQa0ekEsieIAC49lp45BH485/h+9+PuxpJh337wslVX3wR\net6kbuw//xzWrSs4f82aYaPepk1yY5+4nXIKHHdcPOshUo6yKwh69Ai77xMnKggqqu3bwwZ+5cqw\nsU/cEs/XrAlNPKmaNg0b9iFDwv2ppybv69fXAVrJetkVBGYwZgz88pewZEn4FSiZ5ZtvwkZ9+fIw\n5v2KFQUfb9pUcP7c3NB807w5DBwY7k85Jdw3bw4tWoRmQREpVnYFAYQ9gbvugnHj4KGH4q4m++za\nFTb0RW3kly8Pfe5TVasWNuYtW4aLl7doEW6Jjf1JJ6mPvcgxyr4gaNgwjKc+cSL85jdQr17cFVU+\nW7aE9vilS8Mt9XHhNvrc3LBRb9kSRoxIbvQT9w0b6qpWImmW1iAwsyHA74Ac4DF3v7fQ682BiUCd\naJ6x7p7+q6//5Cfw+OPhuqm33Zb2t6t03OGrr5Ib+MIb/cLNN40ahfb4f/iH0Daf2NC3aBEGSNOG\nXiRWaRuG2sxygE+Bc4HVwGzgSndfnDLPBOBDd3/YzNoBU9y9RUnLLWoY6qMyeHA4EWjZMg3DW5QD\nB8KB16I29EuXhjNpE6pUCc00p56a7FqZ6GbZqpVGvRTJAHENQ90dWOruy6IiJgEXAYtT5nEg0aG/\nNlDo9Mw0uuWWcIr/44/DD39Ybm+bUfbtC+31iY186oZ+2bKC4+Dk5iZ72vTrV3Bj36KFulmKVGDp\n3CO4DBji7tdFz68Gerj7TSnzNAJeB+oCNYBz3H1uEcu6HrgeoHnz5mcd7pKPpeIeLri9bFnYM6hZ\n89iXmYm+/TYciE3d2CeGRlixInk9Wghnw6Zu4FN/4TdrpoOyIhVYJl+Y5krgj+7+X2bWC3jSzDq4\ne4GO4O4+AZgAoWmoTN7ZDP7zP8Ml+saOhQcfLJPFxmLPnhBoqRv7xAZ/5cqC/epr1QpXperaFa64\nIrnRP+00OPlk9akXyULpDII1QLOU502jaam+DwwBcPeZZpYHNAC+SmNdSb16wc03w+9+B5deGvqh\nZ6LEwdnEMAiJIRIS9198kRy7HsKYSq1bQ8+eMHp0ckPfujU0aKCNvYgUkM4gmA20NrOWhAAYBXyn\n0DxfAIOBP5pZWyAPKGEs6TT4t3+Dl18O5xd8+GF848ds357cyBfe0K9YEcbISXXyyaHNvk+fsIFP\n/WWvK1SJyBFIWxC4+z4zuwl4jdA19HF3X2RmdwNz3P0F4OfAo2b2T4QDx2O8vC9hVb06/PGPMGAA\nXHYZvPhies9EPXAgvMesWQU3+Bs3FpyvVq3QzfL00wt2u0x0vdToliJSRtJ2sDhdyqz7aGETJ4bh\nJ3r3hiefDBvcsrRzZ1juAw+E4S2qVg0nUqVu4BOPW7UKJ7qpCUdEykgmHyzOHNdeC8cfD9/7XhiD\nqEeP0De+Zk345JNw8PW88+Dyy8P486XtLrl6NYwfDxMmwObN0KULPP10WI4uUCIiGUB7BIWtXh0O\nHr/3XrgQyZYtoXmmWTOYMiUMitawYdh7GDIE8vNDM06qbdvglVfguedg8uRwIHfECPjpT0Obvn7p\ni0g5K2mPQEFwJHbvhjfegD/8IbTz798fNuoNG4bBz/buDcGRGE+nYcPQa+cf/zG064uIxERNQ8Dz\nz4eTiJ9//hiGtsnLg2HDwm3zZnj/fZgzB1atgvXrw9m3tWuHjf7AgaF7qk7CEpEMlzVBsHFj+BG/\nfHk4UfaY1asHQ4eGm4hIBZY1wz527Rru582Ltw4RkUyTNUHQvn1ouVEQiIgUlDVBUK1aCAMFgYhI\nQVkTBBCah+bNKzgsj4hItsu6INi4MZwqICIiQdYFAYSx5UREJMiqIOjUKZxDoOMEIiJJWRUENWqE\nYYQUBCIiSVkVBBDGfFMQiIgkZV0QdO0Ka9aEESFERCRLgwB0wFhEJCHrgqBz53Cv5iERkSDrgqBO\nnTDonPYIRESCrAsCSJ5hLCIiWRoEXbqEa8Zv3hx3JSIi8cvKIOjRI9zPmhVvHSIimSBrgyAnB959\nN+5KRETil5VBUKNGaB5SEIiIZGkQAPTuDR98EK43LyKSzbI6CHbtUjdSEZGsDgJQ85CISNYGQePG\n0LIlvPNO3JWIiMQrrUFgZkPM7O9mttTMxhbx+m/NbH50+9TMtqSznsJ69w57BLp0pYhks7QFgZnl\nAOOBoUA74Eoza5c6j7v/k7t3dvfOwO+Bv6arnqL06RNGIV26tDzfVUQks6Rzj6A7sNTdl7n7t8Ak\n4KIS5r8S+FMa6znE4MHh/m9/K893FRHJLOkMgibAqpTnq6NphzCzU4CWwFvFvH69mc0xszkbNmwo\nswJPPTUcJ3j99TJbpIhIhZMpB4tHAc+5+/6iXnT3Ce6e7+75J554Ypm9qRmcdx689ZbOJxCR7JXO\nIFgDNEt53jSaVpRRlHOzUMJ558H27fD++3G8u4hI/NIZBLOB1mbW0syOI2zsXyg8k5m1AeoCM9NY\nS7EGDYIqVdQ8JCLZK21B4O77gJuA14BPgGfdfZGZ3W1mw1NmHQVMco+nE2edOmEQOgWBiGSrqulc\nuLtPAaYUmnZ7oed3prOG0jjvPPjXfw3XJ6hXL+5qRETKV6YcLI7VkCFw4AC8+mrclYiIlD8FAdC9\nOzRsCC8ccgRDRKTyUxAQDhZfeCG88gp8+23c1YiIlC8FQWT4cNi2DaZPj7sSEZHypSCInHMO5OWp\neUhEso+CIFK9Opx7bggCjUYqItlEQZBi+HBYuRI+/jjuSkREyo+CIMWwYeFezUMikk0UBClOPjmc\nZawgEJFsUqogMLMaZlYleny6mQ03s9z0lhaP4cNh9mxYU9zweCIilUxp9wimA3lm1gR4Hbga+GO6\niorTxReH+7+W67XSRETiU9ogMHffCVwCPOTulwPt01dWfNq2hY4dYdKkuCsRESkfpQ4CM+sFXAW8\nHE3LSU9J8Rs5Et57D1atOvy8IiIVXWmD4KfAL4DJ0VDSrYCp6SsrXiNHhvtnn423DhGR8mBHehmA\n6KBxTXfflp6SSpafn+9z5swph/cJl7KcPTvtbyUiknZmNtfd84t6rbS9hp42sxPMrAawEFhsZreW\nZZGZZtQomDMHPv887kpERNKrtE1D7aI9gBHAK0BLQs+hSuuKK8L9M8/EW4eISLqVNghyo/MGRgAv\nuPteoFKPyNO8OZx9toJARCq/0gbBfwMrgBrAdDM7BYjlGEF5GjkSFiyATz6JuxIRkfQpVRC4+zh3\nb+Lu53uwEhiY5tpid8UV4aI1Tz0VdyUiIulT2oPFtc3sfjObE93+i7B3UKmdfHIYmvrJJ8M1jUVE\nKqPSNg09DmwHrohu24An0lVUJrnmGvjiC5gxI+5KRETSo7RBcKq73+Huy6LbXUCrdBaWKUaMgJo1\nw16BiEhlVNog2GVmfRJPzKw3sCs9JWWW6tXh0kvhz3+GXVmxxiKSbUobBD8CxpvZCjNbATwI/DBt\nVWWYq68OF7bXdQpEpDIqba+hj9z9TKAT0MnduwCD0lpZBhkwAJo2VfOQiFROR3SFMnffljLG0M/S\nUE9GysmBq66CV1+F9evjrkZEpGwdy6Uq7bAzmA0xs7+b2VIzG1vMPFeY2WIzW2RmTx9DPWl17bWw\nfz+MGxd3JSIiZetYgqDEIbBQOl4AABLpSURBVCbMLAcYDwwF2gFXmlm7QvO0Jgxv3dvd2xOGu85I\nbdvClVfCb3+ry1iKSOVSYhCY2XYz21bEbTvQ+DDL7g4sjbqbfgtMAi4qNM8PgPHu/jWAu391lOtR\nLu65J+wV3H573JWIiJSdEoPA3Wu5+wlF3Gq5e9XDLLsJkHqNr9XRtFSnA6eb2btmNsvMhhz5KpSf\nli3hppvgiSd0nQIRqTyOpWmoLFQFWgMDgCuBR82sTuGZzOz6xPAWGzZsKOcSC/rVr0IPovPPh0WL\nYi1FRKRMpDMI1gDNUp43jaalWk00rLW7Lwc+JQRDAe4+wd3z3T3/xBNPTFvBpVGvHrz5JlStCuec\nA599Fms5IiLHLJ1BMBtobWYtzew4YBRQ+JSs5wl7A5hZA0JT0bI01lQmWrcOYbBvHwwaBCtWxF2R\niMjRS1sQuPs+4CbgNeAT4Nnowvd3m9nwaLbXgE1mthiYCtzq7pvSVVNZatcO/vY32LEjhMHq1XFX\nJCJydI744vVxK6+L15fW7NkweDA0agTTpoWhq0VEMs0xX7xeitetG0yZEvYIzjkHNm6MuyIRkSOj\nICgDffrAiy/C0qVw3nnw9ddxVyQiUnoKgjIyaBBMngwLF8LQobB9e9wViYiUjoKgDA0dCs88A3Pm\nhPMMFAYiUhEoCMrYxRfDn/4EM2cqDESkYlAQpMHll8PTTyfDYMeOuCsSESmegiBNrrgCnnpKYSAi\nmU9BkEYjR4YwePfdcPxg27bD/42ISHlTEKTZyJEwaRLMmhVOPNu8Oe6KREQKUhCUg8svD11LP/44\nXP9Yl7sUkUyiICgnw4bBSy/B559Dv34am0hEMoeCoBydcw689hqsWwd9+8KHH0IFG+pJRCohBUE5\n69MnDGG9dSt07QrNmsGYMfC//xsCQkSkvB3ucpOSBt26haubvfRSGMr6xRdh4sTwWvv2cO65Ye+h\nXz+oVSveWkWk8tMw1BngwAGYPx/eeCMEw4wZsGdPuApar14hFM45JwRIbm7c1YpIRVTSMNQKggy0\naxe8914yGObNC8cSatWCgQOTwdCmDZjFXa2IVAQKggpu0yaYOjUEwxtvhJ5HAE2aJEMhcXEcEZGi\nKAgqmeXLk6Hw5pshKAA6dEgGQ//+ULNmvHWKSOZQEFRiqccX3ngjHF/YvfvQ4wvdu4dpIpKdFARZ\nZPfucHzhb38LwTB3bvL4woABoQlp0KDQO6mKOg+LZA0FQRbbvDkcX0gEQ+L4woknhgPPgwaF22mn\n6cCzSGWmIJCDvvgiBMNbb4XjC2vWhOlNmyZDYeBAaN483jpFpGwpCKRI7rB0aQiFxG3jxvDaaacl\n9xgGDoSTToq3VhE5NgoCKZUDB8IZz4lQePvt5DUU2rdP7jH07w9168ZaqogcIQWBHJV9+8LAeIlg\nmDEjnOxmFsZJSgRDnz7qqiqS6RQEUib27IEPPkgGw8yZsHdv6Jbao0cyGHr2hLy8uKsVkVQKAkmL\nnTvDZTgTwTBnTmheysuD3r2TwZCfr3MYROKmIJBysXUrTJ+eDIYFC8L0WrXC9RcSwXDmmTqHQaS8\nlRQEaf2dZmZDgN8BOcBj7n5vodfHAP8JRJ0YedDdH0tnTZI+tWvDhReGG8CGDeGAcyIYpkwJ0+vV\nCwecBwwItw4dFAwicUrbHoGZ5QCfAucCq4HZwJXuvjhlnjFAvrvfVNrlao+g4lq9OnkOw9tvw4oV\nYXoiGBLh0LGjgkGkrMW1R9AdWOruy6IiJgEXAYtL/CuptJo2hauvDjcIQTBtWgiFt9+GyZPD9Hr1\nwkV5BgwI4dCpk4JBJJ3SGQRNgFUpz1cDPYqY71Iz60fYe/gnd19VeAYzux64HqC5TnmtNFq0CLdr\nrw3PV64sGAzPPx+m162bDIYBAxQMImUtnU1DlwFD3P266PnVQI/UZiAzqw/scPc9ZvZDYKS7Dypp\nuWoayh5ffFEwGJYtC9Pr1Dk0GHJy4qtTpCKIq2loDdAs5XlTkgeFAXD3TSlPHwP+I431SAXTvHnB\npqRVqwoGwwsvhOm1axcMhjPPVDCIHIl0BsFsoLWZtSQEwCjgO6kzmFkjd18XPR0OfJLGeqSCa9YM\nRo8ON0gGQyIcXnwxTK9dO3RXTQRD584KBpGSpC0I3H2fmd0EvEboPvq4uy8ys7uBOe7+AnCzmQ0H\n9gGbgTHpqkcqn8LBsHp1wWB46aUw/YQTwh5DoldS5846wU0klU4ok0przZpkKEybBp9+GqafcELY\nY0gEQ5cuCgap/HRmsQiwdm3BYPj738P0xJnPiaYkBYNURgoCkSKsW1fw4HNqMPTpkwyGrl0VDFLx\nKQhESuHLLwsGw5IlYXrNmocGQ25ufHWKHA0FgchR+PLLMIheIhg+ifq01awZRldNBMNZZykYJPMp\nCETKwPr1BXslLY4GS6lRo2Aw5OcrGCTzKAhE0mD9+rDHkAiGRYvC9NRg6N8/BMNxx8VZqYiCQKRc\nfPVVsilp2jRYuDBMr169YDB066ZgkPKnIBCJwYYNBYPh44/D9OOPL9iUpGCQ8qAgEMkAGzcWPPic\nGgxnn10wGKpVi69OqZwUBCIZaONGmDEjGQyJS3sefzz06pUMhu7dFQxy7BQEIhXApk2HBoM75OUV\nDIYePRQMcuQUBCIV0ObNBYPho48KBkNirKQePcI0kZIoCEQqgUQwJLqrzp8fgqFateQeQ//+0LOn\ngkEOpSAQqYS+/rpgMHz4YTIYevZMNiUpGAQUBCJZYcuWZFPStGkhGA4cCF1TCwfD8cfHXKyUOwWB\nSBbasgXeeSd5jCE1GHr0SAZDr14KhmygIBARtm4tGAzz5iWDoXv3gsFQvXq8tUrZUxCIyCG2boV3\n300Gw9y5IRhycwsGw9lnKxgqAwWBiBzWtm2HBsP+/QWDoX//EAw1asRcrBwxBYGIHLFEMCR6Jc2Z\nE4KhatVDg6FmzbirlcNREIjIMdu+vWAwzJ6dDIZu3Qo2JSkYMo+CQETK3Pbt8N57ye6qs2fDvn0h\nGPLzk8HQu7eCIRMoCEQk7XbsSAZDYo9h3z7IyTk0GGrVirfWbKQgEJFy9803BYPhgw+SwXDWWclg\n6NNHwVAeFAQiErtvvoGZMwsGw969yWBIDKLXpw+ccELMxVZCCgIRyTg7dxYMhvffD8FQpUpyj6F/\n/xAMtWvHXGwloCAQkYyXCIZEr6RZs5LB0LVrwaYkBcORiy0IzGwI8DsgB3jM3e8tZr5LgeeAbu5e\n4lZeQSCSHXbuDGGQGgzffhuCoUuXgsFQp07MxVYAsQSBmeUAnwLnAquB2cCV7r640Hy1gJeB44Cb\nFAQiUpRdu0IYJLqrzpwZgsGsYDD07atgKEpJQVA1je/bHVjq7suiIiYBFwGLC833r8D/A25NYy0i\nUsEdfzwMHBhuEILh/feTxxjGj4f7708GQ+Lgc9++ULdujIVXAOkMgibAqpTnq4EeqTOYWVegmbu/\nbGbFBoGZXQ9cD9C8efM0lCoiFc3xxyf3AgB27y4YDA89BL/9bQiGzp0LBkO9erGVnZHS2TR0GTDE\n3a+Lnl8N9HD3m6LnVYC3gDHuvsLM3gZuUdOQiJSF3btDF9VEMMycGaaZwZlnJnsl9euXHcEQ1zGC\nXsCd7v4P0fNfALj7v0fPawOfAzuiPzkZ2AwMLykMFAQicjT27CkYDO+9lwyGjh2TodC3L5x0UtzV\nlr24gqAq4WDxYGAN4WDxd9x9UTHzv432CESknOzZE4bBmDoVpk8PwbBzZ3itTZsQCv36hYBo2jTe\nWstCLAeL3X2fmd0EvEboPvq4uy8ys7uBOe7+QrreW0TkcKpVC11P+/QJz/fuDddgmD493CZNggkT\nwmstWyb3GPr1g1atwp5EZaETykREirB/PyxYEEJh2rRwv2lTeK1Jk4J7DG3aZH4w6MxiEZFjdOAA\nLFmSDIVp02DduvDaiSeGYwuJvYaOHcMYSplEQSAiUsbc4fPPk01J06bBihXhtTp1QpNTYq+ha9dw\nyc84xXVCmYhIpWUGp50Wbt/7Xpj2xRfJYJg+HV56KUyvUSNcuS2xx9CtG+TlxVd7YdojEBFJky+/\nhBkzksGwYEGYXq0a9OyZ7K7as2f6r8mgpiERkQyweTO8806yKWnevHDsIScnnP3ct2+49ekDDRuW\n7XsrCEREMtD27eGM5xkzwu3998NJbgCnn54Mhr59QxfWY+mZpCAQEakA9uwJewmJYHjnHdiyJbzW\nuDHcdx9ceeXRLVsHi0VEKoBq1aBXr3D7538OzUaLFiVDoVGj9LyvgkBEJENVqRLOSejYEW68MY3v\nk75Fi4hIRaAgEBHJcgoCEZEspyAQEclyCgIRkSynIBARyXIKAhGRLKcgEBHJchVuiAkz2wCsPIo/\nbQBsLONy4qJ1yUxal8ykdQlOcfcTi3qhwgXB0TKzOcWNs1HRaF0yk9YlM2ldDk9NQyIiWU5BICKS\n5bIpCCbEXUAZ0rpkJq1LZtK6HEbWHCMQEZGiZdMegYiIFEFBICKS5bIiCMxsiJn93cyWmtnYuOs5\nUma2wsw+NrP5ZjYnmlbPzP5mZp9F93XjrrMoZva4mX1lZgtTphVZuwXjou9pgZl1ja/yQxWzLnea\n2Zrou5lvZuenvPaLaF3+bmb/EE/VhzKzZmY21cwWm9kiM/tJNL3CfS8lrEtF/F7yzOwDM/soWpe7\nouktzez9qOZnzOy4aHq16PnS6PUWR/3m7l6pb0AO8DnQCjgO+AhoF3ddR7gOK4AGhab9BzA2ejwW\n+H9x11lM7f2ArsDCw9UOnA+8AhjQE3g/7vpLsS53ArcUMW+76N9aNaBl9G8wJ+51iGprBHSNHtcC\nPo3qrXDfSwnrUhG/FwNqRo9zgfejz/tZYFQ0/RHghujxjcAj0eNRwDNH+97ZsEfQHVjq7svc/Vtg\nEnBRzDWVhYuAidHjicCIGGsplrtPBzYXmlxc7RcB/+PBLKCOmaXpKq1Hrph1Kc5FwCR33+Puy4Gl\nhH+LsXP3de4+L3q8HfgEaEIF/F5KWJfiZPL34u6+I3qaG90cGAQ8F00v/L0kvq/ngMFmZkfz3tkQ\nBE2AVSnPV1PyP5RM5MDrZjbXzK6Ppp3k7uuix18CJ8VT2lEprvaK+l3dFDWZPJ7SRFch1iVqTuhC\n+PVZob+XQusCFfB7MbMcM5sPfAX8jbDHssXd90WzpNZ7cF2i17cC9Y/mfbMhCCqDPu7eFRgK/NjM\n+qW+6GHfsEL2A67ItUceBk4FOgPrgP+Kt5zSM7OawF+An7r7ttTXKtr3UsS6VMjvxd33u3tnoClh\nT6VNebxvNgTBGqBZyvOm0bQKw93XRPdfAZMJ/0DWJ3bPo/uv4qvwiBVXe4X7rtx9ffSf9wDwKMlm\nhoxeFzPLJWw4n3L3v0aTK+T3UtS6VNTvJcHdtwBTgV6Epriq0Uup9R5cl+j12sCmo3m/bAiC2UDr\n6Mj7cYSDKi/EXFOpmVkNM6uVeAycBywkrMO10WzXAv8XT4VHpbjaXwCuiXqp9AS2pjRVZKRCbeUX\nE74bCOsyKurZ0RJoDXxQ3vUVJWpH/gPwibvfn/JShfteiluXCvq9nGhmdaLHxwPnEo55TAUui2Yr\n/L0kvq/LgLeiPbkjF/eR8vK4EXo9fEpob/tV3PUcYe2tCL0cPgIWJeontAW+CXwGvAHUi7vWYur/\nE2HXfC+hffP7xdVO6DUxPvqePgby466/FOvyZFTrgug/ZqOU+X8VrcvfgaFx159SVx9Cs88CYH50\nO78ifi8lrEtF/F46AR9GNS8Ebo+mtyKE1VLgz0C1aHpe9Hxp9Hqro31vDTEhIpLlsqFpSERESqAg\nEBHJcgoCEZEspyAQEclyCgIRkSynIBCJmNn+lNEq51sZjlRrZi1SRy0VySRVDz+LSNbY5eH0fpGs\noj0CkcOwcD2I/7BwTYgPzOy0aHoLM3srGtjsTTNrHk0/ycwmR+PKf2RmZ0eLyjGzR6Ox5l+Pzh7F\nzG6OxtNfYGaTYlpNyWIKApGk4ws1DY1MeW2ru3cEHgQeiKb9Hpjo7p2Ap4Bx0fRxwDR3P5Nw/YJF\n0fTWwHh3bw9sAS6Npo8FukTL+VG6Vk6kODqzWCRiZjvcvWYR01cAg9x9WTTA2ZfuXt/MNhKGLtgb\nTV/n7g3MbAPQ1N33pCyjBfA3d28dPb8NyHX335jZq8AO4HngeU+OSS9SLrRHIFI6XszjI7En5fF+\nksfoLiCM5dMVmJ0y0qRIuVAQiJTOyJT7mdHj9wij2QJcBcyIHr8J3AAHLzRSu7iFmlkVoJm7TwVu\nIwwlfMheiUg66ZeHSNLx0dWhEl5190QX0rpmtoDwq/7KaNo/Ak+Y2a3ABuC70fSfABPM7PuEX/43\nEEYtLUoO8L9RWBgwzsNY9CLlRscIRA4jOkaQ7+4b465FJB3UNCQikuW0RyAikuW0RyAikuUUBCIi\nWU5BICKS5RQEIiJZTkEgIpLl/j+TqDQMmPlyxQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "7e31ab6e-340f-4f29-e9ca-757eaff7a736",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fbc2fe81630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nY4wJOEsExhgTcJYIjDEm4CwRGGNMwP0/e881hoyVaLIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "cb4d81ed-c0f4-4b0f-aa05-994f5c26f9df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_big_labels, epochs= num_epochs, batch_size=68, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "102/102 [==============================] - 0s 4ms/step - loss: 0.8321 - acc: 0.4412\n",
            "Epoch 2/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.7646 - acc: 0.5000\n",
            "Epoch 3/300\n",
            "102/102 [==============================] - 0s 94us/step - loss: 0.7192 - acc: 0.5980\n",
            "Epoch 4/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.6878 - acc: 0.6569\n",
            "Epoch 5/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.6688 - acc: 0.6863\n",
            "Epoch 6/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.6488 - acc: 0.6667\n",
            "Epoch 7/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.6349 - acc: 0.7059\n",
            "Epoch 8/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.6245 - acc: 0.7255\n",
            "Epoch 9/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.6144 - acc: 0.7255\n",
            "Epoch 10/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.6054 - acc: 0.7451\n",
            "Epoch 11/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.5959 - acc: 0.7451\n",
            "Epoch 12/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.5864 - acc: 0.7549\n",
            "Epoch 13/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.5785 - acc: 0.7451\n",
            "Epoch 14/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.5701 - acc: 0.7451\n",
            "Epoch 15/300\n",
            "102/102 [==============================] - 0s 96us/step - loss: 0.5624 - acc: 0.7451\n",
            "Epoch 16/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.5554 - acc: 0.7451\n",
            "Epoch 17/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.5481 - acc: 0.7549\n",
            "Epoch 18/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.5411 - acc: 0.7549\n",
            "Epoch 19/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.5355 - acc: 0.7647\n",
            "Epoch 20/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.5295 - acc: 0.7647\n",
            "Epoch 21/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.5226 - acc: 0.7647\n",
            "Epoch 22/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.5156 - acc: 0.7843\n",
            "Epoch 23/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.5082 - acc: 0.7941\n",
            "Epoch 24/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.5024 - acc: 0.7941\n",
            "Epoch 25/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.4956 - acc: 0.8039\n",
            "Epoch 26/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.4892 - acc: 0.8039\n",
            "Epoch 27/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.4827 - acc: 0.8137\n",
            "Epoch 28/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.4758 - acc: 0.8235\n",
            "Epoch 29/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.4685 - acc: 0.8235\n",
            "Epoch 30/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.4623 - acc: 0.8333\n",
            "Epoch 31/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.4562 - acc: 0.8333\n",
            "Epoch 32/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.4484 - acc: 0.8333\n",
            "Epoch 33/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.4424 - acc: 0.8333\n",
            "Epoch 34/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.4359 - acc: 0.8431\n",
            "Epoch 35/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.4294 - acc: 0.8529\n",
            "Epoch 36/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.4239 - acc: 0.8725\n",
            "Epoch 37/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.4172 - acc: 0.8725\n",
            "Epoch 38/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.4117 - acc: 0.8725\n",
            "Epoch 39/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.4054 - acc: 0.8725\n",
            "Epoch 40/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.3981 - acc: 0.8725\n",
            "Epoch 41/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.3935 - acc: 0.8725\n",
            "Epoch 42/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.3878 - acc: 0.8824\n",
            "Epoch 43/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.3812 - acc: 0.8824\n",
            "Epoch 44/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.3748 - acc: 0.8922\n",
            "Epoch 45/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.3709 - acc: 0.9020\n",
            "Epoch 46/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.3650 - acc: 0.8922\n",
            "Epoch 47/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.3589 - acc: 0.8922\n",
            "Epoch 48/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.3552 - acc: 0.8922\n",
            "Epoch 49/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.3483 - acc: 0.9020\n",
            "Epoch 50/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.3419 - acc: 0.9118\n",
            "Epoch 51/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.3366 - acc: 0.9216\n",
            "Epoch 52/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.3319 - acc: 0.9216\n",
            "Epoch 53/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.3285 - acc: 0.9216\n",
            "Epoch 54/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.3231 - acc: 0.9216\n",
            "Epoch 55/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.3162 - acc: 0.9216\n",
            "Epoch 56/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.3113 - acc: 0.9216\n",
            "Epoch 57/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.3067 - acc: 0.9314\n",
            "Epoch 58/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.3010 - acc: 0.9314\n",
            "Epoch 59/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.2966 - acc: 0.9314\n",
            "Epoch 60/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.2912 - acc: 0.9314\n",
            "Epoch 61/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.2877 - acc: 0.9314\n",
            "Epoch 62/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.2837 - acc: 0.9314\n",
            "Epoch 63/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.2785 - acc: 0.9314\n",
            "Epoch 64/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.2739 - acc: 0.9314\n",
            "Epoch 65/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.2704 - acc: 0.9510\n",
            "Epoch 66/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.2671 - acc: 0.9510\n",
            "Epoch 67/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.2638 - acc: 0.9510\n",
            "Epoch 68/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.2576 - acc: 0.9510\n",
            "Epoch 69/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.2522 - acc: 0.9412\n",
            "Epoch 70/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.2503 - acc: 0.9510\n",
            "Epoch 71/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.2460 - acc: 0.9510\n",
            "Epoch 72/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.2409 - acc: 0.9510\n",
            "Epoch 73/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.2362 - acc: 0.9608\n",
            "Epoch 74/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.2362 - acc: 0.9608\n",
            "Epoch 75/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.2344 - acc: 0.9706\n",
            "Epoch 76/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.2273 - acc: 0.9608\n",
            "Epoch 77/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.2219 - acc: 0.9608\n",
            "Epoch 78/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.2210 - acc: 0.9608\n",
            "Epoch 79/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.2204 - acc: 0.9608\n",
            "Epoch 80/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.2164 - acc: 0.9608\n",
            "Epoch 81/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.2105 - acc: 0.9608\n",
            "Epoch 82/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.2062 - acc: 0.9706\n",
            "Epoch 83/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.2040 - acc: 0.9804\n",
            "Epoch 84/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.2013 - acc: 0.9706\n",
            "Epoch 85/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.1976 - acc: 0.9706\n",
            "Epoch 86/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.1939 - acc: 0.9804\n",
            "Epoch 87/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.1916 - acc: 0.9608\n",
            "Epoch 88/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.1890 - acc: 0.9608\n",
            "Epoch 89/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.1858 - acc: 0.9706\n",
            "Epoch 90/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.1818 - acc: 0.9804\n",
            "Epoch 91/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1808 - acc: 0.9804\n",
            "Epoch 92/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.1781 - acc: 0.9902\n",
            "Epoch 93/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.1745 - acc: 0.9902\n",
            "Epoch 94/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.1710 - acc: 0.9902\n",
            "Epoch 95/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1699 - acc: 0.9804\n",
            "Epoch 96/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.1683 - acc: 0.9804\n",
            "Epoch 97/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.1656 - acc: 0.9804\n",
            "Epoch 98/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.1615 - acc: 0.9902\n",
            "Epoch 99/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.1607 - acc: 0.9902\n",
            "Epoch 100/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.1583 - acc: 0.9902\n",
            "Epoch 101/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.1551 - acc: 1.0000\n",
            "Epoch 102/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.1523 - acc: 0.9902\n",
            "Epoch 103/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.1498 - acc: 1.0000\n",
            "Epoch 104/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.1478 - acc: 1.0000\n",
            "Epoch 105/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.1459 - acc: 1.0000\n",
            "Epoch 106/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.1438 - acc: 1.0000\n",
            "Epoch 107/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1412 - acc: 1.0000\n",
            "Epoch 108/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.1392 - acc: 1.0000\n",
            "Epoch 109/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.1374 - acc: 1.0000\n",
            "Epoch 110/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.1350 - acc: 1.0000\n",
            "Epoch 111/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.1329 - acc: 1.0000\n",
            "Epoch 112/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.1315 - acc: 1.0000\n",
            "Epoch 113/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.1292 - acc: 1.0000\n",
            "Epoch 114/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1275 - acc: 1.0000\n",
            "Epoch 115/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.1263 - acc: 1.0000\n",
            "Epoch 116/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1250 - acc: 1.0000\n",
            "Epoch 117/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.1231 - acc: 1.0000\n",
            "Epoch 118/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1208 - acc: 1.0000\n",
            "Epoch 119/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.1199 - acc: 1.0000\n",
            "Epoch 120/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1181 - acc: 1.0000\n",
            "Epoch 121/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.1164 - acc: 1.0000\n",
            "Epoch 122/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.1151 - acc: 1.0000\n",
            "Epoch 123/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.1136 - acc: 1.0000\n",
            "Epoch 124/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.1120 - acc: 1.0000\n",
            "Epoch 125/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1105 - acc: 1.0000\n",
            "Epoch 126/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1092 - acc: 1.0000\n",
            "Epoch 127/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.1081 - acc: 1.0000\n",
            "Epoch 128/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.1064 - acc: 1.0000\n",
            "Epoch 129/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.1050 - acc: 1.0000\n",
            "Epoch 130/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.1047 - acc: 1.0000\n",
            "Epoch 131/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.1029 - acc: 1.0000\n",
            "Epoch 132/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.1015 - acc: 1.0000\n",
            "Epoch 133/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.1011 - acc: 1.0000\n",
            "Epoch 134/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.1002 - acc: 1.0000\n",
            "Epoch 135/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0989 - acc: 1.0000\n",
            "Epoch 136/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0973 - acc: 1.0000\n",
            "Epoch 137/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0966 - acc: 1.0000\n",
            "Epoch 138/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0955 - acc: 1.0000\n",
            "Epoch 139/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0942 - acc: 1.0000\n",
            "Epoch 140/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0934 - acc: 1.0000\n",
            "Epoch 141/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0930 - acc: 1.0000\n",
            "Epoch 142/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0918 - acc: 1.0000\n",
            "Epoch 143/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0905 - acc: 1.0000\n",
            "Epoch 144/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0898 - acc: 1.0000\n",
            "Epoch 145/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0890 - acc: 1.0000\n",
            "Epoch 146/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0882 - acc: 1.0000\n",
            "Epoch 147/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0875 - acc: 1.0000\n",
            "Epoch 148/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0866 - acc: 1.0000\n",
            "Epoch 149/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0857 - acc: 1.0000\n",
            "Epoch 150/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0850 - acc: 1.0000\n",
            "Epoch 151/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0841 - acc: 1.0000\n",
            "Epoch 152/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0834 - acc: 1.0000\n",
            "Epoch 153/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0829 - acc: 1.0000\n",
            "Epoch 154/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0823 - acc: 1.0000\n",
            "Epoch 155/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.0816 - acc: 1.0000\n",
            "Epoch 156/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0809 - acc: 1.0000\n",
            "Epoch 157/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0802 - acc: 1.0000\n",
            "Epoch 158/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0796 - acc: 1.0000\n",
            "Epoch 159/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0790 - acc: 1.0000\n",
            "Epoch 160/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0785 - acc: 1.0000\n",
            "Epoch 161/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0779 - acc: 1.0000\n",
            "Epoch 162/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0777 - acc: 1.0000\n",
            "Epoch 163/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0769 - acc: 1.0000\n",
            "Epoch 164/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0763 - acc: 1.0000\n",
            "Epoch 165/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0761 - acc: 1.0000\n",
            "Epoch 166/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0754 - acc: 1.0000\n",
            "Epoch 167/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0748 - acc: 1.0000\n",
            "Epoch 168/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0744 - acc: 1.0000\n",
            "Epoch 169/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0738 - acc: 1.0000\n",
            "Epoch 170/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0736 - acc: 1.0000\n",
            "Epoch 171/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0730 - acc: 1.0000\n",
            "Epoch 172/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0725 - acc: 1.0000\n",
            "Epoch 173/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0721 - acc: 1.0000\n",
            "Epoch 174/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0717 - acc: 1.0000\n",
            "Epoch 175/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0714 - acc: 1.0000\n",
            "Epoch 176/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0710 - acc: 1.0000\n",
            "Epoch 177/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0705 - acc: 1.0000\n",
            "Epoch 178/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0701 - acc: 1.0000\n",
            "Epoch 179/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0697 - acc: 1.0000\n",
            "Epoch 180/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0694 - acc: 1.0000\n",
            "Epoch 181/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0690 - acc: 1.0000\n",
            "Epoch 182/300\n",
            "102/102 [==============================] - 0s 45us/step - loss: 0.0686 - acc: 1.0000\n",
            "Epoch 183/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0684 - acc: 1.0000\n",
            "Epoch 184/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0681 - acc: 1.0000\n",
            "Epoch 185/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0678 - acc: 1.0000\n",
            "Epoch 186/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0674 - acc: 1.0000\n",
            "Epoch 187/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0670 - acc: 1.0000\n",
            "Epoch 188/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0666 - acc: 1.0000\n",
            "Epoch 189/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0665 - acc: 1.0000\n",
            "Epoch 190/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0661 - acc: 1.0000\n",
            "Epoch 191/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0659 - acc: 1.0000\n",
            "Epoch 192/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0656 - acc: 1.0000\n",
            "Epoch 193/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.0653 - acc: 1.0000\n",
            "Epoch 194/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0650 - acc: 1.0000\n",
            "Epoch 195/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.0646 - acc: 1.0000\n",
            "Epoch 196/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0644 - acc: 1.0000\n",
            "Epoch 197/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0641 - acc: 1.0000\n",
            "Epoch 198/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.0639 - acc: 1.0000\n",
            "Epoch 199/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0637 - acc: 1.0000\n",
            "Epoch 200/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0633 - acc: 1.0000\n",
            "Epoch 201/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0631 - acc: 1.0000\n",
            "Epoch 202/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0629 - acc: 1.0000\n",
            "Epoch 203/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0627 - acc: 1.0000\n",
            "Epoch 204/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0624 - acc: 1.0000\n",
            "Epoch 205/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0622 - acc: 1.0000\n",
            "Epoch 206/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0620 - acc: 1.0000\n",
            "Epoch 207/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0618 - acc: 1.0000\n",
            "Epoch 208/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0615 - acc: 1.0000\n",
            "Epoch 209/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0612 - acc: 1.0000\n",
            "Epoch 210/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0611 - acc: 1.0000\n",
            "Epoch 211/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0609 - acc: 1.0000\n",
            "Epoch 212/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0606 - acc: 1.0000\n",
            "Epoch 213/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.0604 - acc: 1.0000\n",
            "Epoch 214/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0603 - acc: 1.0000\n",
            "Epoch 215/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0600 - acc: 1.0000\n",
            "Epoch 216/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0598 - acc: 1.0000\n",
            "Epoch 217/300\n",
            "102/102 [==============================] - 0s 44us/step - loss: 0.0597 - acc: 1.0000\n",
            "Epoch 218/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.0594 - acc: 1.0000\n",
            "Epoch 219/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0593 - acc: 1.0000\n",
            "Epoch 220/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0591 - acc: 1.0000\n",
            "Epoch 221/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0589 - acc: 1.0000\n",
            "Epoch 222/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0587 - acc: 1.0000\n",
            "Epoch 223/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0586 - acc: 1.0000\n",
            "Epoch 224/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0583 - acc: 1.0000\n",
            "Epoch 225/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0581 - acc: 1.0000\n",
            "Epoch 226/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0580 - acc: 1.0000\n",
            "Epoch 227/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0578 - acc: 1.0000\n",
            "Epoch 228/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0577 - acc: 1.0000\n",
            "Epoch 229/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0576 - acc: 1.0000\n",
            "Epoch 230/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0574 - acc: 1.0000\n",
            "Epoch 231/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0572 - acc: 1.0000\n",
            "Epoch 232/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0570 - acc: 1.0000\n",
            "Epoch 233/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0568 - acc: 1.0000\n",
            "Epoch 234/300\n",
            "102/102 [==============================] - 0s 49us/step - loss: 0.0567 - acc: 1.0000\n",
            "Epoch 235/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0565 - acc: 1.0000\n",
            "Epoch 236/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0564 - acc: 1.0000\n",
            "Epoch 237/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0562 - acc: 1.0000\n",
            "Epoch 238/300\n",
            "102/102 [==============================] - 0s 48us/step - loss: 0.0561 - acc: 1.0000\n",
            "Epoch 239/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0559 - acc: 1.0000\n",
            "Epoch 240/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0557 - acc: 1.0000\n",
            "Epoch 241/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0556 - acc: 1.0000\n",
            "Epoch 242/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0554 - acc: 1.0000\n",
            "Epoch 243/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0553 - acc: 1.0000\n",
            "Epoch 244/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0551 - acc: 1.0000\n",
            "Epoch 245/300\n",
            "102/102 [==============================] - 0s 54us/step - loss: 0.0550 - acc: 1.0000\n",
            "Epoch 246/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0548 - acc: 1.0000\n",
            "Epoch 247/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0547 - acc: 1.0000\n",
            "Epoch 248/300\n",
            "102/102 [==============================] - 0s 60us/step - loss: 0.0546 - acc: 1.0000\n",
            "Epoch 249/300\n",
            "102/102 [==============================] - 0s 53us/step - loss: 0.0544 - acc: 1.0000\n",
            "Epoch 250/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0543 - acc: 1.0000\n",
            "Epoch 251/300\n",
            "102/102 [==============================] - 0s 46us/step - loss: 0.0542 - acc: 1.0000\n",
            "Epoch 252/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0540 - acc: 1.0000\n",
            "Epoch 253/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0539 - acc: 1.0000\n",
            "Epoch 254/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0537 - acc: 1.0000\n",
            "Epoch 255/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0536 - acc: 1.0000\n",
            "Epoch 256/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0535 - acc: 1.0000\n",
            "Epoch 257/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0533 - acc: 1.0000\n",
            "Epoch 258/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0532 - acc: 1.0000\n",
            "Epoch 259/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0530 - acc: 1.0000\n",
            "Epoch 260/300\n",
            "102/102 [==============================] - 0s 55us/step - loss: 0.0529 - acc: 1.0000\n",
            "Epoch 261/300\n",
            "102/102 [==============================] - 0s 47us/step - loss: 0.0528 - acc: 1.0000\n",
            "Epoch 262/300\n",
            "102/102 [==============================] - 0s 52us/step - loss: 0.0527 - acc: 1.0000\n",
            "Epoch 263/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0525 - acc: 1.0000\n",
            "Epoch 264/300\n",
            "102/102 [==============================] - 0s 94us/step - loss: 0.0524 - acc: 1.0000\n",
            "Epoch 265/300\n",
            "102/102 [==============================] - 0s 93us/step - loss: 0.0523 - acc: 1.0000\n",
            "Epoch 266/300\n",
            "102/102 [==============================] - 0s 89us/step - loss: 0.0522 - acc: 1.0000\n",
            "Epoch 267/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0520 - acc: 1.0000\n",
            "Epoch 268/300\n",
            "102/102 [==============================] - 0s 57us/step - loss: 0.0519 - acc: 1.0000\n",
            "Epoch 269/300\n",
            "102/102 [==============================] - 0s 56us/step - loss: 0.0518 - acc: 1.0000\n",
            "Epoch 270/300\n",
            "102/102 [==============================] - 0s 51us/step - loss: 0.0517 - acc: 1.0000\n",
            "Epoch 271/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0515 - acc: 1.0000\n",
            "Epoch 272/300\n",
            "102/102 [==============================] - 0s 50us/step - loss: 0.0514 - acc: 1.0000\n",
            "Epoch 273/300\n",
            "102/102 [==============================] - 0s 64us/step - loss: 0.0513 - acc: 1.0000\n",
            "Epoch 274/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0512 - acc: 1.0000\n",
            "Epoch 275/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0510 - acc: 1.0000\n",
            "Epoch 276/300\n",
            "102/102 [==============================] - 0s 77us/step - loss: 0.0509 - acc: 1.0000\n",
            "Epoch 277/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.0508 - acc: 1.0000\n",
            "Epoch 278/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0507 - acc: 1.0000\n",
            "Epoch 279/300\n",
            "102/102 [==============================] - 0s 65us/step - loss: 0.0505 - acc: 1.0000\n",
            "Epoch 280/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0504 - acc: 1.0000\n",
            "Epoch 281/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0503 - acc: 1.0000\n",
            "Epoch 282/300\n",
            "102/102 [==============================] - 0s 68us/step - loss: 0.0502 - acc: 1.0000\n",
            "Epoch 283/300\n",
            "102/102 [==============================] - 0s 80us/step - loss: 0.0500 - acc: 1.0000\n",
            "Epoch 284/300\n",
            "102/102 [==============================] - 0s 74us/step - loss: 0.0500 - acc: 1.0000\n",
            "Epoch 285/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0499 - acc: 1.0000\n",
            "Epoch 286/300\n",
            "102/102 [==============================] - 0s 71us/step - loss: 0.0497 - acc: 1.0000\n",
            "Epoch 287/300\n",
            "102/102 [==============================] - 0s 58us/step - loss: 0.0496 - acc: 1.0000\n",
            "Epoch 288/300\n",
            "102/102 [==============================] - 0s 70us/step - loss: 0.0495 - acc: 1.0000\n",
            "Epoch 289/300\n",
            "102/102 [==============================] - 0s 67us/step - loss: 0.0494 - acc: 1.0000\n",
            "Epoch 290/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0493 - acc: 1.0000\n",
            "Epoch 291/300\n",
            "102/102 [==============================] - 0s 75us/step - loss: 0.0492 - acc: 1.0000\n",
            "Epoch 292/300\n",
            "102/102 [==============================] - 0s 66us/step - loss: 0.0490 - acc: 1.0000\n",
            "Epoch 293/300\n",
            "102/102 [==============================] - 0s 59us/step - loss: 0.0490 - acc: 1.0000\n",
            "Epoch 294/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0488 - acc: 1.0000\n",
            "Epoch 295/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0487 - acc: 1.0000\n",
            "Epoch 296/300\n",
            "102/102 [==============================] - 0s 69us/step - loss: 0.0486 - acc: 1.0000\n",
            "Epoch 297/300\n",
            "102/102 [==============================] - 0s 61us/step - loss: 0.0485 - acc: 1.0000\n",
            "Epoch 298/300\n",
            "102/102 [==============================] - 0s 86us/step - loss: 0.0484 - acc: 1.0000\n",
            "Epoch 299/300\n",
            "102/102 [==============================] - 0s 62us/step - loss: 0.0483 - acc: 1.0000\n",
            "Epoch 300/300\n",
            "102/102 [==============================] - 0s 63us/step - loss: 0.0482 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 15ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "bfcdc956-e7d5-4402-ae3d-eaa3c774f1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "85b0bb5d-1288-441a-ba0e-063b139c948c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5384615659713745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    }
  ]
}
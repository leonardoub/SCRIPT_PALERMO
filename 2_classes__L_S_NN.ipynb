{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_classes _L_S_NN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUJBCM1yWnXJG4jrW/sTB2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/2_classes__L_S_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ucro1fblcxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMGJk36soJp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "3088c47a-710b-4df3-c8cf-a55fcac18faf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4qPpCYboRP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train_LS = df_train[df_train['Histology'] != 'adenocarcinoma']\n",
        "df_test_LS = df_test[df_test['Histology'] != 'adenocarcinoma']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train_LS.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test_LS.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train_LS.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test_LS.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "4d53c085-8836-48d1-efa9-cce8d791a03f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(89, 8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'large cell':0, 'squamous cell carcinoma':1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(107,)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  #kernel_regularizer=regularizers.l2(l=0.001)\n",
        "  model.add(layers.Dense(20, activation='relu'))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(2, activation='sigmoid'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5a1147d3-f479-4f87-a87e-21aeb9236d09",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d2be7779-52e9-48a6-eb9e-86d151c950d4",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, one_hot_train_labels, validation_data=(val_data_stand, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=89, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 89 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "89/89 [==============================] - 2s 21ms/step - loss: 0.7774 - acc: 0.4944 - val_loss: 0.7506 - val_acc: 0.4615\n",
            "Epoch 2/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.7544 - acc: 0.4944 - val_loss: 0.7348 - val_acc: 0.5385\n",
            "Epoch 3/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.7344 - acc: 0.5281 - val_loss: 0.7203 - val_acc: 0.5385\n",
            "Epoch 4/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.7185 - acc: 0.5506 - val_loss: 0.7104 - val_acc: 0.4615\n",
            "Epoch 5/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.7053 - acc: 0.5730 - val_loss: 0.7047 - val_acc: 0.4615\n",
            "Epoch 6/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6939 - acc: 0.5730 - val_loss: 0.7006 - val_acc: 0.4615\n",
            "Epoch 7/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6842 - acc: 0.6067 - val_loss: 0.6977 - val_acc: 0.4615\n",
            "Epoch 8/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6765 - acc: 0.5843 - val_loss: 0.6943 - val_acc: 0.3846\n",
            "Epoch 9/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.6692 - acc: 0.5843 - val_loss: 0.6908 - val_acc: 0.3846\n",
            "Epoch 10/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6625 - acc: 0.5730 - val_loss: 0.6883 - val_acc: 0.3846\n",
            "Epoch 11/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6563 - acc: 0.5730 - val_loss: 0.6868 - val_acc: 0.4615\n",
            "Epoch 12/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6507 - acc: 0.5730 - val_loss: 0.6866 - val_acc: 0.4615\n",
            "Epoch 13/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6452 - acc: 0.5955 - val_loss: 0.6852 - val_acc: 0.4615\n",
            "Epoch 14/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6397 - acc: 0.6517 - val_loss: 0.6835 - val_acc: 0.4615\n",
            "Epoch 15/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6345 - acc: 0.6292 - val_loss: 0.6807 - val_acc: 0.6154\n",
            "Epoch 16/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.6296 - acc: 0.6517 - val_loss: 0.6774 - val_acc: 0.6154\n",
            "Epoch 17/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.6250 - acc: 0.6629 - val_loss: 0.6737 - val_acc: 0.6154\n",
            "Epoch 18/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6203 - acc: 0.7191 - val_loss: 0.6707 - val_acc: 0.6154\n",
            "Epoch 19/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6156 - acc: 0.7191 - val_loss: 0.6687 - val_acc: 0.6154\n",
            "Epoch 20/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.6111 - acc: 0.7191 - val_loss: 0.6674 - val_acc: 0.6154\n",
            "Epoch 21/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6069 - acc: 0.7303 - val_loss: 0.6664 - val_acc: 0.6154\n",
            "Epoch 22/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.6027 - acc: 0.7303 - val_loss: 0.6651 - val_acc: 0.6154\n",
            "Epoch 23/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5986 - acc: 0.7303 - val_loss: 0.6636 - val_acc: 0.6154\n",
            "Epoch 24/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5945 - acc: 0.7303 - val_loss: 0.6621 - val_acc: 0.6154\n",
            "Epoch 25/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.5903 - acc: 0.7416 - val_loss: 0.6607 - val_acc: 0.6154\n",
            "Epoch 26/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5862 - acc: 0.7528 - val_loss: 0.6597 - val_acc: 0.6154\n",
            "Epoch 27/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5821 - acc: 0.7303 - val_loss: 0.6591 - val_acc: 0.6154\n",
            "Epoch 28/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5778 - acc: 0.7528 - val_loss: 0.6588 - val_acc: 0.6154\n",
            "Epoch 29/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5735 - acc: 0.7640 - val_loss: 0.6593 - val_acc: 0.6154\n",
            "Epoch 30/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5691 - acc: 0.7640 - val_loss: 0.6600 - val_acc: 0.6154\n",
            "Epoch 31/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.5648 - acc: 0.7528 - val_loss: 0.6610 - val_acc: 0.6154\n",
            "Epoch 32/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5605 - acc: 0.7528 - val_loss: 0.6618 - val_acc: 0.6154\n",
            "Epoch 33/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.5561 - acc: 0.7416 - val_loss: 0.6625 - val_acc: 0.6154\n",
            "Epoch 34/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.5519 - acc: 0.7528 - val_loss: 0.6627 - val_acc: 0.6154\n",
            "Epoch 35/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5475 - acc: 0.7528 - val_loss: 0.6623 - val_acc: 0.6154\n",
            "Epoch 36/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5432 - acc: 0.7640 - val_loss: 0.6615 - val_acc: 0.5385\n",
            "Epoch 37/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5389 - acc: 0.7640 - val_loss: 0.6604 - val_acc: 0.5385\n",
            "Epoch 38/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.5345 - acc: 0.7753 - val_loss: 0.6592 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 39/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.5303 - acc: 0.7753 - val_loss: 0.6590 - val_acc: 0.5385\n",
            "Epoch 40/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.5298 - acc: 0.7753 - val_loss: 0.6588 - val_acc: 0.5385\n",
            "Epoch 41/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.5294 - acc: 0.7753 - val_loss: 0.6585 - val_acc: 0.5385\n",
            "Epoch 42/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5289 - acc: 0.7753 - val_loss: 0.6582 - val_acc: 0.5385\n",
            "Epoch 43/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5285 - acc: 0.7753 - val_loss: 0.6579 - val_acc: 0.5385\n",
            "Epoch 44/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5280 - acc: 0.7865 - val_loss: 0.6576 - val_acc: 0.5385\n",
            "Epoch 45/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5275 - acc: 0.7865 - val_loss: 0.6572 - val_acc: 0.5385\n",
            "Epoch 46/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5271 - acc: 0.7865 - val_loss: 0.6568 - val_acc: 0.5385\n",
            "Epoch 47/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5266 - acc: 0.7865 - val_loss: 0.6565 - val_acc: 0.5385\n",
            "Epoch 48/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5261 - acc: 0.7865 - val_loss: 0.6561 - val_acc: 0.5385\n",
            "Epoch 49/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5256 - acc: 0.7865 - val_loss: 0.6559 - val_acc: 0.5385\n",
            "Epoch 50/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.5251 - acc: 0.7865 - val_loss: 0.6556 - val_acc: 0.5385\n",
            "Epoch 51/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5246 - acc: 0.7978 - val_loss: 0.6553 - val_acc: 0.5385\n",
            "Epoch 52/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.5242 - acc: 0.7978 - val_loss: 0.6551 - val_acc: 0.5385\n",
            "Epoch 53/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5237 - acc: 0.7978 - val_loss: 0.6549 - val_acc: 0.5385\n",
            "Epoch 54/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5232 - acc: 0.7978 - val_loss: 0.6548 - val_acc: 0.5385\n",
            "Epoch 55/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5227 - acc: 0.7978 - val_loss: 0.6546 - val_acc: 0.5385\n",
            "Epoch 56/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5222 - acc: 0.7978 - val_loss: 0.6545 - val_acc: 0.5385\n",
            "Epoch 57/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.5217 - acc: 0.7978 - val_loss: 0.6543 - val_acc: 0.5385\n",
            "Epoch 58/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5213 - acc: 0.7978 - val_loss: 0.6541 - val_acc: 0.5385\n",
            "Epoch 59/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5208 - acc: 0.7978 - val_loss: 0.6539 - val_acc: 0.5385\n",
            "Epoch 60/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5203 - acc: 0.7978 - val_loss: 0.6537 - val_acc: 0.5385\n",
            "Epoch 61/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.5198 - acc: 0.7978 - val_loss: 0.6535 - val_acc: 0.5385\n",
            "Epoch 62/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5193 - acc: 0.7978 - val_loss: 0.6534 - val_acc: 0.5385\n",
            "Epoch 63/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.5189 - acc: 0.7978 - val_loss: 0.6532 - val_acc: 0.4615\n",
            "Epoch 64/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5184 - acc: 0.7978 - val_loss: 0.6531 - val_acc: 0.4615\n",
            "Epoch 65/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.5179 - acc: 0.7978 - val_loss: 0.6529 - val_acc: 0.4615\n",
            "Epoch 66/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.5174 - acc: 0.7978 - val_loss: 0.6528 - val_acc: 0.4615\n",
            "Epoch 67/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5170 - acc: 0.7978 - val_loss: 0.6527 - val_acc: 0.4615\n",
            "Epoch 68/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.5165 - acc: 0.7978 - val_loss: 0.6526 - val_acc: 0.4615\n",
            "Epoch 69/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.5160 - acc: 0.7978 - val_loss: 0.6524 - val_acc: 0.4615\n",
            "Epoch 70/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.5156 - acc: 0.7978 - val_loss: 0.6523 - val_acc: 0.4615\n",
            "Epoch 71/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.5151 - acc: 0.7978 - val_loss: 0.6520 - val_acc: 0.4615\n",
            "Epoch 72/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5146 - acc: 0.7978 - val_loss: 0.6518 - val_acc: 0.4615\n",
            "Epoch 73/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5142 - acc: 0.7978 - val_loss: 0.6516 - val_acc: 0.4615\n",
            "Epoch 74/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.5137 - acc: 0.7978 - val_loss: 0.6514 - val_acc: 0.4615\n",
            "Epoch 75/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.5132 - acc: 0.7978 - val_loss: 0.6512 - val_acc: 0.5385\n",
            "Epoch 76/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.5128 - acc: 0.7978 - val_loss: 0.6510 - val_acc: 0.5385\n",
            "Epoch 77/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5123 - acc: 0.7978 - val_loss: 0.6508 - val_acc: 0.5385\n",
            "Epoch 78/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.5119 - acc: 0.7978 - val_loss: 0.6507 - val_acc: 0.5385\n",
            "Epoch 79/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.5114 - acc: 0.7978 - val_loss: 0.6505 - val_acc: 0.5385\n",
            "Epoch 80/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.5110 - acc: 0.7978 - val_loss: 0.6503 - val_acc: 0.5385\n",
            "Epoch 81/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5105 - acc: 0.7978 - val_loss: 0.6501 - val_acc: 0.5385\n",
            "Epoch 82/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5100 - acc: 0.7978 - val_loss: 0.6498 - val_acc: 0.5385\n",
            "Epoch 83/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.5096 - acc: 0.7978 - val_loss: 0.6496 - val_acc: 0.5385\n",
            "Epoch 84/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5091 - acc: 0.7978 - val_loss: 0.6493 - val_acc: 0.5385\n",
            "Epoch 85/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.5087 - acc: 0.7978 - val_loss: 0.6491 - val_acc: 0.5385\n",
            "Epoch 86/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.5082 - acc: 0.7978 - val_loss: 0.6488 - val_acc: 0.5385\n",
            "Epoch 87/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.5077 - acc: 0.7978 - val_loss: 0.6486 - val_acc: 0.5385\n",
            "Epoch 88/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.5073 - acc: 0.7978 - val_loss: 0.6484 - val_acc: 0.5385\n",
            "Epoch 89/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5068 - acc: 0.7978 - val_loss: 0.6482 - val_acc: 0.5385\n",
            "Epoch 90/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.5064 - acc: 0.7978 - val_loss: 0.6480 - val_acc: 0.5385\n",
            "Epoch 91/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.5059 - acc: 0.7978 - val_loss: 0.6478 - val_acc: 0.5385\n",
            "Epoch 92/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.5055 - acc: 0.7978 - val_loss: 0.6475 - val_acc: 0.5385\n",
            "Epoch 93/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.5050 - acc: 0.7978 - val_loss: 0.6472 - val_acc: 0.5385\n",
            "Epoch 94/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5046 - acc: 0.7978 - val_loss: 0.6470 - val_acc: 0.5385\n",
            "Epoch 95/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.5041 - acc: 0.7978 - val_loss: 0.6467 - val_acc: 0.5385\n",
            "Epoch 96/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.5037 - acc: 0.7978 - val_loss: 0.6465 - val_acc: 0.5385\n",
            "Epoch 97/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5033 - acc: 0.7978 - val_loss: 0.6462 - val_acc: 0.5385\n",
            "Epoch 98/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.5028 - acc: 0.7978 - val_loss: 0.6459 - val_acc: 0.5385\n",
            "Epoch 99/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.5024 - acc: 0.7978 - val_loss: 0.6457 - val_acc: 0.5385\n",
            "Epoch 100/1000\n",
            "89/89 [==============================] - 0s 84us/step - loss: 0.5019 - acc: 0.7978 - val_loss: 0.6454 - val_acc: 0.5385\n",
            "Epoch 101/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.5015 - acc: 0.7978 - val_loss: 0.6451 - val_acc: 0.5385\n",
            "Epoch 102/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.5010 - acc: 0.7978 - val_loss: 0.6447 - val_acc: 0.5385\n",
            "Epoch 103/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.5006 - acc: 0.7978 - val_loss: 0.6444 - val_acc: 0.5385\n",
            "Epoch 104/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.5002 - acc: 0.7978 - val_loss: 0.6441 - val_acc: 0.5385\n",
            "Epoch 105/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.4997 - acc: 0.7978 - val_loss: 0.6437 - val_acc: 0.5385\n",
            "Epoch 106/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4993 - acc: 0.7978 - val_loss: 0.6433 - val_acc: 0.5385\n",
            "Epoch 107/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4988 - acc: 0.7978 - val_loss: 0.6430 - val_acc: 0.5385\n",
            "Epoch 108/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.4984 - acc: 0.7978 - val_loss: 0.6426 - val_acc: 0.5385\n",
            "Epoch 109/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4979 - acc: 0.7978 - val_loss: 0.6422 - val_acc: 0.5385\n",
            "Epoch 110/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.4975 - acc: 0.7978 - val_loss: 0.6418 - val_acc: 0.5385\n",
            "Epoch 111/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.4970 - acc: 0.7978 - val_loss: 0.6414 - val_acc: 0.5385\n",
            "Epoch 112/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4966 - acc: 0.7978 - val_loss: 0.6410 - val_acc: 0.5385\n",
            "Epoch 113/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.4962 - acc: 0.7978 - val_loss: 0.6406 - val_acc: 0.5385\n",
            "Epoch 114/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4957 - acc: 0.7978 - val_loss: 0.6402 - val_acc: 0.5385\n",
            "Epoch 115/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.4953 - acc: 0.7978 - val_loss: 0.6397 - val_acc: 0.5385\n",
            "Epoch 116/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4948 - acc: 0.7978 - val_loss: 0.6393 - val_acc: 0.5385\n",
            "Epoch 117/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4944 - acc: 0.7978 - val_loss: 0.6389 - val_acc: 0.5385\n",
            "Epoch 118/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.4939 - acc: 0.7978 - val_loss: 0.6386 - val_acc: 0.5385\n",
            "Epoch 119/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4935 - acc: 0.8090 - val_loss: 0.6382 - val_acc: 0.6154\n",
            "Epoch 120/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.4931 - acc: 0.8090 - val_loss: 0.6379 - val_acc: 0.6154\n",
            "Epoch 121/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.4926 - acc: 0.8090 - val_loss: 0.6375 - val_acc: 0.6154\n",
            "Epoch 122/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.4922 - acc: 0.8090 - val_loss: 0.6372 - val_acc: 0.6154\n",
            "Epoch 123/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.4918 - acc: 0.8090 - val_loss: 0.6368 - val_acc: 0.6154\n",
            "Epoch 124/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4913 - acc: 0.8090 - val_loss: 0.6365 - val_acc: 0.6154\n",
            "Epoch 125/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.4909 - acc: 0.8090 - val_loss: 0.6361 - val_acc: 0.6154\n",
            "Epoch 126/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.4904 - acc: 0.8090 - val_loss: 0.6358 - val_acc: 0.6154\n",
            "Epoch 127/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4900 - acc: 0.8090 - val_loss: 0.6354 - val_acc: 0.6154\n",
            "Epoch 128/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.4896 - acc: 0.8090 - val_loss: 0.6350 - val_acc: 0.6154\n",
            "Epoch 129/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.4891 - acc: 0.8090 - val_loss: 0.6346 - val_acc: 0.6154\n",
            "Epoch 130/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.4887 - acc: 0.8090 - val_loss: 0.6342 - val_acc: 0.6154\n",
            "Epoch 131/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4882 - acc: 0.8090 - val_loss: 0.6339 - val_acc: 0.6154\n",
            "Epoch 132/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.4878 - acc: 0.8090 - val_loss: 0.6336 - val_acc: 0.6154\n",
            "Epoch 133/1000\n",
            "89/89 [==============================] - 0s 99us/step - loss: 0.4874 - acc: 0.8090 - val_loss: 0.6333 - val_acc: 0.6154\n",
            "Epoch 134/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4869 - acc: 0.8090 - val_loss: 0.6329 - val_acc: 0.6154\n",
            "Epoch 135/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4865 - acc: 0.8090 - val_loss: 0.6325 - val_acc: 0.6154\n",
            "Epoch 136/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.4861 - acc: 0.8090 - val_loss: 0.6321 - val_acc: 0.6154\n",
            "Epoch 137/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4856 - acc: 0.8090 - val_loss: 0.6317 - val_acc: 0.6154\n",
            "Epoch 138/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.4852 - acc: 0.8090 - val_loss: 0.6314 - val_acc: 0.6154\n",
            "Epoch 139/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.4847 - acc: 0.8090 - val_loss: 0.6310 - val_acc: 0.6154\n",
            "Epoch 140/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4843 - acc: 0.8090 - val_loss: 0.6306 - val_acc: 0.6154\n",
            "Epoch 141/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.4839 - acc: 0.8090 - val_loss: 0.6301 - val_acc: 0.6154\n",
            "Epoch 142/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.4834 - acc: 0.8090 - val_loss: 0.6298 - val_acc: 0.6154\n",
            "Epoch 143/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.4830 - acc: 0.8090 - val_loss: 0.6294 - val_acc: 0.6154\n",
            "Epoch 144/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.4826 - acc: 0.8090 - val_loss: 0.6289 - val_acc: 0.6154\n",
            "Epoch 145/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4821 - acc: 0.8090 - val_loss: 0.6285 - val_acc: 0.6154\n",
            "Epoch 146/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4817 - acc: 0.8090 - val_loss: 0.6280 - val_acc: 0.6154\n",
            "Epoch 147/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4813 - acc: 0.8090 - val_loss: 0.6275 - val_acc: 0.6154\n",
            "Epoch 148/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4808 - acc: 0.8090 - val_loss: 0.6270 - val_acc: 0.6154\n",
            "Epoch 149/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4804 - acc: 0.8090 - val_loss: 0.6266 - val_acc: 0.6154\n",
            "Epoch 150/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.4799 - acc: 0.8090 - val_loss: 0.6261 - val_acc: 0.6154\n",
            "Epoch 151/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4795 - acc: 0.8090 - val_loss: 0.6256 - val_acc: 0.6154\n",
            "Epoch 152/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.4791 - acc: 0.8090 - val_loss: 0.6252 - val_acc: 0.6154\n",
            "Epoch 153/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4786 - acc: 0.8090 - val_loss: 0.6248 - val_acc: 0.6154\n",
            "Epoch 154/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4782 - acc: 0.8090 - val_loss: 0.6244 - val_acc: 0.6154\n",
            "Epoch 155/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4778 - acc: 0.8090 - val_loss: 0.6240 - val_acc: 0.6154\n",
            "Epoch 156/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.4773 - acc: 0.8090 - val_loss: 0.6236 - val_acc: 0.6154\n",
            "Epoch 157/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.4769 - acc: 0.8090 - val_loss: 0.6232 - val_acc: 0.6154\n",
            "Epoch 158/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.4764 - acc: 0.8090 - val_loss: 0.6228 - val_acc: 0.6154\n",
            "Epoch 159/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.4760 - acc: 0.8090 - val_loss: 0.6224 - val_acc: 0.6154\n",
            "Epoch 160/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.4755 - acc: 0.8090 - val_loss: 0.6221 - val_acc: 0.6154\n",
            "Epoch 161/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.4751 - acc: 0.8090 - val_loss: 0.6216 - val_acc: 0.6154\n",
            "Epoch 162/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4746 - acc: 0.8090 - val_loss: 0.6212 - val_acc: 0.6154\n",
            "Epoch 163/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.4742 - acc: 0.8090 - val_loss: 0.6208 - val_acc: 0.6154\n",
            "Epoch 164/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4737 - acc: 0.8090 - val_loss: 0.6203 - val_acc: 0.6154\n",
            "Epoch 165/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.4733 - acc: 0.8090 - val_loss: 0.6199 - val_acc: 0.6154\n",
            "Epoch 166/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4728 - acc: 0.8090 - val_loss: 0.6194 - val_acc: 0.6154\n",
            "Epoch 167/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.4724 - acc: 0.8090 - val_loss: 0.6190 - val_acc: 0.6154\n",
            "Epoch 168/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4720 - acc: 0.8090 - val_loss: 0.6186 - val_acc: 0.6154\n",
            "Epoch 169/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.4715 - acc: 0.8090 - val_loss: 0.6182 - val_acc: 0.6154\n",
            "Epoch 170/1000\n",
            "89/89 [==============================] - 0s 80us/step - loss: 0.4710 - acc: 0.8090 - val_loss: 0.6178 - val_acc: 0.6154\n",
            "Epoch 171/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.4706 - acc: 0.8090 - val_loss: 0.6174 - val_acc: 0.6154\n",
            "Epoch 172/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.4701 - acc: 0.8090 - val_loss: 0.6170 - val_acc: 0.6923\n",
            "Epoch 173/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4697 - acc: 0.8090 - val_loss: 0.6165 - val_acc: 0.6923\n",
            "Epoch 174/1000\n",
            "89/89 [==============================] - 0s 89us/step - loss: 0.4692 - acc: 0.8090 - val_loss: 0.6160 - val_acc: 0.6923\n",
            "Epoch 175/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4688 - acc: 0.8090 - val_loss: 0.6156 - val_acc: 0.6923\n",
            "Epoch 176/1000\n",
            "89/89 [==============================] - 0s 103us/step - loss: 0.4683 - acc: 0.8090 - val_loss: 0.6151 - val_acc: 0.6923\n",
            "Epoch 177/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.4679 - acc: 0.8090 - val_loss: 0.6147 - val_acc: 0.6923\n",
            "Epoch 178/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4674 - acc: 0.8090 - val_loss: 0.6144 - val_acc: 0.6923\n",
            "Epoch 179/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4669 - acc: 0.8090 - val_loss: 0.6140 - val_acc: 0.6923\n",
            "Epoch 180/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4665 - acc: 0.8090 - val_loss: 0.6137 - val_acc: 0.6923\n",
            "Epoch 181/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4660 - acc: 0.8090 - val_loss: 0.6133 - val_acc: 0.6923\n",
            "Epoch 182/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.4656 - acc: 0.8090 - val_loss: 0.6129 - val_acc: 0.6923\n",
            "Epoch 183/1000\n",
            "89/89 [==============================] - 0s 88us/step - loss: 0.4651 - acc: 0.8090 - val_loss: 0.6125 - val_acc: 0.6923\n",
            "Epoch 184/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4646 - acc: 0.8090 - val_loss: 0.6121 - val_acc: 0.6923\n",
            "Epoch 185/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4641 - acc: 0.8090 - val_loss: 0.6118 - val_acc: 0.6923\n",
            "Epoch 186/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.4637 - acc: 0.8090 - val_loss: 0.6113 - val_acc: 0.6923\n",
            "Epoch 187/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.4632 - acc: 0.8090 - val_loss: 0.6109 - val_acc: 0.6923\n",
            "Epoch 188/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.4627 - acc: 0.8090 - val_loss: 0.6105 - val_acc: 0.6923\n",
            "Epoch 189/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.4622 - acc: 0.8090 - val_loss: 0.6100 - val_acc: 0.6923\n",
            "Epoch 190/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.4618 - acc: 0.8090 - val_loss: 0.6097 - val_acc: 0.6923\n",
            "Epoch 191/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.4613 - acc: 0.8090 - val_loss: 0.6093 - val_acc: 0.6923\n",
            "Epoch 192/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4608 - acc: 0.8090 - val_loss: 0.6089 - val_acc: 0.6923\n",
            "Epoch 193/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4603 - acc: 0.8090 - val_loss: 0.6085 - val_acc: 0.6923\n",
            "Epoch 194/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.4598 - acc: 0.8090 - val_loss: 0.6081 - val_acc: 0.6923\n",
            "Epoch 195/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.4593 - acc: 0.8090 - val_loss: 0.6077 - val_acc: 0.6923\n",
            "Epoch 196/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.4589 - acc: 0.8090 - val_loss: 0.6073 - val_acc: 0.6923\n",
            "Epoch 197/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4584 - acc: 0.8090 - val_loss: 0.6069 - val_acc: 0.6923\n",
            "Epoch 198/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4579 - acc: 0.8090 - val_loss: 0.6065 - val_acc: 0.6923\n",
            "Epoch 199/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.4574 - acc: 0.8090 - val_loss: 0.6061 - val_acc: 0.6923\n",
            "Epoch 200/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.4569 - acc: 0.8090 - val_loss: 0.6057 - val_acc: 0.6923\n",
            "Epoch 201/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.4564 - acc: 0.8090 - val_loss: 0.6053 - val_acc: 0.6923\n",
            "Epoch 202/1000\n",
            "89/89 [==============================] - 0s 80us/step - loss: 0.4559 - acc: 0.8090 - val_loss: 0.6049 - val_acc: 0.6923\n",
            "Epoch 203/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.4554 - acc: 0.8090 - val_loss: 0.6044 - val_acc: 0.6923\n",
            "Epoch 204/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.4549 - acc: 0.8090 - val_loss: 0.6040 - val_acc: 0.6923\n",
            "Epoch 205/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.4544 - acc: 0.8090 - val_loss: 0.6037 - val_acc: 0.6923\n",
            "Epoch 206/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.4539 - acc: 0.8090 - val_loss: 0.6033 - val_acc: 0.6923\n",
            "Epoch 207/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.4535 - acc: 0.8090 - val_loss: 0.6029 - val_acc: 0.6923\n",
            "Epoch 208/1000\n",
            "89/89 [==============================] - 0s 98us/step - loss: 0.4530 - acc: 0.8090 - val_loss: 0.6025 - val_acc: 0.6923\n",
            "Epoch 209/1000\n",
            "89/89 [==============================] - 0s 84us/step - loss: 0.4525 - acc: 0.8090 - val_loss: 0.6022 - val_acc: 0.6923\n",
            "Epoch 210/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.4520 - acc: 0.8090 - val_loss: 0.6018 - val_acc: 0.6923\n",
            "Epoch 211/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.4515 - acc: 0.8090 - val_loss: 0.6014 - val_acc: 0.6923\n",
            "Epoch 212/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.4510 - acc: 0.8090 - val_loss: 0.6011 - val_acc: 0.6923\n",
            "Epoch 213/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.4505 - acc: 0.8090 - val_loss: 0.6007 - val_acc: 0.6923\n",
            "Epoch 214/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.4500 - acc: 0.8090 - val_loss: 0.6002 - val_acc: 0.6923\n",
            "Epoch 215/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.4495 - acc: 0.8090 - val_loss: 0.5998 - val_acc: 0.6923\n",
            "Epoch 216/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.4490 - acc: 0.8090 - val_loss: 0.5994 - val_acc: 0.6923\n",
            "Epoch 217/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4485 - acc: 0.8090 - val_loss: 0.5990 - val_acc: 0.6923\n",
            "Epoch 218/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.4481 - acc: 0.8090 - val_loss: 0.5985 - val_acc: 0.6923\n",
            "Epoch 219/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4476 - acc: 0.8090 - val_loss: 0.5981 - val_acc: 0.6923\n",
            "Epoch 220/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.4471 - acc: 0.8090 - val_loss: 0.5976 - val_acc: 0.6923\n",
            "Epoch 221/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.4466 - acc: 0.8090 - val_loss: 0.5971 - val_acc: 0.6923\n",
            "Epoch 222/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.4461 - acc: 0.8090 - val_loss: 0.5967 - val_acc: 0.6923\n",
            "Epoch 223/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.4456 - acc: 0.8090 - val_loss: 0.5963 - val_acc: 0.6923\n",
            "Epoch 224/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.4451 - acc: 0.8090 - val_loss: 0.5959 - val_acc: 0.6923\n",
            "Epoch 225/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.4446 - acc: 0.8090 - val_loss: 0.5954 - val_acc: 0.6923\n",
            "Epoch 226/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4441 - acc: 0.8090 - val_loss: 0.5950 - val_acc: 0.6923\n",
            "Epoch 227/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.4436 - acc: 0.8090 - val_loss: 0.5946 - val_acc: 0.6923\n",
            "Epoch 228/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.4431 - acc: 0.8090 - val_loss: 0.5942 - val_acc: 0.6923\n",
            "Epoch 229/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.4426 - acc: 0.8090 - val_loss: 0.5937 - val_acc: 0.6923\n",
            "Epoch 230/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.4421 - acc: 0.8090 - val_loss: 0.5933 - val_acc: 0.6923\n",
            "Epoch 231/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4416 - acc: 0.8090 - val_loss: 0.5929 - val_acc: 0.6923\n",
            "Epoch 232/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.4411 - acc: 0.8090 - val_loss: 0.5926 - val_acc: 0.6923\n",
            "Epoch 233/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.4406 - acc: 0.8090 - val_loss: 0.5923 - val_acc: 0.6923\n",
            "Epoch 234/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4402 - acc: 0.8090 - val_loss: 0.5919 - val_acc: 0.6923\n",
            "Epoch 235/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.4397 - acc: 0.8090 - val_loss: 0.5915 - val_acc: 0.6923\n",
            "Epoch 236/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.4392 - acc: 0.8202 - val_loss: 0.5910 - val_acc: 0.6923\n",
            "Epoch 237/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4387 - acc: 0.8202 - val_loss: 0.5906 - val_acc: 0.6923\n",
            "Epoch 238/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4382 - acc: 0.8202 - val_loss: 0.5902 - val_acc: 0.6923\n",
            "Epoch 239/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.4377 - acc: 0.8202 - val_loss: 0.5897 - val_acc: 0.6923\n",
            "Epoch 240/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.4372 - acc: 0.8202 - val_loss: 0.5892 - val_acc: 0.6923\n",
            "Epoch 241/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4367 - acc: 0.8202 - val_loss: 0.5888 - val_acc: 0.6923\n",
            "Epoch 242/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.4362 - acc: 0.8202 - val_loss: 0.5884 - val_acc: 0.6923\n",
            "Epoch 243/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.4357 - acc: 0.8315 - val_loss: 0.5881 - val_acc: 0.6923\n",
            "Epoch 244/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.4352 - acc: 0.8315 - val_loss: 0.5877 - val_acc: 0.6923\n",
            "Epoch 245/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4347 - acc: 0.8315 - val_loss: 0.5873 - val_acc: 0.6923\n",
            "Epoch 246/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4342 - acc: 0.8315 - val_loss: 0.5869 - val_acc: 0.6923\n",
            "Epoch 247/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.4337 - acc: 0.8315 - val_loss: 0.5864 - val_acc: 0.6923\n",
            "Epoch 248/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4332 - acc: 0.8315 - val_loss: 0.5860 - val_acc: 0.6923\n",
            "Epoch 249/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4327 - acc: 0.8427 - val_loss: 0.5856 - val_acc: 0.6923\n",
            "Epoch 250/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.4322 - acc: 0.8427 - val_loss: 0.5852 - val_acc: 0.6923\n",
            "Epoch 251/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4316 - acc: 0.8427 - val_loss: 0.5848 - val_acc: 0.6923\n",
            "Epoch 252/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.4311 - acc: 0.8427 - val_loss: 0.5845 - val_acc: 0.6923\n",
            "Epoch 253/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4306 - acc: 0.8427 - val_loss: 0.5842 - val_acc: 0.6923\n",
            "Epoch 254/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4301 - acc: 0.8427 - val_loss: 0.5838 - val_acc: 0.6923\n",
            "Epoch 255/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4296 - acc: 0.8427 - val_loss: 0.5835 - val_acc: 0.6923\n",
            "Epoch 256/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.4291 - acc: 0.8427 - val_loss: 0.5831 - val_acc: 0.6923\n",
            "Epoch 257/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.4286 - acc: 0.8427 - val_loss: 0.5828 - val_acc: 0.6923\n",
            "Epoch 258/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.4281 - acc: 0.8427 - val_loss: 0.5825 - val_acc: 0.6923\n",
            "Epoch 259/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4276 - acc: 0.8427 - val_loss: 0.5821 - val_acc: 0.6923\n",
            "Epoch 260/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.4271 - acc: 0.8427 - val_loss: 0.5817 - val_acc: 0.6923\n",
            "Epoch 261/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.4266 - acc: 0.8427 - val_loss: 0.5813 - val_acc: 0.6923\n",
            "Epoch 262/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.4261 - acc: 0.8427 - val_loss: 0.5810 - val_acc: 0.6923\n",
            "Epoch 263/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.4256 - acc: 0.8427 - val_loss: 0.5807 - val_acc: 0.6923\n",
            "Epoch 264/1000\n",
            "89/89 [==============================] - 0s 80us/step - loss: 0.4252 - acc: 0.8427 - val_loss: 0.5804 - val_acc: 0.6923\n",
            "Epoch 265/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.4247 - acc: 0.8427 - val_loss: 0.5801 - val_acc: 0.6923\n",
            "Epoch 266/1000\n",
            "89/89 [==============================] - 0s 80us/step - loss: 0.4242 - acc: 0.8427 - val_loss: 0.5798 - val_acc: 0.6923\n",
            "Epoch 267/1000\n",
            "89/89 [==============================] - 0s 82us/step - loss: 0.4237 - acc: 0.8427 - val_loss: 0.5795 - val_acc: 0.6923\n",
            "Epoch 268/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4232 - acc: 0.8427 - val_loss: 0.5791 - val_acc: 0.6923\n",
            "Epoch 269/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.4227 - acc: 0.8427 - val_loss: 0.5788 - val_acc: 0.6923\n",
            "Epoch 270/1000\n",
            "89/89 [==============================] - 0s 79us/step - loss: 0.4222 - acc: 0.8427 - val_loss: 0.5786 - val_acc: 0.6923\n",
            "Epoch 271/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.4217 - acc: 0.8427 - val_loss: 0.5783 - val_acc: 0.6923\n",
            "Epoch 272/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.4212 - acc: 0.8427 - val_loss: 0.5780 - val_acc: 0.6923\n",
            "Epoch 273/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4207 - acc: 0.8427 - val_loss: 0.5777 - val_acc: 0.6923\n",
            "Epoch 274/1000\n",
            "89/89 [==============================] - 0s 90us/step - loss: 0.4202 - acc: 0.8427 - val_loss: 0.5774 - val_acc: 0.6923\n",
            "Epoch 275/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.4197 - acc: 0.8427 - val_loss: 0.5771 - val_acc: 0.6923\n",
            "Epoch 276/1000\n",
            "89/89 [==============================] - 0s 91us/step - loss: 0.4192 - acc: 0.8427 - val_loss: 0.5768 - val_acc: 0.6923\n",
            "Epoch 277/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.4187 - acc: 0.8427 - val_loss: 0.5765 - val_acc: 0.6923\n",
            "Epoch 278/1000\n",
            "89/89 [==============================] - 0s 92us/step - loss: 0.4182 - acc: 0.8427 - val_loss: 0.5762 - val_acc: 0.6923\n",
            "Epoch 279/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4177 - acc: 0.8427 - val_loss: 0.5760 - val_acc: 0.6923\n",
            "Epoch 280/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.4172 - acc: 0.8427 - val_loss: 0.5757 - val_acc: 0.6923\n",
            "Epoch 281/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.4167 - acc: 0.8427 - val_loss: 0.5754 - val_acc: 0.6923\n",
            "Epoch 282/1000\n",
            "89/89 [==============================] - 0s 92us/step - loss: 0.4162 - acc: 0.8427 - val_loss: 0.5751 - val_acc: 0.6923\n",
            "Epoch 283/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4157 - acc: 0.8427 - val_loss: 0.5748 - val_acc: 0.6923\n",
            "Epoch 284/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.4152 - acc: 0.8427 - val_loss: 0.5745 - val_acc: 0.6923\n",
            "Epoch 285/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.4147 - acc: 0.8427 - val_loss: 0.5743 - val_acc: 0.6923\n",
            "Epoch 286/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4142 - acc: 0.8427 - val_loss: 0.5740 - val_acc: 0.6923\n",
            "Epoch 287/1000\n",
            "89/89 [==============================] - 0s 91us/step - loss: 0.4137 - acc: 0.8427 - val_loss: 0.5738 - val_acc: 0.6923\n",
            "Epoch 288/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.4132 - acc: 0.8427 - val_loss: 0.5735 - val_acc: 0.6923\n",
            "Epoch 289/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.4127 - acc: 0.8427 - val_loss: 0.5733 - val_acc: 0.6923\n",
            "Epoch 290/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.4122 - acc: 0.8427 - val_loss: 0.5730 - val_acc: 0.6923\n",
            "Epoch 291/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4117 - acc: 0.8427 - val_loss: 0.5728 - val_acc: 0.6923\n",
            "Epoch 292/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.4112 - acc: 0.8427 - val_loss: 0.5725 - val_acc: 0.6923\n",
            "Epoch 293/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.4107 - acc: 0.8427 - val_loss: 0.5722 - val_acc: 0.6923\n",
            "Epoch 294/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.4102 - acc: 0.8427 - val_loss: 0.5719 - val_acc: 0.6923\n",
            "Epoch 295/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4097 - acc: 0.8427 - val_loss: 0.5717 - val_acc: 0.6923\n",
            "Epoch 296/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.4093 - acc: 0.8427 - val_loss: 0.5715 - val_acc: 0.6923\n",
            "Epoch 297/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.4088 - acc: 0.8427 - val_loss: 0.5713 - val_acc: 0.6923\n",
            "Epoch 298/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.4083 - acc: 0.8427 - val_loss: 0.5711 - val_acc: 0.6923\n",
            "Epoch 299/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.4078 - acc: 0.8427 - val_loss: 0.5708 - val_acc: 0.6923\n",
            "Epoch 300/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.4073 - acc: 0.8427 - val_loss: 0.5706 - val_acc: 0.6923\n",
            "Epoch 301/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.4068 - acc: 0.8427 - val_loss: 0.5704 - val_acc: 0.6923\n",
            "Epoch 302/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.4063 - acc: 0.8427 - val_loss: 0.5701 - val_acc: 0.6923\n",
            "Epoch 303/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.4058 - acc: 0.8427 - val_loss: 0.5699 - val_acc: 0.6923\n",
            "Epoch 304/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.4053 - acc: 0.8427 - val_loss: 0.5696 - val_acc: 0.6923\n",
            "Epoch 305/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.4048 - acc: 0.8427 - val_loss: 0.5694 - val_acc: 0.6923\n",
            "Epoch 306/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.4043 - acc: 0.8427 - val_loss: 0.5691 - val_acc: 0.6923\n",
            "Epoch 307/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.4038 - acc: 0.8427 - val_loss: 0.5689 - val_acc: 0.6923\n",
            "Epoch 308/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.4033 - acc: 0.8427 - val_loss: 0.5686 - val_acc: 0.6923\n",
            "Epoch 309/1000\n",
            "89/89 [==============================] - 0s 84us/step - loss: 0.4028 - acc: 0.8539 - val_loss: 0.5684 - val_acc: 0.6923\n",
            "Epoch 310/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.4023 - acc: 0.8539 - val_loss: 0.5683 - val_acc: 0.6923\n",
            "Epoch 311/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.4018 - acc: 0.8539 - val_loss: 0.5681 - val_acc: 0.6923\n",
            "Epoch 312/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4014 - acc: 0.8539 - val_loss: 0.5679 - val_acc: 0.6923\n",
            "Epoch 313/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.4009 - acc: 0.8539 - val_loss: 0.5677 - val_acc: 0.6923\n",
            "Epoch 314/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.4004 - acc: 0.8539 - val_loss: 0.5676 - val_acc: 0.6923\n",
            "Epoch 315/1000\n",
            "89/89 [==============================] - 0s 85us/step - loss: 0.3999 - acc: 0.8539 - val_loss: 0.5674 - val_acc: 0.6923\n",
            "Epoch 316/1000\n",
            "89/89 [==============================] - 0s 78us/step - loss: 0.3994 - acc: 0.8539 - val_loss: 0.5672 - val_acc: 0.6923\n",
            "Epoch 317/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3989 - acc: 0.8539 - val_loss: 0.5671 - val_acc: 0.6923\n",
            "Epoch 318/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3984 - acc: 0.8539 - val_loss: 0.5670 - val_acc: 0.6923\n",
            "Epoch 319/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3979 - acc: 0.8539 - val_loss: 0.5669 - val_acc: 0.6923\n",
            "Epoch 320/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.3974 - acc: 0.8539 - val_loss: 0.5668 - val_acc: 0.6923\n",
            "Epoch 321/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3969 - acc: 0.8539 - val_loss: 0.5666 - val_acc: 0.6923\n",
            "Epoch 322/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3964 - acc: 0.8539 - val_loss: 0.5664 - val_acc: 0.6923\n",
            "Epoch 323/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3959 - acc: 0.8539 - val_loss: 0.5663 - val_acc: 0.6923\n",
            "Epoch 324/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.3954 - acc: 0.8539 - val_loss: 0.5661 - val_acc: 0.6923\n",
            "Epoch 325/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.3949 - acc: 0.8539 - val_loss: 0.5660 - val_acc: 0.6923\n",
            "Epoch 326/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.3944 - acc: 0.8539 - val_loss: 0.5659 - val_acc: 0.6923\n",
            "Epoch 327/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3939 - acc: 0.8539 - val_loss: 0.5658 - val_acc: 0.6923\n",
            "Epoch 328/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.3935 - acc: 0.8539 - val_loss: 0.5657 - val_acc: 0.6923\n",
            "Epoch 329/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3930 - acc: 0.8539 - val_loss: 0.5656 - val_acc: 0.6923\n",
            "Epoch 330/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.3925 - acc: 0.8539 - val_loss: 0.5655 - val_acc: 0.6923\n",
            "Epoch 331/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3920 - acc: 0.8539 - val_loss: 0.5653 - val_acc: 0.6923\n",
            "Epoch 332/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.3915 - acc: 0.8539 - val_loss: 0.5652 - val_acc: 0.6923\n",
            "Epoch 333/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3910 - acc: 0.8539 - val_loss: 0.5650 - val_acc: 0.6923\n",
            "Epoch 334/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3905 - acc: 0.8539 - val_loss: 0.5649 - val_acc: 0.6923\n",
            "Epoch 335/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3900 - acc: 0.8539 - val_loss: 0.5648 - val_acc: 0.6923\n",
            "Epoch 336/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3895 - acc: 0.8539 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 337/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3891 - acc: 0.8539 - val_loss: 0.5645 - val_acc: 0.6923\n",
            "Epoch 338/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3885 - acc: 0.8539 - val_loss: 0.5644 - val_acc: 0.6923\n",
            "Epoch 339/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3880 - acc: 0.8539 - val_loss: 0.5642 - val_acc: 0.6923\n",
            "Epoch 340/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3875 - acc: 0.8539 - val_loss: 0.5641 - val_acc: 0.6923\n",
            "Epoch 341/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3870 - acc: 0.8539 - val_loss: 0.5640 - val_acc: 0.6923\n",
            "Epoch 342/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3865 - acc: 0.8539 - val_loss: 0.5639 - val_acc: 0.6923\n",
            "Epoch 343/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3860 - acc: 0.8539 - val_loss: 0.5637 - val_acc: 0.6923\n",
            "Epoch 344/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3855 - acc: 0.8539 - val_loss: 0.5636 - val_acc: 0.6923\n",
            "Epoch 345/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3850 - acc: 0.8539 - val_loss: 0.5635 - val_acc: 0.6923\n",
            "Epoch 346/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3845 - acc: 0.8539 - val_loss: 0.5633 - val_acc: 0.6923\n",
            "Epoch 347/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3840 - acc: 0.8539 - val_loss: 0.5632 - val_acc: 0.6923\n",
            "Epoch 348/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.3835 - acc: 0.8539 - val_loss: 0.5631 - val_acc: 0.6923\n",
            "Epoch 349/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3829 - acc: 0.8539 - val_loss: 0.5630 - val_acc: 0.6923\n",
            "Epoch 350/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.3824 - acc: 0.8539 - val_loss: 0.5629 - val_acc: 0.6923\n",
            "Epoch 351/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3819 - acc: 0.8539 - val_loss: 0.5627 - val_acc: 0.6923\n",
            "Epoch 352/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.3814 - acc: 0.8539 - val_loss: 0.5625 - val_acc: 0.6923\n",
            "Epoch 353/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3809 - acc: 0.8539 - val_loss: 0.5624 - val_acc: 0.6923\n",
            "Epoch 354/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3804 - acc: 0.8539 - val_loss: 0.5622 - val_acc: 0.6923\n",
            "Epoch 355/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.3799 - acc: 0.8539 - val_loss: 0.5621 - val_acc: 0.6923\n",
            "Epoch 356/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3794 - acc: 0.8539 - val_loss: 0.5619 - val_acc: 0.6923\n",
            "Epoch 357/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3789 - acc: 0.8539 - val_loss: 0.5618 - val_acc: 0.6923\n",
            "Epoch 358/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3784 - acc: 0.8539 - val_loss: 0.5617 - val_acc: 0.6923\n",
            "Epoch 359/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3779 - acc: 0.8539 - val_loss: 0.5615 - val_acc: 0.6923\n",
            "Epoch 360/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3774 - acc: 0.8539 - val_loss: 0.5615 - val_acc: 0.6923\n",
            "Epoch 361/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3769 - acc: 0.8539 - val_loss: 0.5615 - val_acc: 0.6923\n",
            "Epoch 362/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.3764 - acc: 0.8539 - val_loss: 0.5614 - val_acc: 0.6923\n",
            "Epoch 363/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3758 - acc: 0.8539 - val_loss: 0.5613 - val_acc: 0.6923\n",
            "Epoch 364/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3753 - acc: 0.8539 - val_loss: 0.5612 - val_acc: 0.6923\n",
            "Epoch 365/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3748 - acc: 0.8539 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 366/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3743 - acc: 0.8539 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 367/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3738 - acc: 0.8539 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 368/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.3733 - acc: 0.8539 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 369/1000\n",
            "89/89 [==============================] - 0s 126us/step - loss: 0.3728 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 370/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.3723 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 371/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3718 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 372/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3713 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 373/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3708 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 374/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3703 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 375/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.3698 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 376/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.3693 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 377/1000\n",
            "89/89 [==============================] - 0s 90us/step - loss: 0.3688 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 378/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.3683 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 379/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.3678 - acc: 0.8539 - val_loss: 0.5608 - val_acc: 0.6923\n",
            "Epoch 380/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.3673 - acc: 0.8539 - val_loss: 0.5608 - val_acc: 0.6923\n",
            "Epoch 381/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.3668 - acc: 0.8539 - val_loss: 0.5608 - val_acc: 0.6923\n",
            "Epoch 382/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3663 - acc: 0.8539 - val_loss: 0.5608 - val_acc: 0.6923\n",
            "Epoch 383/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.3658 - acc: 0.8539 - val_loss: 0.5608 - val_acc: 0.6923\n",
            "Epoch 384/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.3653 - acc: 0.8539 - val_loss: 0.5609 - val_acc: 0.6923\n",
            "Epoch 385/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3648 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 386/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.3643 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 387/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.3639 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 388/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.3634 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "Epoch 389/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.3629 - acc: 0.8539 - val_loss: 0.5610 - val_acc: 0.6923\n",
            "\n",
            "Epoch 00389: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 390/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.3624 - acc: 0.8652 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 391/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.3619 - acc: 0.8652 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 392/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3614 - acc: 0.8652 - val_loss: 0.5611 - val_acc: 0.6923\n",
            "Epoch 393/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3609 - acc: 0.8652 - val_loss: 0.5612 - val_acc: 0.6923\n",
            "Epoch 394/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.3604 - acc: 0.8652 - val_loss: 0.5612 - val_acc: 0.6923\n",
            "Epoch 395/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3599 - acc: 0.8652 - val_loss: 0.5612 - val_acc: 0.6923\n",
            "Epoch 396/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.3594 - acc: 0.8652 - val_loss: 0.5613 - val_acc: 0.6923\n",
            "Epoch 397/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.3589 - acc: 0.8652 - val_loss: 0.5613 - val_acc: 0.6923\n",
            "Epoch 398/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.3584 - acc: 0.8652 - val_loss: 0.5613 - val_acc: 0.6923\n",
            "Epoch 399/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3579 - acc: 0.8764 - val_loss: 0.5613 - val_acc: 0.6923\n",
            "Epoch 400/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3574 - acc: 0.8764 - val_loss: 0.5613 - val_acc: 0.6923\n",
            "Epoch 401/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3569 - acc: 0.8764 - val_loss: 0.5614 - val_acc: 0.6923\n",
            "Epoch 402/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3564 - acc: 0.8764 - val_loss: 0.5614 - val_acc: 0.6923\n",
            "Epoch 403/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3560 - acc: 0.8764 - val_loss: 0.5616 - val_acc: 0.6923\n",
            "Epoch 404/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3555 - acc: 0.8764 - val_loss: 0.5616 - val_acc: 0.6923\n",
            "Epoch 405/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.3550 - acc: 0.8764 - val_loss: 0.5616 - val_acc: 0.6923\n",
            "Epoch 406/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3545 - acc: 0.8764 - val_loss: 0.5617 - val_acc: 0.6923\n",
            "Epoch 407/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3540 - acc: 0.8764 - val_loss: 0.5618 - val_acc: 0.6923\n",
            "Epoch 408/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3535 - acc: 0.8764 - val_loss: 0.5618 - val_acc: 0.6923\n",
            "Epoch 409/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.3530 - acc: 0.8764 - val_loss: 0.5619 - val_acc: 0.6923\n",
            "Epoch 410/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3525 - acc: 0.8764 - val_loss: 0.5620 - val_acc: 0.6923\n",
            "Epoch 411/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.3520 - acc: 0.8764 - val_loss: 0.5621 - val_acc: 0.6923\n",
            "Epoch 412/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3515 - acc: 0.8764 - val_loss: 0.5622 - val_acc: 0.6923\n",
            "Epoch 413/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3510 - acc: 0.8764 - val_loss: 0.5623 - val_acc: 0.6923\n",
            "Epoch 414/1000\n",
            "89/89 [==============================] - 0s 88us/step - loss: 0.3505 - acc: 0.8764 - val_loss: 0.5623 - val_acc: 0.6923\n",
            "Epoch 415/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3500 - acc: 0.8764 - val_loss: 0.5625 - val_acc: 0.6923\n",
            "Epoch 416/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3495 - acc: 0.8764 - val_loss: 0.5626 - val_acc: 0.6923\n",
            "Epoch 417/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.3491 - acc: 0.8764 - val_loss: 0.5628 - val_acc: 0.6923\n",
            "Epoch 418/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3486 - acc: 0.8764 - val_loss: 0.5629 - val_acc: 0.6923\n",
            "Epoch 419/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.3481 - acc: 0.8764 - val_loss: 0.5630 - val_acc: 0.6923\n",
            "Epoch 420/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3476 - acc: 0.8764 - val_loss: 0.5632 - val_acc: 0.6923\n",
            "Epoch 421/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3471 - acc: 0.8764 - val_loss: 0.5633 - val_acc: 0.6923\n",
            "Epoch 422/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.3466 - acc: 0.8764 - val_loss: 0.5635 - val_acc: 0.6923\n",
            "Epoch 423/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.3461 - acc: 0.8764 - val_loss: 0.5636 - val_acc: 0.6923\n",
            "Epoch 424/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3456 - acc: 0.8876 - val_loss: 0.5638 - val_acc: 0.6923\n",
            "Epoch 425/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.3451 - acc: 0.8876 - val_loss: 0.5639 - val_acc: 0.6923\n",
            "Epoch 426/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3447 - acc: 0.8876 - val_loss: 0.5641 - val_acc: 0.6923\n",
            "Epoch 427/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.3442 - acc: 0.8876 - val_loss: 0.5642 - val_acc: 0.6923\n",
            "Epoch 428/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.3437 - acc: 0.8876 - val_loss: 0.5643 - val_acc: 0.6923\n",
            "Epoch 429/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.3432 - acc: 0.8876 - val_loss: 0.5644 - val_acc: 0.6923\n",
            "Epoch 430/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3427 - acc: 0.8876 - val_loss: 0.5644 - val_acc: 0.6923\n",
            "Epoch 431/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3422 - acc: 0.8876 - val_loss: 0.5645 - val_acc: 0.6923\n",
            "Epoch 432/1000\n",
            "89/89 [==============================] - 0s 81us/step - loss: 0.3417 - acc: 0.8876 - val_loss: 0.5645 - val_acc: 0.6923\n",
            "Epoch 433/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.3413 - acc: 0.8876 - val_loss: 0.5645 - val_acc: 0.6923\n",
            "Epoch 434/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3408 - acc: 0.8876 - val_loss: 0.5646 - val_acc: 0.6923\n",
            "Epoch 435/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.3403 - acc: 0.8876 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 436/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.3398 - acc: 0.8876 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 437/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3393 - acc: 0.8876 - val_loss: 0.5648 - val_acc: 0.6923\n",
            "Epoch 438/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.3388 - acc: 0.8876 - val_loss: 0.5648 - val_acc: 0.6923\n",
            "Epoch 439/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.3383 - acc: 0.8876 - val_loss: 0.5648 - val_acc: 0.6923\n",
            "Epoch 440/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.3378 - acc: 0.8876 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 441/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3374 - acc: 0.8876 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 442/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3369 - acc: 0.8876 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 443/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3364 - acc: 0.8876 - val_loss: 0.5647 - val_acc: 0.6923\n",
            "Epoch 444/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.3359 - acc: 0.8876 - val_loss: 0.5648 - val_acc: 0.6923\n",
            "Epoch 445/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3354 - acc: 0.8876 - val_loss: 0.5648 - val_acc: 0.6923\n",
            "Epoch 446/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.3349 - acc: 0.8876 - val_loss: 0.5649 - val_acc: 0.6923\n",
            "Epoch 447/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.3344 - acc: 0.8876 - val_loss: 0.5650 - val_acc: 0.6923\n",
            "Epoch 448/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3340 - acc: 0.8876 - val_loss: 0.5651 - val_acc: 0.7692\n",
            "Epoch 449/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.3335 - acc: 0.8876 - val_loss: 0.5651 - val_acc: 0.7692\n",
            "Epoch 450/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.3330 - acc: 0.8876 - val_loss: 0.5650 - val_acc: 0.7692\n",
            "Epoch 451/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.3325 - acc: 0.8876 - val_loss: 0.5650 - val_acc: 0.7692\n",
            "Epoch 452/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3320 - acc: 0.8876 - val_loss: 0.5651 - val_acc: 0.7692\n",
            "Epoch 453/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.3315 - acc: 0.8876 - val_loss: 0.5652 - val_acc: 0.7692\n",
            "Epoch 454/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3310 - acc: 0.8876 - val_loss: 0.5653 - val_acc: 0.7692\n",
            "Epoch 455/1000\n",
            "89/89 [==============================] - 0s 83us/step - loss: 0.3305 - acc: 0.8876 - val_loss: 0.5653 - val_acc: 0.7692\n",
            "Epoch 456/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3300 - acc: 0.8876 - val_loss: 0.5653 - val_acc: 0.7692\n",
            "Epoch 457/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3295 - acc: 0.8876 - val_loss: 0.5653 - val_acc: 0.7692\n",
            "Epoch 458/1000\n",
            "89/89 [==============================] - 0s 106us/step - loss: 0.3290 - acc: 0.8876 - val_loss: 0.5654 - val_acc: 0.7692\n",
            "Epoch 459/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3285 - acc: 0.8876 - val_loss: 0.5655 - val_acc: 0.7692\n",
            "Epoch 460/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3280 - acc: 0.8876 - val_loss: 0.5657 - val_acc: 0.7692\n",
            "Epoch 461/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3275 - acc: 0.8876 - val_loss: 0.5659 - val_acc: 0.7692\n",
            "Epoch 462/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3270 - acc: 0.8876 - val_loss: 0.5660 - val_acc: 0.7692\n",
            "Epoch 463/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3265 - acc: 0.8876 - val_loss: 0.5660 - val_acc: 0.7692\n",
            "Epoch 464/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3260 - acc: 0.8876 - val_loss: 0.5660 - val_acc: 0.7692\n",
            "Epoch 465/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3256 - acc: 0.8876 - val_loss: 0.5659 - val_acc: 0.7692\n",
            "Epoch 466/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3251 - acc: 0.8876 - val_loss: 0.5658 - val_acc: 0.7692\n",
            "Epoch 467/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3246 - acc: 0.8876 - val_loss: 0.5658 - val_acc: 0.7692\n",
            "Epoch 468/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3241 - acc: 0.8876 - val_loss: 0.5659 - val_acc: 0.7692\n",
            "Epoch 469/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.3236 - acc: 0.8876 - val_loss: 0.5660 - val_acc: 0.7692\n",
            "Epoch 470/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3231 - acc: 0.8876 - val_loss: 0.5660 - val_acc: 0.7692\n",
            "Epoch 471/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.3226 - acc: 0.8876 - val_loss: 0.5661 - val_acc: 0.7692\n",
            "Epoch 472/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.3221 - acc: 0.8876 - val_loss: 0.5662 - val_acc: 0.7692\n",
            "Epoch 473/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3216 - acc: 0.8876 - val_loss: 0.5663 - val_acc: 0.7692\n",
            "Epoch 474/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.3211 - acc: 0.8876 - val_loss: 0.5664 - val_acc: 0.7692\n",
            "Epoch 475/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3206 - acc: 0.8876 - val_loss: 0.5665 - val_acc: 0.7692\n",
            "Epoch 476/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.3200 - acc: 0.8876 - val_loss: 0.5666 - val_acc: 0.7692\n",
            "Epoch 477/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.3195 - acc: 0.8876 - val_loss: 0.5666 - val_acc: 0.7692\n",
            "Epoch 478/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3190 - acc: 0.8876 - val_loss: 0.5667 - val_acc: 0.7692\n",
            "Epoch 479/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.3185 - acc: 0.8876 - val_loss: 0.5667 - val_acc: 0.7692\n",
            "Epoch 480/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3180 - acc: 0.8876 - val_loss: 0.5668 - val_acc: 0.7692\n",
            "Epoch 481/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3175 - acc: 0.8876 - val_loss: 0.5669 - val_acc: 0.7692\n",
            "Epoch 482/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3170 - acc: 0.8876 - val_loss: 0.5670 - val_acc: 0.7692\n",
            "Epoch 483/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.3165 - acc: 0.8876 - val_loss: 0.5672 - val_acc: 0.7692\n",
            "Epoch 484/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3160 - acc: 0.8876 - val_loss: 0.5672 - val_acc: 0.7692\n",
            "Epoch 485/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.3155 - acc: 0.8876 - val_loss: 0.5673 - val_acc: 0.7692\n",
            "Epoch 486/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.3150 - acc: 0.8876 - val_loss: 0.5674 - val_acc: 0.7692\n",
            "Epoch 487/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3144 - acc: 0.8876 - val_loss: 0.5674 - val_acc: 0.7692\n",
            "Epoch 488/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.3139 - acc: 0.8876 - val_loss: 0.5675 - val_acc: 0.7692\n",
            "Epoch 489/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3134 - acc: 0.8876 - val_loss: 0.5675 - val_acc: 0.7692\n",
            "Epoch 490/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.3129 - acc: 0.8876 - val_loss: 0.5675 - val_acc: 0.7692\n",
            "Epoch 491/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.3124 - acc: 0.8876 - val_loss: 0.5675 - val_acc: 0.7692\n",
            "Epoch 492/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3119 - acc: 0.8876 - val_loss: 0.5676 - val_acc: 0.7692\n",
            "Epoch 493/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.3114 - acc: 0.8876 - val_loss: 0.5677 - val_acc: 0.7692\n",
            "Epoch 494/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.3108 - acc: 0.8876 - val_loss: 0.5677 - val_acc: 0.7692\n",
            "Epoch 495/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.3103 - acc: 0.8876 - val_loss: 0.5678 - val_acc: 0.7692\n",
            "Epoch 496/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.3098 - acc: 0.8876 - val_loss: 0.5679 - val_acc: 0.7692\n",
            "Epoch 497/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3093 - acc: 0.8876 - val_loss: 0.5680 - val_acc: 0.7692\n",
            "Epoch 498/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.3088 - acc: 0.8876 - val_loss: 0.5682 - val_acc: 0.7692\n",
            "Epoch 499/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.3083 - acc: 0.8989 - val_loss: 0.5684 - val_acc: 0.7692\n",
            "Epoch 500/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.3078 - acc: 0.8989 - val_loss: 0.5685 - val_acc: 0.7692\n",
            "Epoch 501/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3072 - acc: 0.8989 - val_loss: 0.5686 - val_acc: 0.7692\n",
            "Epoch 502/1000\n",
            "89/89 [==============================] - 0s 83us/step - loss: 0.3067 - acc: 0.8989 - val_loss: 0.5686 - val_acc: 0.7692\n",
            "Epoch 503/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.3062 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 504/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3057 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 505/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.3052 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 506/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3047 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 507/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.3042 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 508/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.3037 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 509/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.3031 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 510/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.3026 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 511/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3021 - acc: 0.8989 - val_loss: 0.5688 - val_acc: 0.7692\n",
            "Epoch 512/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.3016 - acc: 0.8989 - val_loss: 0.5688 - val_acc: 0.7692\n",
            "Epoch 513/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.3011 - acc: 0.8989 - val_loss: 0.5688 - val_acc: 0.7692\n",
            "Epoch 514/1000\n",
            "89/89 [==============================] - 0s 96us/step - loss: 0.3006 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 515/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.3001 - acc: 0.8989 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 516/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2996 - acc: 0.8989 - val_loss: 0.5686 - val_acc: 0.7692\n",
            "Epoch 517/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2991 - acc: 0.8989 - val_loss: 0.5686 - val_acc: 0.7692\n",
            "Epoch 518/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2986 - acc: 0.8989 - val_loss: 0.5688 - val_acc: 0.7692\n",
            "Epoch 519/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2981 - acc: 0.8989 - val_loss: 0.5689 - val_acc: 0.7692\n",
            "Epoch 520/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2976 - acc: 0.8989 - val_loss: 0.5691 - val_acc: 0.7692\n",
            "Epoch 521/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2971 - acc: 0.8989 - val_loss: 0.5692 - val_acc: 0.7692\n",
            "Epoch 522/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2966 - acc: 0.8989 - val_loss: 0.5693 - val_acc: 0.7692\n",
            "Epoch 523/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2961 - acc: 0.8989 - val_loss: 0.5694 - val_acc: 0.7692\n",
            "Epoch 524/1000\n",
            "89/89 [==============================] - 0s 91us/step - loss: 0.2956 - acc: 0.8989 - val_loss: 0.5695 - val_acc: 0.7692\n",
            "Epoch 525/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.2951 - acc: 0.8989 - val_loss: 0.5696 - val_acc: 0.7692\n",
            "Epoch 526/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2946 - acc: 0.9101 - val_loss: 0.5696 - val_acc: 0.7692\n",
            "Epoch 527/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2941 - acc: 0.9101 - val_loss: 0.5698 - val_acc: 0.7692\n",
            "Epoch 528/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2936 - acc: 0.9101 - val_loss: 0.5700 - val_acc: 0.7692\n",
            "Epoch 529/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2931 - acc: 0.9101 - val_loss: 0.5701 - val_acc: 0.7692\n",
            "Epoch 530/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2926 - acc: 0.9101 - val_loss: 0.5702 - val_acc: 0.7692\n",
            "Epoch 531/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.2921 - acc: 0.9101 - val_loss: 0.5704 - val_acc: 0.7692\n",
            "Epoch 532/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2917 - acc: 0.9101 - val_loss: 0.5706 - val_acc: 0.7692\n",
            "Epoch 533/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2912 - acc: 0.9213 - val_loss: 0.5709 - val_acc: 0.7692\n",
            "Epoch 534/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.2907 - acc: 0.9213 - val_loss: 0.5711 - val_acc: 0.7692\n",
            "Epoch 535/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.2902 - acc: 0.9213 - val_loss: 0.5713 - val_acc: 0.7692\n",
            "Epoch 536/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2897 - acc: 0.9213 - val_loss: 0.5715 - val_acc: 0.7692\n",
            "Epoch 537/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2892 - acc: 0.9213 - val_loss: 0.5717 - val_acc: 0.7692\n",
            "Epoch 538/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2888 - acc: 0.9213 - val_loss: 0.5720 - val_acc: 0.7692\n",
            "Epoch 539/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2883 - acc: 0.9213 - val_loss: 0.5722 - val_acc: 0.7692\n",
            "Epoch 540/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.2878 - acc: 0.9213 - val_loss: 0.5724 - val_acc: 0.7692\n",
            "Epoch 541/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2873 - acc: 0.9213 - val_loss: 0.5726 - val_acc: 0.7692\n",
            "Epoch 542/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2868 - acc: 0.9213 - val_loss: 0.5726 - val_acc: 0.7692\n",
            "Epoch 543/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2864 - acc: 0.9213 - val_loss: 0.5728 - val_acc: 0.7692\n",
            "Epoch 544/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2859 - acc: 0.9213 - val_loss: 0.5730 - val_acc: 0.7692\n",
            "Epoch 545/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2854 - acc: 0.9213 - val_loss: 0.5730 - val_acc: 0.7692\n",
            "Epoch 546/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2849 - acc: 0.9213 - val_loss: 0.5730 - val_acc: 0.7692\n",
            "Epoch 547/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2845 - acc: 0.9213 - val_loss: 0.5730 - val_acc: 0.7692\n",
            "Epoch 548/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2840 - acc: 0.9213 - val_loss: 0.5732 - val_acc: 0.7692\n",
            "Epoch 549/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2835 - acc: 0.9213 - val_loss: 0.5735 - val_acc: 0.7692\n",
            "Epoch 550/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2830 - acc: 0.9213 - val_loss: 0.5737 - val_acc: 0.7692\n",
            "Epoch 551/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2825 - acc: 0.9213 - val_loss: 0.5738 - val_acc: 0.7692\n",
            "Epoch 552/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.2821 - acc: 0.9213 - val_loss: 0.5739 - val_acc: 0.7692\n",
            "Epoch 553/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.2816 - acc: 0.9213 - val_loss: 0.5739 - val_acc: 0.7692\n",
            "Epoch 554/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2811 - acc: 0.9213 - val_loss: 0.5740 - val_acc: 0.7692\n",
            "Epoch 555/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2806 - acc: 0.9213 - val_loss: 0.5742 - val_acc: 0.7692\n",
            "Epoch 556/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2801 - acc: 0.9213 - val_loss: 0.5743 - val_acc: 0.7692\n",
            "Epoch 557/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2797 - acc: 0.9213 - val_loss: 0.5744 - val_acc: 0.7692\n",
            "Epoch 558/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2792 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 559/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2787 - acc: 0.9213 - val_loss: 0.5747 - val_acc: 0.7692\n",
            "Epoch 560/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2782 - acc: 0.9213 - val_loss: 0.5747 - val_acc: 0.7692\n",
            "Epoch 561/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2777 - acc: 0.9213 - val_loss: 0.5747 - val_acc: 0.7692\n",
            "Epoch 562/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2773 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 563/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2768 - acc: 0.9213 - val_loss: 0.5745 - val_acc: 0.7692\n",
            "Epoch 564/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2763 - acc: 0.9213 - val_loss: 0.5745 - val_acc: 0.7692\n",
            "Epoch 565/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2758 - acc: 0.9213 - val_loss: 0.5747 - val_acc: 0.7692\n",
            "Epoch 566/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2753 - acc: 0.9213 - val_loss: 0.5750 - val_acc: 0.7692\n",
            "Epoch 567/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.2749 - acc: 0.9213 - val_loss: 0.5750 - val_acc: 0.7692\n",
            "Epoch 568/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.2744 - acc: 0.9213 - val_loss: 0.5749 - val_acc: 0.7692\n",
            "Epoch 569/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2739 - acc: 0.9213 - val_loss: 0.5749 - val_acc: 0.7692\n",
            "Epoch 570/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.2734 - acc: 0.9213 - val_loss: 0.5748 - val_acc: 0.7692\n",
            "Epoch 571/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2730 - acc: 0.9213 - val_loss: 0.5748 - val_acc: 0.7692\n",
            "Epoch 572/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2725 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 573/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2720 - acc: 0.9213 - val_loss: 0.5745 - val_acc: 0.7692\n",
            "Epoch 574/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.2715 - acc: 0.9213 - val_loss: 0.5743 - val_acc: 0.7692\n",
            "Epoch 575/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.2711 - acc: 0.9213 - val_loss: 0.5742 - val_acc: 0.7692\n",
            "Epoch 576/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2706 - acc: 0.9213 - val_loss: 0.5742 - val_acc: 0.7692\n",
            "Epoch 577/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2701 - acc: 0.9213 - val_loss: 0.5743 - val_acc: 0.7692\n",
            "Epoch 578/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2696 - acc: 0.9213 - val_loss: 0.5743 - val_acc: 0.7692\n",
            "Epoch 579/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2692 - acc: 0.9213 - val_loss: 0.5743 - val_acc: 0.7692\n",
            "Epoch 580/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2687 - acc: 0.9213 - val_loss: 0.5742 - val_acc: 0.7692\n",
            "Epoch 581/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2682 - acc: 0.9213 - val_loss: 0.5742 - val_acc: 0.7692\n",
            "Epoch 582/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2677 - acc: 0.9213 - val_loss: 0.5744 - val_acc: 0.7692\n",
            "Epoch 583/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2673 - acc: 0.9213 - val_loss: 0.5745 - val_acc: 0.7692\n",
            "Epoch 584/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2668 - acc: 0.9213 - val_loss: 0.5745 - val_acc: 0.7692\n",
            "Epoch 585/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.2663 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 586/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2658 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 587/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2654 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 588/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.2649 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 589/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.2644 - acc: 0.9213 - val_loss: 0.5746 - val_acc: 0.7692\n",
            "Epoch 590/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.2640 - acc: 0.9213 - val_loss: 0.5747 - val_acc: 0.7692\n",
            "Epoch 591/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2635 - acc: 0.9213 - val_loss: 0.5747 - val_acc: 0.7692\n",
            "Epoch 592/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2630 - acc: 0.9213 - val_loss: 0.5748 - val_acc: 0.7692\n",
            "Epoch 593/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2626 - acc: 0.9213 - val_loss: 0.5748 - val_acc: 0.7692\n",
            "Epoch 594/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2621 - acc: 0.9213 - val_loss: 0.5748 - val_acc: 0.7692\n",
            "Epoch 595/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2616 - acc: 0.9213 - val_loss: 0.5749 - val_acc: 0.7692\n",
            "Epoch 596/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2612 - acc: 0.9213 - val_loss: 0.5751 - val_acc: 0.7692\n",
            "Epoch 597/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2607 - acc: 0.9213 - val_loss: 0.5752 - val_acc: 0.7692\n",
            "Epoch 598/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.2603 - acc: 0.9326 - val_loss: 0.5752 - val_acc: 0.7692\n",
            "Epoch 599/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2598 - acc: 0.9326 - val_loss: 0.5753 - val_acc: 0.7692\n",
            "Epoch 600/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2593 - acc: 0.9326 - val_loss: 0.5755 - val_acc: 0.7692\n",
            "Epoch 601/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.2588 - acc: 0.9326 - val_loss: 0.5756 - val_acc: 0.7692\n",
            "Epoch 602/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2584 - acc: 0.9326 - val_loss: 0.5757 - val_acc: 0.7692\n",
            "Epoch 603/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2579 - acc: 0.9326 - val_loss: 0.5759 - val_acc: 0.7692\n",
            "Epoch 604/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2574 - acc: 0.9326 - val_loss: 0.5760 - val_acc: 0.7692\n",
            "Epoch 605/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2569 - acc: 0.9326 - val_loss: 0.5762 - val_acc: 0.7692\n",
            "Epoch 606/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2565 - acc: 0.9326 - val_loss: 0.5764 - val_acc: 0.7692\n",
            "Epoch 607/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2560 - acc: 0.9326 - val_loss: 0.5766 - val_acc: 0.7692\n",
            "Epoch 608/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2555 - acc: 0.9326 - val_loss: 0.5767 - val_acc: 0.7692\n",
            "Epoch 609/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2551 - acc: 0.9326 - val_loss: 0.5769 - val_acc: 0.7692\n",
            "Epoch 610/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.2546 - acc: 0.9326 - val_loss: 0.5772 - val_acc: 0.7692\n",
            "Epoch 611/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2541 - acc: 0.9326 - val_loss: 0.5774 - val_acc: 0.7692\n",
            "Epoch 612/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.2536 - acc: 0.9438 - val_loss: 0.5776 - val_acc: 0.7692\n",
            "Epoch 613/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.2532 - acc: 0.9438 - val_loss: 0.5780 - val_acc: 0.7692\n",
            "Epoch 614/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2527 - acc: 0.9438 - val_loss: 0.5782 - val_acc: 0.7692\n",
            "Epoch 615/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.2522 - acc: 0.9438 - val_loss: 0.5785 - val_acc: 0.7692\n",
            "Epoch 616/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2517 - acc: 0.9438 - val_loss: 0.5788 - val_acc: 0.7692\n",
            "Epoch 617/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2513 - acc: 0.9438 - val_loss: 0.5792 - val_acc: 0.7692\n",
            "Epoch 618/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2508 - acc: 0.9438 - val_loss: 0.5794 - val_acc: 0.7692\n",
            "Epoch 619/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2503 - acc: 0.9438 - val_loss: 0.5796 - val_acc: 0.7692\n",
            "Epoch 620/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2499 - acc: 0.9438 - val_loss: 0.5797 - val_acc: 0.7692\n",
            "Epoch 621/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.2494 - acc: 0.9438 - val_loss: 0.5799 - val_acc: 0.7692\n",
            "Epoch 622/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2489 - acc: 0.9438 - val_loss: 0.5801 - val_acc: 0.7692\n",
            "Epoch 623/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2485 - acc: 0.9438 - val_loss: 0.5803 - val_acc: 0.7692\n",
            "Epoch 624/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2480 - acc: 0.9438 - val_loss: 0.5806 - val_acc: 0.7692\n",
            "Epoch 625/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2475 - acc: 0.9438 - val_loss: 0.5809 - val_acc: 0.7692\n",
            "Epoch 626/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.2471 - acc: 0.9438 - val_loss: 0.5812 - val_acc: 0.7692\n",
            "Epoch 627/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2466 - acc: 0.9438 - val_loss: 0.5814 - val_acc: 0.7692\n",
            "Epoch 628/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2461 - acc: 0.9438 - val_loss: 0.5817 - val_acc: 0.7692\n",
            "Epoch 629/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2457 - acc: 0.9438 - val_loss: 0.5820 - val_acc: 0.7692\n",
            "Epoch 630/1000\n",
            "89/89 [==============================] - 0s 87us/step - loss: 0.2452 - acc: 0.9438 - val_loss: 0.5824 - val_acc: 0.7692\n",
            "Epoch 631/1000\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.2447 - acc: 0.9438 - val_loss: 0.5827 - val_acc: 0.7692\n",
            "Epoch 632/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.2443 - acc: 0.9438 - val_loss: 0.5829 - val_acc: 0.7692\n",
            "Epoch 633/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2438 - acc: 0.9551 - val_loss: 0.5831 - val_acc: 0.7692\n",
            "Epoch 634/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2433 - acc: 0.9551 - val_loss: 0.5834 - val_acc: 0.7692\n",
            "Epoch 635/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2429 - acc: 0.9551 - val_loss: 0.5839 - val_acc: 0.7692\n",
            "Epoch 636/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2424 - acc: 0.9551 - val_loss: 0.5843 - val_acc: 0.7692\n",
            "Epoch 637/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2420 - acc: 0.9551 - val_loss: 0.5847 - val_acc: 0.7692\n",
            "Epoch 638/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.2415 - acc: 0.9551 - val_loss: 0.5851 - val_acc: 0.7692\n",
            "Epoch 639/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.2410 - acc: 0.9551 - val_loss: 0.5855 - val_acc: 0.7692\n",
            "Epoch 640/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2406 - acc: 0.9551 - val_loss: 0.5857 - val_acc: 0.7692\n",
            "Epoch 641/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.2401 - acc: 0.9551 - val_loss: 0.5859 - val_acc: 0.7692\n",
            "Epoch 642/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.2397 - acc: 0.9551 - val_loss: 0.5862 - val_acc: 0.7692\n",
            "Epoch 643/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2392 - acc: 0.9551 - val_loss: 0.5865 - val_acc: 0.7692\n",
            "Epoch 644/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2387 - acc: 0.9551 - val_loss: 0.5868 - val_acc: 0.7692\n",
            "Epoch 645/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2383 - acc: 0.9551 - val_loss: 0.5873 - val_acc: 0.7692\n",
            "Epoch 646/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.2378 - acc: 0.9551 - val_loss: 0.5878 - val_acc: 0.7692\n",
            "Epoch 647/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2374 - acc: 0.9551 - val_loss: 0.5881 - val_acc: 0.7692\n",
            "Epoch 648/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2369 - acc: 0.9551 - val_loss: 0.5883 - val_acc: 0.7692\n",
            "Epoch 649/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.2365 - acc: 0.9551 - val_loss: 0.5885 - val_acc: 0.7692\n",
            "Epoch 650/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.2360 - acc: 0.9551 - val_loss: 0.5889 - val_acc: 0.7692\n",
            "Epoch 651/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2355 - acc: 0.9551 - val_loss: 0.5893 - val_acc: 0.7692\n",
            "Epoch 652/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2351 - acc: 0.9551 - val_loss: 0.5898 - val_acc: 0.7692\n",
            "Epoch 653/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2346 - acc: 0.9551 - val_loss: 0.5902 - val_acc: 0.7692\n",
            "Epoch 654/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.2342 - acc: 0.9551 - val_loss: 0.5906 - val_acc: 0.7692\n",
            "Epoch 655/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2337 - acc: 0.9551 - val_loss: 0.5908 - val_acc: 0.7692\n",
            "Epoch 656/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2333 - acc: 0.9551 - val_loss: 0.5909 - val_acc: 0.7692\n",
            "Epoch 657/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.2328 - acc: 0.9551 - val_loss: 0.5913 - val_acc: 0.7692\n",
            "Epoch 658/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2324 - acc: 0.9551 - val_loss: 0.5918 - val_acc: 0.7692\n",
            "Epoch 659/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2319 - acc: 0.9551 - val_loss: 0.5923 - val_acc: 0.7692\n",
            "Epoch 660/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.2315 - acc: 0.9551 - val_loss: 0.5927 - val_acc: 0.7692\n",
            "Epoch 661/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.2310 - acc: 0.9551 - val_loss: 0.5929 - val_acc: 0.7692\n",
            "Epoch 662/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2306 - acc: 0.9551 - val_loss: 0.5931 - val_acc: 0.7692\n",
            "Epoch 663/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2302 - acc: 0.9551 - val_loss: 0.5933 - val_acc: 0.7692\n",
            "Epoch 664/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2297 - acc: 0.9551 - val_loss: 0.5937 - val_acc: 0.7692\n",
            "Epoch 665/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2293 - acc: 0.9551 - val_loss: 0.5940 - val_acc: 0.7692\n",
            "Epoch 666/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2288 - acc: 0.9551 - val_loss: 0.5941 - val_acc: 0.7692\n",
            "Epoch 667/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.2284 - acc: 0.9551 - val_loss: 0.5943 - val_acc: 0.7692\n",
            "Epoch 668/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2279 - acc: 0.9551 - val_loss: 0.5943 - val_acc: 0.7692\n",
            "Epoch 669/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2275 - acc: 0.9551 - val_loss: 0.5944 - val_acc: 0.7692\n",
            "Epoch 670/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.2271 - acc: 0.9551 - val_loss: 0.5945 - val_acc: 0.7692\n",
            "Epoch 671/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2266 - acc: 0.9551 - val_loss: 0.5948 - val_acc: 0.7692\n",
            "Epoch 672/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.2262 - acc: 0.9551 - val_loss: 0.5950 - val_acc: 0.7692\n",
            "Epoch 673/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.2257 - acc: 0.9663 - val_loss: 0.5952 - val_acc: 0.7692\n",
            "Epoch 674/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2253 - acc: 0.9663 - val_loss: 0.5954 - val_acc: 0.7692\n",
            "Epoch 675/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2249 - acc: 0.9663 - val_loss: 0.5957 - val_acc: 0.7692\n",
            "Epoch 676/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.2244 - acc: 0.9663 - val_loss: 0.5961 - val_acc: 0.7692\n",
            "Epoch 677/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.2240 - acc: 0.9663 - val_loss: 0.5965 - val_acc: 0.7692\n",
            "Epoch 678/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.2236 - acc: 0.9663 - val_loss: 0.5970 - val_acc: 0.7692\n",
            "Epoch 679/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2231 - acc: 0.9663 - val_loss: 0.5974 - val_acc: 0.7692\n",
            "Epoch 680/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.2227 - acc: 0.9663 - val_loss: 0.5976 - val_acc: 0.7692\n",
            "Epoch 681/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.2223 - acc: 0.9663 - val_loss: 0.5978 - val_acc: 0.7692\n",
            "Epoch 682/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.2219 - acc: 0.9663 - val_loss: 0.5981 - val_acc: 0.7692\n",
            "Epoch 683/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.2214 - acc: 0.9663 - val_loss: 0.5985 - val_acc: 0.7692\n",
            "Epoch 684/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.2210 - acc: 0.9663 - val_loss: 0.5990 - val_acc: 0.7692\n",
            "Epoch 685/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.2206 - acc: 0.9663 - val_loss: 0.5995 - val_acc: 0.7692\n",
            "Epoch 686/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2201 - acc: 0.9663 - val_loss: 0.6001 - val_acc: 0.7692\n",
            "Epoch 687/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2197 - acc: 0.9663 - val_loss: 0.6005 - val_acc: 0.7692\n",
            "Epoch 688/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2193 - acc: 0.9663 - val_loss: 0.6010 - val_acc: 0.7692\n",
            "Epoch 689/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2189 - acc: 0.9663 - val_loss: 0.6015 - val_acc: 0.7692\n",
            "Epoch 690/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2185 - acc: 0.9663 - val_loss: 0.6021 - val_acc: 0.7692\n",
            "Epoch 691/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2180 - acc: 0.9663 - val_loss: 0.6027 - val_acc: 0.7692\n",
            "Epoch 692/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.2176 - acc: 0.9663 - val_loss: 0.6030 - val_acc: 0.7692\n",
            "Epoch 693/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2172 - acc: 0.9663 - val_loss: 0.6035 - val_acc: 0.7692\n",
            "Epoch 694/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.2168 - acc: 0.9663 - val_loss: 0.6040 - val_acc: 0.7692\n",
            "Epoch 695/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.2164 - acc: 0.9663 - val_loss: 0.6045 - val_acc: 0.7692\n",
            "Epoch 696/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.2159 - acc: 0.9663 - val_loss: 0.6049 - val_acc: 0.6923\n",
            "Epoch 697/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.2155 - acc: 0.9663 - val_loss: 0.6052 - val_acc: 0.6923\n",
            "Epoch 698/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2151 - acc: 0.9663 - val_loss: 0.6055 - val_acc: 0.6923\n",
            "Epoch 699/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2147 - acc: 0.9663 - val_loss: 0.6061 - val_acc: 0.6923\n",
            "Epoch 700/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2143 - acc: 0.9663 - val_loss: 0.6067 - val_acc: 0.6923\n",
            "Epoch 701/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.2139 - acc: 0.9663 - val_loss: 0.6073 - val_acc: 0.6923\n",
            "Epoch 702/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2135 - acc: 0.9663 - val_loss: 0.6077 - val_acc: 0.6923\n",
            "Epoch 703/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2130 - acc: 0.9663 - val_loss: 0.6080 - val_acc: 0.6923\n",
            "Epoch 704/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.2126 - acc: 0.9663 - val_loss: 0.6082 - val_acc: 0.6923\n",
            "Epoch 705/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2122 - acc: 0.9663 - val_loss: 0.6085 - val_acc: 0.6923\n",
            "Epoch 706/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.2118 - acc: 0.9663 - val_loss: 0.6089 - val_acc: 0.6923\n",
            "Epoch 707/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.2114 - acc: 0.9663 - val_loss: 0.6094 - val_acc: 0.6923\n",
            "Epoch 708/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2110 - acc: 0.9663 - val_loss: 0.6100 - val_acc: 0.6923\n",
            "Epoch 709/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2106 - acc: 0.9663 - val_loss: 0.6107 - val_acc: 0.6923\n",
            "Epoch 710/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.2102 - acc: 0.9663 - val_loss: 0.6112 - val_acc: 0.6923\n",
            "Epoch 711/1000\n",
            "89/89 [==============================] - 0s 80us/step - loss: 0.2098 - acc: 0.9663 - val_loss: 0.6116 - val_acc: 0.6923\n",
            "Epoch 712/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.2094 - acc: 0.9663 - val_loss: 0.6120 - val_acc: 0.6923\n",
            "Epoch 713/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.2089 - acc: 0.9663 - val_loss: 0.6121 - val_acc: 0.6923\n",
            "Epoch 714/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2085 - acc: 0.9663 - val_loss: 0.6124 - val_acc: 0.6923\n",
            "Epoch 715/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2081 - acc: 0.9663 - val_loss: 0.6128 - val_acc: 0.6923\n",
            "Epoch 716/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.2077 - acc: 0.9663 - val_loss: 0.6134 - val_acc: 0.6923\n",
            "Epoch 717/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2073 - acc: 0.9663 - val_loss: 0.6140 - val_acc: 0.6923\n",
            "Epoch 718/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2069 - acc: 0.9663 - val_loss: 0.6145 - val_acc: 0.6923\n",
            "Epoch 719/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2065 - acc: 0.9663 - val_loss: 0.6148 - val_acc: 0.6923\n",
            "Epoch 720/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.2061 - acc: 0.9663 - val_loss: 0.6153 - val_acc: 0.6923\n",
            "Epoch 721/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.2057 - acc: 0.9663 - val_loss: 0.6159 - val_acc: 0.6923\n",
            "Epoch 722/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.2053 - acc: 0.9663 - val_loss: 0.6162 - val_acc: 0.6923\n",
            "Epoch 723/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.2049 - acc: 0.9663 - val_loss: 0.6167 - val_acc: 0.6923\n",
            "Epoch 724/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.2045 - acc: 0.9663 - val_loss: 0.6175 - val_acc: 0.6923\n",
            "Epoch 725/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.2041 - acc: 0.9663 - val_loss: 0.6181 - val_acc: 0.6923\n",
            "Epoch 726/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2037 - acc: 0.9663 - val_loss: 0.6187 - val_acc: 0.6923\n",
            "Epoch 727/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.2033 - acc: 0.9663 - val_loss: 0.6193 - val_acc: 0.6923\n",
            "Epoch 728/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.2029 - acc: 0.9663 - val_loss: 0.6200 - val_acc: 0.6923\n",
            "Epoch 729/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.2025 - acc: 0.9663 - val_loss: 0.6206 - val_acc: 0.6923\n",
            "Epoch 730/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.2021 - acc: 0.9663 - val_loss: 0.6213 - val_acc: 0.6923\n",
            "Epoch 731/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.2017 - acc: 0.9663 - val_loss: 0.6220 - val_acc: 0.6923\n",
            "Epoch 732/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.2013 - acc: 0.9663 - val_loss: 0.6227 - val_acc: 0.6923\n",
            "Epoch 733/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2009 - acc: 0.9663 - val_loss: 0.6232 - val_acc: 0.6923\n",
            "Epoch 734/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.2005 - acc: 0.9663 - val_loss: 0.6238 - val_acc: 0.6923\n",
            "Epoch 735/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.2001 - acc: 0.9663 - val_loss: 0.6246 - val_acc: 0.6923\n",
            "Epoch 736/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1997 - acc: 0.9663 - val_loss: 0.6254 - val_acc: 0.6923\n",
            "Epoch 737/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1993 - acc: 0.9663 - val_loss: 0.6261 - val_acc: 0.6923\n",
            "Epoch 738/1000\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1989 - acc: 0.9663 - val_loss: 0.6268 - val_acc: 0.6923\n",
            "Epoch 739/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.1985 - acc: 0.9663 - val_loss: 0.6274 - val_acc: 0.6923\n",
            "Epoch 740/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.1980 - acc: 0.9663 - val_loss: 0.6280 - val_acc: 0.6923\n",
            "Epoch 741/1000\n",
            "89/89 [==============================] - 0s 89us/step - loss: 0.1976 - acc: 0.9663 - val_loss: 0.6287 - val_acc: 0.6923\n",
            "Epoch 742/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1972 - acc: 0.9663 - val_loss: 0.6294 - val_acc: 0.6923\n",
            "Epoch 743/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1968 - acc: 0.9663 - val_loss: 0.6300 - val_acc: 0.6923\n",
            "Epoch 744/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1964 - acc: 0.9663 - val_loss: 0.6307 - val_acc: 0.6923\n",
            "Epoch 745/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1960 - acc: 0.9663 - val_loss: 0.6314 - val_acc: 0.6923\n",
            "Epoch 746/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1956 - acc: 0.9663 - val_loss: 0.6319 - val_acc: 0.6923\n",
            "Epoch 747/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1952 - acc: 0.9663 - val_loss: 0.6325 - val_acc: 0.6923\n",
            "Epoch 748/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1948 - acc: 0.9663 - val_loss: 0.6333 - val_acc: 0.6923\n",
            "Epoch 749/1000\n",
            "89/89 [==============================] - 0s 86us/step - loss: 0.1944 - acc: 0.9663 - val_loss: 0.6342 - val_acc: 0.6923\n",
            "Epoch 750/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1940 - acc: 0.9663 - val_loss: 0.6350 - val_acc: 0.6923\n",
            "Epoch 751/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1936 - acc: 0.9663 - val_loss: 0.6355 - val_acc: 0.6923\n",
            "Epoch 752/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1932 - acc: 0.9663 - val_loss: 0.6360 - val_acc: 0.6923\n",
            "Epoch 753/1000\n",
            "89/89 [==============================] - 0s 75us/step - loss: 0.1928 - acc: 0.9663 - val_loss: 0.6366 - val_acc: 0.6923\n",
            "Epoch 754/1000\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1924 - acc: 0.9663 - val_loss: 0.6373 - val_acc: 0.6923\n",
            "Epoch 755/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1920 - acc: 0.9663 - val_loss: 0.6379 - val_acc: 0.6923\n",
            "Epoch 756/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1916 - acc: 0.9663 - val_loss: 0.6386 - val_acc: 0.6923\n",
            "Epoch 757/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1912 - acc: 0.9663 - val_loss: 0.6391 - val_acc: 0.6923\n",
            "Epoch 758/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1908 - acc: 0.9663 - val_loss: 0.6398 - val_acc: 0.6923\n",
            "Epoch 759/1000\n",
            "89/89 [==============================] - 0s 73us/step - loss: 0.1904 - acc: 0.9663 - val_loss: 0.6403 - val_acc: 0.6923\n",
            "Epoch 760/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1900 - acc: 0.9663 - val_loss: 0.6408 - val_acc: 0.6923\n",
            "Epoch 761/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1896 - acc: 0.9663 - val_loss: 0.6412 - val_acc: 0.6923\n",
            "Epoch 762/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1892 - acc: 0.9775 - val_loss: 0.6415 - val_acc: 0.6923\n",
            "Epoch 763/1000\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.1888 - acc: 0.9775 - val_loss: 0.6420 - val_acc: 0.6923\n",
            "Epoch 764/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1884 - acc: 0.9775 - val_loss: 0.6426 - val_acc: 0.6923\n",
            "Epoch 765/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1880 - acc: 0.9775 - val_loss: 0.6434 - val_acc: 0.6923\n",
            "Epoch 766/1000\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1876 - acc: 0.9775 - val_loss: 0.6440 - val_acc: 0.6923\n",
            "Epoch 767/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1872 - acc: 0.9775 - val_loss: 0.6443 - val_acc: 0.6923\n",
            "Epoch 768/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1868 - acc: 0.9775 - val_loss: 0.6445 - val_acc: 0.6923\n",
            "Epoch 769/1000\n",
            "89/89 [==============================] - 0s 68us/step - loss: 0.1864 - acc: 0.9775 - val_loss: 0.6450 - val_acc: 0.6923\n",
            "Epoch 770/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1860 - acc: 0.9775 - val_loss: 0.6456 - val_acc: 0.6923\n",
            "Epoch 771/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1857 - acc: 0.9775 - val_loss: 0.6464 - val_acc: 0.6923\n",
            "Epoch 772/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1853 - acc: 0.9775 - val_loss: 0.6468 - val_acc: 0.6923\n",
            "Epoch 773/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1849 - acc: 0.9775 - val_loss: 0.6471 - val_acc: 0.6923\n",
            "Epoch 774/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1845 - acc: 0.9775 - val_loss: 0.6475 - val_acc: 0.6923\n",
            "Epoch 775/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1841 - acc: 0.9775 - val_loss: 0.6480 - val_acc: 0.6923\n",
            "Epoch 776/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1837 - acc: 0.9775 - val_loss: 0.6487 - val_acc: 0.6923\n",
            "Epoch 777/1000\n",
            "89/89 [==============================] - 0s 72us/step - loss: 0.1833 - acc: 0.9775 - val_loss: 0.6494 - val_acc: 0.6923\n",
            "Epoch 778/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1830 - acc: 0.9775 - val_loss: 0.6498 - val_acc: 0.6923\n",
            "Epoch 779/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1826 - acc: 0.9775 - val_loss: 0.6500 - val_acc: 0.6923\n",
            "Epoch 780/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1822 - acc: 0.9775 - val_loss: 0.6502 - val_acc: 0.6923\n",
            "Epoch 781/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1818 - acc: 0.9775 - val_loss: 0.6506 - val_acc: 0.6923\n",
            "Epoch 782/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1814 - acc: 0.9775 - val_loss: 0.6513 - val_acc: 0.6923\n",
            "Epoch 783/1000\n",
            "89/89 [==============================] - 0s 70us/step - loss: 0.1811 - acc: 0.9775 - val_loss: 0.6520 - val_acc: 0.6923\n",
            "Epoch 784/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1807 - acc: 0.9775 - val_loss: 0.6527 - val_acc: 0.6923\n",
            "Epoch 785/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1803 - acc: 0.9775 - val_loss: 0.6532 - val_acc: 0.6923\n",
            "Epoch 786/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1799 - acc: 0.9775 - val_loss: 0.6537 - val_acc: 0.6923\n",
            "Epoch 787/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1795 - acc: 0.9775 - val_loss: 0.6542 - val_acc: 0.6923\n",
            "Epoch 788/1000\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.1792 - acc: 0.9775 - val_loss: 0.6547 - val_acc: 0.6923\n",
            "Epoch 789/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1788 - acc: 0.9775 - val_loss: 0.6553 - val_acc: 0.6923\n",
            "Epoch 790/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1784 - acc: 0.9775 - val_loss: 0.6558 - val_acc: 0.6923\n",
            "Epoch 791/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1780 - acc: 0.9775 - val_loss: 0.6562 - val_acc: 0.6923\n",
            "Epoch 792/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1777 - acc: 0.9775 - val_loss: 0.6567 - val_acc: 0.6923\n",
            "Epoch 793/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1773 - acc: 0.9775 - val_loss: 0.6573 - val_acc: 0.6923\n",
            "Epoch 794/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1769 - acc: 0.9775 - val_loss: 0.6578 - val_acc: 0.6923\n",
            "Epoch 795/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1765 - acc: 0.9775 - val_loss: 0.6581 - val_acc: 0.6923\n",
            "Epoch 796/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1762 - acc: 0.9775 - val_loss: 0.6586 - val_acc: 0.6923\n",
            "Epoch 797/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.1758 - acc: 0.9775 - val_loss: 0.6593 - val_acc: 0.6923\n",
            "Epoch 798/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1754 - acc: 0.9775 - val_loss: 0.6600 - val_acc: 0.6923\n",
            "Epoch 799/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1751 - acc: 0.9775 - val_loss: 0.6603 - val_acc: 0.6923\n",
            "Epoch 800/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1747 - acc: 0.9775 - val_loss: 0.6607 - val_acc: 0.6923\n",
            "Epoch 801/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1743 - acc: 0.9775 - val_loss: 0.6613 - val_acc: 0.6923\n",
            "Epoch 802/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.1740 - acc: 0.9775 - val_loss: 0.6620 - val_acc: 0.6923\n",
            "Epoch 803/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1736 - acc: 0.9775 - val_loss: 0.6625 - val_acc: 0.6923\n",
            "Epoch 804/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1733 - acc: 0.9775 - val_loss: 0.6627 - val_acc: 0.6923\n",
            "Epoch 805/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1729 - acc: 0.9775 - val_loss: 0.6631 - val_acc: 0.6923\n",
            "Epoch 806/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1725 - acc: 0.9775 - val_loss: 0.6638 - val_acc: 0.6923\n",
            "Epoch 807/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1722 - acc: 0.9775 - val_loss: 0.6645 - val_acc: 0.6923\n",
            "Epoch 808/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1718 - acc: 0.9775 - val_loss: 0.6649 - val_acc: 0.6923\n",
            "Epoch 809/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1714 - acc: 0.9775 - val_loss: 0.6654 - val_acc: 0.6923\n",
            "Epoch 810/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1711 - acc: 0.9775 - val_loss: 0.6659 - val_acc: 0.6923\n",
            "Epoch 811/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1707 - acc: 0.9775 - val_loss: 0.6666 - val_acc: 0.6923\n",
            "Epoch 812/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1704 - acc: 0.9775 - val_loss: 0.6671 - val_acc: 0.6923\n",
            "Epoch 813/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1700 - acc: 0.9775 - val_loss: 0.6675 - val_acc: 0.6923\n",
            "Epoch 814/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1697 - acc: 0.9775 - val_loss: 0.6681 - val_acc: 0.6923\n",
            "Epoch 815/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1693 - acc: 0.9775 - val_loss: 0.6688 - val_acc: 0.6923\n",
            "Epoch 816/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1690 - acc: 0.9775 - val_loss: 0.6696 - val_acc: 0.6923\n",
            "Epoch 817/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1686 - acc: 0.9775 - val_loss: 0.6698 - val_acc: 0.6923\n",
            "Epoch 818/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1683 - acc: 0.9775 - val_loss: 0.6702 - val_acc: 0.6923\n",
            "Epoch 819/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1679 - acc: 0.9775 - val_loss: 0.6707 - val_acc: 0.6923\n",
            "Epoch 820/1000\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.1676 - acc: 0.9775 - val_loss: 0.6713 - val_acc: 0.6923\n",
            "Epoch 821/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1672 - acc: 0.9775 - val_loss: 0.6717 - val_acc: 0.6923\n",
            "Epoch 822/1000\n",
            "89/89 [==============================] - 0s 61us/step - loss: 0.1669 - acc: 0.9775 - val_loss: 0.6718 - val_acc: 0.6923\n",
            "Epoch 823/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1665 - acc: 0.9775 - val_loss: 0.6722 - val_acc: 0.6923\n",
            "Epoch 824/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1662 - acc: 0.9775 - val_loss: 0.6727 - val_acc: 0.6923\n",
            "Epoch 825/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1658 - acc: 0.9775 - val_loss: 0.6732 - val_acc: 0.6923\n",
            "Epoch 826/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1655 - acc: 0.9775 - val_loss: 0.6736 - val_acc: 0.6923\n",
            "Epoch 827/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1652 - acc: 0.9775 - val_loss: 0.6741 - val_acc: 0.6923\n",
            "Epoch 828/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1648 - acc: 0.9775 - val_loss: 0.6747 - val_acc: 0.6923\n",
            "Epoch 829/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1645 - acc: 0.9775 - val_loss: 0.6751 - val_acc: 0.6923\n",
            "Epoch 830/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1641 - acc: 0.9775 - val_loss: 0.6756 - val_acc: 0.6923\n",
            "Epoch 831/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1638 - acc: 0.9775 - val_loss: 0.6763 - val_acc: 0.6923\n",
            "Epoch 832/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.1634 - acc: 0.9775 - val_loss: 0.6772 - val_acc: 0.6923\n",
            "Epoch 833/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1631 - acc: 0.9775 - val_loss: 0.6780 - val_acc: 0.6923\n",
            "Epoch 834/1000\n",
            "89/89 [==============================] - 0s 109us/step - loss: 0.1628 - acc: 0.9775 - val_loss: 0.6786 - val_acc: 0.6923\n",
            "Epoch 835/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1624 - acc: 0.9775 - val_loss: 0.6789 - val_acc: 0.6923\n",
            "Epoch 836/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1621 - acc: 0.9775 - val_loss: 0.6793 - val_acc: 0.6923\n",
            "Epoch 837/1000\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.1618 - acc: 0.9775 - val_loss: 0.6797 - val_acc: 0.6923\n",
            "Epoch 838/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1614 - acc: 0.9775 - val_loss: 0.6804 - val_acc: 0.6923\n",
            "Epoch 839/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1611 - acc: 0.9775 - val_loss: 0.6812 - val_acc: 0.6923\n",
            "Epoch 840/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1607 - acc: 0.9775 - val_loss: 0.6817 - val_acc: 0.6923\n",
            "Epoch 841/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1604 - acc: 0.9775 - val_loss: 0.6819 - val_acc: 0.6923\n",
            "Epoch 842/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.1601 - acc: 0.9775 - val_loss: 0.6824 - val_acc: 0.6923\n",
            "Epoch 843/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1597 - acc: 0.9775 - val_loss: 0.6829 - val_acc: 0.6923\n",
            "Epoch 844/1000\n",
            "89/89 [==============================] - 0s 67us/step - loss: 0.1594 - acc: 0.9775 - val_loss: 0.6835 - val_acc: 0.6923\n",
            "Epoch 845/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1591 - acc: 0.9775 - val_loss: 0.6843 - val_acc: 0.6923\n",
            "Epoch 846/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1587 - acc: 0.9775 - val_loss: 0.6848 - val_acc: 0.6923\n",
            "Epoch 847/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1584 - acc: 0.9775 - val_loss: 0.6851 - val_acc: 0.6923\n",
            "Epoch 848/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1581 - acc: 0.9775 - val_loss: 0.6857 - val_acc: 0.6923\n",
            "Epoch 849/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1578 - acc: 0.9775 - val_loss: 0.6863 - val_acc: 0.6923\n",
            "Epoch 850/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1574 - acc: 0.9775 - val_loss: 0.6869 - val_acc: 0.6923\n",
            "Epoch 851/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1571 - acc: 0.9775 - val_loss: 0.6876 - val_acc: 0.6923\n",
            "Epoch 852/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1568 - acc: 0.9775 - val_loss: 0.6880 - val_acc: 0.6923\n",
            "Epoch 853/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1565 - acc: 0.9775 - val_loss: 0.6884 - val_acc: 0.6923\n",
            "Epoch 854/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1561 - acc: 0.9775 - val_loss: 0.6890 - val_acc: 0.6923\n",
            "Epoch 855/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1558 - acc: 0.9775 - val_loss: 0.6897 - val_acc: 0.6923\n",
            "Epoch 856/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1555 - acc: 0.9775 - val_loss: 0.6907 - val_acc: 0.6923\n",
            "Epoch 857/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1552 - acc: 0.9775 - val_loss: 0.6912 - val_acc: 0.6923\n",
            "Epoch 858/1000\n",
            "89/89 [==============================] - 0s 66us/step - loss: 0.1548 - acc: 0.9775 - val_loss: 0.6916 - val_acc: 0.6923\n",
            "Epoch 859/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1545 - acc: 0.9775 - val_loss: 0.6921 - val_acc: 0.6923\n",
            "Epoch 860/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1542 - acc: 0.9775 - val_loss: 0.6927 - val_acc: 0.6923\n",
            "Epoch 861/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1539 - acc: 0.9775 - val_loss: 0.6931 - val_acc: 0.6923\n",
            "Epoch 862/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1536 - acc: 0.9775 - val_loss: 0.6936 - val_acc: 0.6923\n",
            "Epoch 863/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1533 - acc: 0.9775 - val_loss: 0.6943 - val_acc: 0.6923\n",
            "Epoch 864/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1529 - acc: 0.9775 - val_loss: 0.6949 - val_acc: 0.6923\n",
            "Epoch 865/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1526 - acc: 0.9775 - val_loss: 0.6952 - val_acc: 0.6923\n",
            "Epoch 866/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1523 - acc: 0.9775 - val_loss: 0.6957 - val_acc: 0.6923\n",
            "Epoch 867/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1520 - acc: 0.9775 - val_loss: 0.6963 - val_acc: 0.6923\n",
            "Epoch 868/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1517 - acc: 0.9775 - val_loss: 0.6971 - val_acc: 0.6923\n",
            "Epoch 869/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1514 - acc: 0.9775 - val_loss: 0.6976 - val_acc: 0.6923\n",
            "Epoch 870/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1511 - acc: 0.9775 - val_loss: 0.6979 - val_acc: 0.6923\n",
            "Epoch 871/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1508 - acc: 0.9775 - val_loss: 0.6982 - val_acc: 0.6923\n",
            "Epoch 872/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1505 - acc: 0.9775 - val_loss: 0.6987 - val_acc: 0.6923\n",
            "Epoch 873/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1502 - acc: 0.9775 - val_loss: 0.6993 - val_acc: 0.6154\n",
            "Epoch 874/1000\n",
            "89/89 [==============================] - 0s 65us/step - loss: 0.1499 - acc: 0.9775 - val_loss: 0.7000 - val_acc: 0.6154\n",
            "Epoch 875/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1496 - acc: 0.9775 - val_loss: 0.7004 - val_acc: 0.6154\n",
            "Epoch 876/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.1493 - acc: 0.9775 - val_loss: 0.7005 - val_acc: 0.6154\n",
            "Epoch 877/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1490 - acc: 0.9775 - val_loss: 0.7007 - val_acc: 0.6154\n",
            "Epoch 878/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.1487 - acc: 0.9775 - val_loss: 0.7010 - val_acc: 0.6154\n",
            "Epoch 879/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1484 - acc: 0.9775 - val_loss: 0.7014 - val_acc: 0.6154\n",
            "Epoch 880/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1481 - acc: 0.9775 - val_loss: 0.7018 - val_acc: 0.6154\n",
            "Epoch 881/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1478 - acc: 0.9775 - val_loss: 0.7020 - val_acc: 0.6154\n",
            "Epoch 882/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1475 - acc: 0.9775 - val_loss: 0.7026 - val_acc: 0.6154\n",
            "Epoch 883/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1472 - acc: 0.9775 - val_loss: 0.7034 - val_acc: 0.6154\n",
            "Epoch 884/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.1469 - acc: 0.9775 - val_loss: 0.7039 - val_acc: 0.6154\n",
            "Epoch 885/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1466 - acc: 0.9775 - val_loss: 0.7043 - val_acc: 0.6154\n",
            "Epoch 886/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1463 - acc: 0.9775 - val_loss: 0.7047 - val_acc: 0.6154\n",
            "Epoch 887/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1460 - acc: 0.9775 - val_loss: 0.7051 - val_acc: 0.6154\n",
            "Epoch 888/1000\n",
            "89/89 [==============================] - 0s 63us/step - loss: 0.1457 - acc: 0.9775 - val_loss: 0.7054 - val_acc: 0.6154\n",
            "Epoch 889/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1454 - acc: 0.9775 - val_loss: 0.7055 - val_acc: 0.6154\n",
            "Epoch 890/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1451 - acc: 0.9775 - val_loss: 0.7058 - val_acc: 0.6154\n",
            "Epoch 891/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1448 - acc: 0.9775 - val_loss: 0.7062 - val_acc: 0.6154\n",
            "Epoch 892/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1445 - acc: 0.9775 - val_loss: 0.7069 - val_acc: 0.6154\n",
            "Epoch 893/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1442 - acc: 0.9775 - val_loss: 0.7074 - val_acc: 0.6154\n",
            "Epoch 894/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1440 - acc: 0.9775 - val_loss: 0.7078 - val_acc: 0.6154\n",
            "Epoch 895/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1437 - acc: 0.9775 - val_loss: 0.7082 - val_acc: 0.6154\n",
            "Epoch 896/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1434 - acc: 0.9775 - val_loss: 0.7088 - val_acc: 0.6154\n",
            "Epoch 897/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1431 - acc: 0.9775 - val_loss: 0.7094 - val_acc: 0.6154\n",
            "Epoch 898/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1428 - acc: 0.9775 - val_loss: 0.7101 - val_acc: 0.6154\n",
            "Epoch 899/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1425 - acc: 0.9775 - val_loss: 0.7104 - val_acc: 0.6154\n",
            "Epoch 900/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1422 - acc: 0.9775 - val_loss: 0.7103 - val_acc: 0.6154\n",
            "Epoch 901/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1420 - acc: 0.9775 - val_loss: 0.7102 - val_acc: 0.6154\n",
            "Epoch 902/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1417 - acc: 0.9775 - val_loss: 0.7105 - val_acc: 0.6154\n",
            "Epoch 903/1000\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.1414 - acc: 0.9775 - val_loss: 0.7109 - val_acc: 0.6154\n",
            "Epoch 904/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1411 - acc: 0.9775 - val_loss: 0.7117 - val_acc: 0.6154\n",
            "Epoch 905/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1408 - acc: 0.9775 - val_loss: 0.7127 - val_acc: 0.6154\n",
            "Epoch 906/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1405 - acc: 0.9775 - val_loss: 0.7131 - val_acc: 0.6154\n",
            "Epoch 907/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1403 - acc: 0.9775 - val_loss: 0.7130 - val_acc: 0.6154\n",
            "Epoch 908/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1400 - acc: 0.9775 - val_loss: 0.7130 - val_acc: 0.6154\n",
            "Epoch 909/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1397 - acc: 0.9775 - val_loss: 0.7131 - val_acc: 0.6154\n",
            "Epoch 910/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1394 - acc: 0.9775 - val_loss: 0.7135 - val_acc: 0.6154\n",
            "Epoch 911/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1391 - acc: 0.9775 - val_loss: 0.7141 - val_acc: 0.6154\n",
            "Epoch 912/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1389 - acc: 0.9775 - val_loss: 0.7147 - val_acc: 0.6154\n",
            "Epoch 913/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1386 - acc: 0.9775 - val_loss: 0.7150 - val_acc: 0.6154\n",
            "Epoch 914/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1383 - acc: 0.9775 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 915/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1380 - acc: 0.9775 - val_loss: 0.7152 - val_acc: 0.6154\n",
            "Epoch 916/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1378 - acc: 0.9775 - val_loss: 0.7156 - val_acc: 0.6154\n",
            "Epoch 917/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1375 - acc: 0.9775 - val_loss: 0.7164 - val_acc: 0.6154\n",
            "Epoch 918/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1372 - acc: 0.9775 - val_loss: 0.7171 - val_acc: 0.6154\n",
            "Epoch 919/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1369 - acc: 0.9775 - val_loss: 0.7179 - val_acc: 0.6154\n",
            "Epoch 920/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1367 - acc: 0.9775 - val_loss: 0.7181 - val_acc: 0.6154\n",
            "Epoch 921/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1364 - acc: 0.9775 - val_loss: 0.7185 - val_acc: 0.6154\n",
            "Epoch 922/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1361 - acc: 0.9775 - val_loss: 0.7189 - val_acc: 0.6154\n",
            "Epoch 923/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1358 - acc: 0.9775 - val_loss: 0.7194 - val_acc: 0.6154\n",
            "Epoch 924/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1356 - acc: 0.9775 - val_loss: 0.7196 - val_acc: 0.6154\n",
            "Epoch 925/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1353 - acc: 0.9775 - val_loss: 0.7198 - val_acc: 0.6154\n",
            "Epoch 926/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1350 - acc: 0.9775 - val_loss: 0.7201 - val_acc: 0.6154\n",
            "Epoch 927/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1348 - acc: 0.9775 - val_loss: 0.7206 - val_acc: 0.6154\n",
            "Epoch 928/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1345 - acc: 0.9775 - val_loss: 0.7210 - val_acc: 0.6154\n",
            "Epoch 929/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1342 - acc: 0.9775 - val_loss: 0.7212 - val_acc: 0.6154\n",
            "Epoch 930/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1340 - acc: 0.9775 - val_loss: 0.7213 - val_acc: 0.6154\n",
            "Epoch 931/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1337 - acc: 0.9775 - val_loss: 0.7217 - val_acc: 0.6154\n",
            "Epoch 932/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1335 - acc: 0.9775 - val_loss: 0.7222 - val_acc: 0.6154\n",
            "Epoch 933/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1332 - acc: 0.9775 - val_loss: 0.7229 - val_acc: 0.6154\n",
            "Epoch 934/1000\n",
            "89/89 [==============================] - 0s 87us/step - loss: 0.1329 - acc: 0.9775 - val_loss: 0.7231 - val_acc: 0.6154\n",
            "Epoch 935/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1326 - acc: 0.9775 - val_loss: 0.7235 - val_acc: 0.6154\n",
            "Epoch 936/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1324 - acc: 0.9775 - val_loss: 0.7241 - val_acc: 0.6154\n",
            "Epoch 937/1000\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.1321 - acc: 0.9775 - val_loss: 0.7243 - val_acc: 0.6154\n",
            "Epoch 938/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1318 - acc: 0.9775 - val_loss: 0.7244 - val_acc: 0.6154\n",
            "Epoch 939/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1316 - acc: 0.9775 - val_loss: 0.7247 - val_acc: 0.6154\n",
            "Epoch 940/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1313 - acc: 0.9775 - val_loss: 0.7251 - val_acc: 0.6154\n",
            "Epoch 941/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1311 - acc: 0.9775 - val_loss: 0.7255 - val_acc: 0.6154\n",
            "Epoch 942/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1308 - acc: 0.9775 - val_loss: 0.7260 - val_acc: 0.6154\n",
            "Epoch 943/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1305 - acc: 0.9775 - val_loss: 0.7263 - val_acc: 0.6154\n",
            "Epoch 944/1000\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.1303 - acc: 0.9775 - val_loss: 0.7266 - val_acc: 0.6154\n",
            "Epoch 945/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1300 - acc: 0.9775 - val_loss: 0.7272 - val_acc: 0.6154\n",
            "Epoch 946/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1297 - acc: 0.9775 - val_loss: 0.7278 - val_acc: 0.6154\n",
            "Epoch 947/1000\n",
            "89/89 [==============================] - 0s 49us/step - loss: 0.1295 - acc: 0.9775 - val_loss: 0.7281 - val_acc: 0.6154\n",
            "Epoch 948/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1292 - acc: 0.9775 - val_loss: 0.7284 - val_acc: 0.6154\n",
            "Epoch 949/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1289 - acc: 0.9775 - val_loss: 0.7289 - val_acc: 0.6154\n",
            "Epoch 950/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1287 - acc: 0.9775 - val_loss: 0.7294 - val_acc: 0.6154\n",
            "Epoch 951/1000\n",
            "89/89 [==============================] - 0s 52us/step - loss: 0.1284 - acc: 0.9775 - val_loss: 0.7298 - val_acc: 0.6154\n",
            "Epoch 952/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1282 - acc: 0.9775 - val_loss: 0.7301 - val_acc: 0.6154\n",
            "Epoch 953/1000\n",
            "89/89 [==============================] - 0s 93us/step - loss: 0.1279 - acc: 0.9775 - val_loss: 0.7307 - val_acc: 0.6154\n",
            "Epoch 954/1000\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1276 - acc: 0.9775 - val_loss: 0.7313 - val_acc: 0.6154\n",
            "Epoch 955/1000\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.1274 - acc: 0.9775 - val_loss: 0.7320 - val_acc: 0.6154\n",
            "Epoch 956/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1271 - acc: 0.9775 - val_loss: 0.7323 - val_acc: 0.6154\n",
            "Epoch 957/1000\n",
            "89/89 [==============================] - 0s 69us/step - loss: 0.1269 - acc: 0.9775 - val_loss: 0.7328 - val_acc: 0.6154\n",
            "Epoch 958/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1266 - acc: 0.9775 - val_loss: 0.7334 - val_acc: 0.6154\n",
            "Epoch 959/1000\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1264 - acc: 0.9775 - val_loss: 0.7338 - val_acc: 0.6154\n",
            "Epoch 960/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1261 - acc: 0.9775 - val_loss: 0.7343 - val_acc: 0.6154\n",
            "Epoch 961/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1258 - acc: 0.9775 - val_loss: 0.7349 - val_acc: 0.6154\n",
            "Epoch 962/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1256 - acc: 0.9775 - val_loss: 0.7356 - val_acc: 0.6154\n",
            "Epoch 963/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1253 - acc: 0.9775 - val_loss: 0.7364 - val_acc: 0.6154\n",
            "Epoch 964/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1251 - acc: 0.9775 - val_loss: 0.7369 - val_acc: 0.6154\n",
            "Epoch 965/1000\n",
            "89/89 [==============================] - 0s 58us/step - loss: 0.1248 - acc: 0.9775 - val_loss: 0.7367 - val_acc: 0.6154\n",
            "Epoch 966/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1246 - acc: 0.9775 - val_loss: 0.7365 - val_acc: 0.6154\n",
            "Epoch 967/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1243 - acc: 0.9775 - val_loss: 0.7365 - val_acc: 0.6154\n",
            "Epoch 968/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1241 - acc: 0.9775 - val_loss: 0.7368 - val_acc: 0.6154\n",
            "Epoch 969/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1238 - acc: 0.9775 - val_loss: 0.7375 - val_acc: 0.6154\n",
            "Epoch 970/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1236 - acc: 0.9775 - val_loss: 0.7385 - val_acc: 0.6154\n",
            "Epoch 971/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1233 - acc: 0.9775 - val_loss: 0.7395 - val_acc: 0.6154\n",
            "Epoch 972/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1231 - acc: 0.9775 - val_loss: 0.7400 - val_acc: 0.6154\n",
            "Epoch 973/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1228 - acc: 0.9775 - val_loss: 0.7402 - val_acc: 0.6154\n",
            "Epoch 974/1000\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1226 - acc: 0.9775 - val_loss: 0.7403 - val_acc: 0.6154\n",
            "Epoch 975/1000\n",
            "89/89 [==============================] - 0s 56us/step - loss: 0.1223 - acc: 0.9775 - val_loss: 0.7406 - val_acc: 0.6154\n",
            "Epoch 976/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1221 - acc: 0.9775 - val_loss: 0.7410 - val_acc: 0.6154\n",
            "Epoch 977/1000\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.1218 - acc: 0.9775 - val_loss: 0.7417 - val_acc: 0.6154\n",
            "Epoch 978/1000\n",
            "89/89 [==============================] - 0s 50us/step - loss: 0.1216 - acc: 0.9775 - val_loss: 0.7423 - val_acc: 0.6154\n",
            "Epoch 979/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1213 - acc: 0.9775 - val_loss: 0.7429 - val_acc: 0.6154\n",
            "Epoch 980/1000\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1211 - acc: 0.9775 - val_loss: 0.7433 - val_acc: 0.6154\n",
            "Epoch 981/1000\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.1209 - acc: 0.9775 - val_loss: 0.7437 - val_acc: 0.6154\n",
            "Epoch 982/1000\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1206 - acc: 0.9775 - val_loss: 0.7441 - val_acc: 0.6154\n",
            "Epoch 983/1000\n",
            "89/89 [==============================] - 0s 53us/step - loss: 0.1204 - acc: 0.9775 - val_loss: 0.7446 - val_acc: 0.6154\n",
            "Epoch 984/1000\n",
            "89/89 [==============================] - 0s 44us/step - loss: 0.1201 - acc: 0.9775 - val_loss: 0.7454 - val_acc: 0.6154\n",
            "Epoch 985/1000\n",
            "89/89 [==============================] - 0s 55us/step - loss: 0.1199 - acc: 0.9775 - val_loss: 0.7464 - val_acc: 0.6154\n",
            "Epoch 986/1000\n",
            "89/89 [==============================] - 0s 76us/step - loss: 0.1196 - acc: 0.9775 - val_loss: 0.7470 - val_acc: 0.6154\n",
            "Epoch 987/1000\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.1194 - acc: 0.9775 - val_loss: 0.7472 - val_acc: 0.6154\n",
            "Epoch 988/1000\n",
            "89/89 [==============================] - 0s 74us/step - loss: 0.1192 - acc: 0.9775 - val_loss: 0.7475 - val_acc: 0.6154\n",
            "Epoch 989/1000\n",
            "89/89 [==============================] - 0s 48us/step - loss: 0.1189 - acc: 0.9775 - val_loss: 0.7480 - val_acc: 0.6154\n",
            "Epoch 990/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1187 - acc: 0.9775 - val_loss: 0.7486 - val_acc: 0.6154\n",
            "Epoch 991/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1184 - acc: 0.9775 - val_loss: 0.7491 - val_acc: 0.6154\n",
            "Epoch 992/1000\n",
            "89/89 [==============================] - 0s 62us/step - loss: 0.1182 - acc: 0.9775 - val_loss: 0.7496 - val_acc: 0.6154\n",
            "Epoch 993/1000\n",
            "89/89 [==============================] - 0s 60us/step - loss: 0.1180 - acc: 0.9775 - val_loss: 0.7500 - val_acc: 0.6154\n",
            "Epoch 994/1000\n",
            "89/89 [==============================] - 0s 51us/step - loss: 0.1177 - acc: 0.9775 - val_loss: 0.7506 - val_acc: 0.6154\n",
            "Epoch 995/1000\n",
            "89/89 [==============================] - 0s 59us/step - loss: 0.1175 - acc: 0.9775 - val_loss: 0.7511 - val_acc: 0.6154\n",
            "Epoch 996/1000\n",
            "89/89 [==============================] - 0s 57us/step - loss: 0.1173 - acc: 0.9775 - val_loss: 0.7513 - val_acc: 0.6154\n",
            "Epoch 997/1000\n",
            "89/89 [==============================] - 0s 77us/step - loss: 0.1170 - acc: 0.9775 - val_loss: 0.7517 - val_acc: 0.6154\n",
            "Epoch 998/1000\n",
            "89/89 [==============================] - 0s 54us/step - loss: 0.1168 - acc: 0.9775 - val_loss: 0.7522 - val_acc: 0.6154\n",
            "Epoch 999/1000\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.1166 - acc: 0.9775 - val_loss: 0.7529 - val_acc: 0.6154\n",
            "Epoch 1000/1000\n",
            "89/89 [==============================] - 0s 64us/step - loss: 0.1163 - acc: 0.9775 - val_loss: 0.7536 - val_acc: 0.6154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fc26d777-b0a0-4d26-d048-b896582a41f1",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb675584cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "30e30218-2c04-49b5-8657-cd2d1958856f",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb6754f55c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngYhIxP1/umoweDf3G48AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "92bc6a3e-f59b-4649-a67f-4ece838a32bd",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_labels, epochs= 500, batch_size=89, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "89/89 [==============================] - 2s 20ms/step - loss: 0.7681 - acc: 0.5393\n",
            "Epoch 2/500\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.7404 - acc: 0.5393\n",
            "Epoch 3/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.7170 - acc: 0.5618\n",
            "Epoch 4/500\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6976 - acc: 0.5506\n",
            "Epoch 5/500\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.6824 - acc: 0.5393\n",
            "Epoch 6/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.6704 - acc: 0.5618\n",
            "Epoch 7/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.6606 - acc: 0.5843\n",
            "Epoch 8/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.6522 - acc: 0.5955\n",
            "Epoch 9/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.6446 - acc: 0.5843\n",
            "Epoch 10/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.6376 - acc: 0.5730\n",
            "Epoch 11/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.6311 - acc: 0.5955\n",
            "Epoch 12/500\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.6251 - acc: 0.6292\n",
            "Epoch 13/500\n",
            "89/89 [==============================] - 0s 46us/step - loss: 0.6186 - acc: 0.6629\n",
            "Epoch 14/500\n",
            "89/89 [==============================] - 0s 43us/step - loss: 0.6123 - acc: 0.6629\n",
            "Epoch 15/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.6063 - acc: 0.6854\n",
            "Epoch 16/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.6005 - acc: 0.7191\n",
            "Epoch 17/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.5947 - acc: 0.7191\n",
            "Epoch 18/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5891 - acc: 0.7191\n",
            "Epoch 19/500\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.5833 - acc: 0.7079\n",
            "Epoch 20/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.5779 - acc: 0.7303\n",
            "Epoch 21/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.5726 - acc: 0.7528\n",
            "Epoch 22/500\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.5674 - acc: 0.7528\n",
            "Epoch 23/500\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.5622 - acc: 0.7528\n",
            "Epoch 24/500\n",
            "89/89 [==============================] - 0s 22us/step - loss: 0.5572 - acc: 0.7528\n",
            "Epoch 25/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.5524 - acc: 0.7640\n",
            "Epoch 26/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.5476 - acc: 0.7753\n",
            "Epoch 27/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.5427 - acc: 0.7865\n",
            "Epoch 28/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5377 - acc: 0.7865\n",
            "Epoch 29/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.5326 - acc: 0.7978\n",
            "Epoch 30/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.5272 - acc: 0.7865\n",
            "Epoch 31/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5219 - acc: 0.7978\n",
            "Epoch 32/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5166 - acc: 0.8090\n",
            "Epoch 33/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.5113 - acc: 0.8090\n",
            "Epoch 34/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.5059 - acc: 0.8090\n",
            "Epoch 35/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.5005 - acc: 0.8090\n",
            "Epoch 36/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.4950 - acc: 0.8202\n",
            "Epoch 37/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4897 - acc: 0.8202\n",
            "Epoch 38/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.4844 - acc: 0.8315\n",
            "Epoch 39/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.4792 - acc: 0.8315\n",
            "Epoch 40/500\n",
            "89/89 [==============================] - 0s 23us/step - loss: 0.4742 - acc: 0.8427\n",
            "Epoch 41/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4692 - acc: 0.8427\n",
            "Epoch 42/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4643 - acc: 0.8539\n",
            "Epoch 43/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4594 - acc: 0.8539\n",
            "Epoch 44/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4547 - acc: 0.8539\n",
            "Epoch 45/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4503 - acc: 0.8539\n",
            "Epoch 46/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4459 - acc: 0.8539\n",
            "Epoch 47/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4415 - acc: 0.8539\n",
            "Epoch 48/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4371 - acc: 0.8539\n",
            "Epoch 49/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4325 - acc: 0.8539\n",
            "Epoch 50/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4280 - acc: 0.8539\n",
            "Epoch 51/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.4236 - acc: 0.8652\n",
            "Epoch 52/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.4194 - acc: 0.8652\n",
            "Epoch 53/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4152 - acc: 0.8764\n",
            "Epoch 54/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.4111 - acc: 0.8764\n",
            "Epoch 55/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.4070 - acc: 0.8764\n",
            "Epoch 56/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.4029 - acc: 0.8652\n",
            "Epoch 57/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3989 - acc: 0.8652\n",
            "Epoch 58/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3949 - acc: 0.8764\n",
            "Epoch 59/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.3907 - acc: 0.8764\n",
            "Epoch 60/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.3866 - acc: 0.8764\n",
            "Epoch 61/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.3825 - acc: 0.8764\n",
            "Epoch 62/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.3784 - acc: 0.8652\n",
            "Epoch 63/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.3743 - acc: 0.8652\n",
            "Epoch 64/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.3702 - acc: 0.8652\n",
            "Epoch 65/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.3661 - acc: 0.8652\n",
            "Epoch 66/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.3620 - acc: 0.8652\n",
            "Epoch 67/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.3580 - acc: 0.8652\n",
            "Epoch 68/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.3540 - acc: 0.8652\n",
            "Epoch 69/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.3500 - acc: 0.8764\n",
            "Epoch 70/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.3459 - acc: 0.8876\n",
            "Epoch 71/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.3418 - acc: 0.8876\n",
            "Epoch 72/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.3377 - acc: 0.8876\n",
            "Epoch 73/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.3335 - acc: 0.8876\n",
            "Epoch 74/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.3293 - acc: 0.8876\n",
            "Epoch 75/500\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.3251 - acc: 0.8876\n",
            "Epoch 76/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.3210 - acc: 0.8989\n",
            "Epoch 77/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.3167 - acc: 0.8989\n",
            "Epoch 78/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.3125 - acc: 0.8989\n",
            "Epoch 79/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.3083 - acc: 0.9101\n",
            "Epoch 80/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.3040 - acc: 0.9101\n",
            "Epoch 81/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2996 - acc: 0.9101\n",
            "Epoch 82/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2951 - acc: 0.9101\n",
            "Epoch 83/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2906 - acc: 0.9101\n",
            "Epoch 84/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2862 - acc: 0.9213\n",
            "Epoch 85/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2816 - acc: 0.9326\n",
            "Epoch 86/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2771 - acc: 0.9326\n",
            "Epoch 87/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2726 - acc: 0.9326\n",
            "Epoch 88/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.2682 - acc: 0.9326\n",
            "Epoch 89/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.2638 - acc: 0.9326\n",
            "Epoch 90/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.2595 - acc: 0.9438\n",
            "Epoch 91/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.2552 - acc: 0.9438\n",
            "Epoch 92/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2510 - acc: 0.9438\n",
            "Epoch 93/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2468 - acc: 0.9438\n",
            "Epoch 94/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.2426 - acc: 0.9438\n",
            "Epoch 95/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.2384 - acc: 0.9438\n",
            "Epoch 96/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2343 - acc: 0.9438\n",
            "Epoch 97/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.2303 - acc: 0.9438\n",
            "Epoch 98/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2262 - acc: 0.9438\n",
            "Epoch 99/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.2222 - acc: 0.9438\n",
            "Epoch 100/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2182 - acc: 0.9438\n",
            "Epoch 101/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2141 - acc: 0.9551\n",
            "Epoch 102/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.2102 - acc: 0.9551\n",
            "Epoch 103/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2062 - acc: 0.9551\n",
            "Epoch 104/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.2024 - acc: 0.9551\n",
            "Epoch 105/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1984 - acc: 0.9551\n",
            "Epoch 106/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.1947 - acc: 0.9663\n",
            "Epoch 107/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1910 - acc: 0.9663\n",
            "Epoch 108/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.1872 - acc: 0.9663\n",
            "Epoch 109/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1834 - acc: 0.9663\n",
            "Epoch 110/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1798 - acc: 0.9663\n",
            "Epoch 111/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1762 - acc: 0.9663\n",
            "Epoch 112/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1726 - acc: 0.9663\n",
            "Epoch 113/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.1690 - acc: 0.9663\n",
            "Epoch 114/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.1655 - acc: 0.9663\n",
            "Epoch 115/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1621 - acc: 0.9663\n",
            "Epoch 116/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1586 - acc: 0.9663\n",
            "Epoch 117/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1551 - acc: 0.9663\n",
            "Epoch 118/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1518 - acc: 0.9663\n",
            "Epoch 119/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1484 - acc: 0.9663\n",
            "Epoch 120/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1451 - acc: 0.9775\n",
            "Epoch 121/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.1419 - acc: 0.9775\n",
            "Epoch 122/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1386 - acc: 0.9775\n",
            "Epoch 123/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1355 - acc: 0.9775\n",
            "Epoch 124/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1324 - acc: 0.9775\n",
            "Epoch 125/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.1294 - acc: 0.9775\n",
            "Epoch 126/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1264 - acc: 0.9775\n",
            "Epoch 127/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1236 - acc: 0.9775\n",
            "Epoch 128/500\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.1208 - acc: 0.9888\n",
            "Epoch 129/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.1181 - acc: 0.9888\n",
            "Epoch 130/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.1154 - acc: 0.9888\n",
            "Epoch 131/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.1128 - acc: 0.9888\n",
            "Epoch 132/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.1102 - acc: 0.9888\n",
            "Epoch 133/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1077 - acc: 0.9888\n",
            "Epoch 134/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.1054 - acc: 0.9888\n",
            "Epoch 135/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.1028 - acc: 0.9888\n",
            "Epoch 136/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.1006 - acc: 0.9888\n",
            "Epoch 137/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0984 - acc: 0.9888\n",
            "Epoch 138/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0960 - acc: 0.9888\n",
            "Epoch 139/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0940 - acc: 0.9888\n",
            "Epoch 140/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0919 - acc: 0.9888\n",
            "Epoch 141/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0897 - acc: 0.9888\n",
            "Epoch 142/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0878 - acc: 1.0000\n",
            "Epoch 143/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0859 - acc: 1.0000\n",
            "Epoch 144/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0839 - acc: 1.0000\n",
            "Epoch 145/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0820 - acc: 1.0000\n",
            "Epoch 146/500\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0803 - acc: 1.0000\n",
            "Epoch 147/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0785 - acc: 1.0000\n",
            "Epoch 148/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0768 - acc: 1.0000\n",
            "Epoch 149/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0751 - acc: 1.0000\n",
            "Epoch 150/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0735 - acc: 1.0000\n",
            "Epoch 151/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0719 - acc: 1.0000\n",
            "Epoch 152/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0703 - acc: 1.0000\n",
            "Epoch 153/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0689 - acc: 1.0000\n",
            "Epoch 154/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0674 - acc: 1.0000\n",
            "Epoch 155/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0660 - acc: 1.0000\n",
            "Epoch 156/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0647 - acc: 1.0000\n",
            "Epoch 157/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0634 - acc: 1.0000\n",
            "Epoch 158/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0620 - acc: 1.0000\n",
            "Epoch 159/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0608 - acc: 1.0000\n",
            "Epoch 160/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0596 - acc: 1.0000\n",
            "Epoch 161/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0583 - acc: 1.0000\n",
            "Epoch 162/500\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0572 - acc: 1.0000\n",
            "Epoch 163/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0560 - acc: 1.0000\n",
            "Epoch 164/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0549 - acc: 1.0000\n",
            "Epoch 165/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0538 - acc: 1.0000\n",
            "Epoch 166/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0527 - acc: 1.0000\n",
            "Epoch 167/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0517 - acc: 1.0000\n",
            "Epoch 168/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0506 - acc: 1.0000\n",
            "Epoch 169/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0497 - acc: 1.0000\n",
            "Epoch 170/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0487 - acc: 1.0000\n",
            "Epoch 171/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0478 - acc: 1.0000\n",
            "Epoch 172/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0468 - acc: 1.0000\n",
            "Epoch 173/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0459 - acc: 1.0000\n",
            "Epoch 174/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0450 - acc: 1.0000\n",
            "Epoch 175/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0442 - acc: 1.0000\n",
            "Epoch 176/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0434 - acc: 1.0000\n",
            "Epoch 177/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0425 - acc: 1.0000\n",
            "Epoch 178/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0418 - acc: 1.0000\n",
            "Epoch 179/500\n",
            "89/89 [==============================] - 0s 47us/step - loss: 0.0410 - acc: 1.0000\n",
            "Epoch 180/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0402 - acc: 1.0000\n",
            "Epoch 181/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0395 - acc: 1.0000\n",
            "Epoch 182/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0387 - acc: 1.0000\n",
            "Epoch 183/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0381 - acc: 1.0000\n",
            "Epoch 184/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0373 - acc: 1.0000\n",
            "Epoch 185/500\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0366 - acc: 1.0000\n",
            "Epoch 186/500\n",
            "89/89 [==============================] - 0s 24us/step - loss: 0.0360 - acc: 1.0000\n",
            "Epoch 187/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0353 - acc: 1.0000\n",
            "Epoch 188/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0347 - acc: 1.0000\n",
            "Epoch 189/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0340 - acc: 1.0000\n",
            "Epoch 190/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0334 - acc: 1.0000\n",
            "Epoch 191/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0328 - acc: 1.0000\n",
            "Epoch 192/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0323 - acc: 1.0000\n",
            "Epoch 193/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0317 - acc: 1.0000\n",
            "Epoch 194/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0311 - acc: 1.0000\n",
            "Epoch 195/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0306 - acc: 1.0000\n",
            "Epoch 196/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0300 - acc: 1.0000\n",
            "Epoch 197/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0295 - acc: 1.0000\n",
            "Epoch 198/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0290 - acc: 1.0000\n",
            "Epoch 199/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0285 - acc: 1.0000\n",
            "Epoch 200/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0280 - acc: 1.0000\n",
            "Epoch 201/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0276 - acc: 1.0000\n",
            "Epoch 202/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0271 - acc: 1.0000\n",
            "Epoch 203/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0266 - acc: 1.0000\n",
            "Epoch 204/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0262 - acc: 1.0000\n",
            "Epoch 205/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0258 - acc: 1.0000\n",
            "Epoch 206/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0254 - acc: 1.0000\n",
            "Epoch 207/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0250 - acc: 1.0000\n",
            "Epoch 208/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0246 - acc: 1.0000\n",
            "Epoch 209/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0242 - acc: 1.0000\n",
            "Epoch 210/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0238 - acc: 1.0000\n",
            "Epoch 211/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0234 - acc: 1.0000\n",
            "Epoch 212/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0231 - acc: 1.0000\n",
            "Epoch 213/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0227 - acc: 1.0000\n",
            "Epoch 214/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0224 - acc: 1.0000\n",
            "Epoch 215/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0220 - acc: 1.0000\n",
            "Epoch 216/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0217 - acc: 1.0000\n",
            "Epoch 217/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0214 - acc: 1.0000\n",
            "Epoch 218/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0211 - acc: 1.0000\n",
            "Epoch 219/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0208 - acc: 1.0000\n",
            "Epoch 220/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0204 - acc: 1.0000\n",
            "Epoch 221/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0201 - acc: 1.0000\n",
            "Epoch 222/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0199 - acc: 1.0000\n",
            "Epoch 223/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0196 - acc: 1.0000\n",
            "Epoch 224/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0193 - acc: 1.0000\n",
            "Epoch 225/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0190 - acc: 1.0000\n",
            "Epoch 226/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0187 - acc: 1.0000\n",
            "Epoch 227/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0185 - acc: 1.0000\n",
            "Epoch 228/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0182 - acc: 1.0000\n",
            "Epoch 229/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0180 - acc: 1.0000\n",
            "Epoch 230/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0177 - acc: 1.0000\n",
            "Epoch 231/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0175 - acc: 1.0000\n",
            "Epoch 232/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0172 - acc: 1.0000\n",
            "Epoch 233/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0170 - acc: 1.0000\n",
            "Epoch 234/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0168 - acc: 1.0000\n",
            "Epoch 235/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0166 - acc: 1.0000\n",
            "Epoch 236/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0163 - acc: 1.0000\n",
            "Epoch 237/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0161 - acc: 1.0000\n",
            "Epoch 238/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0159 - acc: 1.0000\n",
            "Epoch 239/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0157 - acc: 1.0000\n",
            "Epoch 240/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0155 - acc: 1.0000\n",
            "Epoch 241/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0153 - acc: 1.0000\n",
            "Epoch 242/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0151 - acc: 1.0000\n",
            "Epoch 243/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0149 - acc: 1.0000\n",
            "Epoch 244/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0147 - acc: 1.0000\n",
            "Epoch 245/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0145 - acc: 1.0000\n",
            "Epoch 246/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0144 - acc: 1.0000\n",
            "Epoch 247/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0142 - acc: 1.0000\n",
            "Epoch 248/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0140 - acc: 1.0000\n",
            "Epoch 249/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0138 - acc: 1.0000\n",
            "Epoch 250/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0137 - acc: 1.0000\n",
            "Epoch 251/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0135 - acc: 1.0000\n",
            "Epoch 252/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0133 - acc: 1.0000\n",
            "Epoch 253/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0132 - acc: 1.0000\n",
            "Epoch 254/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0130 - acc: 1.0000\n",
            "Epoch 255/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0129 - acc: 1.0000\n",
            "Epoch 256/500\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0127 - acc: 1.0000\n",
            "Epoch 257/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0126 - acc: 1.0000\n",
            "Epoch 258/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0124 - acc: 1.0000\n",
            "Epoch 259/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0123 - acc: 1.0000\n",
            "Epoch 260/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0122 - acc: 1.0000\n",
            "Epoch 261/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0120 - acc: 1.0000\n",
            "Epoch 262/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0119 - acc: 1.0000\n",
            "Epoch 263/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0118 - acc: 1.0000\n",
            "Epoch 264/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 265/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0115 - acc: 1.0000\n",
            "Epoch 266/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0114 - acc: 1.0000\n",
            "Epoch 267/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0112 - acc: 1.0000\n",
            "Epoch 268/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0111 - acc: 1.0000\n",
            "Epoch 269/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0110 - acc: 1.0000\n",
            "Epoch 270/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0109 - acc: 1.0000\n",
            "Epoch 271/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0108 - acc: 1.0000\n",
            "Epoch 272/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0106 - acc: 1.0000\n",
            "Epoch 273/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0105 - acc: 1.0000\n",
            "Epoch 274/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0104 - acc: 1.0000\n",
            "Epoch 275/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0103 - acc: 1.0000\n",
            "Epoch 276/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0102 - acc: 1.0000\n",
            "Epoch 277/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0101 - acc: 1.0000\n",
            "Epoch 278/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0100 - acc: 1.0000\n",
            "Epoch 279/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 280/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0098 - acc: 1.0000\n",
            "Epoch 281/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0097 - acc: 1.0000\n",
            "Epoch 282/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0096 - acc: 1.0000\n",
            "Epoch 283/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0095 - acc: 1.0000\n",
            "Epoch 284/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0094 - acc: 1.0000\n",
            "Epoch 285/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0093 - acc: 1.0000\n",
            "Epoch 286/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0092 - acc: 1.0000\n",
            "Epoch 287/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0091 - acc: 1.0000\n",
            "Epoch 288/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0090 - acc: 1.0000\n",
            "Epoch 289/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0089 - acc: 1.0000\n",
            "Epoch 290/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0089 - acc: 1.0000\n",
            "Epoch 291/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0088 - acc: 1.0000\n",
            "Epoch 292/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0087 - acc: 1.0000\n",
            "Epoch 293/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0086 - acc: 1.0000\n",
            "Epoch 294/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0085 - acc: 1.0000\n",
            "Epoch 295/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0084 - acc: 1.0000\n",
            "Epoch 296/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0084 - acc: 1.0000\n",
            "Epoch 297/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0083 - acc: 1.0000\n",
            "Epoch 298/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0082 - acc: 1.0000\n",
            "Epoch 299/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0081 - acc: 1.0000\n",
            "Epoch 300/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0081 - acc: 1.0000\n",
            "Epoch 301/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0080 - acc: 1.0000\n",
            "Epoch 302/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0079 - acc: 1.0000\n",
            "Epoch 303/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 304/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0078 - acc: 1.0000\n",
            "Epoch 305/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0077 - acc: 1.0000\n",
            "Epoch 306/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0076 - acc: 1.0000\n",
            "Epoch 307/500\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 308/500\n",
            "89/89 [==============================] - 0s 42us/step - loss: 0.0075 - acc: 1.0000\n",
            "Epoch 309/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0074 - acc: 1.0000\n",
            "Epoch 310/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 311/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0073 - acc: 1.0000\n",
            "Epoch 312/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0072 - acc: 1.0000\n",
            "Epoch 313/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0072 - acc: 1.0000\n",
            "Epoch 314/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0071 - acc: 1.0000\n",
            "Epoch 315/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 316/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0070 - acc: 1.0000\n",
            "Epoch 317/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 318/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0069 - acc: 1.0000\n",
            "Epoch 319/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0068 - acc: 1.0000\n",
            "Epoch 320/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 321/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0067 - acc: 1.0000\n",
            "Epoch 322/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 323/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0066 - acc: 1.0000\n",
            "Epoch 324/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 325/500\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0065 - acc: 1.0000\n",
            "Epoch 326/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 327/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0064 - acc: 1.0000\n",
            "Epoch 328/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0063 - acc: 1.0000\n",
            "Epoch 329/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 330/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0062 - acc: 1.0000\n",
            "Epoch 331/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 332/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0061 - acc: 1.0000\n",
            "Epoch 333/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 334/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0060 - acc: 1.0000\n",
            "Epoch 335/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 336/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 337/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0059 - acc: 1.0000\n",
            "Epoch 338/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 339/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0058 - acc: 1.0000\n",
            "Epoch 340/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 341/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0057 - acc: 1.0000\n",
            "Epoch 342/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 343/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0056 - acc: 1.0000\n",
            "Epoch 344/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 345/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 346/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0055 - acc: 1.0000\n",
            "Epoch 347/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 348/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0054 - acc: 1.0000\n",
            "Epoch 349/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 350/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 351/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0053 - acc: 1.0000\n",
            "Epoch 352/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 353/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0052 - acc: 1.0000\n",
            "Epoch 354/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 355/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 356/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0051 - acc: 1.0000\n",
            "Epoch 357/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 358/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 359/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 360/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 361/500\n",
            "89/89 [==============================] - 0s 41us/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 362/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0049 - acc: 1.0000\n",
            "Epoch 363/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 364/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0048 - acc: 1.0000\n",
            "Epoch 365/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 366/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 367/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0047 - acc: 1.0000\n",
            "Epoch 368/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 369/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 370/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 371/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0046 - acc: 1.0000\n",
            "Epoch 372/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 373/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 374/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0045 - acc: 1.0000\n",
            "Epoch 375/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 376/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 377/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0044 - acc: 1.0000\n",
            "Epoch 378/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 379/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 380/500\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 381/500\n",
            "89/89 [==============================] - 0s 45us/step - loss: 0.0043 - acc: 1.0000\n",
            "Epoch 382/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 383/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 384/500\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0042 - acc: 1.0000\n",
            "Epoch 385/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 386/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 387/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 388/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0041 - acc: 1.0000\n",
            "Epoch 389/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 390/500\n",
            "89/89 [==============================] - 0s 71us/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 391/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 392/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0040 - acc: 1.0000\n",
            "Epoch 393/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 394/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 395/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 396/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0039 - acc: 1.0000\n",
            "Epoch 397/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 398/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 399/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 400/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0038 - acc: 1.0000\n",
            "Epoch 401/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 402/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 403/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 404/500\n",
            "89/89 [==============================] - 0s 40us/step - loss: 0.0037 - acc: 1.0000\n",
            "Epoch 405/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 406/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 407/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 408/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 409/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0036 - acc: 1.0000\n",
            "Epoch 410/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 411/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 412/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 413/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 414/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0035 - acc: 1.0000\n",
            "Epoch 415/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 416/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 417/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 418/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 419/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0034 - acc: 1.0000\n",
            "Epoch 420/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 421/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 422/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 423/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 424/500\n",
            "89/89 [==============================] - 0s 36us/step - loss: 0.0033 - acc: 1.0000\n",
            "Epoch 425/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 426/500\n",
            "89/89 [==============================] - 0s 21us/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 427/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 428/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 429/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0032 - acc: 1.0000\n",
            "Epoch 430/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 431/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 432/500\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 433/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 434/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 435/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0031 - acc: 1.0000\n",
            "Epoch 436/500\n",
            "89/89 [==============================] - 0s 29us/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 437/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 438/500\n",
            "89/89 [==============================] - 0s 28us/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 439/500\n",
            "89/89 [==============================] - 0s 26us/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 440/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 441/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0030 - acc: 1.0000\n",
            "Epoch 442/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 443/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 444/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 445/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 446/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 447/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0029 - acc: 1.0000\n",
            "Epoch 448/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 449/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 450/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 451/500\n",
            "89/89 [==============================] - 0s 30us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 452/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 453/500\n",
            "89/89 [==============================] - 0s 39us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 454/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0028 - acc: 1.0000\n",
            "Epoch 455/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 456/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 457/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 458/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 459/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 460/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 461/500\n",
            "89/89 [==============================] - 0s 35us/step - loss: 0.0027 - acc: 1.0000\n",
            "Epoch 462/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 463/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 464/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 465/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 466/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 467/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 468/500\n",
            "89/89 [==============================] - 0s 25us/step - loss: 0.0026 - acc: 1.0000\n",
            "Epoch 469/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 470/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 471/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 472/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 473/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 474/500\n",
            "89/89 [==============================] - 0s 34us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 475/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 476/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
            "Epoch 477/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 478/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 479/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 480/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 481/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 482/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 483/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 484/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 485/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 486/500\n",
            "89/89 [==============================] - 0s 31us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 487/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 488/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 489/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 490/500\n",
            "89/89 [==============================] - 0s 27us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 491/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 492/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 493/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 494/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
            "Epoch 495/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 496/500\n",
            "89/89 [==============================] - 0s 38us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 497/500\n",
            "89/89 [==============================] - 0s 37us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 498/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 499/500\n",
            "89/89 [==============================] - 0s 33us/step - loss: 0.0022 - acc: 1.0000\n",
            "Epoch 500/500\n",
            "89/89 [==============================] - 0s 32us/step - loss: 0.0022 - acc: 1.0000\n",
            "13/13 [==============================] - 1s 86ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "98d81b41-b506-43bd-bb4a-8122b36461c7",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c14347e1-523c-478c-ab46-ececf372479e",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7692307829856873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}
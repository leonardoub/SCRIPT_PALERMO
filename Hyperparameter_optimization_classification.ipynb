{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter_optimization_classification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Hyperparameter_optimization_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YN6Hc9lFNNk",
        "colab_type": "text"
      },
      "source": [
        "#Optimization of hyperparameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Py6sopCQLbAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        },
        "outputId": "cff7c232-8e06-4421-dfa4-b4c253e13e40"
      },
      "source": [
        "!pip install hyperas"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting hyperas\n",
            "  Downloading https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from hyperas) (1.0.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from hyperas) (5.6.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from hyperas) (2.2.5)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.1.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.6/dist-packages (from hyperas) (0.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from hyperas) (4.4.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (4.6.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->hyperas) (7.5.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (1.4.2)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.6.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.6.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (3.1.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.4.4)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.1.3)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (4.3.3)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (0.8.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->hyperas) (2.10.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.17.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->hyperas) (1.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (2.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (4.28.1)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (3.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt->hyperas) (0.16.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (0.2.0)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->hyperas) (2.6.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (0.8.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (4.5.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->hyperas) (5.3.4)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->hyperas) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->hyperas) (1.0.18)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->hyperas) (3.5.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->hyperas) (0.5.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->hyperas) (4.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->hyperas) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->hyperas) (0.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->notebook->jupyter->hyperas) (17.0.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (42.0.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.7.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
            "Installing collected packages: hyperas\n",
            "Successfully installed hyperas-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import hyperas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "95586027-2e37-4a8c-a022-1cd74359cb74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "022ed24e-b47f-413b-a54c-431b51a9f5e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "7bc89e2d-d4b3-4b08-bc88-69c57ed952e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(6, activation='relu', input_shape=(9,)))\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "  sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "767b172a-5503-47c1-a68f-f080d007c8a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "5c2ad09e-6c75-45e8-9cd2-6715fd6fac8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   4   5   8   9  11  12  14  15  16  17  19  20  22  23  24  25\n",
            "  27  29  30  33  34  36  37  38  39  40  41  42  44  45  46  48  51  52\n",
            "  53  56  57  58  59  60  62  63  65  66  67  69  72  76  77  78  79  80\n",
            "  81  83  84  85  87  88  89  90  92  96  97  98 100 101 102 103 104 105\n",
            " 107 109 110 111 113 115 117 120 121 122 124 125 127 128] TEST: [  2   3   6   7  10  13  18  21  26  28  31  32  35  43  47  49  50  54\n",
            "  55  61  64  68  70  71  73  74  75  82  86  91  93  94  95  99 106 108\n",
            " 112 114 116 118 119 123 126 129 130]\n",
            "TRAIN: [  2   3   5   6   7   8   9  10  11  12  13  18  20  21  25  26  27  28\n",
            "  29  30  31  32  34  35  36  38  39  43  44  45  46  47  48  49  50  53\n",
            "  54  55  57  58  61  63  64  65  66  68  70  71  73  74  75  76  78  82\n",
            "  84  85  86  87  90  91  92  93  94  95  96  99 100 101 102 105 106 108\n",
            " 109 111 112 114 115 116 118 119 122 123 124 125 126 127 129 130] TEST: [  0   1   4  14  15  16  17  19  22  23  24  33  37  40  41  42  51  52\n",
            "  56  59  60  62  67  69  72  77  79  80  81  83  88  89  97  98 103 104\n",
            " 107 110 113 117 120 121 128]\n",
            "TRAIN: [  0   1   2   3   4   6   7  10  13  14  15  16  17  18  19  21  22  23\n",
            "  24  26  28  31  32  33  35  37  40  41  42  43  47  49  50  51  52  54\n",
            "  55  56  59  60  61  62  64  67  68  69  70  71  72  73  74  75  77  79\n",
            "  80  81  82  83  86  88  89  91  93  94  95  97  98  99 103 104 106 107\n",
            " 108 110 112 113 114 116 117 118 119 120 121 123 126 128 129 130] TEST: [  5   8   9  11  12  20  25  27  29  30  34  36  38  39  44  45  46  48\n",
            "  53  57  58  63  65  66  76  78  84  85  87  90  92  96 100 101 102 105\n",
            " 109 111 115 122 124 125 127]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "ed589f05-8608-410e-ca6a-3f28fc954456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "13610212-c4c6-48fe-aab0-519c5b655f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=10)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 86 samples, validate on 45 samples\n",
            "Epoch 1/500\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 2.5963 - acc: 0.3372 - val_loss: 2.4555 - val_acc: 0.2889\n",
            "Epoch 2/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 2.5080 - acc: 0.3605 - val_loss: 2.3613 - val_acc: 0.3333\n",
            "Epoch 3/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 2.4286 - acc: 0.3605 - val_loss: 2.2733 - val_acc: 0.3333\n",
            "Epoch 4/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 2.3507 - acc: 0.3605 - val_loss: 2.1916 - val_acc: 0.3333\n",
            "Epoch 5/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 2.2794 - acc: 0.3721 - val_loss: 2.1164 - val_acc: 0.3333\n",
            "Epoch 6/500\n",
            "86/86 [==============================] - 0s 282us/step - loss: 2.2163 - acc: 0.3837 - val_loss: 2.0452 - val_acc: 0.3556\n",
            "Epoch 7/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 2.1487 - acc: 0.3837 - val_loss: 1.9777 - val_acc: 0.3556\n",
            "Epoch 8/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 2.0857 - acc: 0.3837 - val_loss: 1.9131 - val_acc: 0.3778\n",
            "Epoch 9/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 2.0286 - acc: 0.3837 - val_loss: 1.8543 - val_acc: 0.3778\n",
            "Epoch 10/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 1.9750 - acc: 0.4070 - val_loss: 1.7991 - val_acc: 0.3778\n",
            "Epoch 11/500\n",
            "86/86 [==============================] - 0s 277us/step - loss: 1.9230 - acc: 0.4302 - val_loss: 1.7486 - val_acc: 0.3778\n",
            "Epoch 12/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 1.8770 - acc: 0.4302 - val_loss: 1.7022 - val_acc: 0.3778\n",
            "Epoch 13/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 1.8288 - acc: 0.4419 - val_loss: 1.6576 - val_acc: 0.3778\n",
            "Epoch 14/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 1.7859 - acc: 0.4419 - val_loss: 1.6173 - val_acc: 0.3778\n",
            "Epoch 15/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 1.7427 - acc: 0.4651 - val_loss: 1.5792 - val_acc: 0.3556\n",
            "Epoch 16/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 1.7054 - acc: 0.4651 - val_loss: 1.5445 - val_acc: 0.3778\n",
            "Epoch 17/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 1.6679 - acc: 0.4651 - val_loss: 1.5116 - val_acc: 0.3778\n",
            "Epoch 18/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 1.6344 - acc: 0.4767 - val_loss: 1.4824 - val_acc: 0.3778\n",
            "Epoch 19/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 1.6051 - acc: 0.4767 - val_loss: 1.4553 - val_acc: 0.4000\n",
            "Epoch 20/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 1.5741 - acc: 0.4767 - val_loss: 1.4300 - val_acc: 0.4222\n",
            "Epoch 21/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 1.5457 - acc: 0.4884 - val_loss: 1.4078 - val_acc: 0.4222\n",
            "Epoch 22/500\n",
            "86/86 [==============================] - 0s 199us/step - loss: 1.5193 - acc: 0.4884 - val_loss: 1.3861 - val_acc: 0.4222\n",
            "Epoch 23/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 1.4953 - acc: 0.4884 - val_loss: 1.3667 - val_acc: 0.4222\n",
            "Epoch 24/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 1.4714 - acc: 0.4884 - val_loss: 1.3478 - val_acc: 0.4222\n",
            "Epoch 25/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 1.4480 - acc: 0.5116 - val_loss: 1.3320 - val_acc: 0.4222\n",
            "Epoch 26/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 1.4276 - acc: 0.5116 - val_loss: 1.3153 - val_acc: 0.4222\n",
            "Epoch 27/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 1.4056 - acc: 0.5116 - val_loss: 1.3005 - val_acc: 0.4222\n",
            "Epoch 28/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 1.3869 - acc: 0.5116 - val_loss: 1.2875 - val_acc: 0.4222\n",
            "Epoch 29/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 1.3686 - acc: 0.5116 - val_loss: 1.2738 - val_acc: 0.4222\n",
            "Epoch 30/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 1.3508 - acc: 0.5000 - val_loss: 1.2617 - val_acc: 0.4444\n",
            "Epoch 31/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 1.3362 - acc: 0.5000 - val_loss: 1.2521 - val_acc: 0.4444\n",
            "Epoch 32/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 1.3214 - acc: 0.5000 - val_loss: 1.2423 - val_acc: 0.4444\n",
            "Epoch 33/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 1.3081 - acc: 0.5000 - val_loss: 1.2327 - val_acc: 0.4222\n",
            "Epoch 34/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 1.2941 - acc: 0.4884 - val_loss: 1.2244 - val_acc: 0.4222\n",
            "Epoch 35/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 1.2821 - acc: 0.5116 - val_loss: 1.2155 - val_acc: 0.4222\n",
            "Epoch 36/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 1.2686 - acc: 0.5000 - val_loss: 1.2058 - val_acc: 0.4444\n",
            "Epoch 37/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 1.2566 - acc: 0.5000 - val_loss: 1.1991 - val_acc: 0.4444\n",
            "Epoch 38/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 1.2446 - acc: 0.5116 - val_loss: 1.1941 - val_acc: 0.4444\n",
            "Epoch 39/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 1.2355 - acc: 0.5233 - val_loss: 1.1881 - val_acc: 0.4444\n",
            "Epoch 40/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 1.2243 - acc: 0.5233 - val_loss: 1.1810 - val_acc: 0.4889\n",
            "Epoch 41/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 1.2151 - acc: 0.5000 - val_loss: 1.1761 - val_acc: 0.4889\n",
            "Epoch 42/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 1.2052 - acc: 0.5116 - val_loss: 1.1708 - val_acc: 0.4889\n",
            "Epoch 43/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 1.1954 - acc: 0.5116 - val_loss: 1.1648 - val_acc: 0.4889\n",
            "Epoch 44/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 1.1885 - acc: 0.5116 - val_loss: 1.1594 - val_acc: 0.4889\n",
            "Epoch 45/500\n",
            "86/86 [==============================] - 0s 201us/step - loss: 1.1800 - acc: 0.5116 - val_loss: 1.1528 - val_acc: 0.4889\n",
            "Epoch 46/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 1.1715 - acc: 0.5116 - val_loss: 1.1481 - val_acc: 0.4889\n",
            "Epoch 47/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 1.1616 - acc: 0.5000 - val_loss: 1.1438 - val_acc: 0.4889\n",
            "Epoch 48/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 1.1546 - acc: 0.5000 - val_loss: 1.1385 - val_acc: 0.4889\n",
            "Epoch 49/500\n",
            "86/86 [==============================] - 0s 265us/step - loss: 1.1463 - acc: 0.5000 - val_loss: 1.1346 - val_acc: 0.4667\n",
            "Epoch 50/500\n",
            "86/86 [==============================] - 0s 263us/step - loss: 1.1394 - acc: 0.5000 - val_loss: 1.1309 - val_acc: 0.4667\n",
            "Epoch 51/500\n",
            "86/86 [==============================] - 0s 378us/step - loss: 1.1291 - acc: 0.5116 - val_loss: 1.1266 - val_acc: 0.4667\n",
            "Epoch 52/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 1.1214 - acc: 0.5116 - val_loss: 1.1224 - val_acc: 0.4667\n",
            "Epoch 53/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 1.1140 - acc: 0.5116 - val_loss: 1.1184 - val_acc: 0.4667\n",
            "Epoch 54/500\n",
            "86/86 [==============================] - 0s 261us/step - loss: 1.1062 - acc: 0.5116 - val_loss: 1.1146 - val_acc: 0.4444\n",
            "Epoch 55/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 1.0996 - acc: 0.5000 - val_loss: 1.1115 - val_acc: 0.4444\n",
            "Epoch 56/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 1.0934 - acc: 0.5000 - val_loss: 1.1089 - val_acc: 0.4667\n",
            "Epoch 57/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 1.0862 - acc: 0.5000 - val_loss: 1.1062 - val_acc: 0.4667\n",
            "Epoch 58/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 1.0794 - acc: 0.5000 - val_loss: 1.1044 - val_acc: 0.4667\n",
            "Epoch 59/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 1.0736 - acc: 0.5000 - val_loss: 1.1021 - val_acc: 0.4667\n",
            "Epoch 60/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 1.0665 - acc: 0.5116 - val_loss: 1.0995 - val_acc: 0.4667\n",
            "Epoch 61/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 1.0625 - acc: 0.5116 - val_loss: 1.0968 - val_acc: 0.4667\n",
            "Epoch 62/500\n",
            "86/86 [==============================] - 0s 264us/step - loss: 1.0568 - acc: 0.5116 - val_loss: 1.0942 - val_acc: 0.4667\n",
            "Epoch 63/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 1.0527 - acc: 0.5116 - val_loss: 1.0922 - val_acc: 0.4667\n",
            "Epoch 64/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 1.0481 - acc: 0.5233 - val_loss: 1.0907 - val_acc: 0.4889\n",
            "Epoch 65/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 1.0438 - acc: 0.5233 - val_loss: 1.0894 - val_acc: 0.4889\n",
            "Epoch 66/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 1.0395 - acc: 0.5233 - val_loss: 1.0883 - val_acc: 0.4889\n",
            "Epoch 67/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 1.0354 - acc: 0.5465 - val_loss: 1.0863 - val_acc: 0.4889\n",
            "Epoch 68/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 1.0317 - acc: 0.5349 - val_loss: 1.0846 - val_acc: 0.4889\n",
            "Epoch 69/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 1.0280 - acc: 0.5581 - val_loss: 1.0836 - val_acc: 0.4667\n",
            "Epoch 70/500\n",
            "86/86 [==============================] - 0s 190us/step - loss: 1.0231 - acc: 0.5698 - val_loss: 1.0820 - val_acc: 0.4667\n",
            "Epoch 71/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 1.0206 - acc: 0.5698 - val_loss: 1.0809 - val_acc: 0.4667\n",
            "Epoch 72/500\n",
            "86/86 [==============================] - 0s 277us/step - loss: 1.0156 - acc: 0.5581 - val_loss: 1.0798 - val_acc: 0.4667\n",
            "Epoch 73/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 1.0131 - acc: 0.5698 - val_loss: 1.0781 - val_acc: 0.4667\n",
            "Epoch 74/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 1.0089 - acc: 0.5581 - val_loss: 1.0769 - val_acc: 0.4667\n",
            "Epoch 75/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 1.0067 - acc: 0.5581 - val_loss: 1.0755 - val_acc: 0.4667\n",
            "Epoch 76/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 1.0033 - acc: 0.5581 - val_loss: 1.0741 - val_acc: 0.4667\n",
            "Epoch 77/500\n",
            "86/86 [==============================] - 0s 260us/step - loss: 1.0001 - acc: 0.5581 - val_loss: 1.0737 - val_acc: 0.4667\n",
            "Epoch 78/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.9979 - acc: 0.5581 - val_loss: 1.0731 - val_acc: 0.4667\n",
            "Epoch 79/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.9941 - acc: 0.5581 - val_loss: 1.0723 - val_acc: 0.4667\n",
            "Epoch 80/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.9912 - acc: 0.5581 - val_loss: 1.0717 - val_acc: 0.4667\n",
            "Epoch 81/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.9901 - acc: 0.5581 - val_loss: 1.0716 - val_acc: 0.4667\n",
            "Epoch 82/500\n",
            "86/86 [==============================] - 0s 252us/step - loss: 0.9858 - acc: 0.5581 - val_loss: 1.0712 - val_acc: 0.4667\n",
            "Epoch 83/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.9844 - acc: 0.5465 - val_loss: 1.0708 - val_acc: 0.4667\n",
            "Epoch 84/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.9811 - acc: 0.5465 - val_loss: 1.0701 - val_acc: 0.4667\n",
            "Epoch 85/500\n",
            "86/86 [==============================] - 0s 255us/step - loss: 0.9788 - acc: 0.5465 - val_loss: 1.0703 - val_acc: 0.4667\n",
            "Epoch 86/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.9762 - acc: 0.5581 - val_loss: 1.0702 - val_acc: 0.4667\n",
            "Epoch 87/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.9742 - acc: 0.5581 - val_loss: 1.0703 - val_acc: 0.4667\n",
            "Epoch 88/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.9727 - acc: 0.5581 - val_loss: 1.0697 - val_acc: 0.4667\n",
            "Epoch 89/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.9696 - acc: 0.5581 - val_loss: 1.0687 - val_acc: 0.4667\n",
            "Epoch 90/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.9672 - acc: 0.5698 - val_loss: 1.0680 - val_acc: 0.4667\n",
            "Epoch 91/500\n",
            "86/86 [==============================] - 0s 282us/step - loss: 0.9658 - acc: 0.5698 - val_loss: 1.0680 - val_acc: 0.4889\n",
            "Epoch 92/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.9644 - acc: 0.5581 - val_loss: 1.0680 - val_acc: 0.4889\n",
            "Epoch 93/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.9629 - acc: 0.5814 - val_loss: 1.0674 - val_acc: 0.4889\n",
            "Epoch 94/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.9603 - acc: 0.5814 - val_loss: 1.0669 - val_acc: 0.4889\n",
            "Epoch 95/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.9585 - acc: 0.5814 - val_loss: 1.0665 - val_acc: 0.4889\n",
            "Epoch 96/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.9564 - acc: 0.5814 - val_loss: 1.0659 - val_acc: 0.4667\n",
            "Epoch 97/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.9547 - acc: 0.5814 - val_loss: 1.0658 - val_acc: 0.4889\n",
            "Epoch 98/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.9539 - acc: 0.5814 - val_loss: 1.0656 - val_acc: 0.4889\n",
            "Epoch 99/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.9524 - acc: 0.5814 - val_loss: 1.0654 - val_acc: 0.4889\n",
            "Epoch 100/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.9504 - acc: 0.5814 - val_loss: 1.0652 - val_acc: 0.4889\n",
            "Epoch 101/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.9496 - acc: 0.5814 - val_loss: 1.0642 - val_acc: 0.4889\n",
            "Epoch 102/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.9473 - acc: 0.5814 - val_loss: 1.0636 - val_acc: 0.4889\n",
            "Epoch 103/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 0.9459 - acc: 0.5930 - val_loss: 1.0629 - val_acc: 0.4667\n",
            "Epoch 104/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 0.9453 - acc: 0.5930 - val_loss: 1.0625 - val_acc: 0.4667\n",
            "Epoch 105/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.9433 - acc: 0.5930 - val_loss: 1.0617 - val_acc: 0.4667\n",
            "Epoch 106/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.9419 - acc: 0.5930 - val_loss: 1.0608 - val_acc: 0.4667\n",
            "Epoch 107/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.9409 - acc: 0.5930 - val_loss: 1.0601 - val_acc: 0.4667\n",
            "Epoch 108/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.9388 - acc: 0.5930 - val_loss: 1.0598 - val_acc: 0.4667\n",
            "Epoch 109/500\n",
            "86/86 [==============================] - 0s 265us/step - loss: 0.9372 - acc: 0.5930 - val_loss: 1.0591 - val_acc: 0.4667\n",
            "Epoch 110/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.9374 - acc: 0.5930 - val_loss: 1.0590 - val_acc: 0.4667\n",
            "Epoch 111/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.9346 - acc: 0.6047 - val_loss: 1.0597 - val_acc: 0.4889\n",
            "Epoch 112/500\n",
            "86/86 [==============================] - 0s 283us/step - loss: 0.9344 - acc: 0.6047 - val_loss: 1.0596 - val_acc: 0.4889\n",
            "Epoch 113/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.9316 - acc: 0.6047 - val_loss: 1.0590 - val_acc: 0.4667\n",
            "Epoch 114/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.9305 - acc: 0.6047 - val_loss: 1.0587 - val_acc: 0.4667\n",
            "Epoch 115/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.9290 - acc: 0.5930 - val_loss: 1.0588 - val_acc: 0.4889\n",
            "Epoch 116/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.9278 - acc: 0.5930 - val_loss: 1.0582 - val_acc: 0.4889\n",
            "Epoch 117/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.9275 - acc: 0.5930 - val_loss: 1.0577 - val_acc: 0.4667\n",
            "Epoch 118/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 0.9259 - acc: 0.6047 - val_loss: 1.0579 - val_acc: 0.4889\n",
            "Epoch 119/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.9245 - acc: 0.5930 - val_loss: 1.0580 - val_acc: 0.4889\n",
            "Epoch 120/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.9233 - acc: 0.6047 - val_loss: 1.0581 - val_acc: 0.4889\n",
            "Epoch 121/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.9219 - acc: 0.5930 - val_loss: 1.0580 - val_acc: 0.4889\n",
            "Epoch 122/500\n",
            "86/86 [==============================] - 0s 260us/step - loss: 0.9211 - acc: 0.5930 - val_loss: 1.0582 - val_acc: 0.5111\n",
            "Epoch 123/500\n",
            "86/86 [==============================] - 0s 175us/step - loss: 0.9197 - acc: 0.5930 - val_loss: 1.0581 - val_acc: 0.5111\n",
            "Epoch 124/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.9193 - acc: 0.5930 - val_loss: 1.0579 - val_acc: 0.5111\n",
            "Epoch 125/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.9182 - acc: 0.5930 - val_loss: 1.0574 - val_acc: 0.5111\n",
            "Epoch 126/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.9170 - acc: 0.6047 - val_loss: 1.0566 - val_acc: 0.5111\n",
            "Epoch 127/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.9152 - acc: 0.6047 - val_loss: 1.0559 - val_acc: 0.5111\n",
            "Epoch 128/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.9148 - acc: 0.5930 - val_loss: 1.0559 - val_acc: 0.5111\n",
            "Epoch 129/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.9132 - acc: 0.6047 - val_loss: 1.0562 - val_acc: 0.5111\n",
            "Epoch 130/500\n",
            "86/86 [==============================] - 0s 251us/step - loss: 0.9127 - acc: 0.6047 - val_loss: 1.0560 - val_acc: 0.5111\n",
            "Epoch 131/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.9122 - acc: 0.6047 - val_loss: 1.0555 - val_acc: 0.5111\n",
            "Epoch 132/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.9115 - acc: 0.6047 - val_loss: 1.0547 - val_acc: 0.5111\n",
            "Epoch 133/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 0.9101 - acc: 0.6047 - val_loss: 1.0551 - val_acc: 0.5111\n",
            "Epoch 134/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.9094 - acc: 0.6047 - val_loss: 1.0551 - val_acc: 0.5111\n",
            "Epoch 135/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.9081 - acc: 0.6047 - val_loss: 1.0549 - val_acc: 0.5111\n",
            "Epoch 136/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.9068 - acc: 0.6047 - val_loss: 1.0544 - val_acc: 0.5111\n",
            "Epoch 137/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 0.9072 - acc: 0.6047 - val_loss: 1.0539 - val_acc: 0.5111\n",
            "Epoch 138/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.9049 - acc: 0.6047 - val_loss: 1.0535 - val_acc: 0.5111\n",
            "Epoch 139/500\n",
            "86/86 [==============================] - 0s 275us/step - loss: 0.9046 - acc: 0.6047 - val_loss: 1.0531 - val_acc: 0.5111\n",
            "Epoch 140/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.9034 - acc: 0.6047 - val_loss: 1.0536 - val_acc: 0.5111\n",
            "Epoch 141/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.9022 - acc: 0.6047 - val_loss: 1.0538 - val_acc: 0.5111\n",
            "Epoch 142/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.9017 - acc: 0.6047 - val_loss: 1.0533 - val_acc: 0.5111\n",
            "Epoch 143/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.9015 - acc: 0.6163 - val_loss: 1.0524 - val_acc: 0.5111\n",
            "Epoch 144/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.9001 - acc: 0.6163 - val_loss: 1.0518 - val_acc: 0.5111\n",
            "Epoch 145/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8993 - acc: 0.6047 - val_loss: 1.0516 - val_acc: 0.5111\n",
            "Epoch 146/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8983 - acc: 0.6163 - val_loss: 1.0514 - val_acc: 0.5111\n",
            "Epoch 147/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.8975 - acc: 0.6163 - val_loss: 1.0515 - val_acc: 0.5111\n",
            "Epoch 148/500\n",
            "86/86 [==============================] - 0s 270us/step - loss: 0.8970 - acc: 0.6163 - val_loss: 1.0516 - val_acc: 0.5111\n",
            "Epoch 149/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8966 - acc: 0.6163 - val_loss: 1.0515 - val_acc: 0.5111\n",
            "Epoch 150/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8957 - acc: 0.6163 - val_loss: 1.0513 - val_acc: 0.5111\n",
            "Epoch 151/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8946 - acc: 0.6163 - val_loss: 1.0519 - val_acc: 0.4889\n",
            "Epoch 152/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8938 - acc: 0.6163 - val_loss: 1.0522 - val_acc: 0.4889\n",
            "Epoch 153/500\n",
            "86/86 [==============================] - 0s 267us/step - loss: 0.8942 - acc: 0.6047 - val_loss: 1.0527 - val_acc: 0.5111\n",
            "Epoch 154/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8923 - acc: 0.6163 - val_loss: 1.0535 - val_acc: 0.5111\n",
            "Epoch 155/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8918 - acc: 0.6279 - val_loss: 1.0536 - val_acc: 0.5111\n",
            "Epoch 156/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8911 - acc: 0.6163 - val_loss: 1.0535 - val_acc: 0.5111\n",
            "Epoch 157/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8911 - acc: 0.6279 - val_loss: 1.0536 - val_acc: 0.5111\n",
            "Epoch 158/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8894 - acc: 0.6279 - val_loss: 1.0533 - val_acc: 0.5111\n",
            "Epoch 159/500\n",
            "86/86 [==============================] - 0s 271us/step - loss: 0.8899 - acc: 0.6279 - val_loss: 1.0535 - val_acc: 0.5111\n",
            "Epoch 160/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8888 - acc: 0.6279 - val_loss: 1.0538 - val_acc: 0.5111\n",
            "Epoch 161/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.8883 - acc: 0.6279 - val_loss: 1.0544 - val_acc: 0.5111\n",
            "Epoch 162/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8866 - acc: 0.6279 - val_loss: 1.0549 - val_acc: 0.5111\n",
            "Epoch 163/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8863 - acc: 0.6279 - val_loss: 1.0544 - val_acc: 0.5111\n",
            "Epoch 164/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8862 - acc: 0.6279 - val_loss: 1.0544 - val_acc: 0.5111\n",
            "Epoch 165/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8856 - acc: 0.6279 - val_loss: 1.0547 - val_acc: 0.5111\n",
            "Epoch 166/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8846 - acc: 0.6279 - val_loss: 1.0543 - val_acc: 0.5111\n",
            "Epoch 167/500\n",
            "86/86 [==============================] - 0s 274us/step - loss: 0.8844 - acc: 0.6395 - val_loss: 1.0538 - val_acc: 0.5111\n",
            "Epoch 168/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8834 - acc: 0.6395 - val_loss: 1.0539 - val_acc: 0.5111\n",
            "Epoch 169/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.8844 - acc: 0.6395 - val_loss: 1.0536 - val_acc: 0.5111\n",
            "Epoch 170/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8841 - acc: 0.6163 - val_loss: 1.0531 - val_acc: 0.5111\n",
            "Epoch 171/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8830 - acc: 0.6163 - val_loss: 1.0531 - val_acc: 0.5111\n",
            "Epoch 172/500\n",
            "86/86 [==============================] - 0s 188us/step - loss: 0.8815 - acc: 0.6395 - val_loss: 1.0529 - val_acc: 0.5111\n",
            "Epoch 173/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8816 - acc: 0.6163 - val_loss: 1.0525 - val_acc: 0.5111\n",
            "Epoch 174/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8806 - acc: 0.6279 - val_loss: 1.0528 - val_acc: 0.5111\n",
            "Epoch 175/500\n",
            "86/86 [==============================] - 0s 277us/step - loss: 0.8802 - acc: 0.6163 - val_loss: 1.0534 - val_acc: 0.5111\n",
            "Epoch 176/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8799 - acc: 0.6279 - val_loss: 1.0538 - val_acc: 0.5111\n",
            "Epoch 177/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8797 - acc: 0.6163 - val_loss: 1.0540 - val_acc: 0.5111\n",
            "Epoch 178/500\n",
            "86/86 [==============================] - 0s 284us/step - loss: 0.8793 - acc: 0.6395 - val_loss: 1.0545 - val_acc: 0.5111\n",
            "Epoch 179/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8788 - acc: 0.6163 - val_loss: 1.0550 - val_acc: 0.5111\n",
            "Epoch 180/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.8780 - acc: 0.6163 - val_loss: 1.0551 - val_acc: 0.5111\n",
            "Epoch 181/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8778 - acc: 0.6279 - val_loss: 1.0548 - val_acc: 0.5111\n",
            "Epoch 182/500\n",
            "86/86 [==============================] - 0s 263us/step - loss: 0.8770 - acc: 0.6395 - val_loss: 1.0549 - val_acc: 0.5111\n",
            "Epoch 183/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8770 - acc: 0.6279 - val_loss: 1.0549 - val_acc: 0.5111\n",
            "Epoch 184/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8760 - acc: 0.6279 - val_loss: 1.0547 - val_acc: 0.5111\n",
            "Epoch 185/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8757 - acc: 0.6279 - val_loss: 1.0545 - val_acc: 0.5111\n",
            "Epoch 186/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8752 - acc: 0.6279 - val_loss: 1.0549 - val_acc: 0.5111\n",
            "Epoch 187/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8752 - acc: 0.6279 - val_loss: 1.0553 - val_acc: 0.5111\n",
            "Epoch 188/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8748 - acc: 0.6279 - val_loss: 1.0554 - val_acc: 0.5111\n",
            "Epoch 189/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8747 - acc: 0.6279 - val_loss: 1.0558 - val_acc: 0.5111\n",
            "Epoch 190/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8745 - acc: 0.6279 - val_loss: 1.0564 - val_acc: 0.4889\n",
            "Epoch 191/500\n",
            "86/86 [==============================] - 0s 259us/step - loss: 0.8731 - acc: 0.6395 - val_loss: 1.0564 - val_acc: 0.4889\n",
            "Epoch 192/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8732 - acc: 0.6395 - val_loss: 1.0561 - val_acc: 0.4889\n",
            "Epoch 193/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8727 - acc: 0.6279 - val_loss: 1.0567 - val_acc: 0.4889\n",
            "Epoch 194/500\n",
            "86/86 [==============================] - 0s 296us/step - loss: 0.8734 - acc: 0.6395 - val_loss: 1.0568 - val_acc: 0.4889\n",
            "Epoch 195/500\n",
            "86/86 [==============================] - 0s 366us/step - loss: 0.8715 - acc: 0.6395 - val_loss: 1.0571 - val_acc: 0.4889\n",
            "Epoch 196/500\n",
            "86/86 [==============================] - 0s 297us/step - loss: 0.8709 - acc: 0.6395 - val_loss: 1.0571 - val_acc: 0.4889\n",
            "Epoch 197/500\n",
            "86/86 [==============================] - 0s 290us/step - loss: 0.8708 - acc: 0.6279 - val_loss: 1.0572 - val_acc: 0.4889\n",
            "Epoch 198/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8703 - acc: 0.6279 - val_loss: 1.0576 - val_acc: 0.4889\n",
            "Epoch 199/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8698 - acc: 0.6512 - val_loss: 1.0575 - val_acc: 0.4667\n",
            "Epoch 200/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8699 - acc: 0.6395 - val_loss: 1.0568 - val_acc: 0.4667\n",
            "Epoch 201/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8688 - acc: 0.6395 - val_loss: 1.0573 - val_acc: 0.4667\n",
            "Epoch 202/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8689 - acc: 0.6395 - val_loss: 1.0575 - val_acc: 0.4667\n",
            "Epoch 203/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8682 - acc: 0.6395 - val_loss: 1.0576 - val_acc: 0.4667\n",
            "Epoch 204/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8679 - acc: 0.6395 - val_loss: 1.0577 - val_acc: 0.4667\n",
            "Epoch 205/500\n",
            "86/86 [==============================] - 0s 283us/step - loss: 0.8674 - acc: 0.6395 - val_loss: 1.0576 - val_acc: 0.4667\n",
            "Epoch 206/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.8676 - acc: 0.6395 - val_loss: 1.0575 - val_acc: 0.4667\n",
            "Epoch 207/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8671 - acc: 0.6395 - val_loss: 1.0576 - val_acc: 0.4667\n",
            "Epoch 208/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8668 - acc: 0.6395 - val_loss: 1.0580 - val_acc: 0.4667\n",
            "Epoch 209/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8664 - acc: 0.6395 - val_loss: 1.0588 - val_acc: 0.4667\n",
            "Epoch 210/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8660 - acc: 0.6395 - val_loss: 1.0598 - val_acc: 0.4667\n",
            "Epoch 211/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8662 - acc: 0.6395 - val_loss: 1.0597 - val_acc: 0.4667\n",
            "Epoch 212/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8654 - acc: 0.6395 - val_loss: 1.0597 - val_acc: 0.4667\n",
            "Epoch 213/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8653 - acc: 0.6279 - val_loss: 1.0592 - val_acc: 0.4667\n",
            "Epoch 214/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8640 - acc: 0.6395 - val_loss: 1.0590 - val_acc: 0.4667\n",
            "Epoch 215/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.8645 - acc: 0.6395 - val_loss: 1.0590 - val_acc: 0.4667\n",
            "Epoch 216/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8634 - acc: 0.6395 - val_loss: 1.0589 - val_acc: 0.4667\n",
            "Epoch 217/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8628 - acc: 0.6395 - val_loss: 1.0593 - val_acc: 0.4667\n",
            "Epoch 218/500\n",
            "86/86 [==============================] - 0s 287us/step - loss: 0.8628 - acc: 0.6395 - val_loss: 1.0595 - val_acc: 0.4667\n",
            "Epoch 219/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8623 - acc: 0.6395 - val_loss: 1.0593 - val_acc: 0.4667\n",
            "Epoch 220/500\n",
            "86/86 [==============================] - 0s 255us/step - loss: 0.8616 - acc: 0.6512 - val_loss: 1.0598 - val_acc: 0.4667\n",
            "Epoch 221/500\n",
            "86/86 [==============================] - 0s 281us/step - loss: 0.8621 - acc: 0.6395 - val_loss: 1.0605 - val_acc: 0.4667\n",
            "Epoch 222/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8617 - acc: 0.6395 - val_loss: 1.0606 - val_acc: 0.4667\n",
            "Epoch 223/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8607 - acc: 0.6395 - val_loss: 1.0603 - val_acc: 0.4667\n",
            "Epoch 224/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8605 - acc: 0.6512 - val_loss: 1.0608 - val_acc: 0.4667\n",
            "Epoch 225/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8612 - acc: 0.6512 - val_loss: 1.0606 - val_acc: 0.4667\n",
            "Epoch 226/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.8600 - acc: 0.6512 - val_loss: 1.0604 - val_acc: 0.4667\n",
            "Epoch 227/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8591 - acc: 0.6512 - val_loss: 1.0609 - val_acc: 0.4667\n",
            "Epoch 228/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8595 - acc: 0.6395 - val_loss: 1.0615 - val_acc: 0.4889\n",
            "Epoch 229/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8590 - acc: 0.6512 - val_loss: 1.0622 - val_acc: 0.4889\n",
            "Epoch 230/500\n",
            "86/86 [==============================] - 0s 263us/step - loss: 0.8582 - acc: 0.6512 - val_loss: 1.0614 - val_acc: 0.4889\n",
            "Epoch 231/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8576 - acc: 0.6512 - val_loss: 1.0610 - val_acc: 0.4889\n",
            "Epoch 232/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8573 - acc: 0.6512 - val_loss: 1.0616 - val_acc: 0.4889\n",
            "Epoch 233/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8577 - acc: 0.6512 - val_loss: 1.0620 - val_acc: 0.4889\n",
            "Epoch 234/500\n",
            "86/86 [==============================] - 0s 305us/step - loss: 0.8567 - acc: 0.6512 - val_loss: 1.0617 - val_acc: 0.4889\n",
            "Epoch 235/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8565 - acc: 0.6512 - val_loss: 1.0617 - val_acc: 0.4889\n",
            "Epoch 236/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8563 - acc: 0.6395 - val_loss: 1.0619 - val_acc: 0.5111\n",
            "Epoch 237/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8561 - acc: 0.6512 - val_loss: 1.0619 - val_acc: 0.4889\n",
            "Epoch 238/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8556 - acc: 0.6512 - val_loss: 1.0620 - val_acc: 0.5111\n",
            "Epoch 239/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.8551 - acc: 0.6512 - val_loss: 1.0624 - val_acc: 0.5111\n",
            "Epoch 240/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8550 - acc: 0.6512 - val_loss: 1.0630 - val_acc: 0.5111\n",
            "Epoch 241/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8546 - acc: 0.6512 - val_loss: 1.0627 - val_acc: 0.4889\n",
            "Epoch 242/500\n",
            "86/86 [==============================] - 0s 197us/step - loss: 0.8545 - acc: 0.6512 - val_loss: 1.0625 - val_acc: 0.4889\n",
            "Epoch 243/500\n",
            "86/86 [==============================] - 0s 261us/step - loss: 0.8546 - acc: 0.6512 - val_loss: 1.0620 - val_acc: 0.4889\n",
            "Epoch 244/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8538 - acc: 0.6512 - val_loss: 1.0617 - val_acc: 0.4889\n",
            "Epoch 245/500\n",
            "86/86 [==============================] - 0s 324us/step - loss: 0.8537 - acc: 0.6512 - val_loss: 1.0617 - val_acc: 0.4889\n",
            "Epoch 246/500\n",
            "86/86 [==============================] - 0s 199us/step - loss: 0.8527 - acc: 0.6628 - val_loss: 1.0622 - val_acc: 0.4889\n",
            "Epoch 247/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8525 - acc: 0.6512 - val_loss: 1.0629 - val_acc: 0.4889\n",
            "Epoch 248/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8524 - acc: 0.6628 - val_loss: 1.0632 - val_acc: 0.4889\n",
            "Epoch 249/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8522 - acc: 0.6512 - val_loss: 1.0633 - val_acc: 0.5111\n",
            "Epoch 250/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.8527 - acc: 0.6512 - val_loss: 1.0633 - val_acc: 0.5111\n",
            "Epoch 251/500\n",
            "86/86 [==============================] - 0s 201us/step - loss: 0.8518 - acc: 0.6628 - val_loss: 1.0630 - val_acc: 0.5111\n",
            "Epoch 252/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8512 - acc: 0.6628 - val_loss: 1.0630 - val_acc: 0.5111\n",
            "Epoch 253/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8517 - acc: 0.6628 - val_loss: 1.0634 - val_acc: 0.5111\n",
            "Epoch 254/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8508 - acc: 0.6628 - val_loss: 1.0636 - val_acc: 0.5111\n",
            "Epoch 255/500\n",
            "86/86 [==============================] - 0s 287us/step - loss: 0.8512 - acc: 0.6628 - val_loss: 1.0635 - val_acc: 0.5111\n",
            "Epoch 256/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8501 - acc: 0.6628 - val_loss: 1.0642 - val_acc: 0.5111\n",
            "Epoch 257/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8501 - acc: 0.6628 - val_loss: 1.0647 - val_acc: 0.5111\n",
            "Epoch 258/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8503 - acc: 0.6628 - val_loss: 1.0648 - val_acc: 0.4889\n",
            "Epoch 259/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8495 - acc: 0.6512 - val_loss: 1.0647 - val_acc: 0.5111\n",
            "Epoch 260/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8488 - acc: 0.6628 - val_loss: 1.0651 - val_acc: 0.5333\n",
            "Epoch 261/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8485 - acc: 0.6628 - val_loss: 1.0656 - val_acc: 0.5333\n",
            "Epoch 262/500\n",
            "86/86 [==============================] - 0s 285us/step - loss: 0.8486 - acc: 0.6628 - val_loss: 1.0663 - val_acc: 0.5556\n",
            "Epoch 263/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8485 - acc: 0.6628 - val_loss: 1.0670 - val_acc: 0.5556\n",
            "Epoch 264/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8485 - acc: 0.6628 - val_loss: 1.0670 - val_acc: 0.5333\n",
            "Epoch 265/500\n",
            "86/86 [==============================] - 0s 302us/step - loss: 0.8482 - acc: 0.6628 - val_loss: 1.0670 - val_acc: 0.5556\n",
            "Epoch 266/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.8471 - acc: 0.6628 - val_loss: 1.0668 - val_acc: 0.5333\n",
            "Epoch 267/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8467 - acc: 0.6744 - val_loss: 1.0665 - val_acc: 0.5333\n",
            "Epoch 268/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8477 - acc: 0.6628 - val_loss: 1.0664 - val_acc: 0.5111\n",
            "Epoch 269/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8463 - acc: 0.6744 - val_loss: 1.0659 - val_acc: 0.5111\n",
            "Epoch 270/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8461 - acc: 0.6744 - val_loss: 1.0654 - val_acc: 0.5111\n",
            "Epoch 271/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8456 - acc: 0.6860 - val_loss: 1.0652 - val_acc: 0.5111\n",
            "Epoch 272/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8458 - acc: 0.6628 - val_loss: 1.0652 - val_acc: 0.5111\n",
            "Epoch 273/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8451 - acc: 0.6744 - val_loss: 1.0649 - val_acc: 0.5111\n",
            "Epoch 274/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8453 - acc: 0.6628 - val_loss: 1.0645 - val_acc: 0.5111\n",
            "Epoch 275/500\n",
            "86/86 [==============================] - 0s 281us/step - loss: 0.8445 - acc: 0.6860 - val_loss: 1.0643 - val_acc: 0.5111\n",
            "Epoch 276/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8443 - acc: 0.6628 - val_loss: 1.0640 - val_acc: 0.5333\n",
            "Epoch 277/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8445 - acc: 0.6860 - val_loss: 1.0637 - val_acc: 0.5333\n",
            "Epoch 278/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.8446 - acc: 0.6744 - val_loss: 1.0634 - val_acc: 0.5333\n",
            "Epoch 279/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8443 - acc: 0.6744 - val_loss: 1.0633 - val_acc: 0.5111\n",
            "Epoch 280/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8433 - acc: 0.6744 - val_loss: 1.0634 - val_acc: 0.5333\n",
            "Epoch 281/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8431 - acc: 0.6744 - val_loss: 1.0633 - val_acc: 0.5333\n",
            "Epoch 282/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8434 - acc: 0.6628 - val_loss: 1.0633 - val_acc: 0.5333\n",
            "Epoch 283/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.8429 - acc: 0.6744 - val_loss: 1.0627 - val_acc: 0.5333\n",
            "Epoch 284/500\n",
            "86/86 [==============================] - 0s 252us/step - loss: 0.8431 - acc: 0.6860 - val_loss: 1.0625 - val_acc: 0.5333\n",
            "Epoch 285/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8428 - acc: 0.6860 - val_loss: 1.0630 - val_acc: 0.5333\n",
            "Epoch 286/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8422 - acc: 0.6860 - val_loss: 1.0630 - val_acc: 0.5333\n",
            "Epoch 287/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8418 - acc: 0.6860 - val_loss: 1.0631 - val_acc: 0.5333\n",
            "Epoch 288/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.8421 - acc: 0.6744 - val_loss: 1.0631 - val_acc: 0.5333\n",
            "Epoch 289/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8413 - acc: 0.6860 - val_loss: 1.0634 - val_acc: 0.5333\n",
            "Epoch 290/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8415 - acc: 0.6744 - val_loss: 1.0634 - val_acc: 0.5333\n",
            "Epoch 291/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8407 - acc: 0.6860 - val_loss: 1.0634 - val_acc: 0.5333\n",
            "Epoch 292/500\n",
            "86/86 [==============================] - 0s 277us/step - loss: 0.8400 - acc: 0.6860 - val_loss: 1.0637 - val_acc: 0.5333\n",
            "Epoch 293/500\n",
            "86/86 [==============================] - 0s 282us/step - loss: 0.8404 - acc: 0.6744 - val_loss: 1.0637 - val_acc: 0.5333\n",
            "Epoch 294/500\n",
            "86/86 [==============================] - 0s 273us/step - loss: 0.8400 - acc: 0.6860 - val_loss: 1.0638 - val_acc: 0.5333\n",
            "Epoch 295/500\n",
            "86/86 [==============================] - 0s 254us/step - loss: 0.8396 - acc: 0.6744 - val_loss: 1.0638 - val_acc: 0.5333\n",
            "Epoch 296/500\n",
            "86/86 [==============================] - 0s 272us/step - loss: 0.8396 - acc: 0.6860 - val_loss: 1.0639 - val_acc: 0.5333\n",
            "Epoch 297/500\n",
            "86/86 [==============================] - 0s 265us/step - loss: 0.8392 - acc: 0.6860 - val_loss: 1.0637 - val_acc: 0.5333\n",
            "Epoch 298/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8390 - acc: 0.6860 - val_loss: 1.0633 - val_acc: 0.5333\n",
            "Epoch 299/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8387 - acc: 0.6860 - val_loss: 1.0635 - val_acc: 0.5556\n",
            "Epoch 300/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8382 - acc: 0.6860 - val_loss: 1.0641 - val_acc: 0.5333\n",
            "Epoch 301/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8389 - acc: 0.6860 - val_loss: 1.0643 - val_acc: 0.5333\n",
            "Epoch 302/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8383 - acc: 0.6860 - val_loss: 1.0647 - val_acc: 0.5333\n",
            "Epoch 303/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8376 - acc: 0.6860 - val_loss: 1.0653 - val_acc: 0.5333\n",
            "Epoch 304/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8382 - acc: 0.6860 - val_loss: 1.0656 - val_acc: 0.5333\n",
            "Epoch 305/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8373 - acc: 0.6860 - val_loss: 1.0651 - val_acc: 0.5111\n",
            "Epoch 306/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8367 - acc: 0.6744 - val_loss: 1.0648 - val_acc: 0.5111\n",
            "Epoch 307/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8375 - acc: 0.6860 - val_loss: 1.0644 - val_acc: 0.5111\n",
            "Epoch 308/500\n",
            "86/86 [==============================] - 0s 276us/step - loss: 0.8365 - acc: 0.6744 - val_loss: 1.0639 - val_acc: 0.5111\n",
            "Epoch 309/500\n",
            "86/86 [==============================] - 0s 302us/step - loss: 0.8365 - acc: 0.6860 - val_loss: 1.0638 - val_acc: 0.5333\n",
            "Epoch 310/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8360 - acc: 0.6744 - val_loss: 1.0636 - val_acc: 0.5556\n",
            "Epoch 311/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8365 - acc: 0.6860 - val_loss: 1.0641 - val_acc: 0.5556\n",
            "Epoch 312/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8361 - acc: 0.6860 - val_loss: 1.0643 - val_acc: 0.5333\n",
            "Epoch 313/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8355 - acc: 0.6860 - val_loss: 1.0648 - val_acc: 0.5333\n",
            "Epoch 314/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8352 - acc: 0.6860 - val_loss: 1.0656 - val_acc: 0.5333\n",
            "Epoch 315/500\n",
            "86/86 [==============================] - 0s 266us/step - loss: 0.8352 - acc: 0.6744 - val_loss: 1.0663 - val_acc: 0.5333\n",
            "Epoch 316/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8346 - acc: 0.6860 - val_loss: 1.0667 - val_acc: 0.5333\n",
            "Epoch 317/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8345 - acc: 0.6860 - val_loss: 1.0669 - val_acc: 0.5333\n",
            "Epoch 318/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8344 - acc: 0.6860 - val_loss: 1.0673 - val_acc: 0.5333\n",
            "Epoch 319/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8341 - acc: 0.6860 - val_loss: 1.0679 - val_acc: 0.5333\n",
            "Epoch 320/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8342 - acc: 0.6860 - val_loss: 1.0679 - val_acc: 0.5333\n",
            "Epoch 321/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8342 - acc: 0.6744 - val_loss: 1.0681 - val_acc: 0.5333\n",
            "Epoch 322/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8333 - acc: 0.6744 - val_loss: 1.0682 - val_acc: 0.5333\n",
            "Epoch 323/500\n",
            "86/86 [==============================] - 0s 191us/step - loss: 0.8331 - acc: 0.6860 - val_loss: 1.0680 - val_acc: 0.5333\n",
            "Epoch 324/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8333 - acc: 0.6860 - val_loss: 1.0682 - val_acc: 0.5333\n",
            "Epoch 325/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8322 - acc: 0.6860 - val_loss: 1.0681 - val_acc: 0.5333\n",
            "Epoch 326/500\n",
            "86/86 [==============================] - 0s 258us/step - loss: 0.8322 - acc: 0.6744 - val_loss: 1.0679 - val_acc: 0.5556\n",
            "Epoch 327/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8332 - acc: 0.6744 - val_loss: 1.0675 - val_acc: 0.5556\n",
            "Epoch 328/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8323 - acc: 0.6744 - val_loss: 1.0676 - val_acc: 0.5556\n",
            "Epoch 329/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8325 - acc: 0.6744 - val_loss: 1.0679 - val_acc: 0.5556\n",
            "Epoch 330/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8319 - acc: 0.6744 - val_loss: 1.0679 - val_acc: 0.5556\n",
            "Epoch 331/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8315 - acc: 0.6860 - val_loss: 1.0676 - val_acc: 0.5556\n",
            "Epoch 332/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8308 - acc: 0.6744 - val_loss: 1.0677 - val_acc: 0.5556\n",
            "Epoch 333/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8308 - acc: 0.6744 - val_loss: 1.0678 - val_acc: 0.5556\n",
            "Epoch 334/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8304 - acc: 0.6744 - val_loss: 1.0677 - val_acc: 0.5556\n",
            "Epoch 335/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8306 - acc: 0.6744 - val_loss: 1.0674 - val_acc: 0.5556\n",
            "Epoch 336/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8304 - acc: 0.6744 - val_loss: 1.0681 - val_acc: 0.5556\n",
            "Epoch 337/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8297 - acc: 0.6744 - val_loss: 1.0681 - val_acc: 0.5556\n",
            "Epoch 338/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8297 - acc: 0.6744 - val_loss: 1.0682 - val_acc: 0.5556\n",
            "Epoch 339/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8297 - acc: 0.6744 - val_loss: 1.0689 - val_acc: 0.5556\n",
            "Epoch 340/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8294 - acc: 0.6744 - val_loss: 1.0689 - val_acc: 0.5556\n",
            "Epoch 341/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8293 - acc: 0.6744 - val_loss: 1.0693 - val_acc: 0.5556\n",
            "Epoch 342/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8289 - acc: 0.6744 - val_loss: 1.0696 - val_acc: 0.5556\n",
            "Epoch 343/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8290 - acc: 0.6744 - val_loss: 1.0694 - val_acc: 0.5556\n",
            "Epoch 344/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8289 - acc: 0.6744 - val_loss: 1.0694 - val_acc: 0.5556\n",
            "Epoch 345/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8287 - acc: 0.6744 - val_loss: 1.0690 - val_acc: 0.5556\n",
            "Epoch 346/500\n",
            "86/86 [==============================] - 0s 312us/step - loss: 0.8283 - acc: 0.6744 - val_loss: 1.0693 - val_acc: 0.5556\n",
            "Epoch 347/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8281 - acc: 0.6744 - val_loss: 1.0700 - val_acc: 0.5333\n",
            "Epoch 348/500\n",
            "86/86 [==============================] - 0s 256us/step - loss: 0.8282 - acc: 0.6744 - val_loss: 1.0705 - val_acc: 0.5333\n",
            "Epoch 349/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8278 - acc: 0.6744 - val_loss: 1.0707 - val_acc: 0.5333\n",
            "Epoch 350/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.8273 - acc: 0.6744 - val_loss: 1.0701 - val_acc: 0.5333\n",
            "Epoch 351/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8274 - acc: 0.6744 - val_loss: 1.0698 - val_acc: 0.5333\n",
            "Epoch 352/500\n",
            "86/86 [==============================] - 0s 183us/step - loss: 0.8271 - acc: 0.6744 - val_loss: 1.0706 - val_acc: 0.5333\n",
            "Epoch 353/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8271 - acc: 0.6744 - val_loss: 1.0711 - val_acc: 0.5333\n",
            "Epoch 354/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8265 - acc: 0.6744 - val_loss: 1.0706 - val_acc: 0.5111\n",
            "Epoch 355/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8271 - acc: 0.6744 - val_loss: 1.0708 - val_acc: 0.5333\n",
            "Epoch 356/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8269 - acc: 0.6744 - val_loss: 1.0717 - val_acc: 0.5333\n",
            "Epoch 357/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8260 - acc: 0.6744 - val_loss: 1.0721 - val_acc: 0.5333\n",
            "Epoch 358/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8260 - acc: 0.6744 - val_loss: 1.0716 - val_acc: 0.5333\n",
            "Epoch 359/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8265 - acc: 0.6744 - val_loss: 1.0714 - val_acc: 0.5111\n",
            "Epoch 360/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8258 - acc: 0.6744 - val_loss: 1.0716 - val_acc: 0.5111\n",
            "Epoch 361/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.8257 - acc: 0.6744 - val_loss: 1.0717 - val_acc: 0.5111\n",
            "Epoch 362/500\n",
            "86/86 [==============================] - 0s 333us/step - loss: 0.8249 - acc: 0.6744 - val_loss: 1.0722 - val_acc: 0.5111\n",
            "Epoch 363/500\n",
            "86/86 [==============================] - 0s 254us/step - loss: 0.8247 - acc: 0.6744 - val_loss: 1.0725 - val_acc: 0.5333\n",
            "Epoch 364/500\n",
            "86/86 [==============================] - 0s 252us/step - loss: 0.8251 - acc: 0.6744 - val_loss: 1.0727 - val_acc: 0.5333\n",
            "Epoch 365/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8251 - acc: 0.6744 - val_loss: 1.0728 - val_acc: 0.5333\n",
            "Epoch 366/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8242 - acc: 0.6744 - val_loss: 1.0729 - val_acc: 0.5333\n",
            "Epoch 367/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8239 - acc: 0.6744 - val_loss: 1.0728 - val_acc: 0.5111\n",
            "Epoch 368/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8237 - acc: 0.6744 - val_loss: 1.0729 - val_acc: 0.5111\n",
            "Epoch 369/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8238 - acc: 0.6744 - val_loss: 1.0730 - val_acc: 0.5111\n",
            "Epoch 370/500\n",
            "86/86 [==============================] - 0s 258us/step - loss: 0.8239 - acc: 0.6744 - val_loss: 1.0734 - val_acc: 0.5111\n",
            "Epoch 371/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8237 - acc: 0.6744 - val_loss: 1.0739 - val_acc: 0.5111\n",
            "Epoch 372/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8228 - acc: 0.6744 - val_loss: 1.0735 - val_acc: 0.5111\n",
            "Epoch 373/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8226 - acc: 0.6744 - val_loss: 1.0726 - val_acc: 0.5111\n",
            "Epoch 374/500\n",
            "86/86 [==============================] - 0s 274us/step - loss: 0.8227 - acc: 0.6744 - val_loss: 1.0727 - val_acc: 0.5111\n",
            "Epoch 375/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8225 - acc: 0.6744 - val_loss: 1.0731 - val_acc: 0.5111\n",
            "Epoch 376/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8224 - acc: 0.6744 - val_loss: 1.0734 - val_acc: 0.5111\n",
            "Epoch 377/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8227 - acc: 0.6744 - val_loss: 1.0739 - val_acc: 0.5111\n",
            "Epoch 378/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8220 - acc: 0.6744 - val_loss: 1.0736 - val_acc: 0.5333\n",
            "Epoch 379/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8221 - acc: 0.6860 - val_loss: 1.0743 - val_acc: 0.5333\n",
            "Epoch 380/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8220 - acc: 0.6860 - val_loss: 1.0751 - val_acc: 0.5111\n",
            "Epoch 381/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.8216 - acc: 0.6744 - val_loss: 1.0758 - val_acc: 0.5111\n",
            "Epoch 382/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8208 - acc: 0.6744 - val_loss: 1.0765 - val_acc: 0.5111\n",
            "Epoch 383/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8214 - acc: 0.6860 - val_loss: 1.0769 - val_acc: 0.5111\n",
            "Epoch 384/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8214 - acc: 0.6860 - val_loss: 1.0769 - val_acc: 0.5111\n",
            "Epoch 385/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.8209 - acc: 0.6860 - val_loss: 1.0774 - val_acc: 0.5111\n",
            "Epoch 386/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 0.8215 - acc: 0.6744 - val_loss: 1.0782 - val_acc: 0.5111\n",
            "Epoch 387/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8203 - acc: 0.6744 - val_loss: 1.0780 - val_acc: 0.5111\n",
            "Epoch 388/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8204 - acc: 0.6744 - val_loss: 1.0773 - val_acc: 0.5111\n",
            "Epoch 389/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.8202 - acc: 0.6744 - val_loss: 1.0770 - val_acc: 0.5111\n",
            "Epoch 390/500\n",
            "86/86 [==============================] - 0s 326us/step - loss: 0.8198 - acc: 0.6744 - val_loss: 1.0770 - val_acc: 0.5111\n",
            "Epoch 391/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8201 - acc: 0.6744 - val_loss: 1.0776 - val_acc: 0.5111\n",
            "Epoch 392/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8196 - acc: 0.6744 - val_loss: 1.0774 - val_acc: 0.5111\n",
            "Epoch 393/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8196 - acc: 0.6860 - val_loss: 1.0771 - val_acc: 0.5111\n",
            "Epoch 394/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8190 - acc: 0.6744 - val_loss: 1.0765 - val_acc: 0.5111\n",
            "Epoch 395/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8188 - acc: 0.6744 - val_loss: 1.0768 - val_acc: 0.5111\n",
            "Epoch 396/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8192 - acc: 0.6744 - val_loss: 1.0774 - val_acc: 0.5111\n",
            "Epoch 397/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8183 - acc: 0.6744 - val_loss: 1.0782 - val_acc: 0.5111\n",
            "Epoch 398/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8183 - acc: 0.6744 - val_loss: 1.0790 - val_acc: 0.5111\n",
            "Epoch 399/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8184 - acc: 0.6744 - val_loss: 1.0792 - val_acc: 0.5111\n",
            "Epoch 400/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8183 - acc: 0.6744 - val_loss: 1.0788 - val_acc: 0.5111\n",
            "Epoch 401/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8183 - acc: 0.6744 - val_loss: 1.0788 - val_acc: 0.5111\n",
            "Epoch 402/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8182 - acc: 0.6744 - val_loss: 1.0797 - val_acc: 0.5111\n",
            "Epoch 403/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8186 - acc: 0.6744 - val_loss: 1.0805 - val_acc: 0.5111\n",
            "Epoch 404/500\n",
            "86/86 [==============================] - 0s 262us/step - loss: 0.8178 - acc: 0.6744 - val_loss: 1.0809 - val_acc: 0.5111\n",
            "Epoch 405/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.8169 - acc: 0.6744 - val_loss: 1.0815 - val_acc: 0.5111\n",
            "Epoch 406/500\n",
            "86/86 [==============================] - 0s 259us/step - loss: 0.8167 - acc: 0.6744 - val_loss: 1.0818 - val_acc: 0.5111\n",
            "Epoch 407/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.8167 - acc: 0.6744 - val_loss: 1.0815 - val_acc: 0.5111\n",
            "Epoch 408/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8164 - acc: 0.6744 - val_loss: 1.0816 - val_acc: 0.5111\n",
            "Epoch 409/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8169 - acc: 0.6744 - val_loss: 1.0808 - val_acc: 0.5111\n",
            "Epoch 410/500\n",
            "86/86 [==============================] - 0s 276us/step - loss: 0.8162 - acc: 0.6744 - val_loss: 1.0804 - val_acc: 0.5111\n",
            "Epoch 411/500\n",
            "86/86 [==============================] - 0s 186us/step - loss: 0.8164 - acc: 0.6744 - val_loss: 1.0811 - val_acc: 0.5111\n",
            "Epoch 412/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8156 - acc: 0.6744 - val_loss: 1.0814 - val_acc: 0.5111\n",
            "Epoch 413/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8161 - acc: 0.6744 - val_loss: 1.0817 - val_acc: 0.5111\n",
            "Epoch 414/500\n",
            "86/86 [==============================] - 0s 185us/step - loss: 0.8161 - acc: 0.6744 - val_loss: 1.0819 - val_acc: 0.5111\n",
            "Epoch 415/500\n",
            "86/86 [==============================] - 0s 263us/step - loss: 0.8155 - acc: 0.6744 - val_loss: 1.0819 - val_acc: 0.5111\n",
            "Epoch 416/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 0.8150 - acc: 0.6744 - val_loss: 1.0819 - val_acc: 0.5111\n",
            "Epoch 417/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 0.8149 - acc: 0.6744 - val_loss: 1.0822 - val_acc: 0.5111\n",
            "Epoch 418/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8154 - acc: 0.6744 - val_loss: 1.0818 - val_acc: 0.5111\n",
            "Epoch 419/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8151 - acc: 0.6744 - val_loss: 1.0809 - val_acc: 0.5111\n",
            "Epoch 420/500\n",
            "86/86 [==============================] - 0s 297us/step - loss: 0.8150 - acc: 0.6744 - val_loss: 1.0810 - val_acc: 0.5111\n",
            "Epoch 421/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8140 - acc: 0.6744 - val_loss: 1.0814 - val_acc: 0.5111\n",
            "Epoch 422/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8143 - acc: 0.6744 - val_loss: 1.0818 - val_acc: 0.5111\n",
            "Epoch 423/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8142 - acc: 0.6744 - val_loss: 1.0822 - val_acc: 0.5111\n",
            "Epoch 424/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8132 - acc: 0.6744 - val_loss: 1.0824 - val_acc: 0.5111\n",
            "Epoch 425/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8131 - acc: 0.6744 - val_loss: 1.0826 - val_acc: 0.5111\n",
            "Epoch 426/500\n",
            "86/86 [==============================] - 0s 273us/step - loss: 0.8133 - acc: 0.6744 - val_loss: 1.0823 - val_acc: 0.5111\n",
            "Epoch 427/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8137 - acc: 0.6744 - val_loss: 1.0820 - val_acc: 0.5111\n",
            "Epoch 428/500\n",
            "86/86 [==============================] - 0s 278us/step - loss: 0.8131 - acc: 0.6744 - val_loss: 1.0818 - val_acc: 0.5111\n",
            "Epoch 429/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.8128 - acc: 0.6744 - val_loss: 1.0825 - val_acc: 0.5111\n",
            "Epoch 430/500\n",
            "86/86 [==============================] - 0s 256us/step - loss: 0.8127 - acc: 0.6744 - val_loss: 1.0829 - val_acc: 0.5111\n",
            "Epoch 431/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8125 - acc: 0.6860 - val_loss: 1.0831 - val_acc: 0.5111\n",
            "Epoch 432/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8122 - acc: 0.6744 - val_loss: 1.0831 - val_acc: 0.5111\n",
            "Epoch 433/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8118 - acc: 0.6860 - val_loss: 1.0830 - val_acc: 0.5111\n",
            "Epoch 434/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8121 - acc: 0.6860 - val_loss: 1.0830 - val_acc: 0.5111\n",
            "Epoch 435/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8113 - acc: 0.6860 - val_loss: 1.0835 - val_acc: 0.5111\n",
            "Epoch 436/500\n",
            "86/86 [==============================] - 0s 290us/step - loss: 0.8117 - acc: 0.6860 - val_loss: 1.0833 - val_acc: 0.5111\n",
            "Epoch 437/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8117 - acc: 0.6860 - val_loss: 1.0833 - val_acc: 0.5111\n",
            "Epoch 438/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8112 - acc: 0.6860 - val_loss: 1.0834 - val_acc: 0.5111\n",
            "Epoch 439/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8112 - acc: 0.6860 - val_loss: 1.0831 - val_acc: 0.5111\n",
            "Epoch 440/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8109 - acc: 0.6860 - val_loss: 1.0840 - val_acc: 0.5111\n",
            "Epoch 441/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8101 - acc: 0.6860 - val_loss: 1.0851 - val_acc: 0.5111\n",
            "Epoch 442/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8101 - acc: 0.6860 - val_loss: 1.0859 - val_acc: 0.5111\n",
            "Epoch 443/500\n",
            "86/86 [==============================] - 0s 252us/step - loss: 0.8101 - acc: 0.6860 - val_loss: 1.0864 - val_acc: 0.5111\n",
            "Epoch 444/500\n",
            "86/86 [==============================] - 0s 255us/step - loss: 0.8096 - acc: 0.6860 - val_loss: 1.0869 - val_acc: 0.5111\n",
            "Epoch 445/500\n",
            "86/86 [==============================] - 0s 282us/step - loss: 0.8096 - acc: 0.6860 - val_loss: 1.0869 - val_acc: 0.5111\n",
            "Epoch 446/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8092 - acc: 0.6860 - val_loss: 1.0869 - val_acc: 0.5111\n",
            "Epoch 447/500\n",
            "86/86 [==============================] - 0s 292us/step - loss: 0.8094 - acc: 0.6860 - val_loss: 1.0865 - val_acc: 0.5111\n",
            "Epoch 448/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8094 - acc: 0.6860 - val_loss: 1.0862 - val_acc: 0.5111\n",
            "Epoch 449/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8089 - acc: 0.6860 - val_loss: 1.0862 - val_acc: 0.5111\n",
            "Epoch 450/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8089 - acc: 0.6860 - val_loss: 1.0863 - val_acc: 0.5111\n",
            "Epoch 451/500\n",
            "86/86 [==============================] - 0s 254us/step - loss: 0.8085 - acc: 0.6860 - val_loss: 1.0861 - val_acc: 0.5111\n",
            "Epoch 452/500\n",
            "86/86 [==============================] - 0s 249us/step - loss: 0.8086 - acc: 0.6860 - val_loss: 1.0863 - val_acc: 0.5111\n",
            "Epoch 453/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8082 - acc: 0.6860 - val_loss: 1.0861 - val_acc: 0.5111\n",
            "Epoch 454/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8076 - acc: 0.6860 - val_loss: 1.0859 - val_acc: 0.5111\n",
            "Epoch 455/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8075 - acc: 0.6860 - val_loss: 1.0858 - val_acc: 0.5111\n",
            "Epoch 456/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.8074 - acc: 0.6860 - val_loss: 1.0859 - val_acc: 0.5111\n",
            "Epoch 457/500\n",
            "86/86 [==============================] - 0s 261us/step - loss: 0.8079 - acc: 0.6860 - val_loss: 1.0860 - val_acc: 0.5111\n",
            "Epoch 458/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8069 - acc: 0.6860 - val_loss: 1.0854 - val_acc: 0.5111\n",
            "Epoch 459/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8065 - acc: 0.6860 - val_loss: 1.0850 - val_acc: 0.5111\n",
            "Epoch 460/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8066 - acc: 0.6860 - val_loss: 1.0854 - val_acc: 0.5111\n",
            "Epoch 461/500\n",
            "86/86 [==============================] - 0s 269us/step - loss: 0.8058 - acc: 0.6860 - val_loss: 1.0855 - val_acc: 0.5111\n",
            "Epoch 462/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8060 - acc: 0.6860 - val_loss: 1.0847 - val_acc: 0.5111\n",
            "Epoch 463/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8061 - acc: 0.6860 - val_loss: 1.0840 - val_acc: 0.5111\n",
            "Epoch 464/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8055 - acc: 0.6860 - val_loss: 1.0839 - val_acc: 0.5111\n",
            "Epoch 465/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8055 - acc: 0.6860 - val_loss: 1.0837 - val_acc: 0.5111\n",
            "Epoch 466/500\n",
            "86/86 [==============================] - 0s 264us/step - loss: 0.8057 - acc: 0.6860 - val_loss: 1.0840 - val_acc: 0.5111\n",
            "Epoch 467/500\n",
            "86/86 [==============================] - 0s 254us/step - loss: 0.8046 - acc: 0.6860 - val_loss: 1.0847 - val_acc: 0.5111\n",
            "Epoch 468/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8039 - acc: 0.6860 - val_loss: 1.0852 - val_acc: 0.5111\n",
            "Epoch 469/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.8044 - acc: 0.6860 - val_loss: 1.0846 - val_acc: 0.5111\n",
            "Epoch 470/500\n",
            "86/86 [==============================] - 0s 280us/step - loss: 0.8045 - acc: 0.6860 - val_loss: 1.0843 - val_acc: 0.5111\n",
            "Epoch 471/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8039 - acc: 0.6860 - val_loss: 1.0846 - val_acc: 0.5111\n",
            "Epoch 472/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8036 - acc: 0.6860 - val_loss: 1.0852 - val_acc: 0.5111\n",
            "Epoch 473/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8041 - acc: 0.6860 - val_loss: 1.0862 - val_acc: 0.5111\n",
            "Epoch 474/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8031 - acc: 0.6860 - val_loss: 1.0861 - val_acc: 0.5111\n",
            "Epoch 475/500\n",
            "86/86 [==============================] - 0s 282us/step - loss: 0.8032 - acc: 0.6860 - val_loss: 1.0867 - val_acc: 0.5111\n",
            "Epoch 476/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8025 - acc: 0.6860 - val_loss: 1.0873 - val_acc: 0.5111\n",
            "Epoch 477/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8032 - acc: 0.6860 - val_loss: 1.0884 - val_acc: 0.5111\n",
            "Epoch 478/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8027 - acc: 0.6860 - val_loss: 1.0887 - val_acc: 0.5111\n",
            "Epoch 479/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8024 - acc: 0.6860 - val_loss: 1.0886 - val_acc: 0.5111\n",
            "Epoch 480/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8022 - acc: 0.6860 - val_loss: 1.0881 - val_acc: 0.5111\n",
            "Epoch 481/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8017 - acc: 0.6860 - val_loss: 1.0885 - val_acc: 0.5111\n",
            "Epoch 482/500\n",
            "86/86 [==============================] - 0s 292us/step - loss: 0.8016 - acc: 0.6860 - val_loss: 1.0883 - val_acc: 0.5111\n",
            "Epoch 483/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8017 - acc: 0.6860 - val_loss: 1.0879 - val_acc: 0.5111\n",
            "Epoch 484/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.8016 - acc: 0.6860 - val_loss: 1.0880 - val_acc: 0.5111\n",
            "Epoch 485/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.8009 - acc: 0.6860 - val_loss: 1.0880 - val_acc: 0.5111\n",
            "Epoch 486/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8015 - acc: 0.6860 - val_loss: 1.0884 - val_acc: 0.5111\n",
            "Epoch 487/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8012 - acc: 0.6860 - val_loss: 1.0888 - val_acc: 0.5111\n",
            "Epoch 488/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8002 - acc: 0.6860 - val_loss: 1.0885 - val_acc: 0.5111\n",
            "Epoch 489/500\n",
            "86/86 [==============================] - 0s 302us/step - loss: 0.8001 - acc: 0.6860 - val_loss: 1.0893 - val_acc: 0.5111\n",
            "Epoch 490/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8000 - acc: 0.6860 - val_loss: 1.0899 - val_acc: 0.5111\n",
            "Epoch 491/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.7997 - acc: 0.6860 - val_loss: 1.0905 - val_acc: 0.5111\n",
            "Epoch 492/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.7994 - acc: 0.6860 - val_loss: 1.0912 - val_acc: 0.5111\n",
            "Epoch 493/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.7990 - acc: 0.6860 - val_loss: 1.0912 - val_acc: 0.5111\n",
            "Epoch 494/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.7993 - acc: 0.6860 - val_loss: 1.0914 - val_acc: 0.5111\n",
            "Epoch 495/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.7993 - acc: 0.6860 - val_loss: 1.0918 - val_acc: 0.5111\n",
            "Epoch 496/500\n",
            "86/86 [==============================] - 0s 275us/step - loss: 0.7986 - acc: 0.6860 - val_loss: 1.0910 - val_acc: 0.5111\n",
            "Epoch 497/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.7984 - acc: 0.6860 - val_loss: 1.0918 - val_acc: 0.5111\n",
            "Epoch 498/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.7979 - acc: 0.6860 - val_loss: 1.0918 - val_acc: 0.5111\n",
            "Epoch 499/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.7982 - acc: 0.6860 - val_loss: 1.0916 - val_acc: 0.5111\n",
            "Epoch 500/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.7975 - acc: 0.6860 - val_loss: 1.0920 - val_acc: 0.5111\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/500\n",
            "88/88 [==============================] - 0s 4ms/step - loss: 2.1066 - acc: 0.3182 - val_loss: 1.8953 - val_acc: 0.2558\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 2.0388 - acc: 0.3182 - val_loss: 1.8243 - val_acc: 0.3023\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.9764 - acc: 0.3295 - val_loss: 1.7591 - val_acc: 0.3023\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.9177 - acc: 0.3409 - val_loss: 1.6989 - val_acc: 0.2558\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.8607 - acc: 0.3409 - val_loss: 1.6474 - val_acc: 0.2558\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.8119 - acc: 0.3636 - val_loss: 1.6029 - val_acc: 0.2791\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.7638 - acc: 0.3636 - val_loss: 1.5625 - val_acc: 0.2558\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.7224 - acc: 0.3864 - val_loss: 1.5277 - val_acc: 0.2791\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.6833 - acc: 0.3864 - val_loss: 1.4974 - val_acc: 0.2791\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.6478 - acc: 0.3977 - val_loss: 1.4714 - val_acc: 0.2791\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.6117 - acc: 0.3977 - val_loss: 1.4475 - val_acc: 0.2791\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.5797 - acc: 0.4091 - val_loss: 1.4254 - val_acc: 0.3023\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.5487 - acc: 0.4091 - val_loss: 1.4047 - val_acc: 0.3023\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.5205 - acc: 0.4091 - val_loss: 1.3862 - val_acc: 0.3023\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.4934 - acc: 0.4432 - val_loss: 1.3691 - val_acc: 0.3023\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.4720 - acc: 0.4318 - val_loss: 1.3536 - val_acc: 0.3023\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 0s 270us/step - loss: 1.4473 - acc: 0.4318 - val_loss: 1.3396 - val_acc: 0.3023\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.4233 - acc: 0.4318 - val_loss: 1.3270 - val_acc: 0.3256\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.4067 - acc: 0.4318 - val_loss: 1.3162 - val_acc: 0.3488\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 0s 286us/step - loss: 1.3834 - acc: 0.4318 - val_loss: 1.3072 - val_acc: 0.3488\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.3639 - acc: 0.4432 - val_loss: 1.2985 - val_acc: 0.3488\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 0s 284us/step - loss: 1.3459 - acc: 0.4432 - val_loss: 1.2904 - val_acc: 0.3256\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.3267 - acc: 0.4432 - val_loss: 1.2824 - val_acc: 0.3256\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.3124 - acc: 0.4432 - val_loss: 1.2762 - val_acc: 0.3488\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.2933 - acc: 0.4318 - val_loss: 1.2702 - val_acc: 0.3488\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.2720 - acc: 0.4318 - val_loss: 1.2652 - val_acc: 0.3256\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.2567 - acc: 0.4432 - val_loss: 1.2599 - val_acc: 0.3256\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 1.2394 - acc: 0.4659 - val_loss: 1.2514 - val_acc: 0.3256\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.2220 - acc: 0.4659 - val_loss: 1.2429 - val_acc: 0.3256\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 0s 285us/step - loss: 1.2073 - acc: 0.4659 - val_loss: 1.2359 - val_acc: 0.3488\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 1.1960 - acc: 0.4659 - val_loss: 1.2286 - val_acc: 0.3488\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.1833 - acc: 0.4659 - val_loss: 1.2228 - val_acc: 0.3488\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 0s 288us/step - loss: 1.1724 - acc: 0.4773 - val_loss: 1.2166 - val_acc: 0.3488\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 1.1617 - acc: 0.4886 - val_loss: 1.2104 - val_acc: 0.3488\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 1.1527 - acc: 0.4886 - val_loss: 1.2054 - val_acc: 0.4186\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.1456 - acc: 0.4659 - val_loss: 1.2001 - val_acc: 0.4186\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.1349 - acc: 0.4773 - val_loss: 1.1954 - val_acc: 0.4186\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 1.1273 - acc: 0.4773 - val_loss: 1.1896 - val_acc: 0.4186\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.1209 - acc: 0.4773 - val_loss: 1.1851 - val_acc: 0.4186\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 0s 328us/step - loss: 1.1147 - acc: 0.4773 - val_loss: 1.1802 - val_acc: 0.4186\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 1.1069 - acc: 0.4659 - val_loss: 1.1759 - val_acc: 0.4186\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 1.1008 - acc: 0.4659 - val_loss: 1.1722 - val_acc: 0.4186\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0954 - acc: 0.4659 - val_loss: 1.1694 - val_acc: 0.4186\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0896 - acc: 0.4659 - val_loss: 1.1663 - val_acc: 0.4186\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0829 - acc: 0.4545 - val_loss: 1.1632 - val_acc: 0.4186\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0766 - acc: 0.4659 - val_loss: 1.1597 - val_acc: 0.4186\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0719 - acc: 0.4659 - val_loss: 1.1565 - val_acc: 0.4186\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0669 - acc: 0.4659 - val_loss: 1.1534 - val_acc: 0.4186\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0642 - acc: 0.4659 - val_loss: 1.1510 - val_acc: 0.4186\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 1.0580 - acc: 0.4545 - val_loss: 1.1482 - val_acc: 0.4186\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0531 - acc: 0.4659 - val_loss: 1.1460 - val_acc: 0.4186\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0493 - acc: 0.4659 - val_loss: 1.1437 - val_acc: 0.4186\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0442 - acc: 0.4659 - val_loss: 1.1413 - val_acc: 0.4186\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0399 - acc: 0.4659 - val_loss: 1.1395 - val_acc: 0.4186\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 0s 296us/step - loss: 1.0355 - acc: 0.4659 - val_loss: 1.1371 - val_acc: 0.4186\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.0309 - acc: 0.4659 - val_loss: 1.1355 - val_acc: 0.4186\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 1.0267 - acc: 0.4659 - val_loss: 1.1339 - val_acc: 0.4186\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 1.0232 - acc: 0.4773 - val_loss: 1.1337 - val_acc: 0.4186\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0189 - acc: 0.4886 - val_loss: 1.1329 - val_acc: 0.4186\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 1.0150 - acc: 0.4886 - val_loss: 1.1310 - val_acc: 0.4419\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.0109 - acc: 0.4773 - val_loss: 1.1296 - val_acc: 0.4651\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0069 - acc: 0.4886 - val_loss: 1.1281 - val_acc: 0.4651\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0049 - acc: 0.4886 - val_loss: 1.1262 - val_acc: 0.4651\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9998 - acc: 0.5000 - val_loss: 1.1252 - val_acc: 0.4651\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.9981 - acc: 0.5114 - val_loss: 1.1246 - val_acc: 0.4651\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.9937 - acc: 0.5000 - val_loss: 1.1242 - val_acc: 0.4651\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.9908 - acc: 0.5114 - val_loss: 1.1231 - val_acc: 0.4651\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.9886 - acc: 0.5114 - val_loss: 1.1217 - val_acc: 0.4884\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.9865 - acc: 0.5114 - val_loss: 1.1210 - val_acc: 0.4884\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9830 - acc: 0.5455 - val_loss: 1.1197 - val_acc: 0.4884\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.9794 - acc: 0.5341 - val_loss: 1.1189 - val_acc: 0.4884\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.9755 - acc: 0.5455 - val_loss: 1.1181 - val_acc: 0.4884\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.9733 - acc: 0.5455 - val_loss: 1.1173 - val_acc: 0.4884\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9713 - acc: 0.5455 - val_loss: 1.1163 - val_acc: 0.5116\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.9682 - acc: 0.5455 - val_loss: 1.1154 - val_acc: 0.5116\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.9647 - acc: 0.5455 - val_loss: 1.1151 - val_acc: 0.4884\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.9629 - acc: 0.5455 - val_loss: 1.1151 - val_acc: 0.4884\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9601 - acc: 0.5568 - val_loss: 1.1147 - val_acc: 0.4884\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.9578 - acc: 0.5455 - val_loss: 1.1135 - val_acc: 0.4884\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.9548 - acc: 0.5455 - val_loss: 1.1131 - val_acc: 0.4884\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9527 - acc: 0.5568 - val_loss: 1.1127 - val_acc: 0.5116\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9510 - acc: 0.5568 - val_loss: 1.1117 - val_acc: 0.5349\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.9486 - acc: 0.5682 - val_loss: 1.1105 - val_acc: 0.5349\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9459 - acc: 0.5682 - val_loss: 1.1097 - val_acc: 0.5349\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9436 - acc: 0.5682 - val_loss: 1.1085 - val_acc: 0.5349\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.9421 - acc: 0.5568 - val_loss: 1.1072 - val_acc: 0.5349\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9402 - acc: 0.5795 - val_loss: 1.1066 - val_acc: 0.5349\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.9382 - acc: 0.5682 - val_loss: 1.1058 - val_acc: 0.5349\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.9362 - acc: 0.5682 - val_loss: 1.1049 - val_acc: 0.5349\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.9336 - acc: 0.5682 - val_loss: 1.1045 - val_acc: 0.5349\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.9322 - acc: 0.5682 - val_loss: 1.1040 - val_acc: 0.5349\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.9302 - acc: 0.5682 - val_loss: 1.1042 - val_acc: 0.5349\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.9290 - acc: 0.5682 - val_loss: 1.1036 - val_acc: 0.5349\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.9263 - acc: 0.5682 - val_loss: 1.1041 - val_acc: 0.5116\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.9252 - acc: 0.5682 - val_loss: 1.1037 - val_acc: 0.5116\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9235 - acc: 0.5682 - val_loss: 1.1035 - val_acc: 0.5116\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.9217 - acc: 0.5682 - val_loss: 1.1038 - val_acc: 0.5116\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.9206 - acc: 0.5682 - val_loss: 1.1031 - val_acc: 0.5116\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9191 - acc: 0.5682 - val_loss: 1.1031 - val_acc: 0.5116\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.9172 - acc: 0.5682 - val_loss: 1.1029 - val_acc: 0.5116\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9152 - acc: 0.5682 - val_loss: 1.1026 - val_acc: 0.5116\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.9147 - acc: 0.5682 - val_loss: 1.1021 - val_acc: 0.5116\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.9131 - acc: 0.5682 - val_loss: 1.1013 - val_acc: 0.5116\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.9115 - acc: 0.5682 - val_loss: 1.1006 - val_acc: 0.5349\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.9099 - acc: 0.5682 - val_loss: 1.1003 - val_acc: 0.5349\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9092 - acc: 0.5682 - val_loss: 1.1004 - val_acc: 0.5581\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.9087 - acc: 0.5682 - val_loss: 1.1005 - val_acc: 0.5581\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9067 - acc: 0.5682 - val_loss: 1.1016 - val_acc: 0.5349\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.9056 - acc: 0.5682 - val_loss: 1.1015 - val_acc: 0.5349\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.9048 - acc: 0.5682 - val_loss: 1.1017 - val_acc: 0.5349\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9030 - acc: 0.5795 - val_loss: 1.1020 - val_acc: 0.5349\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.9023 - acc: 0.5795 - val_loss: 1.1022 - val_acc: 0.5349\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.9004 - acc: 0.5909 - val_loss: 1.1030 - val_acc: 0.5349\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8993 - acc: 0.5909 - val_loss: 1.1025 - val_acc: 0.5581\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.9003 - acc: 0.5909 - val_loss: 1.1021 - val_acc: 0.5581\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.8975 - acc: 0.5909 - val_loss: 1.1027 - val_acc: 0.5581\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8966 - acc: 0.5909 - val_loss: 1.1034 - val_acc: 0.5581\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8952 - acc: 0.5909 - val_loss: 1.1029 - val_acc: 0.5581\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8937 - acc: 0.5909 - val_loss: 1.1033 - val_acc: 0.5581\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8936 - acc: 0.5909 - val_loss: 1.1030 - val_acc: 0.5581\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.8923 - acc: 0.5795 - val_loss: 1.1033 - val_acc: 0.5581\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8903 - acc: 0.5909 - val_loss: 1.1043 - val_acc: 0.5581\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8907 - acc: 0.5909 - val_loss: 1.1049 - val_acc: 0.5581\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8889 - acc: 0.5909 - val_loss: 1.1051 - val_acc: 0.5581\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8880 - acc: 0.5795 - val_loss: 1.1053 - val_acc: 0.5581\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8871 - acc: 0.5909 - val_loss: 1.1060 - val_acc: 0.5581\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8865 - acc: 0.5795 - val_loss: 1.1060 - val_acc: 0.5581\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8855 - acc: 0.5795 - val_loss: 1.1059 - val_acc: 0.5581\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8840 - acc: 0.5909 - val_loss: 1.1065 - val_acc: 0.5581\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8831 - acc: 0.5909 - val_loss: 1.1071 - val_acc: 0.5581\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8830 - acc: 0.5795 - val_loss: 1.1080 - val_acc: 0.5581\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 0s 300us/step - loss: 0.8820 - acc: 0.5909 - val_loss: 1.1086 - val_acc: 0.5581\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8816 - acc: 0.5909 - val_loss: 1.1091 - val_acc: 0.5581\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8815 - acc: 0.5909 - val_loss: 1.1089 - val_acc: 0.5581\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8791 - acc: 0.5909 - val_loss: 1.1087 - val_acc: 0.5581\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8795 - acc: 0.5909 - val_loss: 1.1094 - val_acc: 0.5581\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8788 - acc: 0.5909 - val_loss: 1.1101 - val_acc: 0.5581\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8775 - acc: 0.5909 - val_loss: 1.1097 - val_acc: 0.5581\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8774 - acc: 0.5795 - val_loss: 1.1104 - val_acc: 0.5581\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.8777 - acc: 0.5909 - val_loss: 1.1105 - val_acc: 0.5581\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 0s 349us/step - loss: 0.8763 - acc: 0.5795 - val_loss: 1.1114 - val_acc: 0.5581\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8758 - acc: 0.5909 - val_loss: 1.1121 - val_acc: 0.5581\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8747 - acc: 0.5909 - val_loss: 1.1119 - val_acc: 0.5581\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.8751 - acc: 0.5909 - val_loss: 1.1122 - val_acc: 0.5581\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8736 - acc: 0.5909 - val_loss: 1.1134 - val_acc: 0.5581\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8732 - acc: 0.5909 - val_loss: 1.1144 - val_acc: 0.5581\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8729 - acc: 0.5909 - val_loss: 1.1146 - val_acc: 0.5581\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8721 - acc: 0.5795 - val_loss: 1.1152 - val_acc: 0.5581\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.8713 - acc: 0.5795 - val_loss: 1.1153 - val_acc: 0.5581\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8715 - acc: 0.5795 - val_loss: 1.1155 - val_acc: 0.5349\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8709 - acc: 0.5795 - val_loss: 1.1151 - val_acc: 0.5349\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 0s 301us/step - loss: 0.8699 - acc: 0.5909 - val_loss: 1.1148 - val_acc: 0.5581\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8702 - acc: 0.5909 - val_loss: 1.1148 - val_acc: 0.5581\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8698 - acc: 0.5795 - val_loss: 1.1155 - val_acc: 0.5581\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8688 - acc: 0.5909 - val_loss: 1.1156 - val_acc: 0.5581\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8681 - acc: 0.5795 - val_loss: 1.1158 - val_acc: 0.5349\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8678 - acc: 0.5795 - val_loss: 1.1167 - val_acc: 0.5349\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8678 - acc: 0.5909 - val_loss: 1.1177 - val_acc: 0.5349\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8676 - acc: 0.5909 - val_loss: 1.1178 - val_acc: 0.5349\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8667 - acc: 0.5909 - val_loss: 1.1179 - val_acc: 0.5349\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8660 - acc: 0.5795 - val_loss: 1.1180 - val_acc: 0.5349\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 0s 356us/step - loss: 0.8660 - acc: 0.5795 - val_loss: 1.1184 - val_acc: 0.5349\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8652 - acc: 0.5795 - val_loss: 1.1189 - val_acc: 0.5349\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8645 - acc: 0.5795 - val_loss: 1.1203 - val_acc: 0.5349\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8647 - acc: 0.5909 - val_loss: 1.1212 - val_acc: 0.5349\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8638 - acc: 0.5909 - val_loss: 1.1214 - val_acc: 0.5349\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8626 - acc: 0.5795 - val_loss: 1.1211 - val_acc: 0.5349\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8636 - acc: 0.5795 - val_loss: 1.1211 - val_acc: 0.5349\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8629 - acc: 0.5795 - val_loss: 1.1221 - val_acc: 0.5349\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8619 - acc: 0.5909 - val_loss: 1.1225 - val_acc: 0.5349\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8609 - acc: 0.5909 - val_loss: 1.1233 - val_acc: 0.5349\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8606 - acc: 0.5909 - val_loss: 1.1239 - val_acc: 0.5349\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8606 - acc: 0.5909 - val_loss: 1.1239 - val_acc: 0.5349\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8604 - acc: 0.5795 - val_loss: 1.1235 - val_acc: 0.5581\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8597 - acc: 0.5795 - val_loss: 1.1243 - val_acc: 0.5581\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8588 - acc: 0.5795 - val_loss: 1.1243 - val_acc: 0.5581\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8582 - acc: 0.5795 - val_loss: 1.1250 - val_acc: 0.5581\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8589 - acc: 0.5909 - val_loss: 1.1251 - val_acc: 0.5581\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8575 - acc: 0.5909 - val_loss: 1.1251 - val_acc: 0.5581\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8566 - acc: 0.5909 - val_loss: 1.1246 - val_acc: 0.5581\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8562 - acc: 0.5909 - val_loss: 1.1250 - val_acc: 0.5581\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 0s 292us/step - loss: 0.8569 - acc: 0.5909 - val_loss: 1.1249 - val_acc: 0.5581\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8553 - acc: 0.5909 - val_loss: 1.1256 - val_acc: 0.5581\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8545 - acc: 0.5909 - val_loss: 1.1260 - val_acc: 0.5581\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8541 - acc: 0.5909 - val_loss: 1.1262 - val_acc: 0.5581\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8535 - acc: 0.5909 - val_loss: 1.1268 - val_acc: 0.5581\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8532 - acc: 0.5909 - val_loss: 1.1266 - val_acc: 0.5581\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8534 - acc: 0.5909 - val_loss: 1.1270 - val_acc: 0.5581\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8523 - acc: 0.5909 - val_loss: 1.1271 - val_acc: 0.5581\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8521 - acc: 0.5909 - val_loss: 1.1269 - val_acc: 0.5581\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8507 - acc: 0.5909 - val_loss: 1.1273 - val_acc: 0.5581\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.8507 - acc: 0.5909 - val_loss: 1.1279 - val_acc: 0.5581\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8502 - acc: 0.5909 - val_loss: 1.1280 - val_acc: 0.5581\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.8508 - acc: 0.5909 - val_loss: 1.1290 - val_acc: 0.5581\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8497 - acc: 0.5909 - val_loss: 1.1292 - val_acc: 0.5581\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8492 - acc: 0.5909 - val_loss: 1.1297 - val_acc: 0.5581\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.8492 - acc: 0.5909 - val_loss: 1.1295 - val_acc: 0.5581\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8493 - acc: 0.5909 - val_loss: 1.1311 - val_acc: 0.5581\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8477 - acc: 0.6023 - val_loss: 1.1318 - val_acc: 0.5349\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8486 - acc: 0.5909 - val_loss: 1.1321 - val_acc: 0.5349\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8475 - acc: 0.6364 - val_loss: 1.1317 - val_acc: 0.5349\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8470 - acc: 0.6023 - val_loss: 1.1318 - val_acc: 0.5349\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8468 - acc: 0.6250 - val_loss: 1.1323 - val_acc: 0.5349\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8462 - acc: 0.6250 - val_loss: 1.1319 - val_acc: 0.5349\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8454 - acc: 0.6364 - val_loss: 1.1323 - val_acc: 0.5349\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8461 - acc: 0.6250 - val_loss: 1.1329 - val_acc: 0.5349\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8448 - acc: 0.6477 - val_loss: 1.1326 - val_acc: 0.5349\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8441 - acc: 0.6250 - val_loss: 1.1331 - val_acc: 0.5349\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.8445 - acc: 0.6477 - val_loss: 1.1337 - val_acc: 0.5349\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8429 - acc: 0.6477 - val_loss: 1.1340 - val_acc: 0.5349\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8426 - acc: 0.6364 - val_loss: 1.1343 - val_acc: 0.5349\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8433 - acc: 0.6364 - val_loss: 1.1347 - val_acc: 0.5349\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8427 - acc: 0.6136 - val_loss: 1.1355 - val_acc: 0.5349\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8423 - acc: 0.6250 - val_loss: 1.1357 - val_acc: 0.5349\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 0s 310us/step - loss: 0.8409 - acc: 0.6591 - val_loss: 1.1357 - val_acc: 0.5349\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8416 - acc: 0.6364 - val_loss: 1.1357 - val_acc: 0.5349\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.8407 - acc: 0.6591 - val_loss: 1.1364 - val_acc: 0.5349\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8404 - acc: 0.6364 - val_loss: 1.1364 - val_acc: 0.5349\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8401 - acc: 0.6591 - val_loss: 1.1373 - val_acc: 0.5349\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8396 - acc: 0.6591 - val_loss: 1.1380 - val_acc: 0.5349\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8405 - acc: 0.6364 - val_loss: 1.1388 - val_acc: 0.5349\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8395 - acc: 0.6591 - val_loss: 1.1394 - val_acc: 0.5116\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8384 - acc: 0.6591 - val_loss: 1.1390 - val_acc: 0.5116\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8381 - acc: 0.6477 - val_loss: 1.1391 - val_acc: 0.5116\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8390 - acc: 0.6591 - val_loss: 1.1393 - val_acc: 0.5116\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8376 - acc: 0.6591 - val_loss: 1.1396 - val_acc: 0.5116\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8372 - acc: 0.6477 - val_loss: 1.1406 - val_acc: 0.5116\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8373 - acc: 0.6477 - val_loss: 1.1414 - val_acc: 0.5116\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8369 - acc: 0.6477 - val_loss: 1.1422 - val_acc: 0.5116\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8357 - acc: 0.6591 - val_loss: 1.1435 - val_acc: 0.5116\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8370 - acc: 0.6250 - val_loss: 1.1442 - val_acc: 0.5116\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8357 - acc: 0.6705 - val_loss: 1.1442 - val_acc: 0.5116\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8346 - acc: 0.6705 - val_loss: 1.1445 - val_acc: 0.5116\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8346 - acc: 0.6705 - val_loss: 1.1447 - val_acc: 0.5116\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.8339 - acc: 0.6591 - val_loss: 1.1454 - val_acc: 0.4884\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8329 - acc: 0.6705 - val_loss: 1.1457 - val_acc: 0.4884\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8334 - acc: 0.6705 - val_loss: 1.1461 - val_acc: 0.4884\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8334 - acc: 0.6705 - val_loss: 1.1466 - val_acc: 0.4884\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8323 - acc: 0.6705 - val_loss: 1.1469 - val_acc: 0.5116\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8324 - acc: 0.6705 - val_loss: 1.1474 - val_acc: 0.4884\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8314 - acc: 0.6705 - val_loss: 1.1483 - val_acc: 0.4884\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8317 - acc: 0.6591 - val_loss: 1.1485 - val_acc: 0.4884\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8309 - acc: 0.6591 - val_loss: 1.1483 - val_acc: 0.4884\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 0s 297us/step - loss: 0.8311 - acc: 0.6705 - val_loss: 1.1487 - val_acc: 0.4884\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8307 - acc: 0.6705 - val_loss: 1.1494 - val_acc: 0.4884\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8305 - acc: 0.6591 - val_loss: 1.1501 - val_acc: 0.4884\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8307 - acc: 0.6591 - val_loss: 1.1507 - val_acc: 0.4884\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8291 - acc: 0.6591 - val_loss: 1.1515 - val_acc: 0.4884\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8294 - acc: 0.6705 - val_loss: 1.1522 - val_acc: 0.4651\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8293 - acc: 0.6591 - val_loss: 1.1523 - val_acc: 0.4651\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8288 - acc: 0.6705 - val_loss: 1.1524 - val_acc: 0.4651\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 0.8281 - acc: 0.6705 - val_loss: 1.1532 - val_acc: 0.4651\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8279 - acc: 0.6705 - val_loss: 1.1539 - val_acc: 0.4651\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8270 - acc: 0.6591 - val_loss: 1.1540 - val_acc: 0.4651\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8269 - acc: 0.6705 - val_loss: 1.1551 - val_acc: 0.4651\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8274 - acc: 0.6591 - val_loss: 1.1559 - val_acc: 0.4651\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8262 - acc: 0.6591 - val_loss: 1.1565 - val_acc: 0.4651\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8256 - acc: 0.6591 - val_loss: 1.1566 - val_acc: 0.4651\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8260 - acc: 0.6591 - val_loss: 1.1567 - val_acc: 0.4651\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8253 - acc: 0.6591 - val_loss: 1.1572 - val_acc: 0.4651\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8253 - acc: 0.6591 - val_loss: 1.1571 - val_acc: 0.4651\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8252 - acc: 0.6705 - val_loss: 1.1575 - val_acc: 0.4651\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8248 - acc: 0.6591 - val_loss: 1.1574 - val_acc: 0.4651\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 0s 292us/step - loss: 0.8244 - acc: 0.6705 - val_loss: 1.1585 - val_acc: 0.4651\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8241 - acc: 0.6591 - val_loss: 1.1590 - val_acc: 0.4651\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8233 - acc: 0.6705 - val_loss: 1.1592 - val_acc: 0.4651\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8234 - acc: 0.6705 - val_loss: 1.1608 - val_acc: 0.4651\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8226 - acc: 0.6591 - val_loss: 1.1616 - val_acc: 0.4651\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8225 - acc: 0.6705 - val_loss: 1.1628 - val_acc: 0.4651\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8225 - acc: 0.6591 - val_loss: 1.1634 - val_acc: 0.4651\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8217 - acc: 0.6705 - val_loss: 1.1639 - val_acc: 0.4651\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8220 - acc: 0.6705 - val_loss: 1.1652 - val_acc: 0.4651\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8220 - acc: 0.6591 - val_loss: 1.1661 - val_acc: 0.4651\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8217 - acc: 0.6705 - val_loss: 1.1668 - val_acc: 0.4651\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 0s 299us/step - loss: 0.8206 - acc: 0.6591 - val_loss: 1.1673 - val_acc: 0.4651\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8216 - acc: 0.6705 - val_loss: 1.1676 - val_acc: 0.4651\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8200 - acc: 0.6705 - val_loss: 1.1687 - val_acc: 0.4651\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8204 - acc: 0.6591 - val_loss: 1.1694 - val_acc: 0.4651\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8207 - acc: 0.6705 - val_loss: 1.1699 - val_acc: 0.4651\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8200 - acc: 0.6591 - val_loss: 1.1711 - val_acc: 0.4651\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8195 - acc: 0.6591 - val_loss: 1.1712 - val_acc: 0.4651\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8189 - acc: 0.6591 - val_loss: 1.1718 - val_acc: 0.4651\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8190 - acc: 0.6477 - val_loss: 1.1727 - val_acc: 0.4651\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8177 - acc: 0.6591 - val_loss: 1.1728 - val_acc: 0.4651\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8178 - acc: 0.6591 - val_loss: 1.1732 - val_acc: 0.4651\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8175 - acc: 0.6591 - val_loss: 1.1737 - val_acc: 0.4651\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8171 - acc: 0.6705 - val_loss: 1.1737 - val_acc: 0.4651\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8183 - acc: 0.6591 - val_loss: 1.1748 - val_acc: 0.4651\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 0s 303us/step - loss: 0.8173 - acc: 0.6705 - val_loss: 1.1755 - val_acc: 0.4651\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.8163 - acc: 0.6705 - val_loss: 1.1759 - val_acc: 0.4651\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.8161 - acc: 0.6591 - val_loss: 1.1764 - val_acc: 0.4651\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8169 - acc: 0.6591 - val_loss: 1.1768 - val_acc: 0.4651\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8160 - acc: 0.6705 - val_loss: 1.1770 - val_acc: 0.4651\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8161 - acc: 0.6818 - val_loss: 1.1777 - val_acc: 0.4651\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8160 - acc: 0.6705 - val_loss: 1.1781 - val_acc: 0.4651\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8149 - acc: 0.6818 - val_loss: 1.1793 - val_acc: 0.4651\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8158 - acc: 0.6818 - val_loss: 1.1796 - val_acc: 0.4651\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8154 - acc: 0.6705 - val_loss: 1.1802 - val_acc: 0.4651\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.8148 - acc: 0.6705 - val_loss: 1.1807 - val_acc: 0.4651\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8147 - acc: 0.6477 - val_loss: 1.1815 - val_acc: 0.4651\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8147 - acc: 0.6705 - val_loss: 1.1814 - val_acc: 0.4651\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8140 - acc: 0.6705 - val_loss: 1.1820 - val_acc: 0.4651\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8140 - acc: 0.6477 - val_loss: 1.1824 - val_acc: 0.4651\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8135 - acc: 0.6705 - val_loss: 1.1830 - val_acc: 0.4651\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 0s 298us/step - loss: 0.8133 - acc: 0.6818 - val_loss: 1.1833 - val_acc: 0.4651\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8130 - acc: 0.6705 - val_loss: 1.1841 - val_acc: 0.4651\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8122 - acc: 0.6591 - val_loss: 1.1848 - val_acc: 0.4651\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8123 - acc: 0.6477 - val_loss: 1.1849 - val_acc: 0.4651\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8119 - acc: 0.6705 - val_loss: 1.1853 - val_acc: 0.4651\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8124 - acc: 0.6818 - val_loss: 1.1863 - val_acc: 0.4651\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.8120 - acc: 0.6591 - val_loss: 1.1872 - val_acc: 0.4651\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8114 - acc: 0.6705 - val_loss: 1.1873 - val_acc: 0.4651\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8113 - acc: 0.6705 - val_loss: 1.1873 - val_acc: 0.4651\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8107 - acc: 0.6705 - val_loss: 1.1876 - val_acc: 0.4651\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8104 - acc: 0.6705 - val_loss: 1.1882 - val_acc: 0.4651\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8105 - acc: 0.6705 - val_loss: 1.1886 - val_acc: 0.4651\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8102 - acc: 0.6705 - val_loss: 1.1889 - val_acc: 0.4651\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8100 - acc: 0.6705 - val_loss: 1.1891 - val_acc: 0.4651\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8098 - acc: 0.6705 - val_loss: 1.1898 - val_acc: 0.4651\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8096 - acc: 0.6705 - val_loss: 1.1904 - val_acc: 0.4651\n",
            "Epoch 321/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8098 - acc: 0.6705 - val_loss: 1.1919 - val_acc: 0.4651\n",
            "Epoch 322/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.8103 - acc: 0.6818 - val_loss: 1.1922 - val_acc: 0.4651\n",
            "Epoch 323/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8090 - acc: 0.6705 - val_loss: 1.1932 - val_acc: 0.4884\n",
            "Epoch 324/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8090 - acc: 0.6591 - val_loss: 1.1931 - val_acc: 0.4884\n",
            "Epoch 325/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8082 - acc: 0.6591 - val_loss: 1.1936 - val_acc: 0.4884\n",
            "Epoch 326/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8086 - acc: 0.6705 - val_loss: 1.1944 - val_acc: 0.4884\n",
            "Epoch 327/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8081 - acc: 0.6591 - val_loss: 1.1948 - val_acc: 0.4884\n",
            "Epoch 328/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8072 - acc: 0.6705 - val_loss: 1.1942 - val_acc: 0.4651\n",
            "Epoch 329/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8076 - acc: 0.6705 - val_loss: 1.1947 - val_acc: 0.4651\n",
            "Epoch 330/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8072 - acc: 0.6705 - val_loss: 1.1956 - val_acc: 0.4884\n",
            "Epoch 331/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8070 - acc: 0.6705 - val_loss: 1.1957 - val_acc: 0.4884\n",
            "Epoch 332/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8068 - acc: 0.6591 - val_loss: 1.1956 - val_acc: 0.4884\n",
            "Epoch 333/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8071 - acc: 0.6591 - val_loss: 1.1965 - val_acc: 0.4884\n",
            "Epoch 334/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8064 - acc: 0.6477 - val_loss: 1.1965 - val_acc: 0.4651\n",
            "Epoch 335/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8063 - acc: 0.6591 - val_loss: 1.1973 - val_acc: 0.4884\n",
            "Epoch 336/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8062 - acc: 0.6591 - val_loss: 1.1978 - val_acc: 0.4884\n",
            "Epoch 337/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8063 - acc: 0.6705 - val_loss: 1.1986 - val_acc: 0.4884\n",
            "Epoch 338/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8050 - acc: 0.6591 - val_loss: 1.1989 - val_acc: 0.4884\n",
            "Epoch 339/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8054 - acc: 0.6591 - val_loss: 1.1994 - val_acc: 0.4884\n",
            "Epoch 340/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8045 - acc: 0.6705 - val_loss: 1.2002 - val_acc: 0.4884\n",
            "Epoch 341/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8049 - acc: 0.6705 - val_loss: 1.2001 - val_acc: 0.4884\n",
            "Epoch 342/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8045 - acc: 0.6591 - val_loss: 1.2002 - val_acc: 0.4884\n",
            "Epoch 343/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8043 - acc: 0.6591 - val_loss: 1.2013 - val_acc: 0.4884\n",
            "Epoch 344/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8040 - acc: 0.6477 - val_loss: 1.2018 - val_acc: 0.4884\n",
            "Epoch 345/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8034 - acc: 0.6591 - val_loss: 1.2021 - val_acc: 0.4884\n",
            "Epoch 346/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8037 - acc: 0.6591 - val_loss: 1.2026 - val_acc: 0.4884\n",
            "Epoch 347/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8030 - acc: 0.6591 - val_loss: 1.2037 - val_acc: 0.4884\n",
            "Epoch 348/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8029 - acc: 0.6591 - val_loss: 1.2044 - val_acc: 0.4884\n",
            "Epoch 349/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8027 - acc: 0.6591 - val_loss: 1.2046 - val_acc: 0.4884\n",
            "Epoch 350/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.8025 - acc: 0.6477 - val_loss: 1.2055 - val_acc: 0.4884\n",
            "Epoch 351/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8022 - acc: 0.6591 - val_loss: 1.2061 - val_acc: 0.4884\n",
            "Epoch 352/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8023 - acc: 0.6477 - val_loss: 1.2074 - val_acc: 0.4884\n",
            "Epoch 353/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8018 - acc: 0.6591 - val_loss: 1.2077 - val_acc: 0.4884\n",
            "Epoch 354/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8021 - acc: 0.6591 - val_loss: 1.2077 - val_acc: 0.4884\n",
            "Epoch 355/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8027 - acc: 0.6477 - val_loss: 1.2080 - val_acc: 0.4884\n",
            "Epoch 356/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8017 - acc: 0.6477 - val_loss: 1.2083 - val_acc: 0.4884\n",
            "Epoch 357/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8010 - acc: 0.6477 - val_loss: 1.2088 - val_acc: 0.4884\n",
            "Epoch 358/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8012 - acc: 0.6591 - val_loss: 1.2089 - val_acc: 0.4884\n",
            "Epoch 359/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8005 - acc: 0.6591 - val_loss: 1.2089 - val_acc: 0.4884\n",
            "Epoch 360/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.8004 - acc: 0.6591 - val_loss: 1.2086 - val_acc: 0.4884\n",
            "Epoch 361/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8002 - acc: 0.6477 - val_loss: 1.2090 - val_acc: 0.4884\n",
            "Epoch 362/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8002 - acc: 0.6477 - val_loss: 1.2100 - val_acc: 0.4884\n",
            "Epoch 363/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.7995 - acc: 0.6591 - val_loss: 1.2105 - val_acc: 0.4884\n",
            "Epoch 364/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7997 - acc: 0.6591 - val_loss: 1.2106 - val_acc: 0.4884\n",
            "Epoch 365/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.7991 - acc: 0.6591 - val_loss: 1.2112 - val_acc: 0.4884\n",
            "Epoch 366/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7986 - acc: 0.6591 - val_loss: 1.2111 - val_acc: 0.4884\n",
            "Epoch 367/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7987 - acc: 0.6591 - val_loss: 1.2110 - val_acc: 0.4884\n",
            "Epoch 368/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.7983 - acc: 0.6591 - val_loss: 1.2111 - val_acc: 0.4884\n",
            "Epoch 369/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7981 - acc: 0.6591 - val_loss: 1.2112 - val_acc: 0.4884\n",
            "Epoch 370/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.7980 - acc: 0.6591 - val_loss: 1.2115 - val_acc: 0.4884\n",
            "Epoch 371/500\n",
            "88/88 [==============================] - 0s 283us/step - loss: 0.7976 - acc: 0.6591 - val_loss: 1.2118 - val_acc: 0.4884\n",
            "Epoch 372/500\n",
            "88/88 [==============================] - 0s 317us/step - loss: 0.7981 - acc: 0.6591 - val_loss: 1.2113 - val_acc: 0.4884\n",
            "Epoch 373/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7968 - acc: 0.6591 - val_loss: 1.2120 - val_acc: 0.4884\n",
            "Epoch 374/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7968 - acc: 0.6591 - val_loss: 1.2126 - val_acc: 0.4884\n",
            "Epoch 375/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7968 - acc: 0.6591 - val_loss: 1.2124 - val_acc: 0.4884\n",
            "Epoch 376/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7965 - acc: 0.6705 - val_loss: 1.2122 - val_acc: 0.4884\n",
            "Epoch 377/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7967 - acc: 0.6591 - val_loss: 1.2119 - val_acc: 0.4884\n",
            "Epoch 378/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7964 - acc: 0.6705 - val_loss: 1.2122 - val_acc: 0.5116\n",
            "Epoch 379/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.7962 - acc: 0.6591 - val_loss: 1.2130 - val_acc: 0.5116\n",
            "Epoch 380/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.7958 - acc: 0.6591 - val_loss: 1.2130 - val_acc: 0.5116\n",
            "Epoch 381/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.7958 - acc: 0.6591 - val_loss: 1.2129 - val_acc: 0.5116\n",
            "Epoch 382/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7958 - acc: 0.6591 - val_loss: 1.2140 - val_acc: 0.4884\n",
            "Epoch 383/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7949 - acc: 0.6705 - val_loss: 1.2142 - val_acc: 0.4651\n",
            "Epoch 384/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7955 - acc: 0.6477 - val_loss: 1.2143 - val_acc: 0.4884\n",
            "Epoch 385/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7954 - acc: 0.6591 - val_loss: 1.2150 - val_acc: 0.4884\n",
            "Epoch 386/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.7945 - acc: 0.6591 - val_loss: 1.2152 - val_acc: 0.4884\n",
            "Epoch 387/500\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.7947 - acc: 0.6705 - val_loss: 1.2161 - val_acc: 0.4651\n",
            "Epoch 388/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7938 - acc: 0.6705 - val_loss: 1.2156 - val_acc: 0.5116\n",
            "Epoch 389/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.7939 - acc: 0.6818 - val_loss: 1.2151 - val_acc: 0.5116\n",
            "Epoch 390/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7938 - acc: 0.6705 - val_loss: 1.2158 - val_acc: 0.5116\n",
            "Epoch 391/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7938 - acc: 0.6705 - val_loss: 1.2172 - val_acc: 0.4884\n",
            "Epoch 392/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.7930 - acc: 0.6818 - val_loss: 1.2189 - val_acc: 0.4651\n",
            "Epoch 393/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7932 - acc: 0.6705 - val_loss: 1.2191 - val_acc: 0.4884\n",
            "Epoch 394/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7936 - acc: 0.6705 - val_loss: 1.2188 - val_acc: 0.4884\n",
            "Epoch 395/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7934 - acc: 0.6705 - val_loss: 1.2186 - val_acc: 0.4884\n",
            "Epoch 396/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7927 - acc: 0.6705 - val_loss: 1.2182 - val_acc: 0.4884\n",
            "Epoch 397/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7925 - acc: 0.6705 - val_loss: 1.2185 - val_acc: 0.4884\n",
            "Epoch 398/500\n",
            "88/88 [==============================] - 0s 283us/step - loss: 0.7929 - acc: 0.6705 - val_loss: 1.2194 - val_acc: 0.4884\n",
            "Epoch 399/500\n",
            "88/88 [==============================] - 0s 283us/step - loss: 0.7928 - acc: 0.6705 - val_loss: 1.2198 - val_acc: 0.4884\n",
            "Epoch 400/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.7921 - acc: 0.6705 - val_loss: 1.2205 - val_acc: 0.4884\n",
            "Epoch 401/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7919 - acc: 0.6705 - val_loss: 1.2199 - val_acc: 0.4884\n",
            "Epoch 402/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7917 - acc: 0.6818 - val_loss: 1.2201 - val_acc: 0.4884\n",
            "Epoch 403/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7912 - acc: 0.6818 - val_loss: 1.2210 - val_acc: 0.4884\n",
            "Epoch 404/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.7912 - acc: 0.6705 - val_loss: 1.2205 - val_acc: 0.4884\n",
            "Epoch 405/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.7909 - acc: 0.6818 - val_loss: 1.2205 - val_acc: 0.4884\n",
            "Epoch 406/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.7916 - acc: 0.6705 - val_loss: 1.2211 - val_acc: 0.4884\n",
            "Epoch 407/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7909 - acc: 0.6818 - val_loss: 1.2211 - val_acc: 0.4884\n",
            "Epoch 408/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.7907 - acc: 0.6705 - val_loss: 1.2220 - val_acc: 0.4884\n",
            "Epoch 409/500\n",
            "88/88 [==============================] - 0s 296us/step - loss: 0.7903 - acc: 0.6818 - val_loss: 1.2218 - val_acc: 0.4884\n",
            "Epoch 410/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7906 - acc: 0.6705 - val_loss: 1.2229 - val_acc: 0.4884\n",
            "Epoch 411/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7900 - acc: 0.6705 - val_loss: 1.2229 - val_acc: 0.4884\n",
            "Epoch 412/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7900 - acc: 0.6705 - val_loss: 1.2228 - val_acc: 0.4884\n",
            "Epoch 413/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7903 - acc: 0.6705 - val_loss: 1.2238 - val_acc: 0.4884\n",
            "Epoch 414/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.7893 - acc: 0.6705 - val_loss: 1.2242 - val_acc: 0.4884\n",
            "Epoch 415/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7894 - acc: 0.6705 - val_loss: 1.2238 - val_acc: 0.4884\n",
            "Epoch 416/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.7895 - acc: 0.6818 - val_loss: 1.2240 - val_acc: 0.4884\n",
            "Epoch 417/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7886 - acc: 0.6705 - val_loss: 1.2237 - val_acc: 0.4884\n",
            "Epoch 418/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.7890 - acc: 0.6818 - val_loss: 1.2242 - val_acc: 0.4884\n",
            "Epoch 419/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7887 - acc: 0.6705 - val_loss: 1.2246 - val_acc: 0.4884\n",
            "Epoch 420/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.7892 - acc: 0.6705 - val_loss: 1.2251 - val_acc: 0.4884\n",
            "Epoch 421/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.7879 - acc: 0.6705 - val_loss: 1.2252 - val_acc: 0.4884\n",
            "Epoch 422/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7887 - acc: 0.6705 - val_loss: 1.2251 - val_acc: 0.4884\n",
            "Epoch 423/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7874 - acc: 0.6705 - val_loss: 1.2250 - val_acc: 0.4884\n",
            "Epoch 424/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7886 - acc: 0.6705 - val_loss: 1.2256 - val_acc: 0.4884\n",
            "Epoch 425/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7873 - acc: 0.6705 - val_loss: 1.2248 - val_acc: 0.4884\n",
            "Epoch 426/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.7878 - acc: 0.6818 - val_loss: 1.2250 - val_acc: 0.4884\n",
            "Epoch 427/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.7873 - acc: 0.6705 - val_loss: 1.2254 - val_acc: 0.4884\n",
            "Epoch 428/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.7872 - acc: 0.6705 - val_loss: 1.2259 - val_acc: 0.4884\n",
            "Epoch 429/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.7870 - acc: 0.6818 - val_loss: 1.2269 - val_acc: 0.4884\n",
            "Epoch 430/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7871 - acc: 0.6818 - val_loss: 1.2279 - val_acc: 0.4884\n",
            "Epoch 431/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.7867 - acc: 0.6705 - val_loss: 1.2281 - val_acc: 0.4884\n",
            "Epoch 432/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7859 - acc: 0.6818 - val_loss: 1.2281 - val_acc: 0.4884\n",
            "Epoch 433/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7870 - acc: 0.6705 - val_loss: 1.2282 - val_acc: 0.4884\n",
            "Epoch 434/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7859 - acc: 0.6818 - val_loss: 1.2282 - val_acc: 0.4884\n",
            "Epoch 435/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7858 - acc: 0.6818 - val_loss: 1.2287 - val_acc: 0.4884\n",
            "Epoch 436/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7863 - acc: 0.6705 - val_loss: 1.2295 - val_acc: 0.4884\n",
            "Epoch 437/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7867 - acc: 0.6818 - val_loss: 1.2299 - val_acc: 0.4884\n",
            "Epoch 438/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.7855 - acc: 0.6705 - val_loss: 1.2295 - val_acc: 0.4884\n",
            "Epoch 439/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7856 - acc: 0.6705 - val_loss: 1.2299 - val_acc: 0.4884\n",
            "Epoch 440/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.7854 - acc: 0.6705 - val_loss: 1.2301 - val_acc: 0.4884\n",
            "Epoch 441/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7846 - acc: 0.6818 - val_loss: 1.2305 - val_acc: 0.4884\n",
            "Epoch 442/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7848 - acc: 0.6818 - val_loss: 1.2305 - val_acc: 0.4884\n",
            "Epoch 443/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.7847 - acc: 0.6932 - val_loss: 1.2308 - val_acc: 0.4884\n",
            "Epoch 444/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7846 - acc: 0.6818 - val_loss: 1.2316 - val_acc: 0.4884\n",
            "Epoch 445/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.7851 - acc: 0.6818 - val_loss: 1.2313 - val_acc: 0.4884\n",
            "Epoch 446/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7848 - acc: 0.6705 - val_loss: 1.2315 - val_acc: 0.4884\n",
            "Epoch 447/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.7847 - acc: 0.6932 - val_loss: 1.2327 - val_acc: 0.4884\n",
            "Epoch 448/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.7847 - acc: 0.6705 - val_loss: 1.2330 - val_acc: 0.4884\n",
            "Epoch 449/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.7838 - acc: 0.6705 - val_loss: 1.2330 - val_acc: 0.4884\n",
            "Epoch 450/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.7835 - acc: 0.6818 - val_loss: 1.2328 - val_acc: 0.4884\n",
            "Epoch 451/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.7831 - acc: 0.6705 - val_loss: 1.2332 - val_acc: 0.4884\n",
            "Epoch 452/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7838 - acc: 0.6932 - val_loss: 1.2333 - val_acc: 0.4884\n",
            "Epoch 453/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.7833 - acc: 0.6705 - val_loss: 1.2338 - val_acc: 0.4884\n",
            "Epoch 454/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7829 - acc: 0.6818 - val_loss: 1.2335 - val_acc: 0.4884\n",
            "Epoch 455/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7826 - acc: 0.6818 - val_loss: 1.2343 - val_acc: 0.4884\n",
            "Epoch 456/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7831 - acc: 0.6705 - val_loss: 1.2347 - val_acc: 0.4884\n",
            "Epoch 457/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.7836 - acc: 0.6932 - val_loss: 1.2344 - val_acc: 0.4884\n",
            "Epoch 458/500\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.7821 - acc: 0.6705 - val_loss: 1.2359 - val_acc: 0.4884\n",
            "Epoch 459/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.7826 - acc: 0.6705 - val_loss: 1.2358 - val_acc: 0.4884\n",
            "Epoch 460/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7823 - acc: 0.6818 - val_loss: 1.2356 - val_acc: 0.4884\n",
            "Epoch 461/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.7822 - acc: 0.6818 - val_loss: 1.2361 - val_acc: 0.4884\n",
            "Epoch 462/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7817 - acc: 0.6705 - val_loss: 1.2367 - val_acc: 0.4884\n",
            "Epoch 463/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7822 - acc: 0.6818 - val_loss: 1.2359 - val_acc: 0.4884\n",
            "Epoch 464/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7813 - acc: 0.6932 - val_loss: 1.2361 - val_acc: 0.4884\n",
            "Epoch 465/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.7819 - acc: 0.6818 - val_loss: 1.2372 - val_acc: 0.4884\n",
            "Epoch 466/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7816 - acc: 0.6818 - val_loss: 1.2368 - val_acc: 0.4884\n",
            "Epoch 467/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7819 - acc: 0.6705 - val_loss: 1.2369 - val_acc: 0.4884\n",
            "Epoch 468/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.7804 - acc: 0.6932 - val_loss: 1.2371 - val_acc: 0.4884\n",
            "Epoch 469/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7811 - acc: 0.6818 - val_loss: 1.2382 - val_acc: 0.4884\n",
            "Epoch 470/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7807 - acc: 0.6705 - val_loss: 1.2373 - val_acc: 0.4884\n",
            "Epoch 471/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.7811 - acc: 0.6932 - val_loss: 1.2376 - val_acc: 0.4884\n",
            "Epoch 472/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7808 - acc: 0.6932 - val_loss: 1.2378 - val_acc: 0.4884\n",
            "Epoch 473/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7809 - acc: 0.6818 - val_loss: 1.2384 - val_acc: 0.4884\n",
            "Epoch 474/500\n",
            "88/88 [==============================] - 0s 303us/step - loss: 0.7802 - acc: 0.6932 - val_loss: 1.2378 - val_acc: 0.4884\n",
            "Epoch 475/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.7802 - acc: 0.6705 - val_loss: 1.2395 - val_acc: 0.4884\n",
            "Epoch 476/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.7805 - acc: 0.6705 - val_loss: 1.2399 - val_acc: 0.4884\n",
            "Epoch 477/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7805 - acc: 0.6932 - val_loss: 1.2400 - val_acc: 0.4884\n",
            "Epoch 478/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7801 - acc: 0.6818 - val_loss: 1.2407 - val_acc: 0.4884\n",
            "Epoch 479/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7804 - acc: 0.6818 - val_loss: 1.2407 - val_acc: 0.4884\n",
            "Epoch 480/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.7795 - acc: 0.6818 - val_loss: 1.2408 - val_acc: 0.4884\n",
            "Epoch 481/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.7793 - acc: 0.6932 - val_loss: 1.2411 - val_acc: 0.4884\n",
            "Epoch 482/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7790 - acc: 0.6818 - val_loss: 1.2414 - val_acc: 0.4884\n",
            "Epoch 483/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7800 - acc: 0.6818 - val_loss: 1.2418 - val_acc: 0.4884\n",
            "Epoch 484/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7787 - acc: 0.6818 - val_loss: 1.2416 - val_acc: 0.4884\n",
            "Epoch 485/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7792 - acc: 0.6932 - val_loss: 1.2420 - val_acc: 0.4884\n",
            "Epoch 486/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.7789 - acc: 0.6932 - val_loss: 1.2421 - val_acc: 0.4884\n",
            "Epoch 487/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7787 - acc: 0.6932 - val_loss: 1.2421 - val_acc: 0.4884\n",
            "Epoch 488/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.7788 - acc: 0.6818 - val_loss: 1.2428 - val_acc: 0.4884\n",
            "Epoch 489/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7790 - acc: 0.6705 - val_loss: 1.2436 - val_acc: 0.4884\n",
            "Epoch 490/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.7793 - acc: 0.6818 - val_loss: 1.2437 - val_acc: 0.4884\n",
            "Epoch 491/500\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.7783 - acc: 0.6705 - val_loss: 1.2439 - val_acc: 0.4884\n",
            "Epoch 492/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.7784 - acc: 0.6818 - val_loss: 1.2440 - val_acc: 0.4884\n",
            "Epoch 493/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.7784 - acc: 0.6932 - val_loss: 1.2445 - val_acc: 0.4884\n",
            "Epoch 494/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.7790 - acc: 0.6705 - val_loss: 1.2451 - val_acc: 0.4884\n",
            "Epoch 495/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.7776 - acc: 0.6705 - val_loss: 1.2446 - val_acc: 0.4884\n",
            "Epoch 496/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.7780 - acc: 0.6932 - val_loss: 1.2449 - val_acc: 0.4884\n",
            "Epoch 497/500\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.7786 - acc: 0.6705 - val_loss: 1.2461 - val_acc: 0.4884\n",
            "Epoch 498/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.7773 - acc: 0.6818 - val_loss: 1.2456 - val_acc: 0.4884\n",
            "Epoch 499/500\n",
            "88/88 [==============================] - 0s 309us/step - loss: 0.7769 - acc: 0.6818 - val_loss: 1.2464 - val_acc: 0.4884\n",
            "Epoch 500/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.7771 - acc: 0.6932 - val_loss: 1.2476 - val_acc: 0.4651\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/500\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 2.6409 - acc: 0.1818 - val_loss: 3.1733 - val_acc: 0.2326\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 2.4772 - acc: 0.1705 - val_loss: 3.0339 - val_acc: 0.2326\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 2.3225 - acc: 0.1705 - val_loss: 2.9030 - val_acc: 0.2326\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 2.1855 - acc: 0.1932 - val_loss: 2.7965 - val_acc: 0.2093\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 2.0686 - acc: 0.2045 - val_loss: 2.6983 - val_acc: 0.2093\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 0s 269us/step - loss: 1.9639 - acc: 0.2045 - val_loss: 2.6088 - val_acc: 0.2326\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.8706 - acc: 0.2045 - val_loss: 2.5291 - val_acc: 0.2093\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 1.7903 - acc: 0.2273 - val_loss: 2.4560 - val_acc: 0.2093\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.7210 - acc: 0.2386 - val_loss: 2.3902 - val_acc: 0.2093\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.6615 - acc: 0.2273 - val_loss: 2.3311 - val_acc: 0.2093\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.6058 - acc: 0.2386 - val_loss: 2.2760 - val_acc: 0.2326\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.5569 - acc: 0.2500 - val_loss: 2.2245 - val_acc: 0.2093\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 1.5155 - acc: 0.2614 - val_loss: 2.1826 - val_acc: 0.2558\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.4777 - acc: 0.2727 - val_loss: 2.1460 - val_acc: 0.2558\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 1.4444 - acc: 0.2614 - val_loss: 2.1118 - val_acc: 0.2558\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.4162 - acc: 0.2841 - val_loss: 2.0810 - val_acc: 0.2791\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.3923 - acc: 0.3068 - val_loss: 2.0548 - val_acc: 0.2791\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.3707 - acc: 0.3295 - val_loss: 2.0332 - val_acc: 0.2791\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.3511 - acc: 0.3409 - val_loss: 2.0129 - val_acc: 0.2791\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 1.3352 - acc: 0.3409 - val_loss: 1.9935 - val_acc: 0.2791\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.3183 - acc: 0.3409 - val_loss: 1.9740 - val_acc: 0.3023\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 0s 293us/step - loss: 1.3049 - acc: 0.3523 - val_loss: 1.9564 - val_acc: 0.3488\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.2926 - acc: 0.3523 - val_loss: 1.9401 - val_acc: 0.3488\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.2795 - acc: 0.3864 - val_loss: 1.9233 - val_acc: 0.3488\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.2674 - acc: 0.3864 - val_loss: 1.9078 - val_acc: 0.3488\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.2560 - acc: 0.3977 - val_loss: 1.8936 - val_acc: 0.3023\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 1.2467 - acc: 0.4205 - val_loss: 1.8804 - val_acc: 0.3023\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.2371 - acc: 0.4205 - val_loss: 1.8673 - val_acc: 0.3023\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.2277 - acc: 0.4205 - val_loss: 1.8548 - val_acc: 0.3023\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.2184 - acc: 0.4318 - val_loss: 1.8426 - val_acc: 0.3023\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.2101 - acc: 0.4432 - val_loss: 1.8306 - val_acc: 0.3023\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.2023 - acc: 0.4432 - val_loss: 1.8202 - val_acc: 0.3023\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 1.1937 - acc: 0.4432 - val_loss: 1.8092 - val_acc: 0.3256\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.1864 - acc: 0.4545 - val_loss: 1.7982 - val_acc: 0.3256\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1784 - acc: 0.4545 - val_loss: 1.7840 - val_acc: 0.3256\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 0s 314us/step - loss: 1.1693 - acc: 0.4773 - val_loss: 1.7707 - val_acc: 0.3256\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.1604 - acc: 0.4659 - val_loss: 1.7579 - val_acc: 0.3256\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.1543 - acc: 0.4773 - val_loss: 1.7454 - val_acc: 0.3488\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.1473 - acc: 0.4545 - val_loss: 1.7333 - val_acc: 0.3488\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.1383 - acc: 0.4659 - val_loss: 1.7212 - val_acc: 0.3488\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.1325 - acc: 0.4773 - val_loss: 1.7099 - val_acc: 0.3721\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.1258 - acc: 0.5000 - val_loss: 1.6999 - val_acc: 0.3953\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.1197 - acc: 0.5000 - val_loss: 1.6884 - val_acc: 0.3953\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.1127 - acc: 0.5114 - val_loss: 1.6779 - val_acc: 0.3953\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1052 - acc: 0.5000 - val_loss: 1.6676 - val_acc: 0.3953\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0992 - acc: 0.5227 - val_loss: 1.6580 - val_acc: 0.3953\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0933 - acc: 0.5114 - val_loss: 1.6484 - val_acc: 0.3953\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0870 - acc: 0.5000 - val_loss: 1.6390 - val_acc: 0.3953\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0806 - acc: 0.5114 - val_loss: 1.6303 - val_acc: 0.3953\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0754 - acc: 0.5227 - val_loss: 1.6216 - val_acc: 0.3953\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0699 - acc: 0.5341 - val_loss: 1.6133 - val_acc: 0.3953\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 1.0657 - acc: 0.5455 - val_loss: 1.6046 - val_acc: 0.3953\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0622 - acc: 0.5568 - val_loss: 1.5965 - val_acc: 0.3953\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.0573 - acc: 0.5568 - val_loss: 1.5884 - val_acc: 0.3953\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0533 - acc: 0.5568 - val_loss: 1.5810 - val_acc: 0.3953\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.0487 - acc: 0.5568 - val_loss: 1.5737 - val_acc: 0.3953\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0445 - acc: 0.5568 - val_loss: 1.5660 - val_acc: 0.3953\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0416 - acc: 0.5568 - val_loss: 1.5589 - val_acc: 0.4186\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 0s 274us/step - loss: 1.0389 - acc: 0.5568 - val_loss: 1.5518 - val_acc: 0.4186\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0357 - acc: 0.5568 - val_loss: 1.5447 - val_acc: 0.4186\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.0325 - acc: 0.5568 - val_loss: 1.5379 - val_acc: 0.4186\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0311 - acc: 0.5568 - val_loss: 1.5308 - val_acc: 0.4186\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 1.0257 - acc: 0.5568 - val_loss: 1.5244 - val_acc: 0.4419\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.0229 - acc: 0.5568 - val_loss: 1.5173 - val_acc: 0.4419\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 0s 322us/step - loss: 1.0196 - acc: 0.5682 - val_loss: 1.5106 - val_acc: 0.4419\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0174 - acc: 0.5682 - val_loss: 1.5044 - val_acc: 0.4419\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 0s 282us/step - loss: 1.0152 - acc: 0.5682 - val_loss: 1.4978 - val_acc: 0.4419\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 1.0115 - acc: 0.5682 - val_loss: 1.4911 - val_acc: 0.4419\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 0s 274us/step - loss: 1.0090 - acc: 0.5568 - val_loss: 1.4854 - val_acc: 0.4419\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0065 - acc: 0.5568 - val_loss: 1.4800 - val_acc: 0.4419\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 1.0041 - acc: 0.5568 - val_loss: 1.4738 - val_acc: 0.4651\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0022 - acc: 0.5568 - val_loss: 1.4684 - val_acc: 0.4651\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9992 - acc: 0.5568 - val_loss: 1.4628 - val_acc: 0.4651\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9976 - acc: 0.5568 - val_loss: 1.4574 - val_acc: 0.4884\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9942 - acc: 0.5568 - val_loss: 1.4519 - val_acc: 0.4884\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.9933 - acc: 0.5568 - val_loss: 1.4468 - val_acc: 0.4884\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.9907 - acc: 0.5568 - val_loss: 1.4415 - val_acc: 0.4884\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.9887 - acc: 0.5568 - val_loss: 1.4366 - val_acc: 0.4884\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.9867 - acc: 0.5568 - val_loss: 1.4314 - val_acc: 0.4884\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9838 - acc: 0.5568 - val_loss: 1.4266 - val_acc: 0.4884\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.9816 - acc: 0.5568 - val_loss: 1.4216 - val_acc: 0.4884\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.9794 - acc: 0.5682 - val_loss: 1.4162 - val_acc: 0.5116\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9773 - acc: 0.5568 - val_loss: 1.4113 - val_acc: 0.5116\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9750 - acc: 0.5568 - val_loss: 1.4066 - val_acc: 0.5116\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.9734 - acc: 0.5682 - val_loss: 1.4016 - val_acc: 0.5116\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9708 - acc: 0.5682 - val_loss: 1.3973 - val_acc: 0.5116\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.9692 - acc: 0.5682 - val_loss: 1.3922 - val_acc: 0.4884\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9679 - acc: 0.5682 - val_loss: 1.3877 - val_acc: 0.4884\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.9661 - acc: 0.5682 - val_loss: 1.3832 - val_acc: 0.4884\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.9637 - acc: 0.5682 - val_loss: 1.3786 - val_acc: 0.4884\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9638 - acc: 0.5682 - val_loss: 1.3742 - val_acc: 0.4884\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.9605 - acc: 0.5682 - val_loss: 1.3701 - val_acc: 0.4884\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.9590 - acc: 0.5682 - val_loss: 1.3663 - val_acc: 0.4884\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9577 - acc: 0.5795 - val_loss: 1.3624 - val_acc: 0.4884\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9580 - acc: 0.5682 - val_loss: 1.3586 - val_acc: 0.4884\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.9542 - acc: 0.5795 - val_loss: 1.3550 - val_acc: 0.4884\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.9523 - acc: 0.5795 - val_loss: 1.3506 - val_acc: 0.4884\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.9499 - acc: 0.5795 - val_loss: 1.3468 - val_acc: 0.4884\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.9490 - acc: 0.5795 - val_loss: 1.3434 - val_acc: 0.4884\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.9467 - acc: 0.5795 - val_loss: 1.3396 - val_acc: 0.4884\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9456 - acc: 0.5795 - val_loss: 1.3359 - val_acc: 0.4884\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.9440 - acc: 0.5795 - val_loss: 1.3324 - val_acc: 0.4884\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.9427 - acc: 0.5795 - val_loss: 1.3287 - val_acc: 0.4884\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.9406 - acc: 0.5795 - val_loss: 1.3252 - val_acc: 0.4884\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.9388 - acc: 0.5795 - val_loss: 1.3216 - val_acc: 0.4884\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.9363 - acc: 0.5909 - val_loss: 1.3184 - val_acc: 0.4884\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9357 - acc: 0.5909 - val_loss: 1.3149 - val_acc: 0.4884\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9354 - acc: 0.5909 - val_loss: 1.3115 - val_acc: 0.4884\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9339 - acc: 0.5795 - val_loss: 1.3082 - val_acc: 0.4884\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.9315 - acc: 0.5909 - val_loss: 1.3046 - val_acc: 0.4884\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.9301 - acc: 0.5909 - val_loss: 1.3021 - val_acc: 0.4884\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.9289 - acc: 0.5909 - val_loss: 1.2990 - val_acc: 0.4884\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9281 - acc: 0.5909 - val_loss: 1.2960 - val_acc: 0.4884\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.9271 - acc: 0.5909 - val_loss: 1.2927 - val_acc: 0.4884\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.9246 - acc: 0.5909 - val_loss: 1.2891 - val_acc: 0.4884\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.9232 - acc: 0.5909 - val_loss: 1.2864 - val_acc: 0.4884\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.9217 - acc: 0.5909 - val_loss: 1.2841 - val_acc: 0.5116\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.9210 - acc: 0.5909 - val_loss: 1.2813 - val_acc: 0.5116\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9189 - acc: 0.5909 - val_loss: 1.2780 - val_acc: 0.5116\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.9180 - acc: 0.5909 - val_loss: 1.2752 - val_acc: 0.5116\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.9171 - acc: 0.5909 - val_loss: 1.2731 - val_acc: 0.5116\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.9160 - acc: 0.6023 - val_loss: 1.2698 - val_acc: 0.5116\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9135 - acc: 0.6023 - val_loss: 1.2673 - val_acc: 0.5116\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9128 - acc: 0.6023 - val_loss: 1.2647 - val_acc: 0.5116\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.9120 - acc: 0.6023 - val_loss: 1.2618 - val_acc: 0.5116\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.9102 - acc: 0.6023 - val_loss: 1.2591 - val_acc: 0.5116\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.9093 - acc: 0.6023 - val_loss: 1.2566 - val_acc: 0.5116\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.9078 - acc: 0.6023 - val_loss: 1.2542 - val_acc: 0.5116\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.9079 - acc: 0.6023 - val_loss: 1.2515 - val_acc: 0.5116\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9057 - acc: 0.6023 - val_loss: 1.2489 - val_acc: 0.5116\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.9041 - acc: 0.6136 - val_loss: 1.2466 - val_acc: 0.5116\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9027 - acc: 0.6136 - val_loss: 1.2448 - val_acc: 0.5116\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.9025 - acc: 0.6136 - val_loss: 1.2423 - val_acc: 0.5116\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.9001 - acc: 0.6136 - val_loss: 1.2400 - val_acc: 0.4884\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.9000 - acc: 0.6136 - val_loss: 1.2381 - val_acc: 0.4884\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8982 - acc: 0.6136 - val_loss: 1.2361 - val_acc: 0.4884\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8974 - acc: 0.6136 - val_loss: 1.2335 - val_acc: 0.4884\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8957 - acc: 0.6136 - val_loss: 1.2308 - val_acc: 0.4884\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8960 - acc: 0.6136 - val_loss: 1.2282 - val_acc: 0.4884\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8943 - acc: 0.6136 - val_loss: 1.2259 - val_acc: 0.4884\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8934 - acc: 0.6136 - val_loss: 1.2244 - val_acc: 0.4884\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8928 - acc: 0.6136 - val_loss: 1.2223 - val_acc: 0.4651\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8913 - acc: 0.6136 - val_loss: 1.2196 - val_acc: 0.4651\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8909 - acc: 0.6136 - val_loss: 1.2183 - val_acc: 0.4651\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8901 - acc: 0.6136 - val_loss: 1.2161 - val_acc: 0.4651\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.8884 - acc: 0.6250 - val_loss: 1.2138 - val_acc: 0.4651\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8872 - acc: 0.6136 - val_loss: 1.2119 - val_acc: 0.4651\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8878 - acc: 0.6136 - val_loss: 1.2094 - val_acc: 0.4651\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8854 - acc: 0.6250 - val_loss: 1.2070 - val_acc: 0.4651\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8847 - acc: 0.6136 - val_loss: 1.2052 - val_acc: 0.4651\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8836 - acc: 0.6136 - val_loss: 1.2036 - val_acc: 0.4651\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8825 - acc: 0.6250 - val_loss: 1.2010 - val_acc: 0.4651\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8811 - acc: 0.6364 - val_loss: 1.1988 - val_acc: 0.4651\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8813 - acc: 0.6136 - val_loss: 1.1971 - val_acc: 0.4651\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8795 - acc: 0.6364 - val_loss: 1.1948 - val_acc: 0.4651\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8791 - acc: 0.6364 - val_loss: 1.1927 - val_acc: 0.4651\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8782 - acc: 0.6364 - val_loss: 1.1909 - val_acc: 0.4651\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8775 - acc: 0.6364 - val_loss: 1.1887 - val_acc: 0.4651\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8770 - acc: 0.6364 - val_loss: 1.1878 - val_acc: 0.4651\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8750 - acc: 0.6364 - val_loss: 1.1860 - val_acc: 0.4419\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8738 - acc: 0.6364 - val_loss: 1.1843 - val_acc: 0.4419\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8739 - acc: 0.6364 - val_loss: 1.1830 - val_acc: 0.4651\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8729 - acc: 0.6364 - val_loss: 1.1808 - val_acc: 0.4651\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.8725 - acc: 0.6364 - val_loss: 1.1791 - val_acc: 0.4651\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8710 - acc: 0.6477 - val_loss: 1.1773 - val_acc: 0.4651\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8695 - acc: 0.6477 - val_loss: 1.1756 - val_acc: 0.4651\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8695 - acc: 0.6477 - val_loss: 1.1743 - val_acc: 0.4651\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8675 - acc: 0.6591 - val_loss: 1.1729 - val_acc: 0.4651\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8671 - acc: 0.6591 - val_loss: 1.1717 - val_acc: 0.4651\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8672 - acc: 0.6591 - val_loss: 1.1701 - val_acc: 0.4651\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8646 - acc: 0.6591 - val_loss: 1.1683 - val_acc: 0.4651\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8658 - acc: 0.6591 - val_loss: 1.1665 - val_acc: 0.4651\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8634 - acc: 0.6591 - val_loss: 1.1646 - val_acc: 0.4651\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.8631 - acc: 0.6477 - val_loss: 1.1633 - val_acc: 0.4651\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8620 - acc: 0.6591 - val_loss: 1.1622 - val_acc: 0.4651\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8605 - acc: 0.6591 - val_loss: 1.1607 - val_acc: 0.4651\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8597 - acc: 0.6591 - val_loss: 1.1591 - val_acc: 0.4651\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.8591 - acc: 0.6591 - val_loss: 1.1572 - val_acc: 0.4651\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.8582 - acc: 0.6591 - val_loss: 1.1554 - val_acc: 0.4651\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8569 - acc: 0.6591 - val_loss: 1.1544 - val_acc: 0.4651\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8561 - acc: 0.6705 - val_loss: 1.1531 - val_acc: 0.4651\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8550 - acc: 0.6591 - val_loss: 1.1518 - val_acc: 0.4651\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8542 - acc: 0.6705 - val_loss: 1.1512 - val_acc: 0.4651\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8539 - acc: 0.6705 - val_loss: 1.1500 - val_acc: 0.4651\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8525 - acc: 0.6705 - val_loss: 1.1486 - val_acc: 0.4651\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8516 - acc: 0.6705 - val_loss: 1.1484 - val_acc: 0.4651\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8503 - acc: 0.6705 - val_loss: 1.1469 - val_acc: 0.4651\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 0s 290us/step - loss: 0.8490 - acc: 0.6705 - val_loss: 1.1462 - val_acc: 0.4651\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8484 - acc: 0.6705 - val_loss: 1.1452 - val_acc: 0.4651\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.8479 - acc: 0.6705 - val_loss: 1.1444 - val_acc: 0.4651\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 0s 303us/step - loss: 0.8470 - acc: 0.6705 - val_loss: 1.1429 - val_acc: 0.4419\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8469 - acc: 0.6705 - val_loss: 1.1424 - val_acc: 0.4651\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8448 - acc: 0.6705 - val_loss: 1.1415 - val_acc: 0.4651\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8441 - acc: 0.6705 - val_loss: 1.1404 - val_acc: 0.4651\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8439 - acc: 0.6705 - val_loss: 1.1395 - val_acc: 0.4884\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.8422 - acc: 0.6705 - val_loss: 1.1381 - val_acc: 0.4884\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8415 - acc: 0.6705 - val_loss: 1.1366 - val_acc: 0.4651\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8410 - acc: 0.6705 - val_loss: 1.1353 - val_acc: 0.4651\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8412 - acc: 0.6705 - val_loss: 1.1342 - val_acc: 0.4884\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8398 - acc: 0.6705 - val_loss: 1.1332 - val_acc: 0.4884\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8389 - acc: 0.6705 - val_loss: 1.1323 - val_acc: 0.4884\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8381 - acc: 0.6705 - val_loss: 1.1307 - val_acc: 0.4651\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8375 - acc: 0.6705 - val_loss: 1.1303 - val_acc: 0.4884\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8375 - acc: 0.6705 - val_loss: 1.1287 - val_acc: 0.4651\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8364 - acc: 0.6705 - val_loss: 1.1275 - val_acc: 0.4651\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.8347 - acc: 0.6705 - val_loss: 1.1267 - val_acc: 0.4884\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8341 - acc: 0.6705 - val_loss: 1.1260 - val_acc: 0.4884\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8334 - acc: 0.6705 - val_loss: 1.1254 - val_acc: 0.5116\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8328 - acc: 0.6705 - val_loss: 1.1239 - val_acc: 0.4884\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8324 - acc: 0.6705 - val_loss: 1.1229 - val_acc: 0.4651\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8307 - acc: 0.6705 - val_loss: 1.1216 - val_acc: 0.4651\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.8306 - acc: 0.6818 - val_loss: 1.1208 - val_acc: 0.4651\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8295 - acc: 0.6705 - val_loss: 1.1201 - val_acc: 0.4651\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8295 - acc: 0.6818 - val_loss: 1.1194 - val_acc: 0.4651\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8276 - acc: 0.6705 - val_loss: 1.1185 - val_acc: 0.4651\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8284 - acc: 0.6705 - val_loss: 1.1169 - val_acc: 0.4651\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8265 - acc: 0.6818 - val_loss: 1.1165 - val_acc: 0.4651\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 0s 297us/step - loss: 0.8263 - acc: 0.6818 - val_loss: 1.1161 - val_acc: 0.4651\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 0s 299us/step - loss: 0.8258 - acc: 0.6818 - val_loss: 1.1157 - val_acc: 0.4884\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8265 - acc: 0.6705 - val_loss: 1.1149 - val_acc: 0.4884\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8254 - acc: 0.6818 - val_loss: 1.1133 - val_acc: 0.4651\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8238 - acc: 0.6705 - val_loss: 1.1127 - val_acc: 0.4651\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8230 - acc: 0.6705 - val_loss: 1.1122 - val_acc: 0.4884\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8230 - acc: 0.6818 - val_loss: 1.1111 - val_acc: 0.4651\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8218 - acc: 0.6818 - val_loss: 1.1109 - val_acc: 0.4651\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8216 - acc: 0.6818 - val_loss: 1.1103 - val_acc: 0.4651\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8208 - acc: 0.6818 - val_loss: 1.1097 - val_acc: 0.4884\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8204 - acc: 0.6818 - val_loss: 1.1081 - val_acc: 0.4651\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 0s 283us/step - loss: 0.8195 - acc: 0.6818 - val_loss: 1.1077 - val_acc: 0.4651\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8192 - acc: 0.6818 - val_loss: 1.1073 - val_acc: 0.4651\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8189 - acc: 0.6818 - val_loss: 1.1067 - val_acc: 0.4651\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8179 - acc: 0.6818 - val_loss: 1.1061 - val_acc: 0.4651\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 0s 337us/step - loss: 0.8171 - acc: 0.6818 - val_loss: 1.1050 - val_acc: 0.4651\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8169 - acc: 0.6818 - val_loss: 1.1045 - val_acc: 0.4651\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8157 - acc: 0.6818 - val_loss: 1.1040 - val_acc: 0.4651\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8155 - acc: 0.6818 - val_loss: 1.1032 - val_acc: 0.4651\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8156 - acc: 0.6818 - val_loss: 1.1018 - val_acc: 0.4651\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8146 - acc: 0.6932 - val_loss: 1.1015 - val_acc: 0.4651\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8138 - acc: 0.6932 - val_loss: 1.1011 - val_acc: 0.4651\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8136 - acc: 0.6932 - val_loss: 1.1003 - val_acc: 0.4651\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8141 - acc: 0.6818 - val_loss: 1.0999 - val_acc: 0.4651\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8124 - acc: 0.6932 - val_loss: 1.0998 - val_acc: 0.4651\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8122 - acc: 0.6932 - val_loss: 1.0981 - val_acc: 0.4651\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8117 - acc: 0.7045 - val_loss: 1.0971 - val_acc: 0.4651\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8111 - acc: 0.7273 - val_loss: 1.0973 - val_acc: 0.4651\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8105 - acc: 0.7159 - val_loss: 1.0973 - val_acc: 0.4651\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8103 - acc: 0.7045 - val_loss: 1.0972 - val_acc: 0.4651\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8095 - acc: 0.7045 - val_loss: 1.0966 - val_acc: 0.4651\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8095 - acc: 0.7159 - val_loss: 1.0965 - val_acc: 0.4651\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8090 - acc: 0.7159 - val_loss: 1.0956 - val_acc: 0.4651\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 0s 288us/step - loss: 0.8083 - acc: 0.7159 - val_loss: 1.0946 - val_acc: 0.4651\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8084 - acc: 0.7159 - val_loss: 1.0944 - val_acc: 0.4651\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.8075 - acc: 0.7159 - val_loss: 1.0937 - val_acc: 0.4651\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8065 - acc: 0.7159 - val_loss: 1.0934 - val_acc: 0.4651\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8069 - acc: 0.7159 - val_loss: 1.0929 - val_acc: 0.4651\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8064 - acc: 0.7159 - val_loss: 1.0918 - val_acc: 0.4651\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8059 - acc: 0.7273 - val_loss: 1.0913 - val_acc: 0.4651\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8054 - acc: 0.7273 - val_loss: 1.0919 - val_acc: 0.4651\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8045 - acc: 0.7159 - val_loss: 1.0920 - val_acc: 0.4651\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8043 - acc: 0.7273 - val_loss: 1.0908 - val_acc: 0.4651\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8043 - acc: 0.7273 - val_loss: 1.0903 - val_acc: 0.4651\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8036 - acc: 0.7159 - val_loss: 1.0904 - val_acc: 0.4651\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8029 - acc: 0.7273 - val_loss: 1.0895 - val_acc: 0.4651\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8030 - acc: 0.7273 - val_loss: 1.0884 - val_acc: 0.4651\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8028 - acc: 0.7273 - val_loss: 1.0883 - val_acc: 0.4651\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8021 - acc: 0.7273 - val_loss: 1.0883 - val_acc: 0.4651\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8017 - acc: 0.7273 - val_loss: 1.0875 - val_acc: 0.4651\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8015 - acc: 0.7273 - val_loss: 1.0875 - val_acc: 0.4884\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8020 - acc: 0.7273 - val_loss: 1.0869 - val_acc: 0.4651\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8006 - acc: 0.7273 - val_loss: 1.0858 - val_acc: 0.4884\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.8011 - acc: 0.7273 - val_loss: 1.0857 - val_acc: 0.4884\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8005 - acc: 0.7273 - val_loss: 1.0853 - val_acc: 0.4884\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7993 - acc: 0.7273 - val_loss: 1.0850 - val_acc: 0.4884\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7992 - acc: 0.7273 - val_loss: 1.0845 - val_acc: 0.4884\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7991 - acc: 0.7273 - val_loss: 1.0848 - val_acc: 0.4884\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7984 - acc: 0.7386 - val_loss: 1.0852 - val_acc: 0.4884\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.7986 - acc: 0.7273 - val_loss: 1.0848 - val_acc: 0.4884\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.7974 - acc: 0.7273 - val_loss: 1.0850 - val_acc: 0.4884\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.7978 - acc: 0.7273 - val_loss: 1.0845 - val_acc: 0.4884\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7970 - acc: 0.7273 - val_loss: 1.0842 - val_acc: 0.4884\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.7970 - acc: 0.7273 - val_loss: 1.0839 - val_acc: 0.4884\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.7966 - acc: 0.7386 - val_loss: 1.0838 - val_acc: 0.4884\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.7967 - acc: 0.7273 - val_loss: 1.0839 - val_acc: 0.4884\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7962 - acc: 0.7273 - val_loss: 1.0836 - val_acc: 0.4884\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7954 - acc: 0.7386 - val_loss: 1.0828 - val_acc: 0.4884\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7951 - acc: 0.7273 - val_loss: 1.0827 - val_acc: 0.4884\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.7949 - acc: 0.7386 - val_loss: 1.0823 - val_acc: 0.4884\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7948 - acc: 0.7386 - val_loss: 1.0821 - val_acc: 0.4884\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.7941 - acc: 0.7386 - val_loss: 1.0819 - val_acc: 0.4884\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.7949 - acc: 0.7273 - val_loss: 1.0815 - val_acc: 0.4884\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.7937 - acc: 0.7273 - val_loss: 1.0807 - val_acc: 0.4884\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7933 - acc: 0.7273 - val_loss: 1.0796 - val_acc: 0.4884\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.7938 - acc: 0.7386 - val_loss: 1.0799 - val_acc: 0.4884\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.7930 - acc: 0.7386 - val_loss: 1.0800 - val_acc: 0.4884\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7923 - acc: 0.7386 - val_loss: 1.0796 - val_acc: 0.4884\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7924 - acc: 0.7386 - val_loss: 1.0796 - val_acc: 0.4884\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7916 - acc: 0.7273 - val_loss: 1.0796 - val_acc: 0.4884\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.7915 - acc: 0.7386 - val_loss: 1.0796 - val_acc: 0.4884\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.7919 - acc: 0.7273 - val_loss: 1.0788 - val_acc: 0.5116\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7913 - acc: 0.7386 - val_loss: 1.0792 - val_acc: 0.5116\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.7911 - acc: 0.7273 - val_loss: 1.0784 - val_acc: 0.5116\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.7910 - acc: 0.7273 - val_loss: 1.0783 - val_acc: 0.5116\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.7901 - acc: 0.7386 - val_loss: 1.0781 - val_acc: 0.5116\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7902 - acc: 0.7273 - val_loss: 1.0780 - val_acc: 0.5116\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.7897 - acc: 0.7386 - val_loss: 1.0784 - val_acc: 0.5116\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.7901 - acc: 0.7386 - val_loss: 1.0780 - val_acc: 0.5116\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.7888 - acc: 0.7386 - val_loss: 1.0784 - val_acc: 0.5116\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.7887 - acc: 0.7386 - val_loss: 1.0782 - val_acc: 0.5116\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7887 - acc: 0.7386 - val_loss: 1.0777 - val_acc: 0.5116\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7890 - acc: 0.7273 - val_loss: 1.0767 - val_acc: 0.5116\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.7881 - acc: 0.7386 - val_loss: 1.0767 - val_acc: 0.5116\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7876 - acc: 0.7386 - val_loss: 1.0762 - val_acc: 0.5116\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7877 - acc: 0.7386 - val_loss: 1.0759 - val_acc: 0.5116\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 0s 180us/step - loss: 0.7877 - acc: 0.7386 - val_loss: 1.0762 - val_acc: 0.5116\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7873 - acc: 0.7386 - val_loss: 1.0757 - val_acc: 0.5116\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.7868 - acc: 0.7386 - val_loss: 1.0759 - val_acc: 0.5116\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7859 - acc: 0.7386 - val_loss: 1.0761 - val_acc: 0.5116\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7865 - acc: 0.7386 - val_loss: 1.0764 - val_acc: 0.5116\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7858 - acc: 0.7386 - val_loss: 1.0755 - val_acc: 0.5116\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7855 - acc: 0.7386 - val_loss: 1.0750 - val_acc: 0.5116\n",
            "Epoch 321/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7849 - acc: 0.7386 - val_loss: 1.0752 - val_acc: 0.5116\n",
            "Epoch 322/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7849 - acc: 0.7386 - val_loss: 1.0755 - val_acc: 0.5116\n",
            "Epoch 323/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.7849 - acc: 0.7386 - val_loss: 1.0753 - val_acc: 0.5116\n",
            "Epoch 324/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7843 - acc: 0.7386 - val_loss: 1.0750 - val_acc: 0.5116\n",
            "Epoch 325/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7843 - acc: 0.7386 - val_loss: 1.0742 - val_acc: 0.5116\n",
            "Epoch 326/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7851 - acc: 0.7273 - val_loss: 1.0732 - val_acc: 0.5116\n",
            "Epoch 327/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.7842 - acc: 0.7386 - val_loss: 1.0733 - val_acc: 0.5116\n",
            "Epoch 328/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7838 - acc: 0.7386 - val_loss: 1.0736 - val_acc: 0.5116\n",
            "Epoch 329/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.7829 - acc: 0.7386 - val_loss: 1.0738 - val_acc: 0.5116\n",
            "Epoch 330/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7836 - acc: 0.7386 - val_loss: 1.0744 - val_acc: 0.5116\n",
            "Epoch 331/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7828 - acc: 0.7386 - val_loss: 1.0745 - val_acc: 0.5116\n",
            "Epoch 332/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.7825 - acc: 0.7386 - val_loss: 1.0742 - val_acc: 0.5349\n",
            "Epoch 333/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.7826 - acc: 0.7386 - val_loss: 1.0741 - val_acc: 0.5349\n",
            "Epoch 334/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7816 - acc: 0.7386 - val_loss: 1.0735 - val_acc: 0.5349\n",
            "Epoch 335/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.7823 - acc: 0.7386 - val_loss: 1.0730 - val_acc: 0.5349\n",
            "Epoch 336/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7814 - acc: 0.7386 - val_loss: 1.0726 - val_acc: 0.5349\n",
            "Epoch 337/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.7814 - acc: 0.7386 - val_loss: 1.0724 - val_acc: 0.5349\n",
            "Epoch 338/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7812 - acc: 0.7386 - val_loss: 1.0727 - val_acc: 0.5349\n",
            "Epoch 339/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.7805 - acc: 0.7386 - val_loss: 1.0730 - val_acc: 0.5349\n",
            "Epoch 340/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.7808 - acc: 0.7386 - val_loss: 1.0728 - val_acc: 0.5349\n",
            "Epoch 341/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7800 - acc: 0.7386 - val_loss: 1.0726 - val_acc: 0.5349\n",
            "Epoch 342/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.7795 - acc: 0.7386 - val_loss: 1.0717 - val_acc: 0.5349\n",
            "Epoch 343/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7798 - acc: 0.7386 - val_loss: 1.0715 - val_acc: 0.5349\n",
            "Epoch 344/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7801 - acc: 0.7386 - val_loss: 1.0719 - val_acc: 0.5349\n",
            "Epoch 345/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.7793 - acc: 0.7386 - val_loss: 1.0717 - val_acc: 0.5349\n",
            "Epoch 346/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7787 - acc: 0.7386 - val_loss: 1.0712 - val_acc: 0.5349\n",
            "Epoch 347/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.7796 - acc: 0.7386 - val_loss: 1.0714 - val_acc: 0.5349\n",
            "Epoch 348/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.7789 - acc: 0.7386 - val_loss: 1.0717 - val_acc: 0.5349\n",
            "Epoch 349/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7779 - acc: 0.7386 - val_loss: 1.0722 - val_acc: 0.5349\n",
            "Epoch 350/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.7783 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5349\n",
            "Epoch 351/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7782 - acc: 0.7386 - val_loss: 1.0712 - val_acc: 0.5349\n",
            "Epoch 352/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7773 - acc: 0.7386 - val_loss: 1.0715 - val_acc: 0.5349\n",
            "Epoch 353/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7772 - acc: 0.7386 - val_loss: 1.0707 - val_acc: 0.5349\n",
            "Epoch 354/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.7773 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5349\n",
            "Epoch 355/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.7775 - acc: 0.7386 - val_loss: 1.0703 - val_acc: 0.5349\n",
            "Epoch 356/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7772 - acc: 0.7386 - val_loss: 1.0703 - val_acc: 0.5349\n",
            "Epoch 357/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7768 - acc: 0.7386 - val_loss: 1.0701 - val_acc: 0.5349\n",
            "Epoch 358/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.7766 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5349\n",
            "Epoch 359/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.7756 - acc: 0.7386 - val_loss: 1.0712 - val_acc: 0.5349\n",
            "Epoch 360/500\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.7757 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5349\n",
            "Epoch 361/500\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.7761 - acc: 0.7386 - val_loss: 1.0707 - val_acc: 0.5349\n",
            "Epoch 362/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7756 - acc: 0.7386 - val_loss: 1.0712 - val_acc: 0.5349\n",
            "Epoch 363/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7749 - acc: 0.7386 - val_loss: 1.0713 - val_acc: 0.5116\n",
            "Epoch 364/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7749 - acc: 0.7386 - val_loss: 1.0717 - val_acc: 0.5116\n",
            "Epoch 365/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7748 - acc: 0.7386 - val_loss: 1.0713 - val_acc: 0.5349\n",
            "Epoch 366/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7744 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5349\n",
            "Epoch 367/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7746 - acc: 0.7386 - val_loss: 1.0710 - val_acc: 0.5349\n",
            "Epoch 368/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7742 - acc: 0.7386 - val_loss: 1.0710 - val_acc: 0.5349\n",
            "Epoch 369/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7744 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5349\n",
            "Epoch 370/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7736 - acc: 0.7386 - val_loss: 1.0709 - val_acc: 0.5116\n",
            "Epoch 371/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7736 - acc: 0.7386 - val_loss: 1.0701 - val_acc: 0.5349\n",
            "Epoch 372/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7730 - acc: 0.7386 - val_loss: 1.0702 - val_acc: 0.5349\n",
            "Epoch 373/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.7727 - acc: 0.7386 - val_loss: 1.0695 - val_acc: 0.5349\n",
            "Epoch 374/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7731 - acc: 0.7386 - val_loss: 1.0691 - val_acc: 0.5349\n",
            "Epoch 375/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.7727 - acc: 0.7386 - val_loss: 1.0694 - val_acc: 0.5349\n",
            "Epoch 376/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7720 - acc: 0.7386 - val_loss: 1.0697 - val_acc: 0.5349\n",
            "Epoch 377/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7716 - acc: 0.7386 - val_loss: 1.0695 - val_acc: 0.5349\n",
            "Epoch 378/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7716 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5349\n",
            "Epoch 379/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.7711 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5116\n",
            "Epoch 380/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7715 - acc: 0.7386 - val_loss: 1.0699 - val_acc: 0.5116\n",
            "Epoch 381/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7714 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5349\n",
            "Epoch 382/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7714 - acc: 0.7386 - val_loss: 1.0698 - val_acc: 0.5349\n",
            "Epoch 383/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7705 - acc: 0.7500 - val_loss: 1.0702 - val_acc: 0.5349\n",
            "Epoch 384/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7707 - acc: 0.7500 - val_loss: 1.0703 - val_acc: 0.5116\n",
            "Epoch 385/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7702 - acc: 0.7500 - val_loss: 1.0709 - val_acc: 0.5116\n",
            "Epoch 386/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.7702 - acc: 0.7500 - val_loss: 1.0702 - val_acc: 0.5349\n",
            "Epoch 387/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7697 - acc: 0.7500 - val_loss: 1.0705 - val_acc: 0.5349\n",
            "Epoch 388/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7693 - acc: 0.7500 - val_loss: 1.0707 - val_acc: 0.5116\n",
            "Epoch 389/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7695 - acc: 0.7500 - val_loss: 1.0708 - val_acc: 0.5116\n",
            "Epoch 390/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.7693 - acc: 0.7500 - val_loss: 1.0710 - val_acc: 0.5116\n",
            "Epoch 391/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7689 - acc: 0.7500 - val_loss: 1.0707 - val_acc: 0.5116\n",
            "Epoch 392/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7686 - acc: 0.7500 - val_loss: 1.0709 - val_acc: 0.5116\n",
            "Epoch 393/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.7688 - acc: 0.7500 - val_loss: 1.0698 - val_acc: 0.5349\n",
            "Epoch 394/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.7687 - acc: 0.7386 - val_loss: 1.0692 - val_acc: 0.5349\n",
            "Epoch 395/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.7688 - acc: 0.7500 - val_loss: 1.0695 - val_acc: 0.5349\n",
            "Epoch 396/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7683 - acc: 0.7500 - val_loss: 1.0698 - val_acc: 0.5349\n",
            "Epoch 397/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.7681 - acc: 0.7500 - val_loss: 1.0702 - val_acc: 0.5349\n",
            "Epoch 398/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.7678 - acc: 0.7500 - val_loss: 1.0702 - val_acc: 0.5116\n",
            "Epoch 399/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.7679 - acc: 0.7500 - val_loss: 1.0701 - val_acc: 0.5116\n",
            "Epoch 400/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7672 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5116\n",
            "Epoch 401/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7667 - acc: 0.7386 - val_loss: 1.0698 - val_acc: 0.5349\n",
            "Epoch 402/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7669 - acc: 0.7386 - val_loss: 1.0700 - val_acc: 0.5349\n",
            "Epoch 403/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.7669 - acc: 0.7386 - val_loss: 1.0699 - val_acc: 0.5349\n",
            "Epoch 404/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7665 - acc: 0.7386 - val_loss: 1.0703 - val_acc: 0.5116\n",
            "Epoch 405/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7662 - acc: 0.7386 - val_loss: 1.0702 - val_acc: 0.5349\n",
            "Epoch 406/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.7659 - acc: 0.7386 - val_loss: 1.0702 - val_acc: 0.5349\n",
            "Epoch 407/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7660 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5349\n",
            "Epoch 408/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.7661 - acc: 0.7386 - val_loss: 1.0691 - val_acc: 0.5581\n",
            "Epoch 409/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.7656 - acc: 0.7386 - val_loss: 1.0692 - val_acc: 0.5581\n",
            "Epoch 410/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7657 - acc: 0.7386 - val_loss: 1.0695 - val_acc: 0.5349\n",
            "Epoch 411/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7652 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5349\n",
            "Epoch 412/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.7651 - acc: 0.7386 - val_loss: 1.0695 - val_acc: 0.5349\n",
            "Epoch 413/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.7652 - acc: 0.7386 - val_loss: 1.0700 - val_acc: 0.5581\n",
            "Epoch 414/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7650 - acc: 0.7386 - val_loss: 1.0697 - val_acc: 0.5581\n",
            "Epoch 415/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.7642 - acc: 0.7500 - val_loss: 1.0690 - val_acc: 0.5581\n",
            "Epoch 416/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7646 - acc: 0.7386 - val_loss: 1.0697 - val_acc: 0.5581\n",
            "Epoch 417/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.7644 - acc: 0.7386 - val_loss: 1.0709 - val_acc: 0.5349\n",
            "Epoch 418/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.7643 - acc: 0.7386 - val_loss: 1.0708 - val_acc: 0.5349\n",
            "Epoch 419/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.7637 - acc: 0.7386 - val_loss: 1.0704 - val_acc: 0.5349\n",
            "Epoch 420/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.7635 - acc: 0.7386 - val_loss: 1.0709 - val_acc: 0.5349\n",
            "Epoch 421/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.7635 - acc: 0.7386 - val_loss: 1.0703 - val_acc: 0.5349\n",
            "Epoch 422/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.7635 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5581\n",
            "Epoch 423/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.7639 - acc: 0.7386 - val_loss: 1.0696 - val_acc: 0.5581\n",
            "Epoch 424/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.7630 - acc: 0.7386 - val_loss: 1.0700 - val_acc: 0.5581\n",
            "Epoch 425/500\n",
            "88/88 [==============================] - 0s 316us/step - loss: 0.7629 - acc: 0.7386 - val_loss: 1.0702 - val_acc: 0.5581\n",
            "Epoch 426/500\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.7626 - acc: 0.7500 - val_loss: 1.0701 - val_acc: 0.5349\n",
            "Epoch 427/500\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.7625 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5349\n",
            "Epoch 428/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.7628 - acc: 0.7500 - val_loss: 1.0701 - val_acc: 0.5581\n",
            "Epoch 429/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7620 - acc: 0.7386 - val_loss: 1.0701 - val_acc: 0.5581\n",
            "Epoch 430/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.7623 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5349\n",
            "Epoch 431/500\n",
            "88/88 [==============================] - 0s 299us/step - loss: 0.7619 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5349\n",
            "Epoch 432/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7624 - acc: 0.7386 - val_loss: 1.0695 - val_acc: 0.5581\n",
            "Epoch 433/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7619 - acc: 0.7386 - val_loss: 1.0699 - val_acc: 0.5581\n",
            "Epoch 434/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7614 - acc: 0.7386 - val_loss: 1.0700 - val_acc: 0.5581\n",
            "Epoch 435/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.7610 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5581\n",
            "Epoch 436/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7621 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5581\n",
            "Epoch 437/500\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.7607 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 438/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7605 - acc: 0.7386 - val_loss: 1.0709 - val_acc: 0.5581\n",
            "Epoch 439/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.7609 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5581\n",
            "Epoch 440/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.7608 - acc: 0.7386 - val_loss: 1.0702 - val_acc: 0.5581\n",
            "Epoch 441/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7604 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5581\n",
            "Epoch 442/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.7604 - acc: 0.7386 - val_loss: 1.0707 - val_acc: 0.5581\n",
            "Epoch 443/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7602 - acc: 0.7500 - val_loss: 1.0699 - val_acc: 0.5581\n",
            "Epoch 444/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.7605 - acc: 0.7386 - val_loss: 1.0704 - val_acc: 0.5581\n",
            "Epoch 445/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7598 - acc: 0.7386 - val_loss: 1.0707 - val_acc: 0.5581\n",
            "Epoch 446/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7598 - acc: 0.7386 - val_loss: 1.0710 - val_acc: 0.5581\n",
            "Epoch 447/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7599 - acc: 0.7386 - val_loss: 1.0713 - val_acc: 0.5581\n",
            "Epoch 448/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7595 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 449/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.7593 - acc: 0.7386 - val_loss: 1.0701 - val_acc: 0.5581\n",
            "Epoch 450/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7595 - acc: 0.7386 - val_loss: 1.0709 - val_acc: 0.5581\n",
            "Epoch 451/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.7591 - acc: 0.7386 - val_loss: 1.0720 - val_acc: 0.5349\n",
            "Epoch 452/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.7597 - acc: 0.7500 - val_loss: 1.0716 - val_acc: 0.5581\n",
            "Epoch 453/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.7589 - acc: 0.7500 - val_loss: 1.0713 - val_acc: 0.5581\n",
            "Epoch 454/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.7589 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 455/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.7584 - acc: 0.7386 - val_loss: 1.0716 - val_acc: 0.5581\n",
            "Epoch 456/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.7585 - acc: 0.7500 - val_loss: 1.0715 - val_acc: 0.5349\n",
            "Epoch 457/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7586 - acc: 0.7500 - val_loss: 1.0707 - val_acc: 0.5581\n",
            "Epoch 458/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7581 - acc: 0.7386 - val_loss: 1.0705 - val_acc: 0.5581\n",
            "Epoch 459/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.7586 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 460/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7581 - acc: 0.7386 - val_loss: 1.0702 - val_acc: 0.5581\n",
            "Epoch 461/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7574 - acc: 0.7386 - val_loss: 1.0707 - val_acc: 0.5581\n",
            "Epoch 462/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.7569 - acc: 0.7386 - val_loss: 1.0706 - val_acc: 0.5581\n",
            "Epoch 463/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7572 - acc: 0.7386 - val_loss: 1.0708 - val_acc: 0.5581\n",
            "Epoch 464/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.7574 - acc: 0.7386 - val_loss: 1.0715 - val_acc: 0.5581\n",
            "Epoch 465/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7577 - acc: 0.7500 - val_loss: 1.0705 - val_acc: 0.5581\n",
            "Epoch 466/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7572 - acc: 0.7500 - val_loss: 1.0704 - val_acc: 0.5581\n",
            "Epoch 467/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7566 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 468/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.7569 - acc: 0.7386 - val_loss: 1.0712 - val_acc: 0.5581\n",
            "Epoch 469/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7561 - acc: 0.7386 - val_loss: 1.0720 - val_acc: 0.5581\n",
            "Epoch 470/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7569 - acc: 0.7386 - val_loss: 1.0709 - val_acc: 0.5581\n",
            "Epoch 471/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7561 - acc: 0.7500 - val_loss: 1.0706 - val_acc: 0.5581\n",
            "Epoch 472/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7565 - acc: 0.7386 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 473/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7560 - acc: 0.7500 - val_loss: 1.0713 - val_acc: 0.5581\n",
            "Epoch 474/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.7561 - acc: 0.7500 - val_loss: 1.0702 - val_acc: 0.5581\n",
            "Epoch 475/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.7560 - acc: 0.7386 - val_loss: 1.0710 - val_acc: 0.5581\n",
            "Epoch 476/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.7558 - acc: 0.7386 - val_loss: 1.0720 - val_acc: 0.5581\n",
            "Epoch 477/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.7558 - acc: 0.7500 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 478/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7552 - acc: 0.7386 - val_loss: 1.0707 - val_acc: 0.5581\n",
            "Epoch 479/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.7551 - acc: 0.7386 - val_loss: 1.0713 - val_acc: 0.5581\n",
            "Epoch 480/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.7552 - acc: 0.7386 - val_loss: 1.0716 - val_acc: 0.5581\n",
            "Epoch 481/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.7549 - acc: 0.7500 - val_loss: 1.0709 - val_acc: 0.5581\n",
            "Epoch 482/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7551 - acc: 0.7386 - val_loss: 1.0714 - val_acc: 0.5581\n",
            "Epoch 483/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7544 - acc: 0.7386 - val_loss: 1.0718 - val_acc: 0.5581\n",
            "Epoch 484/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.7546 - acc: 0.7500 - val_loss: 1.0724 - val_acc: 0.5581\n",
            "Epoch 485/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7544 - acc: 0.7386 - val_loss: 1.0719 - val_acc: 0.5581\n",
            "Epoch 486/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7555 - acc: 0.7386 - val_loss: 1.0714 - val_acc: 0.5581\n",
            "Epoch 487/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7541 - acc: 0.7500 - val_loss: 1.0720 - val_acc: 0.5581\n",
            "Epoch 488/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.7538 - acc: 0.7500 - val_loss: 1.0727 - val_acc: 0.5581\n",
            "Epoch 489/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7544 - acc: 0.7500 - val_loss: 1.0723 - val_acc: 0.5581\n",
            "Epoch 490/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.7541 - acc: 0.7500 - val_loss: 1.0711 - val_acc: 0.5581\n",
            "Epoch 491/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.7536 - acc: 0.7500 - val_loss: 1.0715 - val_acc: 0.5581\n",
            "Epoch 492/500\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.7535 - acc: 0.7500 - val_loss: 1.0712 - val_acc: 0.5581\n",
            "Epoch 493/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.7537 - acc: 0.7500 - val_loss: 1.0714 - val_acc: 0.5581\n",
            "Epoch 494/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7543 - acc: 0.7500 - val_loss: 1.0715 - val_acc: 0.5581\n",
            "Epoch 495/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7531 - acc: 0.7500 - val_loss: 1.0713 - val_acc: 0.5581\n",
            "Epoch 496/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7529 - acc: 0.7500 - val_loss: 1.0708 - val_acc: 0.5581\n",
            "Epoch 497/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.7529 - acc: 0.7500 - val_loss: 1.0713 - val_acc: 0.5581\n",
            "Epoch 498/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7530 - acc: 0.7500 - val_loss: 1.0722 - val_acc: 0.5581\n",
            "Epoch 499/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.7527 - acc: 0.7500 - val_loss: 1.0720 - val_acc: 0.5581\n",
            "Epoch 500/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7526 - acc: 0.7500 - val_loss: 1.0721 - val_acc: 0.5581\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "70c60a80-7081-4136-fd11-454b764a17d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "14f56ee6-4084-4d53-fd46-d5ad35438b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "857e9a06-3883-487e-9411-6a88c66a0c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "242b9cb3-9468-4ffd-bd8e-4bc762c2614e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'bo', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'b', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1bb3e3d3c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niKSlACEiImkpQIiISFoKECIiktb/B5MNY9P5fnPmAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "d0ea9dc2-77a9-4ad7-e909-a8b102d6bee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1bb3dbcfd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}
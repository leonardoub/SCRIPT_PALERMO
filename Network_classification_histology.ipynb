{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "fef88737-6f5e-4f83-c7bb-4eb3c2320e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "4f271172-a9df-490b-9231-e8254efa3bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "a1b206fc-eb96-4e23-9db5-b7786fadd5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "99becb64-8ac9-42da-eb76-4e31556465d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4NQA5c48ju7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "d32899cf-8b6a-4d04-f5cb-76f37945ddbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "bd71590d-ac12-4f6f-a88a-acf10722f076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "ca2cd8a4-4cf0-4bc7-9646-6a66ae92e601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "cc395232-08e9-4c79-be46-935a73ccfafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 2.2157 - acc: 0.3590 - val_loss: 1.6088 - val_acc: 0.5714\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 2.0363 - acc: 0.3675 - val_loss: 1.5092 - val_acc: 0.5714\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 252us/step - loss: 1.8865 - acc: 0.3590 - val_loss: 1.4209 - val_acc: 0.5714\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 1.7589 - acc: 0.3504 - val_loss: 1.3478 - val_acc: 0.5714\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 1.6482 - acc: 0.3675 - val_loss: 1.2824 - val_acc: 0.5714\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 1.5528 - acc: 0.3590 - val_loss: 1.2261 - val_acc: 0.5714\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 1.4695 - acc: 0.3590 - val_loss: 1.1762 - val_acc: 0.5714\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.3988 - acc: 0.3590 - val_loss: 1.1337 - val_acc: 0.5000\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 1.3387 - acc: 0.3590 - val_loss: 1.0975 - val_acc: 0.5000\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.2885 - acc: 0.3504 - val_loss: 1.0691 - val_acc: 0.5000\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 1.2465 - acc: 0.3846 - val_loss: 1.0484 - val_acc: 0.5000\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 1.2113 - acc: 0.3932 - val_loss: 1.0293 - val_acc: 0.5000\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.1811 - acc: 0.4017 - val_loss: 1.0154 - val_acc: 0.5000\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 1.1553 - acc: 0.4274 - val_loss: 1.0035 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.1326 - acc: 0.4359 - val_loss: 0.9947 - val_acc: 0.5000\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 1.1135 - acc: 0.4274 - val_loss: 0.9881 - val_acc: 0.6429\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.0971 - acc: 0.4530 - val_loss: 0.9839 - val_acc: 0.6429\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.0824 - acc: 0.4615 - val_loss: 0.9806 - val_acc: 0.6429\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 1.0705 - acc: 0.4530 - val_loss: 0.9789 - val_acc: 0.6429\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.0584 - acc: 0.4615 - val_loss: 0.9775 - val_acc: 0.7143\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 1.0488 - acc: 0.4701 - val_loss: 0.9780 - val_acc: 0.6429\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 1.0403 - acc: 0.4701 - val_loss: 0.9789 - val_acc: 0.6429\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 1.0329 - acc: 0.4615 - val_loss: 0.9804 - val_acc: 0.6429\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 1.0253 - acc: 0.4615 - val_loss: 0.9815 - val_acc: 0.6429\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 1.0188 - acc: 0.4615 - val_loss: 0.9848 - val_acc: 0.6429\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.0140 - acc: 0.4530 - val_loss: 0.9876 - val_acc: 0.6429\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 1.0087 - acc: 0.4615 - val_loss: 0.9918 - val_acc: 0.5714\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.0044 - acc: 0.4530 - val_loss: 0.9947 - val_acc: 0.5714\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 1.0009 - acc: 0.4530 - val_loss: 0.9977 - val_acc: 0.5714\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9968 - acc: 0.4530 - val_loss: 1.0016 - val_acc: 0.5714\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9936 - acc: 0.4444 - val_loss: 1.0051 - val_acc: 0.5714\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9902 - acc: 0.4530 - val_loss: 1.0086 - val_acc: 0.5714\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.9879 - acc: 0.4615 - val_loss: 1.0132 - val_acc: 0.5714\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9847 - acc: 0.4530 - val_loss: 1.0159 - val_acc: 0.5714\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9823 - acc: 0.4530 - val_loss: 1.0203 - val_acc: 0.5714\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9797 - acc: 0.4701 - val_loss: 1.0245 - val_acc: 0.5714\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.9781 - acc: 0.4786 - val_loss: 1.0282 - val_acc: 0.5714\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9762 - acc: 0.4786 - val_loss: 1.0318 - val_acc: 0.5714\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.9739 - acc: 0.4701 - val_loss: 1.0355 - val_acc: 0.5714\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.9725 - acc: 0.5043 - val_loss: 1.0393 - val_acc: 0.6429\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9696 - acc: 0.4957 - val_loss: 1.0441 - val_acc: 0.6429\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9673 - acc: 0.4957 - val_loss: 1.0485 - val_acc: 0.5714\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.9665 - acc: 0.5043 - val_loss: 1.0528 - val_acc: 0.5714\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9641 - acc: 0.4872 - val_loss: 1.0574 - val_acc: 0.5714\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9617 - acc: 0.5043 - val_loss: 1.0611 - val_acc: 0.5714\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9603 - acc: 0.5043 - val_loss: 1.0643 - val_acc: 0.5714\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.9581 - acc: 0.5214 - val_loss: 1.0678 - val_acc: 0.5714\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9566 - acc: 0.5214 - val_loss: 1.0721 - val_acc: 0.5714\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9550 - acc: 0.5214 - val_loss: 1.0749 - val_acc: 0.5714\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.9534 - acc: 0.5299 - val_loss: 1.0780 - val_acc: 0.5714\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.9526 - acc: 0.5385 - val_loss: 1.0811 - val_acc: 0.5714\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9516 - acc: 0.5385 - val_loss: 1.0850 - val_acc: 0.5714\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9499 - acc: 0.5470 - val_loss: 1.0877 - val_acc: 0.5714\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9490 - acc: 0.5470 - val_loss: 1.0912 - val_acc: 0.5714\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.9467 - acc: 0.5556 - val_loss: 1.0935 - val_acc: 0.5714\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.9458 - acc: 0.5470 - val_loss: 1.0967 - val_acc: 0.5714\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.9442 - acc: 0.5556 - val_loss: 1.0990 - val_acc: 0.5714\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9431 - acc: 0.5556 - val_loss: 1.1019 - val_acc: 0.5714\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.9423 - acc: 0.5556 - val_loss: 1.1037 - val_acc: 0.5714\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9423 - acc: 0.5470 - val_loss: 1.1061 - val_acc: 0.5714\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9400 - acc: 0.5556 - val_loss: 1.1079 - val_acc: 0.5714\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9393 - acc: 0.5556 - val_loss: 1.1104 - val_acc: 0.5714\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9385 - acc: 0.5556 - val_loss: 1.1133 - val_acc: 0.5000\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9378 - acc: 0.5470 - val_loss: 1.1150 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9366 - acc: 0.5556 - val_loss: 1.1178 - val_acc: 0.5000\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9355 - acc: 0.5470 - val_loss: 1.1200 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9352 - acc: 0.5556 - val_loss: 1.1213 - val_acc: 0.5000\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9338 - acc: 0.5641 - val_loss: 1.1227 - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.9330 - acc: 0.5641 - val_loss: 1.1255 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9319 - acc: 0.5726 - val_loss: 1.1262 - val_acc: 0.5000\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.9319 - acc: 0.5641 - val_loss: 1.1284 - val_acc: 0.4286\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9307 - acc: 0.5641 - val_loss: 1.1300 - val_acc: 0.4286\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9303 - acc: 0.5812 - val_loss: 1.1325 - val_acc: 0.4286\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9294 - acc: 0.5812 - val_loss: 1.1331 - val_acc: 0.4286\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.9291 - acc: 0.5812 - val_loss: 1.1355 - val_acc: 0.4286\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9274 - acc: 0.5726 - val_loss: 1.1369 - val_acc: 0.4286\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9272 - acc: 0.5726 - val_loss: 1.1391 - val_acc: 0.4286\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9265 - acc: 0.5641 - val_loss: 1.1414 - val_acc: 0.4286\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 231us/step - loss: 0.9259 - acc: 0.5726 - val_loss: 1.1429 - val_acc: 0.4286\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9265 - acc: 0.5812 - val_loss: 1.1444 - val_acc: 0.4286\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9249 - acc: 0.5812 - val_loss: 1.1456 - val_acc: 0.4286\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9242 - acc: 0.5812 - val_loss: 1.1466 - val_acc: 0.3571\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9233 - acc: 0.5812 - val_loss: 1.1486 - val_acc: 0.3571\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9224 - acc: 0.5897 - val_loss: 1.1497 - val_acc: 0.3571\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9220 - acc: 0.5812 - val_loss: 1.1513 - val_acc: 0.3571\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9212 - acc: 0.5812 - val_loss: 1.1528 - val_acc: 0.3571\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9215 - acc: 0.5726 - val_loss: 1.1529 - val_acc: 0.4286\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9209 - acc: 0.5812 - val_loss: 1.1541 - val_acc: 0.4286\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9198 - acc: 0.5726 - val_loss: 1.1563 - val_acc: 0.4286\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 269us/step - loss: 0.9188 - acc: 0.5726 - val_loss: 1.1570 - val_acc: 0.4286\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9183 - acc: 0.5726 - val_loss: 1.1578 - val_acc: 0.4286\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9178 - acc: 0.5726 - val_loss: 1.1593 - val_acc: 0.4286\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.9171 - acc: 0.5812 - val_loss: 1.1610 - val_acc: 0.4286\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9170 - acc: 0.5812 - val_loss: 1.1624 - val_acc: 0.4286\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9168 - acc: 0.5726 - val_loss: 1.1643 - val_acc: 0.4286\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.9159 - acc: 0.5726 - val_loss: 1.1667 - val_acc: 0.4286\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.9150 - acc: 0.5726 - val_loss: 1.1680 - val_acc: 0.4286\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9145 - acc: 0.5897 - val_loss: 1.1701 - val_acc: 0.4286\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9144 - acc: 0.5812 - val_loss: 1.1716 - val_acc: 0.4286\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9139 - acc: 0.5726 - val_loss: 1.1727 - val_acc: 0.4286\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 1ms/step - loss: 1.1898 - acc: 0.3051 - val_loss: 1.3644 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1560 - acc: 0.3644 - val_loss: 1.3898 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1328 - acc: 0.3983 - val_loss: 1.4146 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1138 - acc: 0.4153 - val_loss: 1.4391 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0970 - acc: 0.4153 - val_loss: 1.4574 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0864 - acc: 0.4576 - val_loss: 1.4706 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0755 - acc: 0.4746 - val_loss: 1.4773 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0676 - acc: 0.4746 - val_loss: 1.4869 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.0594 - acc: 0.4746 - val_loss: 1.4884 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0543 - acc: 0.4746 - val_loss: 1.4906 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0477 - acc: 0.4915 - val_loss: 1.4941 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0422 - acc: 0.4915 - val_loss: 1.5033 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0388 - acc: 0.4915 - val_loss: 1.5063 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0318 - acc: 0.5000 - val_loss: 1.5073 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0269 - acc: 0.5085 - val_loss: 1.5179 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0227 - acc: 0.5000 - val_loss: 1.5079 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0188 - acc: 0.5000 - val_loss: 1.5130 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0163 - acc: 0.5085 - val_loss: 1.5042 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0138 - acc: 0.5169 - val_loss: 1.5077 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0102 - acc: 0.5085 - val_loss: 1.4995 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0064 - acc: 0.5085 - val_loss: 1.5020 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0046 - acc: 0.5169 - val_loss: 1.4970 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0030 - acc: 0.5254 - val_loss: 1.4956 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9996 - acc: 0.5254 - val_loss: 1.4916 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9967 - acc: 0.5339 - val_loss: 1.4971 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9945 - acc: 0.5424 - val_loss: 1.5017 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9922 - acc: 0.5339 - val_loss: 1.4987 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9908 - acc: 0.5424 - val_loss: 1.5005 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9900 - acc: 0.5424 - val_loss: 1.4999 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9878 - acc: 0.5424 - val_loss: 1.5047 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9857 - acc: 0.5424 - val_loss: 1.5028 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9847 - acc: 0.5424 - val_loss: 1.5017 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9835 - acc: 0.5508 - val_loss: 1.4999 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9813 - acc: 0.5424 - val_loss: 1.4978 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9795 - acc: 0.5508 - val_loss: 1.5010 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9796 - acc: 0.5508 - val_loss: 1.5003 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9774 - acc: 0.5508 - val_loss: 1.4972 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9767 - acc: 0.5508 - val_loss: 1.4942 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9752 - acc: 0.5508 - val_loss: 1.5003 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9738 - acc: 0.5508 - val_loss: 1.4969 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9733 - acc: 0.5508 - val_loss: 1.4981 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9729 - acc: 0.5508 - val_loss: 1.4947 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9720 - acc: 0.5424 - val_loss: 1.4892 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9704 - acc: 0.5508 - val_loss: 1.4897 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9694 - acc: 0.5508 - val_loss: 1.4893 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9680 - acc: 0.5508 - val_loss: 1.4886 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9686 - acc: 0.5424 - val_loss: 1.4868 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9663 - acc: 0.5339 - val_loss: 1.4900 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9667 - acc: 0.5424 - val_loss: 1.4889 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9650 - acc: 0.5424 - val_loss: 1.4880 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9653 - acc: 0.5763 - val_loss: 1.4895 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9639 - acc: 0.5678 - val_loss: 1.4883 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9621 - acc: 0.5678 - val_loss: 1.4896 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9614 - acc: 0.5678 - val_loss: 1.4915 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9609 - acc: 0.5593 - val_loss: 1.4912 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9597 - acc: 0.5678 - val_loss: 1.4952 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9593 - acc: 0.5678 - val_loss: 1.4895 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9585 - acc: 0.5678 - val_loss: 1.5004 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9572 - acc: 0.5678 - val_loss: 1.4882 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9572 - acc: 0.5678 - val_loss: 1.4944 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9574 - acc: 0.5678 - val_loss: 1.4968 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9555 - acc: 0.5678 - val_loss: 1.4992 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9544 - acc: 0.5678 - val_loss: 1.5034 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9546 - acc: 0.5678 - val_loss: 1.5093 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9540 - acc: 0.5678 - val_loss: 1.5041 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9535 - acc: 0.5678 - val_loss: 1.5065 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9525 - acc: 0.5593 - val_loss: 1.5081 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9514 - acc: 0.5593 - val_loss: 1.5072 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9512 - acc: 0.5678 - val_loss: 1.5068 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9508 - acc: 0.5678 - val_loss: 1.5098 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9495 - acc: 0.5593 - val_loss: 1.5148 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9495 - acc: 0.5678 - val_loss: 1.5124 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9491 - acc: 0.5593 - val_loss: 1.5168 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9491 - acc: 0.5508 - val_loss: 1.5124 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9471 - acc: 0.5678 - val_loss: 1.5202 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9475 - acc: 0.5593 - val_loss: 1.5206 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9463 - acc: 0.5678 - val_loss: 1.5213 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9463 - acc: 0.5678 - val_loss: 1.5222 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9452 - acc: 0.5678 - val_loss: 1.5227 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9449 - acc: 0.5678 - val_loss: 1.5220 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9441 - acc: 0.5678 - val_loss: 1.5187 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9441 - acc: 0.5763 - val_loss: 1.5230 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9430 - acc: 0.5678 - val_loss: 1.5240 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9419 - acc: 0.5678 - val_loss: 1.5196 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9415 - acc: 0.5678 - val_loss: 1.5223 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9413 - acc: 0.5763 - val_loss: 1.5249 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9418 - acc: 0.5763 - val_loss: 1.5272 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9405 - acc: 0.5678 - val_loss: 1.5241 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9397 - acc: 0.5763 - val_loss: 1.5237 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9396 - acc: 0.5763 - val_loss: 1.5312 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9383 - acc: 0.5763 - val_loss: 1.5261 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9389 - acc: 0.5763 - val_loss: 1.5235 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9382 - acc: 0.5763 - val_loss: 1.5118 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9377 - acc: 0.5678 - val_loss: 1.5151 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9383 - acc: 0.5763 - val_loss: 1.5133 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9369 - acc: 0.5763 - val_loss: 1.5179 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9370 - acc: 0.5763 - val_loss: 1.5175 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9364 - acc: 0.5763 - val_loss: 1.5190 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9364 - acc: 0.5763 - val_loss: 1.5156 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9358 - acc: 0.5763 - val_loss: 1.5175 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.8329 - acc: 0.2712 - val_loss: 2.2177 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 2.6132 - acc: 0.2627 - val_loss: 2.0349 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 2.4263 - acc: 0.2881 - val_loss: 1.8876 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 2.2708 - acc: 0.2966 - val_loss: 1.7552 - val_acc: 0.1538\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 2.1216 - acc: 0.3051 - val_loss: 1.6559 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.9986 - acc: 0.3305 - val_loss: 1.5693 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.8933 - acc: 0.3644 - val_loss: 1.4969 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.7895 - acc: 0.3729 - val_loss: 1.4429 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.6829 - acc: 0.4068 - val_loss: 1.4020 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.5899 - acc: 0.4237 - val_loss: 1.3741 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.5136 - acc: 0.4492 - val_loss: 1.3545 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.4453 - acc: 0.4576 - val_loss: 1.3355 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.3856 - acc: 0.4407 - val_loss: 1.3226 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.3332 - acc: 0.4322 - val_loss: 1.3126 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.2851 - acc: 0.4322 - val_loss: 1.3061 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2430 - acc: 0.4492 - val_loss: 1.2982 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.2027 - acc: 0.4661 - val_loss: 1.2946 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1692 - acc: 0.4661 - val_loss: 1.2906 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.1394 - acc: 0.4746 - val_loss: 1.2855 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1140 - acc: 0.4661 - val_loss: 1.2831 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0892 - acc: 0.4661 - val_loss: 1.2833 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0696 - acc: 0.4661 - val_loss: 1.2833 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0524 - acc: 0.4831 - val_loss: 1.2810 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0359 - acc: 0.5085 - val_loss: 1.2815 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0236 - acc: 0.5000 - val_loss: 1.2807 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0131 - acc: 0.5085 - val_loss: 1.2796 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0044 - acc: 0.5169 - val_loss: 1.2787 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9968 - acc: 0.5254 - val_loss: 1.2778 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9900 - acc: 0.5169 - val_loss: 1.2781 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9840 - acc: 0.5169 - val_loss: 1.2763 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9783 - acc: 0.5169 - val_loss: 1.2764 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9736 - acc: 0.5339 - val_loss: 1.2759 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9694 - acc: 0.5424 - val_loss: 1.2759 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9660 - acc: 0.5508 - val_loss: 1.2742 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9629 - acc: 0.5508 - val_loss: 1.2745 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9588 - acc: 0.5339 - val_loss: 1.2732 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9554 - acc: 0.5424 - val_loss: 1.2729 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9525 - acc: 0.5508 - val_loss: 1.2719 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9503 - acc: 0.5508 - val_loss: 1.2718 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9473 - acc: 0.5593 - val_loss: 1.2712 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9444 - acc: 0.5424 - val_loss: 1.2709 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9419 - acc: 0.5508 - val_loss: 1.2695 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9393 - acc: 0.5508 - val_loss: 1.2702 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9371 - acc: 0.5678 - val_loss: 1.2695 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9355 - acc: 0.5847 - val_loss: 1.2684 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9336 - acc: 0.5932 - val_loss: 1.2674 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9325 - acc: 0.5932 - val_loss: 1.2665 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9298 - acc: 0.5932 - val_loss: 1.2667 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9285 - acc: 0.5932 - val_loss: 1.2662 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9268 - acc: 0.5932 - val_loss: 1.2652 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9254 - acc: 0.6102 - val_loss: 1.2658 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9245 - acc: 0.6186 - val_loss: 1.2655 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9225 - acc: 0.6271 - val_loss: 1.2652 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9214 - acc: 0.6186 - val_loss: 1.2645 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9197 - acc: 0.6271 - val_loss: 1.2645 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9188 - acc: 0.6271 - val_loss: 1.2642 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9176 - acc: 0.6271 - val_loss: 1.2645 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9172 - acc: 0.6186 - val_loss: 1.2642 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 0.9160 - acc: 0.6186 - val_loss: 1.2652 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9152 - acc: 0.6271 - val_loss: 1.2652 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9132 - acc: 0.6271 - val_loss: 1.2656 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9125 - acc: 0.6356 - val_loss: 1.2654 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9119 - acc: 0.6186 - val_loss: 1.2659 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9102 - acc: 0.6271 - val_loss: 1.2669 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9095 - acc: 0.6102 - val_loss: 1.2673 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9081 - acc: 0.6102 - val_loss: 1.2671 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9076 - acc: 0.6186 - val_loss: 1.2680 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9067 - acc: 0.6271 - val_loss: 1.2680 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9055 - acc: 0.6271 - val_loss: 1.2687 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9052 - acc: 0.6271 - val_loss: 1.2691 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9038 - acc: 0.6271 - val_loss: 1.2714 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9032 - acc: 0.6271 - val_loss: 1.2705 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9030 - acc: 0.6271 - val_loss: 1.2717 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9022 - acc: 0.6271 - val_loss: 1.2717 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9009 - acc: 0.6441 - val_loss: 1.2728 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9004 - acc: 0.6271 - val_loss: 1.2744 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8995 - acc: 0.6356 - val_loss: 1.2751 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8984 - acc: 0.6356 - val_loss: 1.2763 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8985 - acc: 0.6356 - val_loss: 1.2780 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8970 - acc: 0.6441 - val_loss: 1.2791 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.8966 - acc: 0.6525 - val_loss: 1.2803 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.8958 - acc: 0.6441 - val_loss: 1.2807 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8953 - acc: 0.6441 - val_loss: 1.2805 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8937 - acc: 0.6525 - val_loss: 1.2817 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8936 - acc: 0.6525 - val_loss: 1.2813 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.8930 - acc: 0.6525 - val_loss: 1.2827 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8924 - acc: 0.6441 - val_loss: 1.2833 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8912 - acc: 0.6525 - val_loss: 1.2837 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.8911 - acc: 0.6525 - val_loss: 1.2849 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8899 - acc: 0.6525 - val_loss: 1.2852 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8898 - acc: 0.6525 - val_loss: 1.2848 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.8894 - acc: 0.6525 - val_loss: 1.2850 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.8893 - acc: 0.6441 - val_loss: 1.2848 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 0.8879 - acc: 0.6525 - val_loss: 1.2852 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.8871 - acc: 0.6441 - val_loss: 1.2864 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8870 - acc: 0.6441 - val_loss: 1.2865 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.8858 - acc: 0.6441 - val_loss: 1.2870 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8854 - acc: 0.6525 - val_loss: 1.2882 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8848 - acc: 0.6525 - val_loss: 1.2892 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8844 - acc: 0.6525 - val_loss: 1.2904 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 3.0650 - acc: 0.3644 - val_loss: 3.0041 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 2.7974 - acc: 0.3644 - val_loss: 2.8555 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 2.5686 - acc: 0.3644 - val_loss: 2.7367 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 2.3700 - acc: 0.3729 - val_loss: 2.6339 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.2024 - acc: 0.3814 - val_loss: 2.5491 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.0574 - acc: 0.3814 - val_loss: 2.4778 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.9211 - acc: 0.3898 - val_loss: 2.4243 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.8070 - acc: 0.4068 - val_loss: 2.3411 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.7106 - acc: 0.4153 - val_loss: 2.2546 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.6265 - acc: 0.4153 - val_loss: 2.1803 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.5525 - acc: 0.4153 - val_loss: 2.1131 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.4881 - acc: 0.4492 - val_loss: 2.0563 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.4340 - acc: 0.4492 - val_loss: 2.0022 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 1.3850 - acc: 0.4661 - val_loss: 1.9567 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.3421 - acc: 0.4746 - val_loss: 1.9147 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3050 - acc: 0.4746 - val_loss: 1.8744 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.2722 - acc: 0.4661 - val_loss: 1.8368 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.2416 - acc: 0.4831 - val_loss: 1.8018 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.2156 - acc: 0.4831 - val_loss: 1.7702 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1894 - acc: 0.4831 - val_loss: 1.7379 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.1679 - acc: 0.4746 - val_loss: 1.7101 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1483 - acc: 0.4831 - val_loss: 1.6831 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1312 - acc: 0.4746 - val_loss: 1.6604 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.1154 - acc: 0.4831 - val_loss: 1.6356 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1005 - acc: 0.4831 - val_loss: 1.6172 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0879 - acc: 0.4831 - val_loss: 1.5973 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0755 - acc: 0.4831 - val_loss: 1.5706 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0644 - acc: 0.4915 - val_loss: 1.5528 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0556 - acc: 0.5000 - val_loss: 1.5347 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0470 - acc: 0.5085 - val_loss: 1.5217 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0390 - acc: 0.5085 - val_loss: 1.5034 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.0310 - acc: 0.5254 - val_loss: 1.4836 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0244 - acc: 0.5254 - val_loss: 1.4699 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0187 - acc: 0.5339 - val_loss: 1.4582 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0132 - acc: 0.5508 - val_loss: 1.4469 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0087 - acc: 0.5593 - val_loss: 1.4367 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0048 - acc: 0.5424 - val_loss: 1.4272 - val_acc: 0.3077\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0009 - acc: 0.5593 - val_loss: 1.4187 - val_acc: 0.3077\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9973 - acc: 0.5424 - val_loss: 1.4105 - val_acc: 0.3077\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9945 - acc: 0.5508 - val_loss: 1.4022 - val_acc: 0.3077\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9910 - acc: 0.5508 - val_loss: 1.3934 - val_acc: 0.3077\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9879 - acc: 0.5593 - val_loss: 1.3858 - val_acc: 0.3077\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9855 - acc: 0.5593 - val_loss: 1.3798 - val_acc: 0.3077\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9825 - acc: 0.5593 - val_loss: 1.3741 - val_acc: 0.3077\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9801 - acc: 0.5593 - val_loss: 1.3648 - val_acc: 0.3077\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9784 - acc: 0.5678 - val_loss: 1.3578 - val_acc: 0.3077\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9758 - acc: 0.5593 - val_loss: 1.3524 - val_acc: 0.3077\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9740 - acc: 0.5508 - val_loss: 1.3476 - val_acc: 0.3077\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9723 - acc: 0.5508 - val_loss: 1.3424 - val_acc: 0.3077\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9704 - acc: 0.5424 - val_loss: 1.3379 - val_acc: 0.3077\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9685 - acc: 0.5424 - val_loss: 1.3354 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9671 - acc: 0.5424 - val_loss: 1.3296 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9653 - acc: 0.5424 - val_loss: 1.3233 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9642 - acc: 0.5508 - val_loss: 1.3197 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9620 - acc: 0.5339 - val_loss: 1.3158 - val_acc: 0.3077\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9608 - acc: 0.5339 - val_loss: 1.3134 - val_acc: 0.3077\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9595 - acc: 0.5424 - val_loss: 1.3093 - val_acc: 0.3077\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9578 - acc: 0.5424 - val_loss: 1.3043 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9568 - acc: 0.5424 - val_loss: 1.3012 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9556 - acc: 0.5424 - val_loss: 1.2989 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9544 - acc: 0.5424 - val_loss: 1.2950 - val_acc: 0.3077\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9534 - acc: 0.5339 - val_loss: 1.2932 - val_acc: 0.3077\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9520 - acc: 0.5424 - val_loss: 1.2908 - val_acc: 0.3077\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9511 - acc: 0.5339 - val_loss: 1.2875 - val_acc: 0.3077\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9501 - acc: 0.5339 - val_loss: 1.2863 - val_acc: 0.3077\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9490 - acc: 0.5339 - val_loss: 1.2834 - val_acc: 0.3077\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9488 - acc: 0.5254 - val_loss: 1.2816 - val_acc: 0.3077\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9471 - acc: 0.5254 - val_loss: 1.2797 - val_acc: 0.3077\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9464 - acc: 0.5339 - val_loss: 1.2778 - val_acc: 0.3077\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9451 - acc: 0.5339 - val_loss: 1.2758 - val_acc: 0.3077\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9447 - acc: 0.5424 - val_loss: 1.2738 - val_acc: 0.3077\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9438 - acc: 0.5339 - val_loss: 1.2724 - val_acc: 0.3077\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9427 - acc: 0.5339 - val_loss: 1.2686 - val_acc: 0.3077\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9423 - acc: 0.5339 - val_loss: 1.2676 - val_acc: 0.3077\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9411 - acc: 0.5339 - val_loss: 1.2659 - val_acc: 0.3077\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9407 - acc: 0.5424 - val_loss: 1.2642 - val_acc: 0.3077\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9404 - acc: 0.5339 - val_loss: 1.2617 - val_acc: 0.3077\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9391 - acc: 0.5339 - val_loss: 1.2604 - val_acc: 0.3077\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9388 - acc: 0.5339 - val_loss: 1.2590 - val_acc: 0.3077\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9380 - acc: 0.5339 - val_loss: 1.2566 - val_acc: 0.3077\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9373 - acc: 0.5339 - val_loss: 1.2556 - val_acc: 0.3077\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9370 - acc: 0.5339 - val_loss: 1.2526 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9364 - acc: 0.5339 - val_loss: 1.2523 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9359 - acc: 0.5339 - val_loss: 1.2509 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9352 - acc: 0.5339 - val_loss: 1.2490 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9345 - acc: 0.5339 - val_loss: 1.2482 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9336 - acc: 0.5339 - val_loss: 1.2470 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9332 - acc: 0.5424 - val_loss: 1.2470 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9331 - acc: 0.5508 - val_loss: 1.2459 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9322 - acc: 0.5424 - val_loss: 1.2449 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9312 - acc: 0.5508 - val_loss: 1.2448 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9309 - acc: 0.5424 - val_loss: 1.2431 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9301 - acc: 0.5508 - val_loss: 1.2428 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9297 - acc: 0.5508 - val_loss: 1.2426 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9292 - acc: 0.5593 - val_loss: 1.2412 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9286 - acc: 0.5508 - val_loss: 1.2410 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9280 - acc: 0.5508 - val_loss: 1.2405 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9274 - acc: 0.5593 - val_loss: 1.2400 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9267 - acc: 0.5593 - val_loss: 1.2388 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9262 - acc: 0.5593 - val_loss: 1.2380 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.6941 - acc: 0.3898 - val_loss: 1.9680 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.6147 - acc: 0.4237 - val_loss: 1.8584 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.5445 - acc: 0.4407 - val_loss: 1.7663 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.4875 - acc: 0.4407 - val_loss: 1.6768 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.4410 - acc: 0.4492 - val_loss: 1.6007 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.4037 - acc: 0.4492 - val_loss: 1.5439 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.3719 - acc: 0.4407 - val_loss: 1.4916 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3397 - acc: 0.4407 - val_loss: 1.4410 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3159 - acc: 0.4576 - val_loss: 1.4043 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2935 - acc: 0.4661 - val_loss: 1.3648 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.2755 - acc: 0.4661 - val_loss: 1.3338 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.2567 - acc: 0.4746 - val_loss: 1.3095 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.2405 - acc: 0.4746 - val_loss: 1.2833 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2259 - acc: 0.4746 - val_loss: 1.2700 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2130 - acc: 0.4746 - val_loss: 1.2422 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1981 - acc: 0.4746 - val_loss: 1.2347 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1858 - acc: 0.4831 - val_loss: 1.2232 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1748 - acc: 0.4915 - val_loss: 1.2066 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1637 - acc: 0.5000 - val_loss: 1.1959 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.1546 - acc: 0.5000 - val_loss: 1.1851 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1444 - acc: 0.5000 - val_loss: 1.1735 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1353 - acc: 0.5000 - val_loss: 1.1669 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1273 - acc: 0.5000 - val_loss: 1.1587 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1188 - acc: 0.4915 - val_loss: 1.1485 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1115 - acc: 0.4831 - val_loss: 1.1416 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.1033 - acc: 0.4831 - val_loss: 1.1396 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0962 - acc: 0.4831 - val_loss: 1.1290 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0896 - acc: 0.4831 - val_loss: 1.1245 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0835 - acc: 0.4915 - val_loss: 1.1182 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0772 - acc: 0.4915 - val_loss: 1.1118 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0729 - acc: 0.4915 - val_loss: 1.1066 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0658 - acc: 0.4915 - val_loss: 1.0987 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0620 - acc: 0.4915 - val_loss: 1.0969 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0564 - acc: 0.5000 - val_loss: 1.0910 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0515 - acc: 0.5000 - val_loss: 1.0882 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0471 - acc: 0.5000 - val_loss: 1.0816 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0429 - acc: 0.5000 - val_loss: 1.0790 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0397 - acc: 0.4661 - val_loss: 1.0763 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0357 - acc: 0.4661 - val_loss: 1.0723 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0325 - acc: 0.4576 - val_loss: 1.0674 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0284 - acc: 0.4915 - val_loss: 1.0670 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0261 - acc: 0.4915 - val_loss: 1.0626 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0234 - acc: 0.5000 - val_loss: 1.0589 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0194 - acc: 0.5000 - val_loss: 1.0551 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0166 - acc: 0.4915 - val_loss: 1.0525 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0154 - acc: 0.4915 - val_loss: 1.0485 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0118 - acc: 0.5000 - val_loss: 1.0448 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0107 - acc: 0.4915 - val_loss: 1.0443 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0073 - acc: 0.5000 - val_loss: 1.0413 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0058 - acc: 0.5000 - val_loss: 1.0369 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0040 - acc: 0.5000 - val_loss: 1.0364 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0017 - acc: 0.5000 - val_loss: 1.0324 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9998 - acc: 0.5000 - val_loss: 1.0307 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9981 - acc: 0.5085 - val_loss: 1.0276 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9956 - acc: 0.5169 - val_loss: 1.0252 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9943 - acc: 0.5169 - val_loss: 1.0235 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9930 - acc: 0.5169 - val_loss: 1.0217 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9921 - acc: 0.5169 - val_loss: 1.0209 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9907 - acc: 0.5169 - val_loss: 1.0187 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9883 - acc: 0.5169 - val_loss: 1.0172 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9883 - acc: 0.5169 - val_loss: 1.0150 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9863 - acc: 0.5169 - val_loss: 1.0132 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9846 - acc: 0.5254 - val_loss: 1.0107 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9836 - acc: 0.5339 - val_loss: 1.0090 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9826 - acc: 0.5339 - val_loss: 1.0076 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9815 - acc: 0.5339 - val_loss: 1.0073 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9808 - acc: 0.5339 - val_loss: 1.0050 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9795 - acc: 0.5339 - val_loss: 1.0035 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9779 - acc: 0.5339 - val_loss: 1.0027 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9771 - acc: 0.5339 - val_loss: 1.0013 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9768 - acc: 0.5339 - val_loss: 0.9991 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 315us/step - loss: 0.9753 - acc: 0.5339 - val_loss: 0.9966 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9740 - acc: 0.5339 - val_loss: 0.9960 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9734 - acc: 0.5339 - val_loss: 0.9942 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9723 - acc: 0.5254 - val_loss: 0.9926 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9711 - acc: 0.5339 - val_loss: 0.9916 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9709 - acc: 0.5254 - val_loss: 0.9903 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9702 - acc: 0.5339 - val_loss: 0.9895 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9698 - acc: 0.5339 - val_loss: 0.9871 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9676 - acc: 0.5254 - val_loss: 0.9861 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9677 - acc: 0.5339 - val_loss: 0.9845 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9665 - acc: 0.5339 - val_loss: 0.9834 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9659 - acc: 0.5424 - val_loss: 0.9823 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9655 - acc: 0.5424 - val_loss: 0.9823 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9646 - acc: 0.5424 - val_loss: 0.9806 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9641 - acc: 0.5424 - val_loss: 0.9807 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9628 - acc: 0.5339 - val_loss: 0.9788 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9620 - acc: 0.5254 - val_loss: 0.9788 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9618 - acc: 0.5339 - val_loss: 0.9778 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9610 - acc: 0.5339 - val_loss: 0.9774 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9604 - acc: 0.5339 - val_loss: 0.9767 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9599 - acc: 0.5339 - val_loss: 0.9765 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9593 - acc: 0.5339 - val_loss: 0.9754 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9589 - acc: 0.5339 - val_loss: 0.9746 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9578 - acc: 0.5339 - val_loss: 0.9735 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9573 - acc: 0.5339 - val_loss: 0.9730 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9571 - acc: 0.5339 - val_loss: 0.9723 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9562 - acc: 0.5339 - val_loss: 0.9713 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9561 - acc: 0.5254 - val_loss: 0.9707 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9552 - acc: 0.5339 - val_loss: 0.9695 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.0367 - acc: 0.4237 - val_loss: 1.1117 - val_acc: 0.5385\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.9307 - acc: 0.4322 - val_loss: 1.0639 - val_acc: 0.6154\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.8378 - acc: 0.4322 - val_loss: 1.0394 - val_acc: 0.6154\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.7552 - acc: 0.4407 - val_loss: 1.0274 - val_acc: 0.6154\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.6878 - acc: 0.4492 - val_loss: 1.0187 - val_acc: 0.6154\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.6309 - acc: 0.4576 - val_loss: 1.0218 - val_acc: 0.6154\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.5837 - acc: 0.4661 - val_loss: 1.0218 - val_acc: 0.6154\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.5453 - acc: 0.4915 - val_loss: 1.0197 - val_acc: 0.6154\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.5088 - acc: 0.4746 - val_loss: 1.0216 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.4800 - acc: 0.4746 - val_loss: 1.0221 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.4542 - acc: 0.4915 - val_loss: 1.0186 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.4306 - acc: 0.4831 - val_loss: 1.0130 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.4057 - acc: 0.4661 - val_loss: 1.0124 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.3836 - acc: 0.4576 - val_loss: 1.0090 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.3527 - acc: 0.4831 - val_loss: 1.0035 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.3257 - acc: 0.4831 - val_loss: 1.0011 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.3044 - acc: 0.4746 - val_loss: 0.9963 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2800 - acc: 0.4746 - val_loss: 0.9892 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.2602 - acc: 0.4576 - val_loss: 0.9828 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.2414 - acc: 0.4661 - val_loss: 0.9766 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2233 - acc: 0.4831 - val_loss: 0.9719 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2060 - acc: 0.4831 - val_loss: 0.9662 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 1.1908 - acc: 0.4831 - val_loss: 0.9613 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.1762 - acc: 0.4915 - val_loss: 0.9549 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.1623 - acc: 0.5085 - val_loss: 0.9526 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1494 - acc: 0.4915 - val_loss: 0.9483 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1374 - acc: 0.4915 - val_loss: 0.9476 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1264 - acc: 0.4915 - val_loss: 0.9462 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1157 - acc: 0.4915 - val_loss: 0.9442 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.1061 - acc: 0.5000 - val_loss: 0.9445 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0967 - acc: 0.4915 - val_loss: 0.9405 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0877 - acc: 0.5000 - val_loss: 0.9382 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0790 - acc: 0.5085 - val_loss: 0.9365 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0710 - acc: 0.5085 - val_loss: 0.9365 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0633 - acc: 0.5000 - val_loss: 0.9354 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0555 - acc: 0.4915 - val_loss: 0.9347 - val_acc: 0.6154\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0494 - acc: 0.4915 - val_loss: 0.9346 - val_acc: 0.6154\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0430 - acc: 0.5000 - val_loss: 0.9333 - val_acc: 0.6154\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0376 - acc: 0.5000 - val_loss: 0.9342 - val_acc: 0.6154\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0319 - acc: 0.5000 - val_loss: 0.9344 - val_acc: 0.6154\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0277 - acc: 0.5000 - val_loss: 0.9332 - val_acc: 0.6154\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0229 - acc: 0.5169 - val_loss: 0.9334 - val_acc: 0.6154\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0178 - acc: 0.5085 - val_loss: 0.9348 - val_acc: 0.6154\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0133 - acc: 0.5085 - val_loss: 0.9341 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0094 - acc: 0.5169 - val_loss: 0.9325 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0052 - acc: 0.5169 - val_loss: 0.9337 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0026 - acc: 0.5000 - val_loss: 0.9337 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9993 - acc: 0.5000 - val_loss: 0.9334 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9950 - acc: 0.5000 - val_loss: 0.9342 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9935 - acc: 0.4831 - val_loss: 0.9351 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9894 - acc: 0.5000 - val_loss: 0.9341 - val_acc: 0.6923\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9864 - acc: 0.5169 - val_loss: 0.9344 - val_acc: 0.6923\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9846 - acc: 0.5085 - val_loss: 0.9342 - val_acc: 0.6923\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9818 - acc: 0.5085 - val_loss: 0.9330 - val_acc: 0.6923\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9794 - acc: 0.5085 - val_loss: 0.9340 - val_acc: 0.7692\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9778 - acc: 0.5085 - val_loss: 0.9329 - val_acc: 0.7692\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9750 - acc: 0.5169 - val_loss: 0.9333 - val_acc: 0.7692\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9729 - acc: 0.5085 - val_loss: 0.9344 - val_acc: 0.7692\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9712 - acc: 0.5000 - val_loss: 0.9344 - val_acc: 0.7692\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9692 - acc: 0.5085 - val_loss: 0.9338 - val_acc: 0.7692\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9675 - acc: 0.5000 - val_loss: 0.9336 - val_acc: 0.7692\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9669 - acc: 0.5000 - val_loss: 0.9334 - val_acc: 0.7692\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9642 - acc: 0.5169 - val_loss: 0.9337 - val_acc: 0.7692\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9631 - acc: 0.5085 - val_loss: 0.9333 - val_acc: 0.7692\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9614 - acc: 0.5254 - val_loss: 0.9332 - val_acc: 0.7692\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9600 - acc: 0.5000 - val_loss: 0.9347 - val_acc: 0.7692\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9586 - acc: 0.5254 - val_loss: 0.9334 - val_acc: 0.7692\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9578 - acc: 0.5254 - val_loss: 0.9334 - val_acc: 0.7692\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9563 - acc: 0.5254 - val_loss: 0.9340 - val_acc: 0.7692\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9551 - acc: 0.5254 - val_loss: 0.9320 - val_acc: 0.7692\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9536 - acc: 0.5254 - val_loss: 0.9328 - val_acc: 0.7692\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9525 - acc: 0.5254 - val_loss: 0.9330 - val_acc: 0.7692\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9514 - acc: 0.5254 - val_loss: 0.9323 - val_acc: 0.7692\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9507 - acc: 0.5254 - val_loss: 0.9315 - val_acc: 0.7692\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9497 - acc: 0.5169 - val_loss: 0.9327 - val_acc: 0.7692\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9486 - acc: 0.5339 - val_loss: 0.9311 - val_acc: 0.7692\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9477 - acc: 0.5339 - val_loss: 0.9303 - val_acc: 0.7692\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9470 - acc: 0.5508 - val_loss: 0.9306 - val_acc: 0.7692\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9466 - acc: 0.5424 - val_loss: 0.9300 - val_acc: 0.7692\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9454 - acc: 0.5508 - val_loss: 0.9295 - val_acc: 0.7692\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9442 - acc: 0.5508 - val_loss: 0.9295 - val_acc: 0.7692\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9437 - acc: 0.5508 - val_loss: 0.9291 - val_acc: 0.7692\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9433 - acc: 0.5424 - val_loss: 0.9293 - val_acc: 0.7692\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9424 - acc: 0.5424 - val_loss: 0.9288 - val_acc: 0.7692\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9410 - acc: 0.5508 - val_loss: 0.9278 - val_acc: 0.7692\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9404 - acc: 0.5424 - val_loss: 0.9281 - val_acc: 0.7692\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9402 - acc: 0.5424 - val_loss: 0.9279 - val_acc: 0.7692\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9396 - acc: 0.5424 - val_loss: 0.9289 - val_acc: 0.7692\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9384 - acc: 0.5593 - val_loss: 0.9278 - val_acc: 0.7692\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9373 - acc: 0.5508 - val_loss: 0.9284 - val_acc: 0.6923\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9372 - acc: 0.5593 - val_loss: 0.9291 - val_acc: 0.6923\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9357 - acc: 0.5593 - val_loss: 0.9289 - val_acc: 0.6923\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9355 - acc: 0.5593 - val_loss: 0.9279 - val_acc: 0.6923\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9347 - acc: 0.5593 - val_loss: 0.9267 - val_acc: 0.6923\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9340 - acc: 0.5593 - val_loss: 0.9266 - val_acc: 0.6923\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9330 - acc: 0.5593 - val_loss: 0.9268 - val_acc: 0.6923\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9322 - acc: 0.5593 - val_loss: 0.9262 - val_acc: 0.6923\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9319 - acc: 0.5593 - val_loss: 0.9255 - val_acc: 0.6923\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9311 - acc: 0.5593 - val_loss: 0.9242 - val_acc: 0.6923\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9306 - acc: 0.5593 - val_loss: 0.9238 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 3.0531 - acc: 0.2288 - val_loss: 2.7569 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 2.6867 - acc: 0.2034 - val_loss: 2.4479 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.4104 - acc: 0.2034 - val_loss: 2.2103 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 2.1971 - acc: 0.2288 - val_loss: 2.0201 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 2.0261 - acc: 0.2373 - val_loss: 1.8737 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.8851 - acc: 0.2373 - val_loss: 1.7549 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.7666 - acc: 0.2458 - val_loss: 1.6454 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.6702 - acc: 0.2712 - val_loss: 1.5604 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.5850 - acc: 0.2881 - val_loss: 1.4876 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.5116 - acc: 0.2966 - val_loss: 1.4261 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.4479 - acc: 0.2966 - val_loss: 1.3750 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.3923 - acc: 0.2966 - val_loss: 1.3314 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.3449 - acc: 0.2966 - val_loss: 1.2927 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.3038 - acc: 0.3136 - val_loss: 1.2619 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.2683 - acc: 0.3136 - val_loss: 1.2358 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2370 - acc: 0.2966 - val_loss: 1.2101 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.2102 - acc: 0.3136 - val_loss: 1.1908 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1874 - acc: 0.3220 - val_loss: 1.1714 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1671 - acc: 0.3390 - val_loss: 1.1581 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1488 - acc: 0.3729 - val_loss: 1.1438 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1330 - acc: 0.3729 - val_loss: 1.1329 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1202 - acc: 0.3644 - val_loss: 1.1222 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1085 - acc: 0.3644 - val_loss: 1.1131 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0972 - acc: 0.3559 - val_loss: 1.1043 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0873 - acc: 0.3559 - val_loss: 1.0970 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0786 - acc: 0.3644 - val_loss: 1.0900 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0709 - acc: 0.3644 - val_loss: 1.0847 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0635 - acc: 0.3814 - val_loss: 1.0788 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0571 - acc: 0.3814 - val_loss: 1.0762 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0509 - acc: 0.3898 - val_loss: 1.0715 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0456 - acc: 0.4153 - val_loss: 1.0680 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0402 - acc: 0.4068 - val_loss: 1.0645 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0353 - acc: 0.4153 - val_loss: 1.0619 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0313 - acc: 0.4153 - val_loss: 1.0588 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0272 - acc: 0.4153 - val_loss: 1.0561 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0240 - acc: 0.4322 - val_loss: 1.0530 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0207 - acc: 0.4322 - val_loss: 1.0515 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0173 - acc: 0.4407 - val_loss: 1.0490 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0139 - acc: 0.4492 - val_loss: 1.0472 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0108 - acc: 0.4576 - val_loss: 1.0451 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0081 - acc: 0.4576 - val_loss: 1.0432 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0054 - acc: 0.4576 - val_loss: 1.0422 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0028 - acc: 0.4576 - val_loss: 1.0399 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0011 - acc: 0.4576 - val_loss: 1.0384 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9977 - acc: 0.4746 - val_loss: 1.0368 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9954 - acc: 0.4661 - val_loss: 1.0361 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9929 - acc: 0.4746 - val_loss: 1.0352 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9902 - acc: 0.4746 - val_loss: 1.0346 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9887 - acc: 0.4746 - val_loss: 1.0338 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9855 - acc: 0.4746 - val_loss: 1.0321 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9840 - acc: 0.4746 - val_loss: 1.0317 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9813 - acc: 0.4746 - val_loss: 1.0311 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9790 - acc: 0.4746 - val_loss: 1.0302 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9774 - acc: 0.4746 - val_loss: 1.0298 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9751 - acc: 0.4746 - val_loss: 1.0287 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9735 - acc: 0.4746 - val_loss: 1.0282 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9717 - acc: 0.4831 - val_loss: 1.0276 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9699 - acc: 0.4831 - val_loss: 1.0272 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9684 - acc: 0.4831 - val_loss: 1.0256 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9669 - acc: 0.4831 - val_loss: 1.0259 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9652 - acc: 0.5085 - val_loss: 1.0238 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9638 - acc: 0.5254 - val_loss: 1.0220 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9631 - acc: 0.5085 - val_loss: 1.0209 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9614 - acc: 0.5339 - val_loss: 1.0200 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9608 - acc: 0.5339 - val_loss: 1.0184 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9597 - acc: 0.5339 - val_loss: 1.0165 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9586 - acc: 0.5339 - val_loss: 1.0151 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9579 - acc: 0.5339 - val_loss: 1.0138 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9564 - acc: 0.5339 - val_loss: 1.0130 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9555 - acc: 0.5339 - val_loss: 1.0114 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9542 - acc: 0.5339 - val_loss: 1.0104 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9541 - acc: 0.5424 - val_loss: 1.0101 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9532 - acc: 0.5593 - val_loss: 1.0092 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9524 - acc: 0.5508 - val_loss: 1.0070 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9511 - acc: 0.5508 - val_loss: 1.0062 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9505 - acc: 0.5508 - val_loss: 1.0049 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9500 - acc: 0.5508 - val_loss: 1.0050 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9491 - acc: 0.5508 - val_loss: 1.0032 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9483 - acc: 0.5508 - val_loss: 1.0018 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9477 - acc: 0.5508 - val_loss: 1.0012 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9471 - acc: 0.5593 - val_loss: 1.0000 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9461 - acc: 0.5678 - val_loss: 0.9992 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9456 - acc: 0.5678 - val_loss: 0.9980 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9454 - acc: 0.5678 - val_loss: 0.9970 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9451 - acc: 0.5678 - val_loss: 0.9960 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9443 - acc: 0.5678 - val_loss: 0.9953 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9432 - acc: 0.5678 - val_loss: 0.9944 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9433 - acc: 0.5678 - val_loss: 0.9938 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9427 - acc: 0.5678 - val_loss: 0.9939 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9420 - acc: 0.5763 - val_loss: 0.9932 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9412 - acc: 0.5678 - val_loss: 0.9928 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9407 - acc: 0.5847 - val_loss: 0.9929 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9405 - acc: 0.5847 - val_loss: 0.9918 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9403 - acc: 0.5847 - val_loss: 0.9908 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9392 - acc: 0.5932 - val_loss: 0.9909 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9386 - acc: 0.6017 - val_loss: 0.9902 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.9384 - acc: 0.5932 - val_loss: 0.9898 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9380 - acc: 0.5932 - val_loss: 0.9891 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9376 - acc: 0.5932 - val_loss: 0.9885 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9373 - acc: 0.5932 - val_loss: 0.9879 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.8106 - acc: 0.2203 - val_loss: 1.2224 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.6721 - acc: 0.2542 - val_loss: 1.1435 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.5557 - acc: 0.3136 - val_loss: 1.0809 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.4712 - acc: 0.3136 - val_loss: 1.0375 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.4032 - acc: 0.3305 - val_loss: 1.0060 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.3480 - acc: 0.3729 - val_loss: 0.9786 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.3052 - acc: 0.3390 - val_loss: 0.9561 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.2671 - acc: 0.3814 - val_loss: 0.9371 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.2368 - acc: 0.3983 - val_loss: 0.9223 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.2106 - acc: 0.4068 - val_loss: 0.9068 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.1891 - acc: 0.4153 - val_loss: 0.8959 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1675 - acc: 0.4492 - val_loss: 0.8869 - val_acc: 0.6923\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.1501 - acc: 0.4576 - val_loss: 0.8783 - val_acc: 0.6923\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1337 - acc: 0.4746 - val_loss: 0.8706 - val_acc: 0.6923\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.1199 - acc: 0.4746 - val_loss: 0.8632 - val_acc: 0.6923\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1065 - acc: 0.4746 - val_loss: 0.8564 - val_acc: 0.6923\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0951 - acc: 0.4746 - val_loss: 0.8501 - val_acc: 0.6923\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0854 - acc: 0.4831 - val_loss: 0.8453 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0756 - acc: 0.4661 - val_loss: 0.8413 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0650 - acc: 0.4661 - val_loss: 0.8373 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0575 - acc: 0.4746 - val_loss: 0.8338 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0496 - acc: 0.4746 - val_loss: 0.8306 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0429 - acc: 0.4661 - val_loss: 0.8277 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0362 - acc: 0.4746 - val_loss: 0.8252 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0299 - acc: 0.4831 - val_loss: 0.8232 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0248 - acc: 0.4831 - val_loss: 0.8213 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0197 - acc: 0.4831 - val_loss: 0.8196 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0150 - acc: 0.4915 - val_loss: 0.8183 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0115 - acc: 0.5085 - val_loss: 0.8160 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0074 - acc: 0.5085 - val_loss: 0.8155 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0032 - acc: 0.5000 - val_loss: 0.8142 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0001 - acc: 0.5000 - val_loss: 0.8133 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9974 - acc: 0.5000 - val_loss: 0.8122 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9935 - acc: 0.5085 - val_loss: 0.8116 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9915 - acc: 0.5085 - val_loss: 0.8115 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9884 - acc: 0.5000 - val_loss: 0.8111 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9860 - acc: 0.5000 - val_loss: 0.8114 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9846 - acc: 0.5085 - val_loss: 0.8106 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9815 - acc: 0.5085 - val_loss: 0.8105 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9801 - acc: 0.5085 - val_loss: 0.8101 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9780 - acc: 0.5169 - val_loss: 0.8102 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9762 - acc: 0.5254 - val_loss: 0.8100 - val_acc: 0.7692\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.9744 - acc: 0.5254 - val_loss: 0.8101 - val_acc: 0.7692\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9739 - acc: 0.5339 - val_loss: 0.8100 - val_acc: 0.7692\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9717 - acc: 0.5254 - val_loss: 0.8098 - val_acc: 0.7692\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9708 - acc: 0.5085 - val_loss: 0.8097 - val_acc: 0.7692\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9699 - acc: 0.5085 - val_loss: 0.8098 - val_acc: 0.7692\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9680 - acc: 0.5000 - val_loss: 0.8098 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9669 - acc: 0.4831 - val_loss: 0.8100 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9661 - acc: 0.4831 - val_loss: 0.8099 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9644 - acc: 0.4831 - val_loss: 0.8097 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9635 - acc: 0.5085 - val_loss: 0.8102 - val_acc: 0.7692\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9629 - acc: 0.5169 - val_loss: 0.8102 - val_acc: 0.7692\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9619 - acc: 0.5085 - val_loss: 0.8099 - val_acc: 0.7692\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9610 - acc: 0.5169 - val_loss: 0.8098 - val_acc: 0.7692\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9600 - acc: 0.5169 - val_loss: 0.8102 - val_acc: 0.7692\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9592 - acc: 0.5254 - val_loss: 0.8102 - val_acc: 0.6923\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9580 - acc: 0.5169 - val_loss: 0.8106 - val_acc: 0.6923\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9576 - acc: 0.5254 - val_loss: 0.8106 - val_acc: 0.6923\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9562 - acc: 0.5254 - val_loss: 0.8101 - val_acc: 0.6923\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9554 - acc: 0.5254 - val_loss: 0.8107 - val_acc: 0.6923\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9550 - acc: 0.5254 - val_loss: 0.8101 - val_acc: 0.6923\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9545 - acc: 0.5254 - val_loss: 0.8104 - val_acc: 0.6923\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9541 - acc: 0.5339 - val_loss: 0.8103 - val_acc: 0.6923\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9530 - acc: 0.5339 - val_loss: 0.8106 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9520 - acc: 0.5339 - val_loss: 0.8106 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9511 - acc: 0.5339 - val_loss: 0.8105 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9502 - acc: 0.5339 - val_loss: 0.8106 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9504 - acc: 0.5424 - val_loss: 0.8102 - val_acc: 0.6923\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9484 - acc: 0.5424 - val_loss: 0.8099 - val_acc: 0.6923\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9479 - acc: 0.5424 - val_loss: 0.8096 - val_acc: 0.6923\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9466 - acc: 0.5424 - val_loss: 0.8091 - val_acc: 0.6923\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9459 - acc: 0.5424 - val_loss: 0.8082 - val_acc: 0.6923\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9446 - acc: 0.5424 - val_loss: 0.8083 - val_acc: 0.6923\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9435 - acc: 0.5424 - val_loss: 0.8081 - val_acc: 0.6923\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9424 - acc: 0.5339 - val_loss: 0.8076 - val_acc: 0.6923\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9409 - acc: 0.5254 - val_loss: 0.8076 - val_acc: 0.6923\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9400 - acc: 0.5339 - val_loss: 0.8079 - val_acc: 0.6923\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9389 - acc: 0.5339 - val_loss: 0.8075 - val_acc: 0.6923\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9379 - acc: 0.5254 - val_loss: 0.8077 - val_acc: 0.6923\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.9364 - acc: 0.5254 - val_loss: 0.8075 - val_acc: 0.6923\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9360 - acc: 0.5339 - val_loss: 0.8077 - val_acc: 0.6923\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9351 - acc: 0.5254 - val_loss: 0.8074 - val_acc: 0.6923\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9337 - acc: 0.5339 - val_loss: 0.8080 - val_acc: 0.6923\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9336 - acc: 0.5339 - val_loss: 0.8079 - val_acc: 0.6923\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9321 - acc: 0.5254 - val_loss: 0.8074 - val_acc: 0.6923\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9312 - acc: 0.5339 - val_loss: 0.8078 - val_acc: 0.6923\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9307 - acc: 0.5254 - val_loss: 0.8077 - val_acc: 0.6923\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9297 - acc: 0.5339 - val_loss: 0.8075 - val_acc: 0.6923\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9292 - acc: 0.5254 - val_loss: 0.8074 - val_acc: 0.6923\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9287 - acc: 0.5339 - val_loss: 0.8072 - val_acc: 0.6923\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9273 - acc: 0.5339 - val_loss: 0.8067 - val_acc: 0.6923\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9265 - acc: 0.5339 - val_loss: 0.8068 - val_acc: 0.6923\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9258 - acc: 0.5339 - val_loss: 0.8071 - val_acc: 0.6923\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9253 - acc: 0.5339 - val_loss: 0.8062 - val_acc: 0.6923\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9246 - acc: 0.5339 - val_loss: 0.8068 - val_acc: 0.6923\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9235 - acc: 0.5339 - val_loss: 0.8062 - val_acc: 0.6923\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9229 - acc: 0.5339 - val_loss: 0.8059 - val_acc: 0.6923\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9222 - acc: 0.5339 - val_loss: 0.8058 - val_acc: 0.6923\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9215 - acc: 0.5339 - val_loss: 0.8062 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 1.1339 - acc: 0.4068 - val_loss: 1.3489 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0986 - acc: 0.4153 - val_loss: 1.2833 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0709 - acc: 0.4237 - val_loss: 1.2306 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0496 - acc: 0.4322 - val_loss: 1.1923 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0343 - acc: 0.4322 - val_loss: 1.1625 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0237 - acc: 0.3983 - val_loss: 1.1416 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0151 - acc: 0.4153 - val_loss: 1.1226 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0084 - acc: 0.4237 - val_loss: 1.1055 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0021 - acc: 0.4407 - val_loss: 1.0916 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9989 - acc: 0.4407 - val_loss: 1.0807 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9965 - acc: 0.4407 - val_loss: 1.0713 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9938 - acc: 0.4407 - val_loss: 1.0651 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9907 - acc: 0.4661 - val_loss: 1.0577 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9890 - acc: 0.4576 - val_loss: 1.0515 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9870 - acc: 0.4576 - val_loss: 1.0460 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9864 - acc: 0.4492 - val_loss: 1.0427 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9860 - acc: 0.4322 - val_loss: 1.0361 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9838 - acc: 0.4407 - val_loss: 1.0328 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9819 - acc: 0.4407 - val_loss: 1.0297 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9807 - acc: 0.4492 - val_loss: 1.0268 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9794 - acc: 0.4576 - val_loss: 1.0240 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9787 - acc: 0.4492 - val_loss: 1.0200 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9776 - acc: 0.4661 - val_loss: 1.0190 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9766 - acc: 0.4661 - val_loss: 1.0162 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9768 - acc: 0.4492 - val_loss: 1.0139 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9743 - acc: 0.4746 - val_loss: 1.0096 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9742 - acc: 0.4576 - val_loss: 1.0075 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9735 - acc: 0.4661 - val_loss: 1.0056 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9736 - acc: 0.4746 - val_loss: 1.0029 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9717 - acc: 0.4661 - val_loss: 1.0017 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9712 - acc: 0.4831 - val_loss: 1.0011 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9711 - acc: 0.4831 - val_loss: 0.9977 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9703 - acc: 0.4831 - val_loss: 0.9954 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9693 - acc: 0.4831 - val_loss: 0.9951 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9699 - acc: 0.4746 - val_loss: 0.9926 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9679 - acc: 0.4746 - val_loss: 0.9904 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9677 - acc: 0.4831 - val_loss: 0.9895 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9672 - acc: 0.4831 - val_loss: 0.9885 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9675 - acc: 0.5000 - val_loss: 0.9856 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9659 - acc: 0.4831 - val_loss: 0.9834 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9655 - acc: 0.4831 - val_loss: 0.9819 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9648 - acc: 0.4915 - val_loss: 0.9802 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9640 - acc: 0.4915 - val_loss: 0.9803 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9646 - acc: 0.4915 - val_loss: 0.9783 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9630 - acc: 0.5000 - val_loss: 0.9792 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9628 - acc: 0.4915 - val_loss: 0.9786 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9616 - acc: 0.4831 - val_loss: 0.9771 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9615 - acc: 0.4915 - val_loss: 0.9762 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9612 - acc: 0.4915 - val_loss: 0.9755 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9605 - acc: 0.4915 - val_loss: 0.9744 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.9595 - acc: 0.4915 - val_loss: 0.9740 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9596 - acc: 0.5000 - val_loss: 0.9740 - val_acc: 0.6154\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9593 - acc: 0.5085 - val_loss: 0.9718 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9587 - acc: 0.4915 - val_loss: 0.9705 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9570 - acc: 0.4915 - val_loss: 0.9688 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9565 - acc: 0.5085 - val_loss: 0.9674 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9561 - acc: 0.5000 - val_loss: 0.9690 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9564 - acc: 0.5085 - val_loss: 0.9673 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9552 - acc: 0.5169 - val_loss: 0.9677 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9552 - acc: 0.4915 - val_loss: 0.9667 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9552 - acc: 0.5085 - val_loss: 0.9665 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9542 - acc: 0.5085 - val_loss: 0.9662 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9541 - acc: 0.5169 - val_loss: 0.9634 - val_acc: 0.6154\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9534 - acc: 0.5085 - val_loss: 0.9634 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9532 - acc: 0.5169 - val_loss: 0.9652 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9528 - acc: 0.5169 - val_loss: 0.9647 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9526 - acc: 0.5169 - val_loss: 0.9633 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9526 - acc: 0.5169 - val_loss: 0.9615 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9521 - acc: 0.5085 - val_loss: 0.9621 - val_acc: 0.6154\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9520 - acc: 0.5254 - val_loss: 0.9616 - val_acc: 0.6154\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9515 - acc: 0.5254 - val_loss: 0.9623 - val_acc: 0.6154\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9512 - acc: 0.5254 - val_loss: 0.9614 - val_acc: 0.6154\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9508 - acc: 0.5254 - val_loss: 0.9608 - val_acc: 0.6154\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9508 - acc: 0.5254 - val_loss: 0.9605 - val_acc: 0.6154\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9505 - acc: 0.5169 - val_loss: 0.9587 - val_acc: 0.6154\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9502 - acc: 0.5254 - val_loss: 0.9587 - val_acc: 0.6154\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9506 - acc: 0.5254 - val_loss: 0.9595 - val_acc: 0.6154\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9509 - acc: 0.5254 - val_loss: 0.9591 - val_acc: 0.6154\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9505 - acc: 0.5254 - val_loss: 0.9582 - val_acc: 0.6154\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9498 - acc: 0.5254 - val_loss: 0.9581 - val_acc: 0.6154\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9493 - acc: 0.5254 - val_loss: 0.9587 - val_acc: 0.6154\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9498 - acc: 0.5254 - val_loss: 0.9589 - val_acc: 0.6154\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9490 - acc: 0.5339 - val_loss: 0.9575 - val_acc: 0.6154\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9488 - acc: 0.5254 - val_loss: 0.9584 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9486 - acc: 0.5169 - val_loss: 0.9563 - val_acc: 0.6154\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9482 - acc: 0.5254 - val_loss: 0.9554 - val_acc: 0.6154\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9479 - acc: 0.5169 - val_loss: 0.9553 - val_acc: 0.6154\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9482 - acc: 0.5000 - val_loss: 0.9552 - val_acc: 0.6154\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9479 - acc: 0.5085 - val_loss: 0.9557 - val_acc: 0.6154\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9479 - acc: 0.5000 - val_loss: 0.9555 - val_acc: 0.6154\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9472 - acc: 0.5254 - val_loss: 0.9569 - val_acc: 0.6154\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9479 - acc: 0.5000 - val_loss: 0.9542 - val_acc: 0.6154\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9478 - acc: 0.5000 - val_loss: 0.9560 - val_acc: 0.6154\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9470 - acc: 0.5169 - val_loss: 0.9555 - val_acc: 0.6154\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9468 - acc: 0.5254 - val_loss: 0.9558 - val_acc: 0.6154\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9480 - acc: 0.5169 - val_loss: 0.9544 - val_acc: 0.6154\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9458 - acc: 0.5169 - val_loss: 0.9548 - val_acc: 0.6154\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9459 - acc: 0.5085 - val_loss: 0.9546 - val_acc: 0.6154\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9464 - acc: 0.5000 - val_loss: 0.9541 - val_acc: 0.6154\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9461 - acc: 0.5000 - val_loss: 0.9554 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 1.9806 - acc: 0.3814 - val_loss: 1.6678 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.8536 - acc: 0.3983 - val_loss: 1.5868 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.7525 - acc: 0.4237 - val_loss: 1.5167 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.6698 - acc: 0.4407 - val_loss: 1.4588 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.6038 - acc: 0.4492 - val_loss: 1.4071 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.5467 - acc: 0.4576 - val_loss: 1.3604 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.4968 - acc: 0.4576 - val_loss: 1.3203 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.4530 - acc: 0.4492 - val_loss: 1.2906 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.4141 - acc: 0.4661 - val_loss: 1.2630 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.3770 - acc: 0.4661 - val_loss: 1.2420 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.3444 - acc: 0.4661 - val_loss: 1.2265 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.3177 - acc: 0.4746 - val_loss: 1.2023 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.2912 - acc: 0.4746 - val_loss: 1.1828 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.2675 - acc: 0.4746 - val_loss: 1.1696 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2449 - acc: 0.4746 - val_loss: 1.1538 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2240 - acc: 0.4746 - val_loss: 1.1407 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.2052 - acc: 0.4915 - val_loss: 1.1295 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.1880 - acc: 0.4915 - val_loss: 1.1170 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1716 - acc: 0.4915 - val_loss: 1.1047 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1575 - acc: 0.5000 - val_loss: 1.0950 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.1443 - acc: 0.4915 - val_loss: 1.0863 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1333 - acc: 0.5000 - val_loss: 1.0763 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1210 - acc: 0.5000 - val_loss: 1.0753 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1102 - acc: 0.5000 - val_loss: 1.0688 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.1004 - acc: 0.5085 - val_loss: 1.0620 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0904 - acc: 0.5085 - val_loss: 1.0559 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0815 - acc: 0.5085 - val_loss: 1.0532 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0733 - acc: 0.5085 - val_loss: 1.0488 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0661 - acc: 0.5000 - val_loss: 1.0421 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0591 - acc: 0.5000 - val_loss: 1.0407 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0526 - acc: 0.4915 - val_loss: 1.0354 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0461 - acc: 0.4915 - val_loss: 1.0323 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0411 - acc: 0.5000 - val_loss: 1.0282 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0348 - acc: 0.5000 - val_loss: 1.0261 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0302 - acc: 0.4915 - val_loss: 1.0265 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0251 - acc: 0.5000 - val_loss: 1.0221 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0212 - acc: 0.5000 - val_loss: 1.0183 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.0167 - acc: 0.5000 - val_loss: 1.0205 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0127 - acc: 0.4915 - val_loss: 1.0166 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0103 - acc: 0.5000 - val_loss: 1.0155 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0063 - acc: 0.5000 - val_loss: 1.0144 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0034 - acc: 0.5000 - val_loss: 1.0114 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0001 - acc: 0.5000 - val_loss: 1.0120 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9978 - acc: 0.5085 - val_loss: 1.0074 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9961 - acc: 0.5085 - val_loss: 1.0079 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9925 - acc: 0.5085 - val_loss: 1.0069 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9903 - acc: 0.5000 - val_loss: 1.0083 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9880 - acc: 0.5169 - val_loss: 1.0066 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9861 - acc: 0.5169 - val_loss: 1.0060 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9852 - acc: 0.5085 - val_loss: 1.0075 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9827 - acc: 0.5000 - val_loss: 1.0047 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9805 - acc: 0.5085 - val_loss: 1.0084 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9792 - acc: 0.5085 - val_loss: 1.0084 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9786 - acc: 0.5085 - val_loss: 1.0070 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9760 - acc: 0.5085 - val_loss: 1.0085 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9744 - acc: 0.5085 - val_loss: 1.0073 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9740 - acc: 0.5085 - val_loss: 1.0051 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9728 - acc: 0.5085 - val_loss: 1.0051 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9714 - acc: 0.5085 - val_loss: 1.0086 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9701 - acc: 0.5085 - val_loss: 1.0069 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9689 - acc: 0.5169 - val_loss: 1.0071 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9681 - acc: 0.5169 - val_loss: 1.0082 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9678 - acc: 0.5085 - val_loss: 1.0092 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9672 - acc: 0.5000 - val_loss: 1.0082 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9652 - acc: 0.5085 - val_loss: 1.0114 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9648 - acc: 0.5085 - val_loss: 1.0118 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9636 - acc: 0.5085 - val_loss: 1.0131 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9631 - acc: 0.5424 - val_loss: 1.0141 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9628 - acc: 0.5508 - val_loss: 1.0142 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9618 - acc: 0.5508 - val_loss: 1.0160 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9618 - acc: 0.5508 - val_loss: 1.0169 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9613 - acc: 0.5508 - val_loss: 1.0169 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9600 - acc: 0.5508 - val_loss: 1.0188 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9600 - acc: 0.5508 - val_loss: 1.0215 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9593 - acc: 0.5508 - val_loss: 1.0212 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9587 - acc: 0.5508 - val_loss: 1.0211 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9583 - acc: 0.5508 - val_loss: 1.0225 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9587 - acc: 0.5508 - val_loss: 1.0234 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.9577 - acc: 0.5508 - val_loss: 1.0243 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9566 - acc: 0.5508 - val_loss: 1.0245 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9563 - acc: 0.5508 - val_loss: 1.0246 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9563 - acc: 0.5508 - val_loss: 1.0246 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9565 - acc: 0.5508 - val_loss: 1.0245 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9552 - acc: 0.5508 - val_loss: 1.0247 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9547 - acc: 0.5508 - val_loss: 1.0256 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9554 - acc: 0.5508 - val_loss: 1.0258 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9544 - acc: 0.5508 - val_loss: 1.0271 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9541 - acc: 0.5508 - val_loss: 1.0284 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9535 - acc: 0.5508 - val_loss: 1.0285 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9535 - acc: 0.5508 - val_loss: 1.0279 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9529 - acc: 0.5508 - val_loss: 1.0283 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9535 - acc: 0.5508 - val_loss: 1.0304 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9521 - acc: 0.5508 - val_loss: 1.0299 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9521 - acc: 0.5508 - val_loss: 1.0319 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9523 - acc: 0.5508 - val_loss: 1.0322 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9514 - acc: 0.5508 - val_loss: 1.0323 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9513 - acc: 0.5508 - val_loss: 1.0349 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9512 - acc: 0.5424 - val_loss: 1.0329 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9511 - acc: 0.5424 - val_loss: 1.0341 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9505 - acc: 0.5508 - val_loss: 1.0357 - val_acc: 0.3846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "f4c555cd-8da7-4a3a-a60a-d1e897a1c434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "68de6398-6f32-4965-da6d-6e8ec01690a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "0fa2656e-1352-4ad2-cf1d-cfc8e5f1873a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "d06259c7-480d-46f3-e5d8-1d070ea3d351",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1e32d40f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERhjTMBZIjDGmID7f8h3oAcugahnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "711d3ecf-8731-49b1-84ba-355dba1207f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1e2da0da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "5bd335d5-1bdd-4f94-f74a-21b7b1d6057a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 4.1188 - acc: 0.3511\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 3.7091 - acc: 0.3588\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 3.3677 - acc: 0.3588\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 3.0606 - acc: 0.3588\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 2.7734 - acc: 0.3664\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 2.5373 - acc: 0.3740\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 2.3285 - acc: 0.3664\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 2.1434 - acc: 0.3664\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.9806 - acc: 0.3664\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.8415 - acc: 0.3740\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.7153 - acc: 0.3893\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 1.6145 - acc: 0.3893\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.5311 - acc: 0.3893\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.4602 - acc: 0.3969\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.3972 - acc: 0.3969\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.3424 - acc: 0.4122\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.2963 - acc: 0.3969\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.2590 - acc: 0.4198\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.2272 - acc: 0.4275\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 221us/step - loss: 1.1983 - acc: 0.4351\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.1721 - acc: 0.4504\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1513 - acc: 0.4580\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.1305 - acc: 0.4733\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.1158 - acc: 0.4885\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.1028 - acc: 0.4656\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0905 - acc: 0.4809\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0820 - acc: 0.4809\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0709 - acc: 0.4809\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.0628 - acc: 0.4809\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0563 - acc: 0.4885\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.0486 - acc: 0.4962\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.0437 - acc: 0.5115\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0385 - acc: 0.5191\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0343 - acc: 0.5115\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0293 - acc: 0.5267\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.0263 - acc: 0.5038\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.0237 - acc: 0.5115\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.0216 - acc: 0.5038\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0175 - acc: 0.5267\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0154 - acc: 0.5344\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0160 - acc: 0.5344\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0113 - acc: 0.5344\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0100 - acc: 0.5267\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0064 - acc: 0.5420\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0033 - acc: 0.5420\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0024 - acc: 0.5496\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0024 - acc: 0.5344\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9981 - acc: 0.5420\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9961 - acc: 0.5573\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9950 - acc: 0.5496\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9924 - acc: 0.5573\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9914 - acc: 0.5649\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9893 - acc: 0.5725\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9874 - acc: 0.5725\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9870 - acc: 0.5802\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9846 - acc: 0.5725\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9844 - acc: 0.5725\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9813 - acc: 0.5878\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9813 - acc: 0.5802\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9799 - acc: 0.5802\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9787 - acc: 0.5725\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9769 - acc: 0.5725\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9759 - acc: 0.5802\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9751 - acc: 0.5878\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9727 - acc: 0.5878\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9723 - acc: 0.5878\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9710 - acc: 0.5878\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9719 - acc: 0.5802\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9699 - acc: 0.5878\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9695 - acc: 0.5878\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9681 - acc: 0.5878\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9669 - acc: 0.5878\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9691 - acc: 0.5802\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9655 - acc: 0.5878\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9646 - acc: 0.5878\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9634 - acc: 0.5802\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9637 - acc: 0.5802\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9629 - acc: 0.5802\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9628 - acc: 0.5878\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9611 - acc: 0.5802\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9606 - acc: 0.5878\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9596 - acc: 0.5802\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9598 - acc: 0.5725\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9592 - acc: 0.5802\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9573 - acc: 0.5725\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9573 - acc: 0.5802\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9567 - acc: 0.5725\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9554 - acc: 0.5802\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9559 - acc: 0.5725\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9544 - acc: 0.5725\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9532 - acc: 0.5802\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9535 - acc: 0.5649\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9526 - acc: 0.5725\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9525 - acc: 0.5725\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9521 - acc: 0.5802\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9513 - acc: 0.5725\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9502 - acc: 0.5649\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9500 - acc: 0.5649\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9504 - acc: 0.5573\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9492 - acc: 0.5649\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "e257c4fe-8f77-4017-aa80-63bd1a317d65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "48407c17-36a5-4252-b3c9-260082d636fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17647058823529413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "23e42fcf-0e88-43dc-a447-ad3f4f9da666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "ee856cc0-c289-457e-bf60-85642f15f590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f34521c9-28fe-4023-b3bb-ed7ad207e4ec",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f1dc6981-3d57-4989-f893-55154afb9102",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "019bb02a-e666-4d08-fd60-f15851dd2838",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "869f9e4f-b3c6-4bcf-e769-255e5744ac87",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 2.2824 - acc: 0.0085 - val_loss: 2.1772 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 2.1351 - acc: 0.0085 - val_loss: 2.0415 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 1.9990 - acc: 0.0085 - val_loss: 1.9145 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 1.8757 - acc: 0.0085 - val_loss: 1.8017 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 1.7630 - acc: 0.0085 - val_loss: 1.6945 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 1.6598 - acc: 0.0085 - val_loss: 1.6009 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.5675 - acc: 0.0085 - val_loss: 1.5139 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.4829 - acc: 0.0085 - val_loss: 1.4369 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 1.4058 - acc: 0.0085 - val_loss: 1.3636 - val_acc: 0.0000e+00\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 1.3351 - acc: 0.0085 - val_loss: 1.2986 - val_acc: 0.0000e+00\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 1.2716 - acc: 0.0000e+00 - val_loss: 1.2377 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.2114 - acc: 0.0171 - val_loss: 1.1824 - val_acc: 0.0714\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.1578 - acc: 0.0342 - val_loss: 1.1321 - val_acc: 0.0714\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 1.1081 - acc: 0.0769 - val_loss: 1.0845 - val_acc: 0.1429\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.0617 - acc: 0.2393 - val_loss: 1.0412 - val_acc: 0.3571\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 1.0184 - acc: 0.4274 - val_loss: 1.0007 - val_acc: 0.4286\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9786 - acc: 0.4530 - val_loss: 0.9633 - val_acc: 0.4286\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9417 - acc: 0.4701 - val_loss: 0.9277 - val_acc: 0.5000\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9068 - acc: 0.5385 - val_loss: 0.8950 - val_acc: 0.6429\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.8745 - acc: 0.6496 - val_loss: 0.8645 - val_acc: 0.6429\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.8440 - acc: 0.7521 - val_loss: 0.8353 - val_acc: 0.7857\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.8152 - acc: 0.7863 - val_loss: 0.8078 - val_acc: 0.7857\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.7879 - acc: 0.7949 - val_loss: 0.7818 - val_acc: 0.8571\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.7621 - acc: 0.8120 - val_loss: 0.7569 - val_acc: 0.8571\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 234us/step - loss: 0.7378 - acc: 0.8291 - val_loss: 0.7336 - val_acc: 0.8571\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.7139 - acc: 0.8547 - val_loss: 0.7113 - val_acc: 0.9286\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.6913 - acc: 0.9231 - val_loss: 0.6895 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.6700 - acc: 0.9316 - val_loss: 0.6689 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.6493 - acc: 0.9487 - val_loss: 0.6488 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.6294 - acc: 0.9573 - val_loss: 0.6295 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.6103 - acc: 0.9573 - val_loss: 0.6107 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.5920 - acc: 0.9658 - val_loss: 0.5925 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.5742 - acc: 0.9658 - val_loss: 0.5748 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.5572 - acc: 0.9658 - val_loss: 0.5576 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.5406 - acc: 0.9658 - val_loss: 0.5410 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.5248 - acc: 0.9658 - val_loss: 0.5250 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.5094 - acc: 0.9658 - val_loss: 0.5093 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.4946 - acc: 0.9658 - val_loss: 0.4942 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.4805 - acc: 0.9658 - val_loss: 0.4796 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.4668 - acc: 0.9658 - val_loss: 0.4657 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.4535 - acc: 0.9658 - val_loss: 0.4521 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.4408 - acc: 0.9658 - val_loss: 0.4390 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.4285 - acc: 0.9658 - val_loss: 0.4264 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.4165 - acc: 0.9658 - val_loss: 0.4141 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.4050 - acc: 0.9658 - val_loss: 0.4019 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.3939 - acc: 0.9658 - val_loss: 0.3903 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.3833 - acc: 0.9658 - val_loss: 0.3793 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.3730 - acc: 0.9658 - val_loss: 0.3685 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.3633 - acc: 0.9658 - val_loss: 0.3581 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.3537 - acc: 0.9658 - val_loss: 0.3481 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.3445 - acc: 0.9658 - val_loss: 0.3386 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.3358 - acc: 0.9658 - val_loss: 0.3293 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.3273 - acc: 0.9658 - val_loss: 0.3204 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.3194 - acc: 0.9658 - val_loss: 0.3119 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.3115 - acc: 0.9658 - val_loss: 0.3036 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.3041 - acc: 0.9658 - val_loss: 0.2956 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.2969 - acc: 0.9658 - val_loss: 0.2880 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.2900 - acc: 0.9658 - val_loss: 0.2805 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.2835 - acc: 0.9658 - val_loss: 0.2734 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.2771 - acc: 0.9658 - val_loss: 0.2666 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.2710 - acc: 0.9658 - val_loss: 0.2599 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.2651 - acc: 0.9658 - val_loss: 0.2535 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.2595 - acc: 0.9744 - val_loss: 0.2473 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.2541 - acc: 0.9744 - val_loss: 0.2413 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.2489 - acc: 0.9744 - val_loss: 0.2355 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.2438 - acc: 0.9744 - val_loss: 0.2301 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.2390 - acc: 0.9744 - val_loss: 0.2247 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.2343 - acc: 0.9744 - val_loss: 0.2195 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.2298 - acc: 0.9744 - val_loss: 0.2145 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 157us/step - loss: 0.2254 - acc: 0.9744 - val_loss: 0.2096 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.2212 - acc: 0.9744 - val_loss: 0.2050 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.2171 - acc: 0.9744 - val_loss: 0.2005 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.2131 - acc: 0.9744 - val_loss: 0.1961 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.2094 - acc: 0.9744 - val_loss: 0.1918 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.2057 - acc: 0.9744 - val_loss: 0.1878 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.2021 - acc: 0.9744 - val_loss: 0.1838 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.1987 - acc: 0.9744 - val_loss: 0.1800 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.1954 - acc: 0.9744 - val_loss: 0.1763 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.1921 - acc: 0.9744 - val_loss: 0.1727 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.1890 - acc: 0.9744 - val_loss: 0.1692 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 149us/step - loss: 0.1860 - acc: 0.9829 - val_loss: 0.1659 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.1831 - acc: 0.9829 - val_loss: 0.1626 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.1803 - acc: 0.9829 - val_loss: 0.1594 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.1775 - acc: 0.9829 - val_loss: 0.1563 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.1749 - acc: 0.9829 - val_loss: 0.1534 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 217us/step - loss: 0.1723 - acc: 0.9829 - val_loss: 0.1505 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.1699 - acc: 0.9829 - val_loss: 0.1477 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.1675 - acc: 0.9829 - val_loss: 0.1450 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.1651 - acc: 0.9829 - val_loss: 0.1424 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.1629 - acc: 0.9829 - val_loss: 0.1399 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.1607 - acc: 0.9829 - val_loss: 0.1374 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.1585 - acc: 0.9829 - val_loss: 0.1350 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.1564 - acc: 0.9829 - val_loss: 0.1326 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.1544 - acc: 0.9829 - val_loss: 0.1304 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.1525 - acc: 0.9829 - val_loss: 0.1282 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.1506 - acc: 0.9829 - val_loss: 0.1260 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.1487 - acc: 0.9829 - val_loss: 0.1239 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.1469 - acc: 0.9829 - val_loss: 0.1219 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.1452 - acc: 0.9829 - val_loss: 0.1199 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.1434 - acc: 0.9829 - val_loss: 0.1180 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 2.9123 - acc: 0.0169 - val_loss: 2.5517 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 2.6629 - acc: 0.0085 - val_loss: 2.3364 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 2.4373 - acc: 0.0085 - val_loss: 2.1453 - val_acc: 0.0769\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 2.2328 - acc: 0.0085 - val_loss: 1.9707 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 2.0489 - acc: 0.0085 - val_loss: 1.8173 - val_acc: 0.0769\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.8888 - acc: 0.0085 - val_loss: 1.6854 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.7547 - acc: 0.0085 - val_loss: 1.5801 - val_acc: 0.0769\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.6467 - acc: 0.0085 - val_loss: 1.4869 - val_acc: 0.0769\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.5510 - acc: 0.0085 - val_loss: 1.4057 - val_acc: 0.0769\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.4663 - acc: 0.0169 - val_loss: 1.3321 - val_acc: 0.0769\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.3906 - acc: 0.0254 - val_loss: 1.2668 - val_acc: 0.0769\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.3224 - acc: 0.0593 - val_loss: 1.2088 - val_acc: 0.0769\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.2596 - acc: 0.1610 - val_loss: 1.1551 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.2028 - acc: 0.2627 - val_loss: 1.1064 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1515 - acc: 0.2966 - val_loss: 1.0626 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1045 - acc: 0.3390 - val_loss: 1.0226 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0611 - acc: 0.3898 - val_loss: 0.9860 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0212 - acc: 0.4153 - val_loss: 0.9524 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9845 - acc: 0.4576 - val_loss: 0.9214 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9505 - acc: 0.4576 - val_loss: 0.8927 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9193 - acc: 0.4746 - val_loss: 0.8665 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8901 - acc: 0.4831 - val_loss: 0.8421 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8630 - acc: 0.5169 - val_loss: 0.8189 - val_acc: 0.7692\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8373 - acc: 0.5508 - val_loss: 0.7975 - val_acc: 0.7692\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8134 - acc: 0.6186 - val_loss: 0.7774 - val_acc: 0.7692\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7910 - acc: 0.7288 - val_loss: 0.7582 - val_acc: 0.8462\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7696 - acc: 0.7542 - val_loss: 0.7403 - val_acc: 0.8462\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7496 - acc: 0.7881 - val_loss: 0.7233 - val_acc: 0.8462\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7306 - acc: 0.8305 - val_loss: 0.7073 - val_acc: 0.8462\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7128 - acc: 0.8559 - val_loss: 0.6920 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6958 - acc: 0.8644 - val_loss: 0.6775 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6796 - acc: 0.8644 - val_loss: 0.6637 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.6642 - acc: 0.8644 - val_loss: 0.6502 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6492 - acc: 0.8729 - val_loss: 0.6376 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6350 - acc: 0.8729 - val_loss: 0.6251 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6212 - acc: 0.8729 - val_loss: 0.6134 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6079 - acc: 0.8729 - val_loss: 0.6020 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5952 - acc: 0.8729 - val_loss: 0.5910 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5830 - acc: 0.8729 - val_loss: 0.5803 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5712 - acc: 0.8729 - val_loss: 0.5700 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.5597 - acc: 0.8729 - val_loss: 0.5602 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5486 - acc: 0.8729 - val_loss: 0.5505 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5378 - acc: 0.8729 - val_loss: 0.5408 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5273 - acc: 0.8729 - val_loss: 0.5318 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.5173 - acc: 0.8729 - val_loss: 0.5228 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5076 - acc: 0.8729 - val_loss: 0.5142 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4980 - acc: 0.8729 - val_loss: 0.5056 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4887 - acc: 0.8729 - val_loss: 0.4973 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4798 - acc: 0.8729 - val_loss: 0.4893 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4710 - acc: 0.8729 - val_loss: 0.4815 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4625 - acc: 0.8729 - val_loss: 0.4737 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4543 - acc: 0.8729 - val_loss: 0.4660 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4461 - acc: 0.8729 - val_loss: 0.4586 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4383 - acc: 0.8729 - val_loss: 0.4514 - val_acc: 0.8462\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4306 - acc: 0.8814 - val_loss: 0.4442 - val_acc: 0.8462\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4230 - acc: 0.8898 - val_loss: 0.4372 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4155 - acc: 0.8898 - val_loss: 0.4302 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.4083 - acc: 0.8983 - val_loss: 0.4233 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4012 - acc: 0.9153 - val_loss: 0.4166 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3941 - acc: 0.9322 - val_loss: 0.4101 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3872 - acc: 0.9407 - val_loss: 0.4036 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3804 - acc: 0.9407 - val_loss: 0.3972 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3738 - acc: 0.9492 - val_loss: 0.3909 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3673 - acc: 0.9576 - val_loss: 0.3849 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3611 - acc: 0.9576 - val_loss: 0.3789 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3548 - acc: 0.9746 - val_loss: 0.3727 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3488 - acc: 0.9746 - val_loss: 0.3670 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3428 - acc: 0.9746 - val_loss: 0.3612 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.3370 - acc: 0.9831 - val_loss: 0.3556 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3314 - acc: 0.9831 - val_loss: 0.3501 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3258 - acc: 0.9831 - val_loss: 0.3447 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.3203 - acc: 0.9831 - val_loss: 0.3394 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3150 - acc: 0.9831 - val_loss: 0.3341 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3097 - acc: 0.9831 - val_loss: 0.3290 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3046 - acc: 0.9831 - val_loss: 0.3239 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2997 - acc: 0.9831 - val_loss: 0.3190 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2947 - acc: 0.9915 - val_loss: 0.3142 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2900 - acc: 0.9915 - val_loss: 0.3094 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2852 - acc: 0.9915 - val_loss: 0.3046 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2806 - acc: 0.9915 - val_loss: 0.3002 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2761 - acc: 0.9915 - val_loss: 0.2956 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2717 - acc: 0.9915 - val_loss: 0.2912 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2674 - acc: 0.9915 - val_loss: 0.2868 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.2632 - acc: 0.9915 - val_loss: 0.2826 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2590 - acc: 0.9915 - val_loss: 0.2785 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2549 - acc: 0.9915 - val_loss: 0.2743 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2510 - acc: 0.9915 - val_loss: 0.2703 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2470 - acc: 0.9915 - val_loss: 0.2662 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2432 - acc: 0.9915 - val_loss: 0.2624 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2395 - acc: 0.9915 - val_loss: 0.2585 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2358 - acc: 0.9915 - val_loss: 0.2550 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2323 - acc: 0.9915 - val_loss: 0.2513 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2287 - acc: 0.9915 - val_loss: 0.2477 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.2253 - acc: 0.9915 - val_loss: 0.2441 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.2219 - acc: 0.9915 - val_loss: 0.2407 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2186 - acc: 0.9915 - val_loss: 0.2374 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2154 - acc: 0.9915 - val_loss: 0.2341 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2122 - acc: 0.9915 - val_loss: 0.2308 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2091 - acc: 0.9915 - val_loss: 0.2277 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2061 - acc: 0.9915 - val_loss: 0.2246 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 1.4428 - acc: 0.2797 - val_loss: 1.3808 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.3770 - acc: 0.2797 - val_loss: 1.3208 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.3176 - acc: 0.2797 - val_loss: 1.2668 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.2642 - acc: 0.2881 - val_loss: 1.2180 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.2159 - acc: 0.4068 - val_loss: 1.1732 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1713 - acc: 0.4237 - val_loss: 1.1325 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1302 - acc: 0.4237 - val_loss: 1.0943 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0925 - acc: 0.4322 - val_loss: 1.0591 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0574 - acc: 0.4407 - val_loss: 1.0263 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0241 - acc: 0.4576 - val_loss: 0.9954 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9931 - acc: 0.4746 - val_loss: 0.9658 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9634 - acc: 0.4746 - val_loss: 0.9379 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9355 - acc: 0.5000 - val_loss: 0.9116 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9089 - acc: 0.5763 - val_loss: 0.8864 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8837 - acc: 0.6356 - val_loss: 0.8623 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8595 - acc: 0.7034 - val_loss: 0.8390 - val_acc: 0.6923\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.8363 - acc: 0.8051 - val_loss: 0.8167 - val_acc: 0.8462\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.8137 - acc: 0.8814 - val_loss: 0.7954 - val_acc: 0.9231\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.7921 - acc: 0.9153 - val_loss: 0.7745 - val_acc: 0.9231\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.7712 - acc: 0.9322 - val_loss: 0.7544 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7509 - acc: 0.9407 - val_loss: 0.7348 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7311 - acc: 0.9407 - val_loss: 0.7157 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7118 - acc: 0.9407 - val_loss: 0.6972 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6930 - acc: 0.9407 - val_loss: 0.6795 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6746 - acc: 0.9407 - val_loss: 0.6622 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6566 - acc: 0.9407 - val_loss: 0.6454 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6392 - acc: 0.9407 - val_loss: 0.6288 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6221 - acc: 0.9492 - val_loss: 0.6128 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6056 - acc: 0.9576 - val_loss: 0.5972 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5895 - acc: 0.9746 - val_loss: 0.5821 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.5739 - acc: 0.9746 - val_loss: 0.5674 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5586 - acc: 0.9746 - val_loss: 0.5530 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5438 - acc: 0.9746 - val_loss: 0.5393 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5293 - acc: 0.9746 - val_loss: 0.5258 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5152 - acc: 0.9831 - val_loss: 0.5128 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5015 - acc: 0.9831 - val_loss: 0.5001 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4882 - acc: 0.9915 - val_loss: 0.4877 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4752 - acc: 0.9915 - val_loss: 0.4758 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4626 - acc: 0.9915 - val_loss: 0.4641 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4504 - acc: 0.9915 - val_loss: 0.4529 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.4385 - acc: 0.9915 - val_loss: 0.4420 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4269 - acc: 1.0000 - val_loss: 0.4314 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4157 - acc: 1.0000 - val_loss: 0.4211 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4049 - acc: 1.0000 - val_loss: 0.4112 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3944 - acc: 1.0000 - val_loss: 0.4016 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3843 - acc: 1.0000 - val_loss: 0.3923 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3745 - acc: 1.0000 - val_loss: 0.3834 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.3650 - acc: 1.0000 - val_loss: 0.3747 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3558 - acc: 1.0000 - val_loss: 0.3663 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3469 - acc: 1.0000 - val_loss: 0.3583 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3384 - acc: 1.0000 - val_loss: 0.3505 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3301 - acc: 1.0000 - val_loss: 0.3430 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3220 - acc: 1.0000 - val_loss: 0.3359 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3144 - acc: 1.0000 - val_loss: 0.3291 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3069 - acc: 1.0000 - val_loss: 0.3225 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2997 - acc: 1.0000 - val_loss: 0.3160 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2927 - acc: 1.0000 - val_loss: 0.3097 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2860 - acc: 1.0000 - val_loss: 0.3038 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2794 - acc: 1.0000 - val_loss: 0.2980 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2731 - acc: 1.0000 - val_loss: 0.2924 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2671 - acc: 1.0000 - val_loss: 0.2870 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2613 - acc: 1.0000 - val_loss: 0.2818 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2556 - acc: 1.0000 - val_loss: 0.2768 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2502 - acc: 1.0000 - val_loss: 0.2719 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2449 - acc: 1.0000 - val_loss: 0.2671 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2398 - acc: 1.0000 - val_loss: 0.2626 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2348 - acc: 1.0000 - val_loss: 0.2582 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2300 - acc: 1.0000 - val_loss: 0.2539 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2253 - acc: 1.0000 - val_loss: 0.2498 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2209 - acc: 1.0000 - val_loss: 0.2458 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2166 - acc: 1.0000 - val_loss: 0.2420 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.2124 - acc: 1.0000 - val_loss: 0.2383 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2083 - acc: 1.0000 - val_loss: 0.2346 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2044 - acc: 1.0000 - val_loss: 0.2311 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2006 - acc: 1.0000 - val_loss: 0.2277 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1970 - acc: 1.0000 - val_loss: 0.2244 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1934 - acc: 1.0000 - val_loss: 0.2213 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1899 - acc: 1.0000 - val_loss: 0.2181 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1866 - acc: 1.0000 - val_loss: 0.2151 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1834 - acc: 1.0000 - val_loss: 0.2123 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1802 - acc: 1.0000 - val_loss: 0.2094 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.1772 - acc: 1.0000 - val_loss: 0.2067 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1742 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1714 - acc: 1.0000 - val_loss: 0.2015 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1686 - acc: 1.0000 - val_loss: 0.1990 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1659 - acc: 1.0000 - val_loss: 0.1966 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1633 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1607 - acc: 1.0000 - val_loss: 0.1920 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1582 - acc: 1.0000 - val_loss: 0.1897 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1558 - acc: 1.0000 - val_loss: 0.1876 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1535 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.1512 - acc: 1.0000 - val_loss: 0.1834 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1490 - acc: 1.0000 - val_loss: 0.1815 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1468 - acc: 1.0000 - val_loss: 0.1795 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1447 - acc: 1.0000 - val_loss: 0.1777 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1427 - acc: 1.0000 - val_loss: 0.1758 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1407 - acc: 1.0000 - val_loss: 0.1740 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1388 - acc: 1.0000 - val_loss: 0.1723 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1369 - acc: 1.0000 - val_loss: 0.1706 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1351 - acc: 1.0000 - val_loss: 0.1690 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.6701 - acc: 0.2373 - val_loss: 1.3865 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.5466 - acc: 0.2458 - val_loss: 1.2974 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.4334 - acc: 0.2797 - val_loss: 1.2156 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3300 - acc: 0.3136 - val_loss: 1.1411 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2361 - acc: 0.3644 - val_loss: 1.0729 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1520 - acc: 0.3729 - val_loss: 1.0145 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0762 - acc: 0.4068 - val_loss: 0.9597 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0079 - acc: 0.4153 - val_loss: 0.9108 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9466 - acc: 0.4492 - val_loss: 0.8646 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8914 - acc: 0.6186 - val_loss: 0.8254 - val_acc: 0.6923\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8426 - acc: 0.7881 - val_loss: 0.7898 - val_acc: 0.7692\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7994 - acc: 0.8390 - val_loss: 0.7571 - val_acc: 0.7692\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.7611 - acc: 0.8644 - val_loss: 0.7278 - val_acc: 0.8462\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7265 - acc: 0.8898 - val_loss: 0.7021 - val_acc: 0.8462\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6958 - acc: 0.8983 - val_loss: 0.6785 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6678 - acc: 0.8983 - val_loss: 0.6558 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6428 - acc: 0.9068 - val_loss: 0.6362 - val_acc: 0.9231\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6203 - acc: 0.9153 - val_loss: 0.6178 - val_acc: 0.9231\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5999 - acc: 0.9322 - val_loss: 0.6010 - val_acc: 0.9231\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5809 - acc: 0.9407 - val_loss: 0.5853 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5633 - acc: 0.9407 - val_loss: 0.5704 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.5473 - acc: 0.9407 - val_loss: 0.5565 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5322 - acc: 0.9407 - val_loss: 0.5435 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5183 - acc: 0.9407 - val_loss: 0.5312 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5050 - acc: 0.9407 - val_loss: 0.5194 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4926 - acc: 0.9492 - val_loss: 0.5085 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4809 - acc: 0.9492 - val_loss: 0.4978 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4697 - acc: 0.9492 - val_loss: 0.4878 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4588 - acc: 0.9492 - val_loss: 0.4781 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4485 - acc: 0.9576 - val_loss: 0.4685 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4386 - acc: 0.9576 - val_loss: 0.4595 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4291 - acc: 0.9746 - val_loss: 0.4505 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4200 - acc: 0.9746 - val_loss: 0.4420 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4114 - acc: 0.9831 - val_loss: 0.4338 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4028 - acc: 0.9831 - val_loss: 0.4260 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3947 - acc: 0.9831 - val_loss: 0.4182 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3868 - acc: 0.9831 - val_loss: 0.4106 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.3792 - acc: 0.9831 - val_loss: 0.4034 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3718 - acc: 0.9831 - val_loss: 0.3963 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3646 - acc: 0.9831 - val_loss: 0.3894 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3576 - acc: 0.9831 - val_loss: 0.3828 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3508 - acc: 0.9831 - val_loss: 0.3763 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3443 - acc: 0.9831 - val_loss: 0.3699 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3379 - acc: 0.9915 - val_loss: 0.3638 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3318 - acc: 0.9915 - val_loss: 0.3579 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3257 - acc: 0.9915 - val_loss: 0.3521 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3199 - acc: 0.9915 - val_loss: 0.3464 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3142 - acc: 0.9915 - val_loss: 0.3409 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3086 - acc: 0.9915 - val_loss: 0.3356 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3032 - acc: 0.9915 - val_loss: 0.3304 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2980 - acc: 0.9915 - val_loss: 0.3253 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2929 - acc: 1.0000 - val_loss: 0.3203 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.2878 - acc: 1.0000 - val_loss: 0.3154 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2830 - acc: 1.0000 - val_loss: 0.3108 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2783 - acc: 1.0000 - val_loss: 0.3062 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2736 - acc: 1.0000 - val_loss: 0.3017 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2692 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2648 - acc: 1.0000 - val_loss: 0.2931 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2605 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2564 - acc: 1.0000 - val_loss: 0.2849 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.2523 - acc: 1.0000 - val_loss: 0.2810 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2483 - acc: 1.0000 - val_loss: 0.2772 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2445 - acc: 1.0000 - val_loss: 0.2734 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2406 - acc: 1.0000 - val_loss: 0.2697 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2369 - acc: 1.0000 - val_loss: 0.2661 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2333 - acc: 1.0000 - val_loss: 0.2626 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2298 - acc: 1.0000 - val_loss: 0.2592 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2264 - acc: 1.0000 - val_loss: 0.2558 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2231 - acc: 1.0000 - val_loss: 0.2525 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2197 - acc: 1.0000 - val_loss: 0.2493 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2166 - acc: 1.0000 - val_loss: 0.2462 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2134 - acc: 1.0000 - val_loss: 0.2431 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2103 - acc: 1.0000 - val_loss: 0.2402 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2074 - acc: 1.0000 - val_loss: 0.2372 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2044 - acc: 1.0000 - val_loss: 0.2344 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2015 - acc: 1.0000 - val_loss: 0.2316 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1987 - acc: 1.0000 - val_loss: 0.2289 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1959 - acc: 1.0000 - val_loss: 0.2262 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1932 - acc: 1.0000 - val_loss: 0.2236 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1906 - acc: 1.0000 - val_loss: 0.2210 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1880 - acc: 1.0000 - val_loss: 0.2184 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1854 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1830 - acc: 1.0000 - val_loss: 0.2135 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1806 - acc: 1.0000 - val_loss: 0.2111 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1782 - acc: 1.0000 - val_loss: 0.2089 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1759 - acc: 1.0000 - val_loss: 0.2065 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1736 - acc: 1.0000 - val_loss: 0.2044 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1714 - acc: 1.0000 - val_loss: 0.2022 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1692 - acc: 1.0000 - val_loss: 0.2001 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1671 - acc: 1.0000 - val_loss: 0.1979 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1650 - acc: 1.0000 - val_loss: 0.1959 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1629 - acc: 1.0000 - val_loss: 0.1938 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.1609 - acc: 1.0000 - val_loss: 0.1919 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1590 - acc: 0.9915 - val_loss: 0.1899 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1570 - acc: 0.9915 - val_loss: 0.1880 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1551 - acc: 0.9915 - val_loss: 0.1861 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1533 - acc: 0.9915 - val_loss: 0.1843 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1514 - acc: 0.9915 - val_loss: 0.1825 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1497 - acc: 0.9915 - val_loss: 0.1807 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1480 - acc: 0.9915 - val_loss: 0.1790 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 2.0476 - acc: 0.3220 - val_loss: 2.0532 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.8642 - acc: 0.3390 - val_loss: 1.8463 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.6905 - acc: 0.3814 - val_loss: 1.6686 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.5331 - acc: 0.4068 - val_loss: 1.5148 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.3772 - acc: 0.3898 - val_loss: 1.3554 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.2390 - acc: 0.4407 - val_loss: 1.2170 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1190 - acc: 0.4068 - val_loss: 1.0935 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0234 - acc: 0.4915 - val_loss: 0.9935 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9469 - acc: 0.4407 - val_loss: 0.9206 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.8855 - acc: 0.7288 - val_loss: 0.8653 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.8335 - acc: 0.9068 - val_loss: 0.8205 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.7924 - acc: 0.9068 - val_loss: 0.7806 - val_acc: 0.8462\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7576 - acc: 0.9237 - val_loss: 0.7466 - val_acc: 0.9231\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.7272 - acc: 0.9322 - val_loss: 0.7166 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6995 - acc: 0.9322 - val_loss: 0.6898 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6718 - acc: 0.9322 - val_loss: 0.6632 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.6432 - acc: 0.9322 - val_loss: 0.6387 - val_acc: 0.9231\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.6160 - acc: 0.9322 - val_loss: 0.6173 - val_acc: 0.9231\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5910 - acc: 0.9407 - val_loss: 0.5977 - val_acc: 0.9231\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.5673 - acc: 0.9407 - val_loss: 0.5802 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5455 - acc: 0.9492 - val_loss: 0.5640 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5250 - acc: 0.9492 - val_loss: 0.5487 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.5053 - acc: 0.9576 - val_loss: 0.5340 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4866 - acc: 0.9576 - val_loss: 0.5162 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4690 - acc: 0.9576 - val_loss: 0.4992 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4519 - acc: 0.9661 - val_loss: 0.4834 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4361 - acc: 0.9661 - val_loss: 0.4687 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4214 - acc: 0.9661 - val_loss: 0.4545 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4069 - acc: 0.9661 - val_loss: 0.4372 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3932 - acc: 0.9661 - val_loss: 0.4200 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3803 - acc: 0.9661 - val_loss: 0.4039 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3683 - acc: 0.9661 - val_loss: 0.3888 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3570 - acc: 0.9661 - val_loss: 0.3748 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3464 - acc: 0.9661 - val_loss: 0.3614 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3364 - acc: 0.9746 - val_loss: 0.3490 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3270 - acc: 0.9746 - val_loss: 0.3366 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3174 - acc: 0.9746 - val_loss: 0.3250 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3084 - acc: 0.9746 - val_loss: 0.3141 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3001 - acc: 0.9746 - val_loss: 0.3038 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2921 - acc: 0.9661 - val_loss: 0.2940 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2845 - acc: 0.9661 - val_loss: 0.2851 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2773 - acc: 0.9661 - val_loss: 0.2762 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2705 - acc: 0.9661 - val_loss: 0.2679 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.2641 - acc: 0.9661 - val_loss: 0.2603 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2579 - acc: 0.9661 - val_loss: 0.2528 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2520 - acc: 0.9661 - val_loss: 0.2456 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2465 - acc: 0.9661 - val_loss: 0.2390 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2411 - acc: 0.9661 - val_loss: 0.2324 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2357 - acc: 0.9746 - val_loss: 0.2261 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2304 - acc: 0.9746 - val_loss: 0.2199 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2252 - acc: 0.9746 - val_loss: 0.2141 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2203 - acc: 0.9746 - val_loss: 0.2085 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.2157 - acc: 0.9746 - val_loss: 0.2032 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2112 - acc: 0.9746 - val_loss: 0.1982 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2069 - acc: 0.9746 - val_loss: 0.1934 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2029 - acc: 0.9746 - val_loss: 0.1888 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1989 - acc: 0.9746 - val_loss: 0.1843 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1952 - acc: 0.9746 - val_loss: 0.1801 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1916 - acc: 0.9746 - val_loss: 0.1761 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1881 - acc: 0.9746 - val_loss: 0.1721 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1846 - acc: 0.9746 - val_loss: 0.1683 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1812 - acc: 0.9746 - val_loss: 0.1646 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1780 - acc: 0.9746 - val_loss: 0.1612 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1749 - acc: 0.9746 - val_loss: 0.1578 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1719 - acc: 0.9746 - val_loss: 0.1546 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1691 - acc: 0.9746 - val_loss: 0.1515 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.1663 - acc: 0.9746 - val_loss: 0.1486 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1636 - acc: 0.9746 - val_loss: 0.1457 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1610 - acc: 0.9746 - val_loss: 0.1429 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.1585 - acc: 0.9746 - val_loss: 0.1402 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1561 - acc: 0.9746 - val_loss: 0.1377 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1538 - acc: 0.9746 - val_loss: 0.1352 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1516 - acc: 0.9746 - val_loss: 0.1328 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1494 - acc: 0.9746 - val_loss: 0.1305 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1473 - acc: 0.9746 - val_loss: 0.1282 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1452 - acc: 0.9746 - val_loss: 0.1261 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1433 - acc: 0.9746 - val_loss: 0.1240 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1414 - acc: 0.9746 - val_loss: 0.1220 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1395 - acc: 0.9746 - val_loss: 0.1201 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1377 - acc: 0.9746 - val_loss: 0.1182 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1360 - acc: 0.9746 - val_loss: 0.1163 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1343 - acc: 0.9746 - val_loss: 0.1145 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1326 - acc: 0.9746 - val_loss: 0.1128 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1310 - acc: 0.9746 - val_loss: 0.1112 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1295 - acc: 0.9831 - val_loss: 0.1095 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1279 - acc: 0.9831 - val_loss: 0.1079 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1265 - acc: 0.9831 - val_loss: 0.1064 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1250 - acc: 0.9831 - val_loss: 0.1049 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1236 - acc: 0.9831 - val_loss: 0.1034 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1222 - acc: 0.9831 - val_loss: 0.1020 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1209 - acc: 0.9831 - val_loss: 0.1006 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1196 - acc: 0.9831 - val_loss: 0.0993 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1184 - acc: 0.9831 - val_loss: 0.0980 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1171 - acc: 0.9831 - val_loss: 0.0968 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1159 - acc: 0.9831 - val_loss: 0.0956 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1148 - acc: 0.9831 - val_loss: 0.0943 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1136 - acc: 0.9831 - val_loss: 0.0932 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1125 - acc: 0.9831 - val_loss: 0.0920 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1114 - acc: 0.9831 - val_loss: 0.0909 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1103 - acc: 0.9831 - val_loss: 0.0899 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.9641 - acc: 0.4661 - val_loss: 1.7752 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.7925 - acc: 0.4661 - val_loss: 1.6283 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.6423 - acc: 0.4661 - val_loss: 1.5005 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.5075 - acc: 0.4661 - val_loss: 1.3855 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.3907 - acc: 0.4661 - val_loss: 1.2865 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.2922 - acc: 0.4661 - val_loss: 1.1996 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.2031 - acc: 0.4576 - val_loss: 1.1268 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.1272 - acc: 0.4576 - val_loss: 1.0614 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0617 - acc: 0.4576 - val_loss: 1.0067 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0046 - acc: 0.4576 - val_loss: 0.9571 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9538 - acc: 0.4492 - val_loss: 0.9143 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9088 - acc: 0.5424 - val_loss: 0.8758 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8687 - acc: 0.5593 - val_loss: 0.8419 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.8325 - acc: 0.5593 - val_loss: 0.8115 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.7996 - acc: 0.5593 - val_loss: 0.7837 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7701 - acc: 0.5593 - val_loss: 0.7584 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.7419 - acc: 0.5593 - val_loss: 0.7351 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.7167 - acc: 0.5593 - val_loss: 0.7133 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.6928 - acc: 0.5678 - val_loss: 0.6930 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6707 - acc: 0.5847 - val_loss: 0.6738 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6496 - acc: 0.6017 - val_loss: 0.6558 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.6299 - acc: 0.6102 - val_loss: 0.6389 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6117 - acc: 0.6695 - val_loss: 0.6225 - val_acc: 0.8462\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5940 - acc: 0.7881 - val_loss: 0.6073 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5772 - acc: 0.8729 - val_loss: 0.5926 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5610 - acc: 0.9237 - val_loss: 0.5780 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5456 - acc: 0.9492 - val_loss: 0.5644 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.5311 - acc: 0.9576 - val_loss: 0.5514 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5167 - acc: 0.9576 - val_loss: 0.5384 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5030 - acc: 0.9576 - val_loss: 0.5263 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4899 - acc: 0.9576 - val_loss: 0.5142 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4772 - acc: 0.9576 - val_loss: 0.5027 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4648 - acc: 0.9661 - val_loss: 0.4918 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4530 - acc: 0.9661 - val_loss: 0.4807 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4415 - acc: 0.9661 - val_loss: 0.4702 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4303 - acc: 0.9661 - val_loss: 0.4600 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4195 - acc: 0.9661 - val_loss: 0.4499 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4089 - acc: 0.9746 - val_loss: 0.4401 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3990 - acc: 0.9746 - val_loss: 0.4305 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3890 - acc: 0.9746 - val_loss: 0.4213 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3796 - acc: 0.9746 - val_loss: 0.4124 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3702 - acc: 0.9746 - val_loss: 0.4034 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3612 - acc: 0.9746 - val_loss: 0.3948 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3525 - acc: 0.9746 - val_loss: 0.3866 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3440 - acc: 0.9746 - val_loss: 0.3783 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3359 - acc: 0.9746 - val_loss: 0.3703 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3279 - acc: 0.9746 - val_loss: 0.3626 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3202 - acc: 0.9746 - val_loss: 0.3551 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3126 - acc: 0.9746 - val_loss: 0.3477 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3053 - acc: 0.9746 - val_loss: 0.3405 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2983 - acc: 0.9746 - val_loss: 0.3335 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2915 - acc: 0.9746 - val_loss: 0.3268 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2850 - acc: 0.9746 - val_loss: 0.3202 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2787 - acc: 0.9746 - val_loss: 0.3139 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2725 - acc: 0.9746 - val_loss: 0.3077 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2666 - acc: 0.9746 - val_loss: 0.3017 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2609 - acc: 0.9746 - val_loss: 0.2958 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2553 - acc: 0.9746 - val_loss: 0.2902 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2499 - acc: 0.9746 - val_loss: 0.2847 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.2447 - acc: 0.9746 - val_loss: 0.2793 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.2397 - acc: 0.9746 - val_loss: 0.2742 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2348 - acc: 0.9746 - val_loss: 0.2691 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2301 - acc: 0.9746 - val_loss: 0.2642 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2255 - acc: 0.9746 - val_loss: 0.2595 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2212 - acc: 0.9746 - val_loss: 0.2550 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2169 - acc: 0.9746 - val_loss: 0.2505 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2128 - acc: 0.9746 - val_loss: 0.2462 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2089 - acc: 0.9746 - val_loss: 0.2420 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2050 - acc: 0.9746 - val_loss: 0.2379 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2013 - acc: 0.9746 - val_loss: 0.2339 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1977 - acc: 0.9746 - val_loss: 0.2301 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1942 - acc: 0.9746 - val_loss: 0.2264 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1909 - acc: 0.9746 - val_loss: 0.2227 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1875 - acc: 0.9746 - val_loss: 0.2191 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1844 - acc: 0.9746 - val_loss: 0.2157 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1813 - acc: 0.9746 - val_loss: 0.2123 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1783 - acc: 0.9746 - val_loss: 0.2091 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1755 - acc: 0.9746 - val_loss: 0.2059 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1726 - acc: 0.9831 - val_loss: 0.2029 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1699 - acc: 0.9831 - val_loss: 0.1998 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1673 - acc: 0.9831 - val_loss: 0.1969 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1647 - acc: 0.9831 - val_loss: 0.1940 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1622 - acc: 0.9831 - val_loss: 0.1913 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1598 - acc: 0.9831 - val_loss: 0.1886 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1575 - acc: 0.9831 - val_loss: 0.1860 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1552 - acc: 0.9831 - val_loss: 0.1834 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1530 - acc: 0.9831 - val_loss: 0.1809 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1509 - acc: 0.9831 - val_loss: 0.1785 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1488 - acc: 0.9831 - val_loss: 0.1761 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1468 - acc: 0.9831 - val_loss: 0.1738 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1448 - acc: 0.9831 - val_loss: 0.1715 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1429 - acc: 0.9831 - val_loss: 0.1693 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.1410 - acc: 0.9831 - val_loss: 0.1672 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1392 - acc: 0.9831 - val_loss: 0.1651 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1375 - acc: 0.9831 - val_loss: 0.1630 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1357 - acc: 0.9831 - val_loss: 0.1610 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1341 - acc: 0.9831 - val_loss: 0.1591 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1324 - acc: 0.9831 - val_loss: 0.1572 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1308 - acc: 0.9831 - val_loss: 0.1553 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1293 - acc: 0.9831 - val_loss: 0.1535 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 2.2514 - acc: 0.3983 - val_loss: 2.1824 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 2.0715 - acc: 0.3983 - val_loss: 2.0233 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.9174 - acc: 0.4068 - val_loss: 1.8801 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.7820 - acc: 0.4068 - val_loss: 1.7587 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.6675 - acc: 0.4068 - val_loss: 1.6544 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.5682 - acc: 0.4153 - val_loss: 1.5656 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.4807 - acc: 0.4237 - val_loss: 1.4849 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.4031 - acc: 0.4237 - val_loss: 1.4165 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.3360 - acc: 0.4322 - val_loss: 1.3571 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.2779 - acc: 0.4322 - val_loss: 1.3050 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.2285 - acc: 0.4237 - val_loss: 1.2614 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.1859 - acc: 0.4237 - val_loss: 1.2240 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1474 - acc: 0.4153 - val_loss: 1.1903 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1128 - acc: 0.4068 - val_loss: 1.1613 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0813 - acc: 0.4153 - val_loss: 1.1350 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0517 - acc: 0.4153 - val_loss: 1.1103 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0241 - acc: 0.4407 - val_loss: 1.0855 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9989 - acc: 0.4407 - val_loss: 1.0638 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9758 - acc: 0.4407 - val_loss: 1.0432 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9544 - acc: 0.4492 - val_loss: 1.0245 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9349 - acc: 0.4492 - val_loss: 1.0072 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9162 - acc: 0.4492 - val_loss: 0.9907 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8987 - acc: 0.4576 - val_loss: 0.9765 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8823 - acc: 0.4746 - val_loss: 0.9628 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8668 - acc: 0.4831 - val_loss: 0.9494 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.8521 - acc: 0.5508 - val_loss: 0.9375 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8382 - acc: 0.5763 - val_loss: 0.9254 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8248 - acc: 0.6441 - val_loss: 0.9138 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8113 - acc: 0.7119 - val_loss: 0.9025 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.7983 - acc: 0.7373 - val_loss: 0.8905 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7854 - acc: 0.7542 - val_loss: 0.8788 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7728 - acc: 0.7712 - val_loss: 0.8674 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7608 - acc: 0.8220 - val_loss: 0.8561 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7491 - acc: 0.8475 - val_loss: 0.8454 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7378 - acc: 0.8644 - val_loss: 0.8341 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.7264 - acc: 0.8898 - val_loss: 0.8227 - val_acc: 0.7692\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7147 - acc: 0.8983 - val_loss: 0.8105 - val_acc: 0.7692\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7032 - acc: 0.8983 - val_loss: 0.7990 - val_acc: 0.7692\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.6916 - acc: 0.9068 - val_loss: 0.7878 - val_acc: 0.7692\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6802 - acc: 0.9068 - val_loss: 0.7763 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6688 - acc: 0.9068 - val_loss: 0.7648 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6575 - acc: 0.9153 - val_loss: 0.7540 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.6463 - acc: 0.9237 - val_loss: 0.7428 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6351 - acc: 0.9322 - val_loss: 0.7317 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6241 - acc: 0.9407 - val_loss: 0.7208 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6131 - acc: 0.9407 - val_loss: 0.7098 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6022 - acc: 0.9407 - val_loss: 0.6986 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5910 - acc: 0.9407 - val_loss: 0.6876 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5801 - acc: 0.9407 - val_loss: 0.6766 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5690 - acc: 0.9407 - val_loss: 0.6654 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5581 - acc: 0.9407 - val_loss: 0.6543 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5471 - acc: 0.9407 - val_loss: 0.6436 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5363 - acc: 0.9492 - val_loss: 0.6328 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.5256 - acc: 0.9576 - val_loss: 0.6220 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.5151 - acc: 0.9661 - val_loss: 0.6116 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5046 - acc: 0.9661 - val_loss: 0.6009 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4943 - acc: 0.9661 - val_loss: 0.5905 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4841 - acc: 0.9661 - val_loss: 0.5803 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4742 - acc: 0.9661 - val_loss: 0.5702 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4641 - acc: 0.9661 - val_loss: 0.5601 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4542 - acc: 0.9661 - val_loss: 0.5503 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4445 - acc: 0.9746 - val_loss: 0.5406 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4350 - acc: 0.9831 - val_loss: 0.5308 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4255 - acc: 0.9831 - val_loss: 0.5213 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4163 - acc: 0.9831 - val_loss: 0.5119 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4072 - acc: 0.9831 - val_loss: 0.5027 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3984 - acc: 0.9831 - val_loss: 0.4935 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3896 - acc: 0.9831 - val_loss: 0.4845 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3812 - acc: 0.9831 - val_loss: 0.4757 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3728 - acc: 0.9831 - val_loss: 0.4671 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3648 - acc: 0.9831 - val_loss: 0.4586 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3569 - acc: 0.9831 - val_loss: 0.4502 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3491 - acc: 0.9831 - val_loss: 0.4421 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3417 - acc: 0.9831 - val_loss: 0.4341 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3343 - acc: 0.9831 - val_loss: 0.4262 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3273 - acc: 0.9831 - val_loss: 0.4185 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3204 - acc: 0.9831 - val_loss: 0.4111 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3136 - acc: 0.9831 - val_loss: 0.4037 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.3071 - acc: 0.9831 - val_loss: 0.3965 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3008 - acc: 0.9831 - val_loss: 0.3895 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2946 - acc: 0.9831 - val_loss: 0.3826 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2887 - acc: 0.9831 - val_loss: 0.3758 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2829 - acc: 0.9831 - val_loss: 0.3692 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.2773 - acc: 0.9831 - val_loss: 0.3628 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.2718 - acc: 0.9831 - val_loss: 0.3565 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2665 - acc: 0.9915 - val_loss: 0.3503 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2614 - acc: 0.9915 - val_loss: 0.3444 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.2564 - acc: 0.9915 - val_loss: 0.3386 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2516 - acc: 0.9915 - val_loss: 0.3329 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2470 - acc: 0.9915 - val_loss: 0.3275 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2425 - acc: 0.9915 - val_loss: 0.3222 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2381 - acc: 0.9915 - val_loss: 0.3169 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2338 - acc: 0.9915 - val_loss: 0.3117 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.2295 - acc: 0.9915 - val_loss: 0.3067 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2254 - acc: 0.9915 - val_loss: 0.3018 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2214 - acc: 0.9915 - val_loss: 0.2971 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2176 - acc: 0.9915 - val_loss: 0.2924 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2138 - acc: 0.9915 - val_loss: 0.2879 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2101 - acc: 0.9915 - val_loss: 0.2835 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2066 - acc: 0.9915 - val_loss: 0.2792 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 3.0205 - acc: 0.2627 - val_loss: 2.8041 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 2.8013 - acc: 0.2797 - val_loss: 2.5774 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 2.6052 - acc: 0.2966 - val_loss: 2.3849 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 2.4270 - acc: 0.2966 - val_loss: 2.2108 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 2.2670 - acc: 0.3136 - val_loss: 2.0552 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 2.1228 - acc: 0.3305 - val_loss: 1.9162 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.9920 - acc: 0.3475 - val_loss: 1.7916 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.8744 - acc: 0.3814 - val_loss: 1.6819 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.7649 - acc: 0.3898 - val_loss: 1.5785 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.6675 - acc: 0.3898 - val_loss: 1.4883 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.5788 - acc: 0.3898 - val_loss: 1.4068 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.4955 - acc: 0.3983 - val_loss: 1.3335 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.4194 - acc: 0.4237 - val_loss: 1.2661 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.3487 - acc: 0.4407 - val_loss: 1.2036 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.2841 - acc: 0.4407 - val_loss: 1.1477 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.2248 - acc: 0.4492 - val_loss: 1.0981 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1695 - acc: 0.4661 - val_loss: 1.0500 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1189 - acc: 0.4661 - val_loss: 1.0075 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0722 - acc: 0.4661 - val_loss: 0.9684 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0282 - acc: 0.4661 - val_loss: 0.9328 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9879 - acc: 0.4746 - val_loss: 0.8995 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9501 - acc: 0.4746 - val_loss: 0.8690 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9160 - acc: 0.4746 - val_loss: 0.8401 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8836 - acc: 0.4831 - val_loss: 0.8138 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.8542 - acc: 0.4831 - val_loss: 0.7896 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8267 - acc: 0.4831 - val_loss: 0.7664 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8019 - acc: 0.4831 - val_loss: 0.7457 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7790 - acc: 0.4915 - val_loss: 0.7262 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7575 - acc: 0.5000 - val_loss: 0.7080 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7381 - acc: 0.5424 - val_loss: 0.6913 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.7198 - acc: 0.6102 - val_loss: 0.6751 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7028 - acc: 0.7034 - val_loss: 0.6600 - val_acc: 0.7692\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6874 - acc: 0.7542 - val_loss: 0.6460 - val_acc: 0.7692\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6728 - acc: 0.8475 - val_loss: 0.6331 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.6594 - acc: 0.8983 - val_loss: 0.6216 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6470 - acc: 0.9237 - val_loss: 0.6123 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6365 - acc: 0.9492 - val_loss: 0.6037 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6269 - acc: 0.9492 - val_loss: 0.5960 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6183 - acc: 0.9492 - val_loss: 0.5885 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6103 - acc: 0.9576 - val_loss: 0.5818 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6029 - acc: 0.9661 - val_loss: 0.5758 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.5963 - acc: 0.9746 - val_loss: 0.5707 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5899 - acc: 0.9746 - val_loss: 0.5656 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5838 - acc: 0.9746 - val_loss: 0.5608 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5780 - acc: 0.9746 - val_loss: 0.5561 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5722 - acc: 0.9746 - val_loss: 0.5516 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5667 - acc: 0.9746 - val_loss: 0.5471 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.5613 - acc: 0.9746 - val_loss: 0.5429 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5561 - acc: 0.9746 - val_loss: 0.5386 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5511 - acc: 0.9746 - val_loss: 0.5346 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.5463 - acc: 0.9746 - val_loss: 0.5305 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.5417 - acc: 0.9746 - val_loss: 0.5266 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5370 - acc: 0.9746 - val_loss: 0.5227 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5325 - acc: 0.9746 - val_loss: 0.5189 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5282 - acc: 0.9746 - val_loss: 0.5152 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5239 - acc: 0.9746 - val_loss: 0.5115 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5197 - acc: 0.9746 - val_loss: 0.5079 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5156 - acc: 0.9746 - val_loss: 0.5043 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5116 - acc: 0.9746 - val_loss: 0.5008 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.5077 - acc: 0.9746 - val_loss: 0.4973 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5039 - acc: 0.9746 - val_loss: 0.4940 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5002 - acc: 0.9746 - val_loss: 0.4906 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4965 - acc: 0.9746 - val_loss: 0.4874 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.4929 - acc: 0.9746 - val_loss: 0.4841 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4893 - acc: 0.9746 - val_loss: 0.4809 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.4858 - acc: 0.9746 - val_loss: 0.4778 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.4824 - acc: 0.9746 - val_loss: 0.4746 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4790 - acc: 0.9746 - val_loss: 0.4716 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4757 - acc: 0.9746 - val_loss: 0.4685 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4724 - acc: 0.9746 - val_loss: 0.4655 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4692 - acc: 0.9746 - val_loss: 0.4626 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4660 - acc: 0.9746 - val_loss: 0.4596 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4628 - acc: 0.9746 - val_loss: 0.4567 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4596 - acc: 0.9746 - val_loss: 0.4539 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.4565 - acc: 0.9746 - val_loss: 0.4510 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4534 - acc: 0.9746 - val_loss: 0.4483 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.4504 - acc: 0.9746 - val_loss: 0.4455 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4473 - acc: 0.9746 - val_loss: 0.4428 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4443 - acc: 0.9746 - val_loss: 0.4400 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4414 - acc: 0.9746 - val_loss: 0.4374 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4386 - acc: 0.9746 - val_loss: 0.4347 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4357 - acc: 0.9746 - val_loss: 0.4320 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4328 - acc: 0.9746 - val_loss: 0.4295 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4300 - acc: 0.9746 - val_loss: 0.4269 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4272 - acc: 0.9746 - val_loss: 0.4243 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4244 - acc: 0.9746 - val_loss: 0.4219 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4218 - acc: 0.9746 - val_loss: 0.4194 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4190 - acc: 0.9746 - val_loss: 0.4170 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4163 - acc: 0.9746 - val_loss: 0.4146 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4137 - acc: 0.9746 - val_loss: 0.4121 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4111 - acc: 0.9746 - val_loss: 0.4097 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4084 - acc: 0.9746 - val_loss: 0.4074 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4058 - acc: 0.9746 - val_loss: 0.4050 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4032 - acc: 0.9746 - val_loss: 0.4027 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4007 - acc: 0.9746 - val_loss: 0.4004 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3982 - acc: 0.9746 - val_loss: 0.3981 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3958 - acc: 0.9746 - val_loss: 0.3959 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3933 - acc: 0.9746 - val_loss: 0.3936 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3909 - acc: 0.9746 - val_loss: 0.3915 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3886 - acc: 0.9746 - val_loss: 0.3892 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 1.8889 - acc: 0.0763 - val_loss: 2.0351 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.7181 - acc: 0.0000e+00 - val_loss: 1.8192 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.5674 - acc: 0.0169 - val_loss: 1.6267 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.4360 - acc: 0.1695 - val_loss: 1.4652 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.3210 - acc: 0.2966 - val_loss: 1.3242 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.2205 - acc: 0.3390 - val_loss: 1.1967 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1323 - acc: 0.3814 - val_loss: 1.0875 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0563 - acc: 0.3983 - val_loss: 0.9919 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9896 - acc: 0.4068 - val_loss: 0.9107 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9312 - acc: 0.4153 - val_loss: 0.8383 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8805 - acc: 0.4407 - val_loss: 0.7757 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8349 - acc: 0.5169 - val_loss: 0.7231 - val_acc: 0.6923\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7947 - acc: 0.6356 - val_loss: 0.6757 - val_acc: 0.9231\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7586 - acc: 0.7542 - val_loss: 0.6347 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7263 - acc: 0.8051 - val_loss: 0.5967 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6972 - acc: 0.8136 - val_loss: 0.5636 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6712 - acc: 0.8220 - val_loss: 0.5343 - val_acc: 0.9231\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.6476 - acc: 0.8136 - val_loss: 0.5080 - val_acc: 0.9231\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6261 - acc: 0.8136 - val_loss: 0.4842 - val_acc: 0.9231\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.6067 - acc: 0.8559 - val_loss: 0.4625 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.5889 - acc: 0.8559 - val_loss: 0.4432 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5727 - acc: 0.8559 - val_loss: 0.4253 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 0.5578 - acc: 0.8559 - val_loss: 0.4091 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5439 - acc: 0.8559 - val_loss: 0.3944 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5311 - acc: 0.8644 - val_loss: 0.3807 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5191 - acc: 0.8644 - val_loss: 0.3682 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.5080 - acc: 0.8644 - val_loss: 0.3566 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4977 - acc: 0.8644 - val_loss: 0.3458 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4882 - acc: 0.8644 - val_loss: 0.3359 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4792 - acc: 0.8644 - val_loss: 0.3267 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4708 - acc: 0.8644 - val_loss: 0.3179 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4629 - acc: 0.8644 - val_loss: 0.3097 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4555 - acc: 0.8644 - val_loss: 0.3022 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4485 - acc: 0.8644 - val_loss: 0.2951 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4419 - acc: 0.8644 - val_loss: 0.2885 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4356 - acc: 0.8644 - val_loss: 0.2822 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4295 - acc: 0.8644 - val_loss: 0.2762 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4235 - acc: 0.8644 - val_loss: 0.2704 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4178 - acc: 0.8644 - val_loss: 0.2650 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4123 - acc: 0.8644 - val_loss: 0.2598 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4070 - acc: 0.8644 - val_loss: 0.2549 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4019 - acc: 0.8644 - val_loss: 0.2502 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3969 - acc: 0.8644 - val_loss: 0.2455 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3919 - acc: 0.8644 - val_loss: 0.2411 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3872 - acc: 0.8644 - val_loss: 0.2369 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3827 - acc: 0.8644 - val_loss: 0.2329 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.3784 - acc: 0.8644 - val_loss: 0.2291 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3742 - acc: 0.8644 - val_loss: 0.2253 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3701 - acc: 0.8644 - val_loss: 0.2218 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3662 - acc: 0.8644 - val_loss: 0.2184 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3623 - acc: 0.8644 - val_loss: 0.2151 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3587 - acc: 0.8644 - val_loss: 0.2120 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3552 - acc: 0.8644 - val_loss: 0.2090 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3518 - acc: 0.8644 - val_loss: 0.2062 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3485 - acc: 0.8644 - val_loss: 0.2034 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3453 - acc: 0.8644 - val_loss: 0.2007 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.3422 - acc: 0.8644 - val_loss: 0.1981 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3392 - acc: 0.8644 - val_loss: 0.1957 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3363 - acc: 0.8644 - val_loss: 0.1933 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3336 - acc: 0.8644 - val_loss: 0.1910 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3308 - acc: 0.8644 - val_loss: 0.1888 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3282 - acc: 0.8644 - val_loss: 0.1867 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3256 - acc: 0.8644 - val_loss: 0.1846 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3232 - acc: 0.8644 - val_loss: 0.1826 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3208 - acc: 0.8644 - val_loss: 0.1807 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.3184 - acc: 0.8644 - val_loss: 0.1788 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3161 - acc: 0.8644 - val_loss: 0.1771 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3139 - acc: 0.8644 - val_loss: 0.1753 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3117 - acc: 0.8644 - val_loss: 0.1736 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3096 - acc: 0.8644 - val_loss: 0.1719 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3075 - acc: 0.8644 - val_loss: 0.1703 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3055 - acc: 0.8644 - val_loss: 0.1688 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3036 - acc: 0.8644 - val_loss: 0.1673 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.3017 - acc: 0.8644 - val_loss: 0.1658 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2998 - acc: 0.8644 - val_loss: 0.1644 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2980 - acc: 0.8644 - val_loss: 0.1630 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2962 - acc: 0.8644 - val_loss: 0.1616 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2945 - acc: 0.8644 - val_loss: 0.1603 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2928 - acc: 0.8644 - val_loss: 0.1590 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2911 - acc: 0.8644 - val_loss: 0.1578 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2895 - acc: 0.8644 - val_loss: 0.1565 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2880 - acc: 0.8644 - val_loss: 0.1554 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2864 - acc: 0.8644 - val_loss: 0.1542 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2849 - acc: 0.8644 - val_loss: 0.1531 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2834 - acc: 0.8644 - val_loss: 0.1520 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2820 - acc: 0.8644 - val_loss: 0.1509 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.2806 - acc: 0.8644 - val_loss: 0.1498 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2792 - acc: 0.8644 - val_loss: 0.1488 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2778 - acc: 0.8644 - val_loss: 0.1478 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2765 - acc: 0.8644 - val_loss: 0.1468 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2752 - acc: 0.8644 - val_loss: 0.1459 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2739 - acc: 0.8644 - val_loss: 0.1449 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2726 - acc: 0.8644 - val_loss: 0.1440 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2713 - acc: 0.8644 - val_loss: 0.1431 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2701 - acc: 0.8644 - val_loss: 0.1422 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2688 - acc: 0.8644 - val_loss: 0.1413 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2676 - acc: 0.8644 - val_loss: 0.1405 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2665 - acc: 0.8644 - val_loss: 0.1396 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2653 - acc: 0.8644 - val_loss: 0.1388 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2642 - acc: 0.8644 - val_loss: 0.1380 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 2.7009 - acc: 0.0085 - val_loss: 2.8032 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 2.5049 - acc: 0.0085 - val_loss: 2.5897 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.3242 - acc: 0.0085 - val_loss: 2.3992 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 2.1604 - acc: 0.0085 - val_loss: 2.2218 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 2.0118 - acc: 0.0085 - val_loss: 2.0583 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.8741 - acc: 0.0085 - val_loss: 1.9060 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.7481 - acc: 0.0085 - val_loss: 1.7646 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.6352 - acc: 0.0169 - val_loss: 1.6410 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.5334 - acc: 0.0169 - val_loss: 1.5244 - val_acc: 0.0000e+00\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4399 - acc: 0.0085 - val_loss: 1.4205 - val_acc: 0.0000e+00\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.3559 - acc: 0.0169 - val_loss: 1.3273 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.2806 - acc: 0.0254 - val_loss: 1.2446 - val_acc: 0.0000e+00\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2133 - acc: 0.0678 - val_loss: 1.1655 - val_acc: 0.0769\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1508 - acc: 0.1610 - val_loss: 1.0928 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0945 - acc: 0.2712 - val_loss: 1.0265 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0423 - acc: 0.3475 - val_loss: 0.9671 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9951 - acc: 0.4576 - val_loss: 0.9141 - val_acc: 0.6923\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9515 - acc: 0.5847 - val_loss: 0.8641 - val_acc: 0.7692\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9119 - acc: 0.6186 - val_loss: 0.8181 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8758 - acc: 0.6695 - val_loss: 0.7774 - val_acc: 0.8462\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.8425 - acc: 0.6780 - val_loss: 0.7399 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8121 - acc: 0.7203 - val_loss: 0.7050 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7840 - acc: 0.7373 - val_loss: 0.6732 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7578 - acc: 0.7458 - val_loss: 0.6436 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7334 - acc: 0.7627 - val_loss: 0.6166 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.7107 - acc: 0.7712 - val_loss: 0.5911 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6895 - acc: 0.7881 - val_loss: 0.5676 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.6698 - acc: 0.7881 - val_loss: 0.5452 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6511 - acc: 0.7966 - val_loss: 0.5245 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6333 - acc: 0.7966 - val_loss: 0.5047 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6164 - acc: 0.8051 - val_loss: 0.4862 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6005 - acc: 0.8305 - val_loss: 0.4685 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5856 - acc: 0.8305 - val_loss: 0.4522 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.5717 - acc: 0.8390 - val_loss: 0.4369 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5586 - acc: 0.8390 - val_loss: 0.4223 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.5464 - acc: 0.8390 - val_loss: 0.4085 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5346 - acc: 0.8475 - val_loss: 0.3958 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5237 - acc: 0.8559 - val_loss: 0.3837 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5134 - acc: 0.8559 - val_loss: 0.3726 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5036 - acc: 0.8559 - val_loss: 0.3618 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4943 - acc: 0.8559 - val_loss: 0.3517 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4856 - acc: 0.8559 - val_loss: 0.3425 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4771 - acc: 0.8559 - val_loss: 0.3334 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4690 - acc: 0.8559 - val_loss: 0.3248 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4611 - acc: 0.8644 - val_loss: 0.3167 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4537 - acc: 0.8644 - val_loss: 0.3088 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4463 - acc: 0.8644 - val_loss: 0.3013 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4391 - acc: 0.8644 - val_loss: 0.2941 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4324 - acc: 0.8644 - val_loss: 0.2874 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4258 - acc: 0.8644 - val_loss: 0.2809 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4195 - acc: 0.8644 - val_loss: 0.2748 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4136 - acc: 0.8644 - val_loss: 0.2689 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4079 - acc: 0.8644 - val_loss: 0.2632 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4023 - acc: 0.8644 - val_loss: 0.2579 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3969 - acc: 0.8644 - val_loss: 0.2527 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3918 - acc: 0.8644 - val_loss: 0.2477 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3868 - acc: 0.8644 - val_loss: 0.2430 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3820 - acc: 0.8644 - val_loss: 0.2384 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3774 - acc: 0.8644 - val_loss: 0.2340 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3730 - acc: 0.8644 - val_loss: 0.2299 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3687 - acc: 0.8644 - val_loss: 0.2259 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3646 - acc: 0.8644 - val_loss: 0.2220 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3606 - acc: 0.8644 - val_loss: 0.2184 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3568 - acc: 0.8644 - val_loss: 0.2148 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.3530 - acc: 0.8644 - val_loss: 0.2115 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3495 - acc: 0.8644 - val_loss: 0.2082 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3460 - acc: 0.8644 - val_loss: 0.2050 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3426 - acc: 0.8644 - val_loss: 0.2020 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3394 - acc: 0.8644 - val_loss: 0.1992 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.3363 - acc: 0.8644 - val_loss: 0.1963 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3332 - acc: 0.8644 - val_loss: 0.1936 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3303 - acc: 0.8644 - val_loss: 0.1909 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3274 - acc: 0.8644 - val_loss: 0.1885 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3246 - acc: 0.8644 - val_loss: 0.1860 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3220 - acc: 0.8644 - val_loss: 0.1837 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3194 - acc: 0.8644 - val_loss: 0.1815 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3168 - acc: 0.8644 - val_loss: 0.1792 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3143 - acc: 0.8644 - val_loss: 0.1771 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3120 - acc: 0.8644 - val_loss: 0.1751 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3096 - acc: 0.8644 - val_loss: 0.1731 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3074 - acc: 0.8644 - val_loss: 0.1712 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3052 - acc: 0.8644 - val_loss: 0.1693 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3030 - acc: 0.8644 - val_loss: 0.1676 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3009 - acc: 0.8644 - val_loss: 0.1658 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2989 - acc: 0.8644 - val_loss: 0.1641 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2969 - acc: 0.8644 - val_loss: 0.1625 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2950 - acc: 0.8644 - val_loss: 0.1609 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2931 - acc: 0.8644 - val_loss: 0.1594 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2913 - acc: 0.8644 - val_loss: 0.1578 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2895 - acc: 0.8644 - val_loss: 0.1564 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2877 - acc: 0.8644 - val_loss: 0.1550 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2860 - acc: 0.8644 - val_loss: 0.1536 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2843 - acc: 0.8644 - val_loss: 0.1523 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2827 - acc: 0.8644 - val_loss: 0.1510 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2811 - acc: 0.8644 - val_loss: 0.1497 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2795 - acc: 0.8644 - val_loss: 0.1484 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2780 - acc: 0.8644 - val_loss: 0.1473 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2764 - acc: 0.8644 - val_loss: 0.1461 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2750 - acc: 0.8644 - val_loss: 0.1449 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2735 - acc: 0.8644 - val_loss: 0.1438 - val_acc: 0.9231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "21c16c7d-8838-4b36-a41c-f783cda236e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.46815712,  2.41613978],\n",
              "       [-2.04213808, -1.20055142],\n",
              "       [-2.26598031, -2.51797746],\n",
              "       [-1.62561816, -1.97201226],\n",
              "       [-2.12949607, -1.76064808],\n",
              "       [-1.32853889, -2.05415965],\n",
              "       [-1.86366758, -1.96105974],\n",
              "       [ 3.1426414 ,  0.23838493],\n",
              "       [ 2.76856724,  0.87507987],\n",
              "       [ 3.33006485,  0.80105572],\n",
              "       [ 3.99255757,  0.1366972 ],\n",
              "       [ 2.79798337,  1.20174003],\n",
              "       [ 1.67824072,  0.59967329]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d9bb7a57-af65-4dc6-ebb1-9cbd5d7ff954",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8ceaaec9-b2b0-45ae-850b-b4df1cb25db6",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4991a82-2b24-48c0-e5b3-ea0e49e2800e",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9476850-36ab-459d-b6e0-7617cb151ff7",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1e25d12e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdd4jKY+pMCaQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fd5b4158-6eab-4035-cef1-f25ca4a8e857",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1e255bda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3e50ea03-682f-4c4b-b28b-f39411de60c1",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 0.6371 - acc: 0.5344\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.6149 - acc: 0.5344\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.5943 - acc: 0.5344\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5748 - acc: 0.5420\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.5561 - acc: 0.5725\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.5392 - acc: 0.7557\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.5237 - acc: 0.8931\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.5090 - acc: 0.9008\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4952 - acc: 0.9084\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4820 - acc: 0.9237\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4696 - acc: 0.9313\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4577 - acc: 0.9389\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4467 - acc: 0.9389\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.4359 - acc: 0.9389\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4257 - acc: 0.9466\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4160 - acc: 0.9466\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4067 - acc: 0.9618\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3978 - acc: 0.9618\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3892 - acc: 0.9618\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.3809 - acc: 0.9618\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.3727 - acc: 0.9618\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.3649 - acc: 0.9542\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3574 - acc: 0.9542\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.3502 - acc: 0.9466\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.3433 - acc: 0.9466\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3362 - acc: 0.9466\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3297 - acc: 0.9466\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3231 - acc: 0.9466\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.3170 - acc: 0.9466\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.3111 - acc: 0.9618\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3051 - acc: 0.9618\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2992 - acc: 0.9618\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2935 - acc: 0.9618\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2880 - acc: 0.9618\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.2825 - acc: 0.9618\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.2773 - acc: 0.9618\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.2722 - acc: 0.9618\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.2674 - acc: 0.9618\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.2626 - acc: 0.9618\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2580 - acc: 0.9695\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2532 - acc: 0.9695\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.2486 - acc: 0.9695\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2442 - acc: 0.9771\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2399 - acc: 0.9771\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.2355 - acc: 0.9771\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.2312 - acc: 0.9771\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.2271 - acc: 0.9771\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.2232 - acc: 0.9924\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2194 - acc: 0.9924\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.2155 - acc: 0.9924\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2118 - acc: 0.9924\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2081 - acc: 0.9924\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.2047 - acc: 0.9924\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2012 - acc: 0.9924\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.1979 - acc: 0.9924\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.1947 - acc: 0.9924\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.1914 - acc: 0.9924\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1884 - acc: 0.9924\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.1853 - acc: 0.9924\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1823 - acc: 0.9924\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1794 - acc: 0.9924\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1766 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.1737 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.1711 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.1684 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.1658 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.1633 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.1609 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.1585 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.1561 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.1538 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1515 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1493 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.1472 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1452 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.1432 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.1412 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.1392 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.1374 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.1355 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.1337 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.1320 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.1303 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.1285 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.1268 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.1252 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.1237 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1222 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.1207 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.1193 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.1178 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.1164 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.1151 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.1137 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1123 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.1110 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1098 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.1085 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.1073 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.1061 - acc: 1.0000\n",
            "34/34 [==============================] - 0s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d638b73b-264a-4e85-d587-a7a28e144cc0",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9bb3bde6-ec8c-422d-d330-6b3872595459",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17647058823529413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iiJh2sP6IiF",
        "colab_type": "text"
      },
      "source": [
        "#Classification without PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZN9I7JNNUyS",
        "colab_type": "text"
      },
      "source": [
        "# Prova remove correlated features with treshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wACff-R4Ib",
        "colab_type": "text"
      },
      "source": [
        "https://campus.datacamp.com/courses/dimensionality-reduction-in-python/feature-selection-i-selecting-for-feature-information?ex=14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNufM8B3NYs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a positive correlation matrix\n",
        "corr_df = train_data_stand.corr().abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8ShRUvN63m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create and apply mask\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DgfSECpOwPu",
        "colab_type": "code",
        "outputId": "ee78981c-e345-4a84-bd00-0280b2515dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [False,  True,  True, ...,  True,  True,  True],\n",
              "       [False, False,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [False, False, False, ...,  True,  True,  True],\n",
              "       [False, False, False, ..., False,  True,  True],\n",
              "       [False, False, False, ..., False, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xQLptVOw6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri_df = corr_df.mask(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDvePEPO8mn",
        "colab_type": "code",
        "outputId": "16922220-37e6-4d32-e933-7dbb43be7459",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tri_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>VoxelVolume</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <td>0.821982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MeshVolume</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <td>0.785606</td>\n",
              "      <td>0.964760</td>\n",
              "      <td>0.785457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sphericity</th>\n",
              "      <td>0.329751</td>\n",
              "      <td>0.678628</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.694906</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coarseness</th>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.586992</td>\n",
              "      <td>0.401370</td>\n",
              "      <td>0.546184</td>\n",
              "      <td>0.356708</td>\n",
              "      <td>0.569919</td>\n",
              "      <td>0.048602</td>\n",
              "      <td>0.813595</td>\n",
              "      <td>0.614989</td>\n",
              "      <td>0.094676</td>\n",
              "      <td>0.471734</td>\n",
              "      <td>0.598815</td>\n",
              "      <td>0.567029</td>\n",
              "      <td>0.554227</td>\n",
              "      <td>0.192052</td>\n",
              "      <td>0.605729</td>\n",
              "      <td>0.246122</td>\n",
              "      <td>0.406714</td>\n",
              "      <td>0.351606</td>\n",
              "      <td>0.669357</td>\n",
              "      <td>0.183179</td>\n",
              "      <td>0.731964</td>\n",
              "      <td>0.417524</td>\n",
              "      <td>0.077465</td>\n",
              "      <td>0.379163</td>\n",
              "      <td>0.437111</td>\n",
              "      <td>0.787425</td>\n",
              "      <td>0.182358</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>0.119708</td>\n",
              "      <td>0.324753</td>\n",
              "      <td>0.661086</td>\n",
              "      <td>0.326758</td>\n",
              "      <td>0.468498</td>\n",
              "      <td>0.481677</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.319663</td>\n",
              "      <td>0.667850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350082</td>\n",
              "      <td>0.292163</td>\n",
              "      <td>0.539925</td>\n",
              "      <td>0.345026</td>\n",
              "      <td>0.148147</td>\n",
              "      <td>0.251303</td>\n",
              "      <td>0.399211</td>\n",
              "      <td>0.336092</td>\n",
              "      <td>0.360524</td>\n",
              "      <td>0.381037</td>\n",
              "      <td>0.556410</td>\n",
              "      <td>0.412333</td>\n",
              "      <td>0.521415</td>\n",
              "      <td>0.480184</td>\n",
              "      <td>0.512788</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.194296</td>\n",
              "      <td>0.595126</td>\n",
              "      <td>0.552550</td>\n",
              "      <td>0.201612</td>\n",
              "      <td>0.272599</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.374723</td>\n",
              "      <td>0.426885</td>\n",
              "      <td>0.417651</td>\n",
              "      <td>0.272696</td>\n",
              "      <td>0.501061</td>\n",
              "      <td>0.667822</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.511486</td>\n",
              "      <td>0.366279</td>\n",
              "      <td>0.704958</td>\n",
              "      <td>0.523589</td>\n",
              "      <td>0.754391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Complexity</th>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.257297</td>\n",
              "      <td>0.277025</td>\n",
              "      <td>0.229141</td>\n",
              "      <td>0.182615</td>\n",
              "      <td>0.268450</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.214902</td>\n",
              "      <td>0.256226</td>\n",
              "      <td>0.019975</td>\n",
              "      <td>0.283738</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>0.441669</td>\n",
              "      <td>0.213055</td>\n",
              "      <td>0.423394</td>\n",
              "      <td>0.392267</td>\n",
              "      <td>0.103854</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.422629</td>\n",
              "      <td>0.058114</td>\n",
              "      <td>0.198407</td>\n",
              "      <td>0.214767</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.125140</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.275899</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.322120</td>\n",
              "      <td>0.410309</td>\n",
              "      <td>0.276311</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>0.275068</td>\n",
              "      <td>0.221594</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.125930</td>\n",
              "      <td>0.298911</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267658</td>\n",
              "      <td>0.198372</td>\n",
              "      <td>0.116352</td>\n",
              "      <td>0.255976</td>\n",
              "      <td>0.464523</td>\n",
              "      <td>0.267090</td>\n",
              "      <td>0.228132</td>\n",
              "      <td>0.195523</td>\n",
              "      <td>0.139835</td>\n",
              "      <td>0.198482</td>\n",
              "      <td>0.296969</td>\n",
              "      <td>0.359280</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>0.168824</td>\n",
              "      <td>0.228273</td>\n",
              "      <td>0.355694</td>\n",
              "      <td>0.220089</td>\n",
              "      <td>0.151630</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.011797</td>\n",
              "      <td>0.469854</td>\n",
              "      <td>0.036549</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.274863</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.351102</td>\n",
              "      <td>0.063147</td>\n",
              "      <td>0.152285</td>\n",
              "      <td>0.074162</td>\n",
              "      <td>0.313890</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.083326</td>\n",
              "      <td>0.166795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strength</th>\n",
              "      <td>0.510155</td>\n",
              "      <td>0.642559</td>\n",
              "      <td>0.509976</td>\n",
              "      <td>0.607597</td>\n",
              "      <td>0.339595</td>\n",
              "      <td>0.645544</td>\n",
              "      <td>0.021965</td>\n",
              "      <td>0.758610</td>\n",
              "      <td>0.670268</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.562243</td>\n",
              "      <td>0.659201</td>\n",
              "      <td>0.624228</td>\n",
              "      <td>0.620794</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.388883</td>\n",
              "      <td>0.341710</td>\n",
              "      <td>0.538344</td>\n",
              "      <td>0.425689</td>\n",
              "      <td>0.458791</td>\n",
              "      <td>0.226763</td>\n",
              "      <td>0.486770</td>\n",
              "      <td>0.290047</td>\n",
              "      <td>0.085559</td>\n",
              "      <td>0.272032</td>\n",
              "      <td>0.325476</td>\n",
              "      <td>0.564383</td>\n",
              "      <td>0.104481</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.099738</td>\n",
              "      <td>0.184149</td>\n",
              "      <td>0.141523</td>\n",
              "      <td>0.432332</td>\n",
              "      <td>0.139576</td>\n",
              "      <td>0.296993</td>\n",
              "      <td>0.271914</td>\n",
              "      <td>0.332061</td>\n",
              "      <td>0.195143</td>\n",
              "      <td>0.422246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126145</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.283099</td>\n",
              "      <td>0.224183</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>0.155122</td>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.240018</td>\n",
              "      <td>0.438144</td>\n",
              "      <td>0.264077</td>\n",
              "      <td>0.333647</td>\n",
              "      <td>0.530074</td>\n",
              "      <td>0.343866</td>\n",
              "      <td>0.337177</td>\n",
              "      <td>0.347487</td>\n",
              "      <td>0.027798</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.372899</td>\n",
              "      <td>0.365016</td>\n",
              "      <td>0.083608</td>\n",
              "      <td>0.329544</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.390861</td>\n",
              "      <td>0.567592</td>\n",
              "      <td>0.552634</td>\n",
              "      <td>0.329531</td>\n",
              "      <td>0.225342</td>\n",
              "      <td>0.451164</td>\n",
              "      <td>0.206294</td>\n",
              "      <td>0.221538</td>\n",
              "      <td>0.253885</td>\n",
              "      <td>0.383804</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.544355</td>\n",
              "      <td>0.611316</td>\n",
              "      <td>0.746738</td>\n",
              "      <td>0.074615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contrast.1</th>\n",
              "      <td>0.458069</td>\n",
              "      <td>0.592542</td>\n",
              "      <td>0.457835</td>\n",
              "      <td>0.555373</td>\n",
              "      <td>0.380133</td>\n",
              "      <td>0.588236</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>0.639843</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.065998</td>\n",
              "      <td>0.515229</td>\n",
              "      <td>0.623406</td>\n",
              "      <td>0.576338</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.699380</td>\n",
              "      <td>0.654218</td>\n",
              "      <td>0.165224</td>\n",
              "      <td>0.399133</td>\n",
              "      <td>0.462206</td>\n",
              "      <td>0.847990</td>\n",
              "      <td>0.404501</td>\n",
              "      <td>0.807251</td>\n",
              "      <td>0.503620</td>\n",
              "      <td>0.040094</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>0.507856</td>\n",
              "      <td>0.811825</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.674231</td>\n",
              "      <td>0.184830</td>\n",
              "      <td>0.427215</td>\n",
              "      <td>0.958897</td>\n",
              "      <td>0.507007</td>\n",
              "      <td>0.941239</td>\n",
              "      <td>0.822280</td>\n",
              "      <td>0.836388</td>\n",
              "      <td>0.858476</td>\n",
              "      <td>0.916872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777488</td>\n",
              "      <td>0.592929</td>\n",
              "      <td>0.882098</td>\n",
              "      <td>0.561351</td>\n",
              "      <td>0.652744</td>\n",
              "      <td>0.457706</td>\n",
              "      <td>0.693751</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.473433</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.601669</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.679507</td>\n",
              "      <td>0.566054</td>\n",
              "      <td>0.635554</td>\n",
              "      <td>0.154862</td>\n",
              "      <td>0.617903</td>\n",
              "      <td>0.665488</td>\n",
              "      <td>0.705727</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.390378</td>\n",
              "      <td>0.398000</td>\n",
              "      <td>0.502327</td>\n",
              "      <td>0.354174</td>\n",
              "      <td>0.437122</td>\n",
              "      <td>0.390572</td>\n",
              "      <td>0.610009</td>\n",
              "      <td>0.852276</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.242269</td>\n",
              "      <td>0.640887</td>\n",
              "      <td>0.494642</td>\n",
              "      <td>0.794082</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.759051</td>\n",
              "      <td>0.551508</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.374697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Busyness</th>\n",
              "      <td>0.937894</td>\n",
              "      <td>0.810919</td>\n",
              "      <td>0.937881</td>\n",
              "      <td>0.773599</td>\n",
              "      <td>0.339451</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.682675</td>\n",
              "      <td>0.843581</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>0.913619</td>\n",
              "      <td>0.867644</td>\n",
              "      <td>0.805965</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>0.176933</td>\n",
              "      <td>0.469093</td>\n",
              "      <td>0.023465</td>\n",
              "      <td>0.890974</td>\n",
              "      <td>0.884836</td>\n",
              "      <td>0.556691</td>\n",
              "      <td>0.356571</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.253053</td>\n",
              "      <td>0.425932</td>\n",
              "      <td>0.366938</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.364459</td>\n",
              "      <td>0.187027</td>\n",
              "      <td>0.262478</td>\n",
              "      <td>0.474823</td>\n",
              "      <td>0.313668</td>\n",
              "      <td>0.411803</td>\n",
              "      <td>0.469366</td>\n",
              "      <td>0.490211</td>\n",
              "      <td>0.354087</td>\n",
              "      <td>0.516006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294360</td>\n",
              "      <td>0.307158</td>\n",
              "      <td>0.382443</td>\n",
              "      <td>0.108961</td>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>0.428333</td>\n",
              "      <td>0.320272</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.365808</td>\n",
              "      <td>0.400702</td>\n",
              "      <td>0.915079</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.448260</td>\n",
              "      <td>0.462777</td>\n",
              "      <td>0.126498</td>\n",
              "      <td>0.276150</td>\n",
              "      <td>0.466663</td>\n",
              "      <td>0.497241</td>\n",
              "      <td>0.130804</td>\n",
              "      <td>0.770513</td>\n",
              "      <td>0.135920</td>\n",
              "      <td>0.372307</td>\n",
              "      <td>0.858147</td>\n",
              "      <td>0.912460</td>\n",
              "      <td>0.770458</td>\n",
              "      <td>0.381146</td>\n",
              "      <td>0.557089</td>\n",
              "      <td>0.516059</td>\n",
              "      <td>0.592938</td>\n",
              "      <td>0.403923</td>\n",
              "      <td>0.370737</td>\n",
              "      <td>0.363137</td>\n",
              "      <td>0.403091</td>\n",
              "      <td>0.382465</td>\n",
              "      <td>0.434561</td>\n",
              "      <td>0.093528</td>\n",
              "      <td>0.602417</td>\n",
              "      <td>0.418697</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "VoxelVolume                NaN                NaN  ...         NaN       NaN\n",
              "Maximum3DDiameter     0.821982                NaN  ...         NaN       NaN\n",
              "MeshVolume            0.999999           0.821800  ...         NaN       NaN\n",
              "MajorAxisLength       0.785606           0.964760  ...         NaN       NaN\n",
              "Sphericity            0.329751           0.678628  ...         NaN       NaN\n",
              "...                        ...                ...  ...         ...       ...\n",
              "Coarseness            0.401634           0.586992  ...         NaN       NaN\n",
              "Complexity            0.277302           0.257297  ...         NaN       NaN\n",
              "Strength              0.510155           0.642559  ...         NaN       NaN\n",
              "Contrast.1            0.458069           0.592542  ...         NaN       NaN\n",
              "Busyness              0.937894           0.810919  ...    0.418697       NaN\n",
              "\n",
              "[107 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1MkWP2O9hU",
        "colab_type": "code",
        "outputId": "f7c386e2-2f7f-4a27-dd60-0182ecf9c611",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Find columns that meet threshold\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.70)]\n",
        "to_drop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VoxelVolume',\n",
              " 'Maximum3DDiameter',\n",
              " 'MeshVolume',\n",
              " 'MajorAxisLength',\n",
              " 'LeastAxisLength',\n",
              " 'Elongation',\n",
              " 'SurfaceVolumeRatio',\n",
              " 'Maximum2DDiameterSlice',\n",
              " 'SurfaceArea',\n",
              " 'MinorAxisLength',\n",
              " 'Maximum2DDiameterColumn',\n",
              " 'Maximum2DDiameterRow',\n",
              " 'GrayLevelVariance',\n",
              " 'HighGrayLevelEmphasis',\n",
              " 'DependenceEntropy',\n",
              " 'DependenceNonUniformity',\n",
              " 'GrayLevelNonUniformity',\n",
              " 'SmallDependenceEmphasis',\n",
              " 'SmallDependenceHighGrayLevelEmphasis',\n",
              " 'DependenceNonUniformityNormalized',\n",
              " 'LargeDependenceEmphasis',\n",
              " 'LargeDependenceLowGrayLevelEmphasis',\n",
              " 'DependenceVariance',\n",
              " 'LargeDependenceHighGrayLevelEmphasis',\n",
              " 'SmallDependenceLowGrayLevelEmphasis',\n",
              " 'LowGrayLevelEmphasis',\n",
              " 'JointAverage',\n",
              " 'SumAverage',\n",
              " 'JointEntropy',\n",
              " 'ClusterShade',\n",
              " 'MaximumProbability',\n",
              " 'Idmn',\n",
              " 'JointEnergy',\n",
              " 'Contrast',\n",
              " 'DifferenceEntropy',\n",
              " 'InverseVariance',\n",
              " 'DifferenceVariance',\n",
              " 'Idn',\n",
              " 'Idm',\n",
              " 'Correlation',\n",
              " 'Autocorrelation',\n",
              " 'SumEntropy',\n",
              " 'SumSquares',\n",
              " 'ClusterProminence',\n",
              " 'Imc2',\n",
              " 'DifferenceAverage',\n",
              " 'Id',\n",
              " 'ClusterTendency',\n",
              " 'InterquartileRange',\n",
              " 'Skewness',\n",
              " 'Uniformity',\n",
              " 'Median',\n",
              " 'Energy',\n",
              " 'RobustMeanAbsoluteDeviation',\n",
              " 'MeanAbsoluteDeviation',\n",
              " 'Maximum',\n",
              " 'RootMeanSquared',\n",
              " 'Minimum',\n",
              " 'Entropy',\n",
              " 'Range',\n",
              " 'Variance',\n",
              " '10Percentile',\n",
              " 'Kurtosis',\n",
              " 'Mean',\n",
              " 'ShortRunLowGrayLevelEmphasis',\n",
              " 'GrayLevelVariance.1',\n",
              " 'LowGrayLevelRunEmphasis',\n",
              " 'GrayLevelNonUniformityNormalized',\n",
              " 'RunVariance',\n",
              " 'GrayLevelNonUniformity.1',\n",
              " 'LongRunEmphasis',\n",
              " 'ShortRunHighGrayLevelEmphasis',\n",
              " 'RunLengthNonUniformity',\n",
              " 'ShortRunEmphasis',\n",
              " 'LongRunHighGrayLevelEmphasis',\n",
              " 'RunPercentage',\n",
              " 'LongRunLowGrayLevelEmphasis',\n",
              " 'RunEntropy',\n",
              " 'HighGrayLevelRunEmphasis',\n",
              " 'RunLengthNonUniformityNormalized',\n",
              " 'ZoneVariance',\n",
              " 'SizeZoneNonUniformityNormalized',\n",
              " 'SizeZoneNonUniformity',\n",
              " 'GrayLevelNonUniformity.2',\n",
              " 'LargeAreaEmphasis',\n",
              " 'SmallAreaHighGrayLevelEmphasis',\n",
              " 'ZonePercentage',\n",
              " 'LowGrayLevelZoneEmphasis',\n",
              " 'SmallAreaLowGrayLevelEmphasis',\n",
              " 'Coarseness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKeereQoci",
        "colab_type": "text"
      },
      "source": [
        "The reason we used the mask to set half of the matrox to NA value is that we eant to avoid removing both features when thay have a strong correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R-tG-WPNXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "train_data_stand_reduced = train_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNUiUoZ0Vdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "test_data_stand_reduced = test_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-uuBifRO8G",
        "colab_type": "code",
        "outputId": "ee6afb66-1c65-4b93-ea25-eaca4d794d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wuGcLV0acQ",
        "colab_type": "code",
        "outputId": "1a356262-7ba4-4f0d-c73f-690a1a994bbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1qR3re7QwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_reduced = train_data_stand_reduced.to_numpy()\n",
        "test_data_stand_reduced = test_data_stand_reduced.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUG3a0o5W1O",
        "colab_type": "code",
        "outputId": "31ab1849-1711-4ca4-e39e-7e3ffaa26d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8wkFES71GJ",
        "colab_type": "code",
        "outputId": "72366e0e-4995-41a8-8327-0ee1350ecd37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljMaQeuSHe1",
        "colab_type": "text"
      },
      "source": [
        "funziona bene, però bisogna stare attenti a basarsi unicamente sul coefficiente di correlazione. Se y = x^2, x e y risulteranno scorrelate secondo il coeffiente di correlazione di Pearson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxuH4u741mLR",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(17,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmeGHBV1xs_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5hBUph6149E"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7NwNpEO149J"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MD1VB0S149O",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9952a109-229d-43c7-9552-cdf986e876f4",
        "id": "VkpRnrey149Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_reduced, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7ee5e368-62be-4d55-cc5e-d3be7f750bdf",
        "id": "F3zK4E6o149g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "12b606c2-110a-4b6e-8808-6f487161036d",
        "id": "BDX_0MCd149o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9QlfChU149v",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvCKZfjj1490",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bjrjhWo1497"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Du7DFfm1498",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1815450e-7713-44d5-9069-3d97a381957f",
        "id": "kHNpJxBL14-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 30\n",
        "all_acc_histories_reduced = []\n",
        "all_loss_histories_reduced = []\n",
        "all_val_acc_histories_reduced = []\n",
        "all_val_loss_histories_reduced = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_reduced[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_reduced[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history_reduced = history.history['acc']\n",
        "  all_acc_histories_reduced.append(acc_history)\n",
        "\n",
        "  loss_history_reduced = history.history['loss']\n",
        "  all_loss_histories_reduced.append(loss_history)\n",
        "\n",
        "  acc_val_history_reduced = history.history['val_acc']\n",
        "  all_val_acc_histories_reduced.append(acc_val_history)\n",
        "\n",
        "  loss_val_history_reduced = history.history['val_loss']\n",
        "  all_val_loss_histories_reduced.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/30\n",
            "117/117 [==============================] - 1s 12ms/step - loss: 1.2475 - acc: 0.3675 - val_loss: 1.0752 - val_acc: 0.5000\n",
            "Epoch 2/30\n",
            "117/117 [==============================] - 0s 211us/step - loss: 1.1480 - acc: 0.4274 - val_loss: 1.0309 - val_acc: 0.5000\n",
            "Epoch 3/30\n",
            "117/117 [==============================] - 0s 198us/step - loss: 1.0909 - acc: 0.4786 - val_loss: 1.0020 - val_acc: 0.4286\n",
            "Epoch 4/30\n",
            "117/117 [==============================] - 0s 182us/step - loss: 1.0505 - acc: 0.5556 - val_loss: 0.9863 - val_acc: 0.3571\n",
            "Epoch 5/30\n",
            "117/117 [==============================] - 0s 190us/step - loss: 1.0221 - acc: 0.5556 - val_loss: 0.9710 - val_acc: 0.3571\n",
            "Epoch 6/30\n",
            "117/117 [==============================] - 0s 189us/step - loss: 1.0012 - acc: 0.5641 - val_loss: 0.9557 - val_acc: 0.3571\n",
            "Epoch 7/30\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.9832 - acc: 0.5812 - val_loss: 0.9497 - val_acc: 0.3571\n",
            "Epoch 8/30\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.9692 - acc: 0.5897 - val_loss: 0.9429 - val_acc: 0.4286\n",
            "Epoch 9/30\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.9557 - acc: 0.5897 - val_loss: 0.9381 - val_acc: 0.4286\n",
            "Epoch 10/30\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.9468 - acc: 0.5812 - val_loss: 0.9372 - val_acc: 0.3571\n",
            "Epoch 11/30\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.9385 - acc: 0.6068 - val_loss: 0.9334 - val_acc: 0.3571\n",
            "Epoch 12/30\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.9295 - acc: 0.5897 - val_loss: 0.9274 - val_acc: 0.3571\n",
            "Epoch 13/30\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.9234 - acc: 0.5726 - val_loss: 0.9271 - val_acc: 0.3571\n",
            "Epoch 14/30\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.9152 - acc: 0.5983 - val_loss: 0.9284 - val_acc: 0.3571\n",
            "Epoch 15/30\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.9089 - acc: 0.6068 - val_loss: 0.9273 - val_acc: 0.3571\n",
            "Epoch 16/30\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.9033 - acc: 0.6068 - val_loss: 0.9279 - val_acc: 0.3571\n",
            "Epoch 17/30\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.8980 - acc: 0.5983 - val_loss: 0.9263 - val_acc: 0.4286\n",
            "Epoch 18/30\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.8926 - acc: 0.5897 - val_loss: 0.9279 - val_acc: 0.3571\n",
            "Epoch 19/30\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.8875 - acc: 0.5897 - val_loss: 0.9297 - val_acc: 0.3571\n",
            "Epoch 20/30\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.8824 - acc: 0.5983 - val_loss: 0.9324 - val_acc: 0.3571\n",
            "Epoch 21/30\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.8780 - acc: 0.5983 - val_loss: 0.9361 - val_acc: 0.3571\n",
            "Epoch 22/30\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.8736 - acc: 0.6154 - val_loss: 0.9387 - val_acc: 0.3571\n",
            "Epoch 23/30\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.8709 - acc: 0.6154 - val_loss: 0.9423 - val_acc: 0.3571\n",
            "Epoch 24/30\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.8640 - acc: 0.6068 - val_loss: 0.9437 - val_acc: 0.3571\n",
            "Epoch 25/30\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.8613 - acc: 0.6154 - val_loss: 0.9456 - val_acc: 0.3571\n",
            "Epoch 26/30\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.8546 - acc: 0.6154 - val_loss: 0.9527 - val_acc: 0.3571\n",
            "Epoch 27/30\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.8504 - acc: 0.6154 - val_loss: 0.9554 - val_acc: 0.3571\n",
            "Epoch 28/30\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.8479 - acc: 0.5983 - val_loss: 0.9576 - val_acc: 0.3571\n",
            "Epoch 29/30\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.8430 - acc: 0.6154 - val_loss: 0.9600 - val_acc: 0.3571\n",
            "Epoch 30/30\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.8404 - acc: 0.6154 - val_loss: 0.9636 - val_acc: 0.3571\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 1s 12ms/step - loss: 1.0838 - acc: 0.4407 - val_loss: 1.0860 - val_acc: 0.3846\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9955 - acc: 0.4831 - val_loss: 1.0827 - val_acc: 0.4615\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9508 - acc: 0.5169 - val_loss: 1.0890 - val_acc: 0.4615\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9269 - acc: 0.5424 - val_loss: 1.0993 - val_acc: 0.3846\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9077 - acc: 0.5678 - val_loss: 1.1136 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8967 - acc: 0.6017 - val_loss: 1.1184 - val_acc: 0.3846\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.8881 - acc: 0.6017 - val_loss: 1.1229 - val_acc: 0.3846\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8830 - acc: 0.6102 - val_loss: 1.1273 - val_acc: 0.3846\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8776 - acc: 0.6102 - val_loss: 1.1261 - val_acc: 0.3077\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8736 - acc: 0.6102 - val_loss: 1.1199 - val_acc: 0.3077\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8667 - acc: 0.5932 - val_loss: 1.1195 - val_acc: 0.3077\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8637 - acc: 0.5932 - val_loss: 1.1161 - val_acc: 0.3077\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8590 - acc: 0.6102 - val_loss: 1.1121 - val_acc: 0.3077\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8557 - acc: 0.6271 - val_loss: 1.1135 - val_acc: 0.3077\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8522 - acc: 0.6186 - val_loss: 1.1175 - val_acc: 0.3077\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8497 - acc: 0.6186 - val_loss: 1.1076 - val_acc: 0.3077\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8468 - acc: 0.6102 - val_loss: 1.1096 - val_acc: 0.3077\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8431 - acc: 0.6186 - val_loss: 1.1043 - val_acc: 0.3077\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 267us/step - loss: 0.8400 - acc: 0.6186 - val_loss: 1.1013 - val_acc: 0.3077\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8388 - acc: 0.6186 - val_loss: 1.0986 - val_acc: 0.3077\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8377 - acc: 0.6102 - val_loss: 1.0979 - val_acc: 0.3077\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8337 - acc: 0.6186 - val_loss: 1.1012 - val_acc: 0.3077\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8283 - acc: 0.6271 - val_loss: 1.0878 - val_acc: 0.3077\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8285 - acc: 0.6102 - val_loss: 1.0914 - val_acc: 0.3077\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8244 - acc: 0.6271 - val_loss: 1.0841 - val_acc: 0.3077\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8242 - acc: 0.6356 - val_loss: 1.0881 - val_acc: 0.3077\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8219 - acc: 0.6271 - val_loss: 1.0862 - val_acc: 0.3077\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.8191 - acc: 0.6356 - val_loss: 1.0813 - val_acc: 0.3077\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8154 - acc: 0.6271 - val_loss: 1.0786 - val_acc: 0.3077\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8133 - acc: 0.6271 - val_loss: 1.0734 - val_acc: 0.3077\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 1.2640 - acc: 0.3051 - val_loss: 1.2640 - val_acc: 0.1538\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 249us/step - loss: 1.1418 - acc: 0.3390 - val_loss: 1.2037 - val_acc: 0.3077\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0631 - acc: 0.4237 - val_loss: 1.1700 - val_acc: 0.6154\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0087 - acc: 0.4831 - val_loss: 1.1610 - val_acc: 0.5385\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9650 - acc: 0.5254 - val_loss: 1.1694 - val_acc: 0.4615\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9297 - acc: 0.5508 - val_loss: 1.1863 - val_acc: 0.3846\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9066 - acc: 0.5424 - val_loss: 1.2071 - val_acc: 0.3846\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8841 - acc: 0.5593 - val_loss: 1.2323 - val_acc: 0.3846\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8682 - acc: 0.5593 - val_loss: 1.2587 - val_acc: 0.4615\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8539 - acc: 0.5678 - val_loss: 1.2799 - val_acc: 0.4615\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8430 - acc: 0.5763 - val_loss: 1.3001 - val_acc: 0.4615\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.8348 - acc: 0.5847 - val_loss: 1.3162 - val_acc: 0.4615\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8269 - acc: 0.6102 - val_loss: 1.3353 - val_acc: 0.4615\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8198 - acc: 0.6186 - val_loss: 1.3516 - val_acc: 0.4615\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8143 - acc: 0.6356 - val_loss: 1.3716 - val_acc: 0.4615\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8069 - acc: 0.6356 - val_loss: 1.3803 - val_acc: 0.4615\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8026 - acc: 0.6356 - val_loss: 1.3931 - val_acc: 0.4615\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.7987 - acc: 0.6525 - val_loss: 1.4089 - val_acc: 0.4615\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7928 - acc: 0.6864 - val_loss: 1.4256 - val_acc: 0.4615\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7892 - acc: 0.6780 - val_loss: 1.4396 - val_acc: 0.4615\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7847 - acc: 0.6610 - val_loss: 1.4524 - val_acc: 0.4615\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7779 - acc: 0.6695 - val_loss: 1.4674 - val_acc: 0.4615\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7774 - acc: 0.6695 - val_loss: 1.4824 - val_acc: 0.4615\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7718 - acc: 0.6864 - val_loss: 1.4944 - val_acc: 0.4615\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7677 - acc: 0.6695 - val_loss: 1.5014 - val_acc: 0.4615\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7633 - acc: 0.6695 - val_loss: 1.5147 - val_acc: 0.4615\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7593 - acc: 0.6780 - val_loss: 1.5272 - val_acc: 0.4615\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7563 - acc: 0.6695 - val_loss: 1.5411 - val_acc: 0.4615\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7514 - acc: 0.6949 - val_loss: 1.5479 - val_acc: 0.4615\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7479 - acc: 0.6949 - val_loss: 1.5590 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 1.3126 - acc: 0.4068 - val_loss: 1.4804 - val_acc: 0.3846\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.1961 - acc: 0.4068 - val_loss: 1.3595 - val_acc: 0.3077\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 246us/step - loss: 1.1242 - acc: 0.4661 - val_loss: 1.2779 - val_acc: 0.3077\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0723 - acc: 0.4746 - val_loss: 1.2216 - val_acc: 0.3077\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0398 - acc: 0.5085 - val_loss: 1.1762 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0175 - acc: 0.4915 - val_loss: 1.1437 - val_acc: 0.3846\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0004 - acc: 0.4915 - val_loss: 1.1152 - val_acc: 0.3846\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9850 - acc: 0.5169 - val_loss: 1.0946 - val_acc: 0.3846\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9740 - acc: 0.5339 - val_loss: 1.0723 - val_acc: 0.3846\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9657 - acc: 0.5424 - val_loss: 1.0541 - val_acc: 0.3846\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9549 - acc: 0.5339 - val_loss: 1.0358 - val_acc: 0.3846\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9469 - acc: 0.5678 - val_loss: 1.0204 - val_acc: 0.4615\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9384 - acc: 0.5678 - val_loss: 1.0065 - val_acc: 0.4615\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9320 - acc: 0.5593 - val_loss: 0.9940 - val_acc: 0.4615\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9244 - acc: 0.5339 - val_loss: 0.9828 - val_acc: 0.4615\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9197 - acc: 0.5339 - val_loss: 0.9715 - val_acc: 0.5385\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9138 - acc: 0.5339 - val_loss: 0.9625 - val_acc: 0.5385\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9079 - acc: 0.5339 - val_loss: 0.9532 - val_acc: 0.5385\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9034 - acc: 0.5424 - val_loss: 0.9466 - val_acc: 0.5385\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8974 - acc: 0.5593 - val_loss: 0.9390 - val_acc: 0.6154\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8934 - acc: 0.5508 - val_loss: 0.9326 - val_acc: 0.6154\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8871 - acc: 0.5339 - val_loss: 0.9276 - val_acc: 0.6154\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8833 - acc: 0.5424 - val_loss: 0.9223 - val_acc: 0.6154\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8758 - acc: 0.5593 - val_loss: 0.9156 - val_acc: 0.6154\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8722 - acc: 0.5593 - val_loss: 0.9126 - val_acc: 0.6154\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8679 - acc: 0.5678 - val_loss: 0.9083 - val_acc: 0.6923\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8621 - acc: 0.5593 - val_loss: 0.9042 - val_acc: 0.6923\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8586 - acc: 0.5763 - val_loss: 0.8996 - val_acc: 0.6923\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8536 - acc: 0.5763 - val_loss: 0.8967 - val_acc: 0.6923\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8500 - acc: 0.5763 - val_loss: 0.8931 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 1.5555 - acc: 0.1610 - val_loss: 1.1510 - val_acc: 0.3846\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.1890 - acc: 0.2966 - val_loss: 1.0667 - val_acc: 0.4615\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0749 - acc: 0.3729 - val_loss: 1.0272 - val_acc: 0.3077\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0239 - acc: 0.3898 - val_loss: 1.0098 - val_acc: 0.3846\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9970 - acc: 0.4407 - val_loss: 0.9960 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9799 - acc: 0.4746 - val_loss: 0.9885 - val_acc: 0.3846\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9687 - acc: 0.5000 - val_loss: 0.9862 - val_acc: 0.3846\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9619 - acc: 0.5169 - val_loss: 0.9819 - val_acc: 0.3846\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9521 - acc: 0.5254 - val_loss: 0.9825 - val_acc: 0.3846\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9460 - acc: 0.5508 - val_loss: 0.9820 - val_acc: 0.3846\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9405 - acc: 0.5508 - val_loss: 0.9802 - val_acc: 0.3846\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9346 - acc: 0.5763 - val_loss: 0.9771 - val_acc: 0.3846\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9308 - acc: 0.5593 - val_loss: 0.9769 - val_acc: 0.3846\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9277 - acc: 0.6017 - val_loss: 0.9776 - val_acc: 0.3846\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9217 - acc: 0.5932 - val_loss: 0.9768 - val_acc: 0.4615\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9187 - acc: 0.6102 - val_loss: 0.9780 - val_acc: 0.4615\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9149 - acc: 0.6102 - val_loss: 0.9797 - val_acc: 0.4615\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9125 - acc: 0.6102 - val_loss: 0.9785 - val_acc: 0.4615\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9070 - acc: 0.6017 - val_loss: 0.9802 - val_acc: 0.4615\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9028 - acc: 0.6017 - val_loss: 0.9777 - val_acc: 0.3846\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9005 - acc: 0.5932 - val_loss: 0.9774 - val_acc: 0.3846\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8972 - acc: 0.6017 - val_loss: 0.9788 - val_acc: 0.3846\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8944 - acc: 0.5847 - val_loss: 0.9821 - val_acc: 0.5385\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8910 - acc: 0.5932 - val_loss: 0.9812 - val_acc: 0.5385\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8869 - acc: 0.5932 - val_loss: 0.9838 - val_acc: 0.5385\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8850 - acc: 0.5847 - val_loss: 0.9826 - val_acc: 0.5385\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8817 - acc: 0.5847 - val_loss: 0.9859 - val_acc: 0.5385\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8820 - acc: 0.5847 - val_loss: 0.9860 - val_acc: 0.4615\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8770 - acc: 0.5847 - val_loss: 0.9898 - val_acc: 0.5385\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8730 - acc: 0.5932 - val_loss: 0.9888 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 1.5758 - acc: 0.2373 - val_loss: 1.5868 - val_acc: 0.1538\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.3011 - acc: 0.3390 - val_loss: 1.3561 - val_acc: 0.0769\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1581 - acc: 0.4153 - val_loss: 1.2272 - val_acc: 0.2308\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0741 - acc: 0.4492 - val_loss: 1.1490 - val_acc: 0.3077\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0202 - acc: 0.5000 - val_loss: 1.0927 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9836 - acc: 0.4831 - val_loss: 1.0606 - val_acc: 0.5385\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9542 - acc: 0.5169 - val_loss: 1.0355 - val_acc: 0.4615\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9365 - acc: 0.5254 - val_loss: 1.0175 - val_acc: 0.5385\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9242 - acc: 0.5424 - val_loss: 1.0052 - val_acc: 0.5385\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9105 - acc: 0.5508 - val_loss: 0.9909 - val_acc: 0.5385\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9007 - acc: 0.5254 - val_loss: 0.9807 - val_acc: 0.6923\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8934 - acc: 0.5508 - val_loss: 0.9732 - val_acc: 0.6923\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8843 - acc: 0.5593 - val_loss: 0.9668 - val_acc: 0.6923\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8780 - acc: 0.5763 - val_loss: 0.9608 - val_acc: 0.6923\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8726 - acc: 0.5847 - val_loss: 0.9573 - val_acc: 0.6923\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8657 - acc: 0.5678 - val_loss: 0.9562 - val_acc: 0.6923\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8607 - acc: 0.5763 - val_loss: 0.9550 - val_acc: 0.6923\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8572 - acc: 0.5847 - val_loss: 0.9497 - val_acc: 0.6923\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8533 - acc: 0.5847 - val_loss: 0.9495 - val_acc: 0.6923\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8512 - acc: 0.5847 - val_loss: 0.9456 - val_acc: 0.6923\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8446 - acc: 0.6102 - val_loss: 0.9444 - val_acc: 0.6923\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8402 - acc: 0.5932 - val_loss: 0.9423 - val_acc: 0.6923\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8373 - acc: 0.6441 - val_loss: 0.9389 - val_acc: 0.6923\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8331 - acc: 0.6186 - val_loss: 0.9396 - val_acc: 0.6923\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8292 - acc: 0.6186 - val_loss: 0.9353 - val_acc: 0.6923\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8247 - acc: 0.6525 - val_loss: 0.9334 - val_acc: 0.6923\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8239 - acc: 0.6441 - val_loss: 0.9366 - val_acc: 0.6923\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.8189 - acc: 0.6525 - val_loss: 0.9330 - val_acc: 0.6923\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8149 - acc: 0.6356 - val_loss: 0.9317 - val_acc: 0.6923\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8125 - acc: 0.6610 - val_loss: 0.9306 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 1.4219 - acc: 0.3051 - val_loss: 1.1953 - val_acc: 0.3846\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.2187 - acc: 0.3814 - val_loss: 1.0611 - val_acc: 0.2308\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1028 - acc: 0.4661 - val_loss: 0.9757 - val_acc: 0.3077\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0348 - acc: 0.4831 - val_loss: 0.9346 - val_acc: 0.5385\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9956 - acc: 0.5000 - val_loss: 0.9080 - val_acc: 0.4615\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9714 - acc: 0.5593 - val_loss: 0.8946 - val_acc: 0.4615\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9570 - acc: 0.5763 - val_loss: 0.8837 - val_acc: 0.5385\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9470 - acc: 0.6356 - val_loss: 0.8771 - val_acc: 0.5385\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9403 - acc: 0.6271 - val_loss: 0.8729 - val_acc: 0.5385\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9319 - acc: 0.6441 - val_loss: 0.8671 - val_acc: 0.5385\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9252 - acc: 0.6186 - val_loss: 0.8689 - val_acc: 0.5385\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9209 - acc: 0.6441 - val_loss: 0.8695 - val_acc: 0.5385\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9145 - acc: 0.6186 - val_loss: 0.8677 - val_acc: 0.5385\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9118 - acc: 0.6102 - val_loss: 0.8657 - val_acc: 0.5385\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9060 - acc: 0.6186 - val_loss: 0.8607 - val_acc: 0.5385\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9037 - acc: 0.6356 - val_loss: 0.8624 - val_acc: 0.5385\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8990 - acc: 0.6356 - val_loss: 0.8633 - val_acc: 0.5385\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8936 - acc: 0.6186 - val_loss: 0.8630 - val_acc: 0.5385\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8895 - acc: 0.6356 - val_loss: 0.8602 - val_acc: 0.5385\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8866 - acc: 0.6441 - val_loss: 0.8636 - val_acc: 0.5385\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.8817 - acc: 0.6441 - val_loss: 0.8564 - val_acc: 0.5385\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8805 - acc: 0.6441 - val_loss: 0.8622 - val_acc: 0.5385\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8751 - acc: 0.6441 - val_loss: 0.8601 - val_acc: 0.5385\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8701 - acc: 0.6525 - val_loss: 0.8585 - val_acc: 0.5385\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8684 - acc: 0.6441 - val_loss: 0.8577 - val_acc: 0.6154\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8647 - acc: 0.6441 - val_loss: 0.8580 - val_acc: 0.6154\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8590 - acc: 0.6525 - val_loss: 0.8611 - val_acc: 0.6154\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8560 - acc: 0.6441 - val_loss: 0.8595 - val_acc: 0.6154\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8545 - acc: 0.6525 - val_loss: 0.8549 - val_acc: 0.6154\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.8477 - acc: 0.6695 - val_loss: 0.8574 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.2397 - acc: 0.3475 - val_loss: 1.2486 - val_acc: 0.3077\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.1437 - acc: 0.3220 - val_loss: 1.1564 - val_acc: 0.3846\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0803 - acc: 0.3814 - val_loss: 1.0998 - val_acc: 0.4615\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0435 - acc: 0.4237 - val_loss: 1.0569 - val_acc: 0.3846\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0191 - acc: 0.4746 - val_loss: 1.0279 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0000 - acc: 0.4831 - val_loss: 1.0077 - val_acc: 0.3846\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9866 - acc: 0.5339 - val_loss: 0.9861 - val_acc: 0.4615\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9740 - acc: 0.5424 - val_loss: 0.9715 - val_acc: 0.4615\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9634 - acc: 0.5678 - val_loss: 0.9576 - val_acc: 0.4615\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9563 - acc: 0.5508 - val_loss: 0.9477 - val_acc: 0.4615\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9491 - acc: 0.5678 - val_loss: 0.9413 - val_acc: 0.4615\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9419 - acc: 0.5593 - val_loss: 0.9308 - val_acc: 0.4615\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9370 - acc: 0.5678 - val_loss: 0.9245 - val_acc: 0.4615\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9299 - acc: 0.5678 - val_loss: 0.9201 - val_acc: 0.4615\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9237 - acc: 0.5763 - val_loss: 0.9161 - val_acc: 0.4615\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9186 - acc: 0.5847 - val_loss: 0.9098 - val_acc: 0.5385\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9133 - acc: 0.5678 - val_loss: 0.9014 - val_acc: 0.5385\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9081 - acc: 0.5678 - val_loss: 0.8971 - val_acc: 0.5385\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9033 - acc: 0.5763 - val_loss: 0.8918 - val_acc: 0.5385\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.8998 - acc: 0.5932 - val_loss: 0.8836 - val_acc: 0.5385\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.8954 - acc: 0.6017 - val_loss: 0.8778 - val_acc: 0.5385\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8891 - acc: 0.6102 - val_loss: 0.8753 - val_acc: 0.5385\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8848 - acc: 0.6102 - val_loss: 0.8726 - val_acc: 0.5385\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8810 - acc: 0.6186 - val_loss: 0.8663 - val_acc: 0.5385\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8772 - acc: 0.6186 - val_loss: 0.8626 - val_acc: 0.5385\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8726 - acc: 0.6441 - val_loss: 0.8596 - val_acc: 0.5385\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8697 - acc: 0.6186 - val_loss: 0.8586 - val_acc: 0.5385\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8664 - acc: 0.6186 - val_loss: 0.8517 - val_acc: 0.5385\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8628 - acc: 0.6356 - val_loss: 0.8500 - val_acc: 0.5385\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8594 - acc: 0.6356 - val_loss: 0.8458 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.4444 - acc: 0.2797 - val_loss: 0.9757 - val_acc: 0.4615\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.2140 - acc: 0.3559 - val_loss: 0.9084 - val_acc: 0.6154\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.1112 - acc: 0.3814 - val_loss: 0.8843 - val_acc: 0.6154\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0560 - acc: 0.4153 - val_loss: 0.8728 - val_acc: 0.6154\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0117 - acc: 0.4576 - val_loss: 0.8617 - val_acc: 0.6923\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9785 - acc: 0.5000 - val_loss: 0.8614 - val_acc: 0.6923\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9538 - acc: 0.5339 - val_loss: 0.8600 - val_acc: 0.6923\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9350 - acc: 0.5339 - val_loss: 0.8629 - val_acc: 0.6923\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9116 - acc: 0.5424 - val_loss: 0.8613 - val_acc: 0.6923\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.8985 - acc: 0.5339 - val_loss: 0.8677 - val_acc: 0.6923\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.8850 - acc: 0.5424 - val_loss: 0.8669 - val_acc: 0.6923\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8732 - acc: 0.5508 - val_loss: 0.8703 - val_acc: 0.6923\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8619 - acc: 0.5339 - val_loss: 0.8771 - val_acc: 0.6923\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8521 - acc: 0.5763 - val_loss: 0.8781 - val_acc: 0.6923\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8443 - acc: 0.5847 - val_loss: 0.8839 - val_acc: 0.6923\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8394 - acc: 0.5763 - val_loss: 0.8870 - val_acc: 0.6923\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8303 - acc: 0.6017 - val_loss: 0.8959 - val_acc: 0.6923\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8238 - acc: 0.6186 - val_loss: 0.9002 - val_acc: 0.6923\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8164 - acc: 0.6186 - val_loss: 0.9026 - val_acc: 0.6923\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8116 - acc: 0.6356 - val_loss: 0.9108 - val_acc: 0.6923\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8061 - acc: 0.6186 - val_loss: 0.9172 - val_acc: 0.6923\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8003 - acc: 0.6695 - val_loss: 0.9157 - val_acc: 0.6923\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7940 - acc: 0.6610 - val_loss: 0.9201 - val_acc: 0.6923\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7900 - acc: 0.6780 - val_loss: 0.9295 - val_acc: 0.6923\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7826 - acc: 0.6864 - val_loss: 0.9367 - val_acc: 0.6923\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7799 - acc: 0.6949 - val_loss: 0.9416 - val_acc: 0.6154\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7746 - acc: 0.6949 - val_loss: 0.9462 - val_acc: 0.6154\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7695 - acc: 0.7034 - val_loss: 0.9546 - val_acc: 0.6154\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7675 - acc: 0.7119 - val_loss: 0.9509 - val_acc: 0.6154\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7617 - acc: 0.6949 - val_loss: 0.9610 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.6207 - acc: 0.2288 - val_loss: 1.7683 - val_acc: 0.2308\n",
            "Epoch 2/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.3264 - acc: 0.2288 - val_loss: 1.5109 - val_acc: 0.3077\n",
            "Epoch 3/30\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.1532 - acc: 0.3051 - val_loss: 1.3595 - val_acc: 0.3077\n",
            "Epoch 4/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0598 - acc: 0.4153 - val_loss: 1.2826 - val_acc: 0.3077\n",
            "Epoch 5/30\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0101 - acc: 0.5339 - val_loss: 1.2341 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9762 - acc: 0.5593 - val_loss: 1.2109 - val_acc: 0.3846\n",
            "Epoch 7/30\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9530 - acc: 0.5763 - val_loss: 1.1956 - val_acc: 0.3846\n",
            "Epoch 8/30\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9340 - acc: 0.6271 - val_loss: 1.1919 - val_acc: 0.4615\n",
            "Epoch 9/30\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9184 - acc: 0.6610 - val_loss: 1.1840 - val_acc: 0.4615\n",
            "Epoch 10/30\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9046 - acc: 0.6186 - val_loss: 1.1853 - val_acc: 0.6154\n",
            "Epoch 11/30\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8924 - acc: 0.6356 - val_loss: 1.1932 - val_acc: 0.5385\n",
            "Epoch 12/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8833 - acc: 0.6271 - val_loss: 1.2049 - val_acc: 0.5385\n",
            "Epoch 13/30\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8724 - acc: 0.6356 - val_loss: 1.2143 - val_acc: 0.5385\n",
            "Epoch 14/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8635 - acc: 0.6610 - val_loss: 1.2250 - val_acc: 0.5385\n",
            "Epoch 15/30\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8560 - acc: 0.6525 - val_loss: 1.2417 - val_acc: 0.5385\n",
            "Epoch 16/30\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8490 - acc: 0.6525 - val_loss: 1.2522 - val_acc: 0.4615\n",
            "Epoch 17/30\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8441 - acc: 0.6525 - val_loss: 1.2723 - val_acc: 0.4615\n",
            "Epoch 18/30\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.8368 - acc: 0.6525 - val_loss: 1.2883 - val_acc: 0.4615\n",
            "Epoch 19/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8324 - acc: 0.6441 - val_loss: 1.3037 - val_acc: 0.4615\n",
            "Epoch 20/30\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.8267 - acc: 0.6525 - val_loss: 1.3261 - val_acc: 0.4615\n",
            "Epoch 21/30\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8214 - acc: 0.6525 - val_loss: 1.3390 - val_acc: 0.4615\n",
            "Epoch 22/30\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8165 - acc: 0.6525 - val_loss: 1.3605 - val_acc: 0.4615\n",
            "Epoch 23/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8118 - acc: 0.6525 - val_loss: 1.3793 - val_acc: 0.4615\n",
            "Epoch 24/30\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8084 - acc: 0.6525 - val_loss: 1.3828 - val_acc: 0.4615\n",
            "Epoch 25/30\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.8048 - acc: 0.6525 - val_loss: 1.4051 - val_acc: 0.4615\n",
            "Epoch 26/30\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7982 - acc: 0.6610 - val_loss: 1.4211 - val_acc: 0.4615\n",
            "Epoch 27/30\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7953 - acc: 0.6610 - val_loss: 1.4392 - val_acc: 0.4615\n",
            "Epoch 28/30\n",
            "118/118 [==============================] - 0s 274us/step - loss: 0.7919 - acc: 0.6525 - val_loss: 1.4535 - val_acc: 0.4615\n",
            "Epoch 29/30\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7870 - acc: 0.6525 - val_loss: 1.4661 - val_acc: 0.5385\n",
            "Epoch 30/30\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7841 - acc: 0.6695 - val_loss: 1.4757 - val_acc: 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0953b1b5-e9a1-4ab0-9b45-055019820db7",
        "id": "6OH7qa6a14-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.34257050e-01,  9.68988268e-02,  3.85769839e-01,\n",
              "         1.61200050e-01, -5.40961150e-01, -3.83658358e-01,\n",
              "        -1.13750969e-01, -2.28591476e-01, -1.15055286e-01,\n",
              "        -3.30299955e-01,  1.35982249e-01,  3.71533408e-01,\n",
              "        -1.21254356e-01, -4.84378805e-01, -5.30458542e-01,\n",
              "        -4.87994766e-01,  1.88481002e-01],\n",
              "       [ 1.54957687e+00,  1.90539890e+00, -2.22052320e-01,\n",
              "         3.87307146e-01, -5.45340485e-01, -1.01385115e+00,\n",
              "        -2.73626628e-01, -6.67618363e-01, -3.33713506e-01,\n",
              "        -3.30970545e-01, -1.05552128e+00, -1.16554466e+00,\n",
              "         7.92773556e-01, -6.04756035e-03, -1.36103435e-01,\n",
              "         2.84618093e-01, -7.68000271e-01],\n",
              "       [ 1.00150735e+00, -8.58201787e-01,  3.11646964e-01,\n",
              "        -1.43442573e+00, -5.49929348e-01, -9.04252402e-01,\n",
              "        -5.28211743e-01, -1.20002808e-01, -3.35769175e-01,\n",
              "        -3.30973760e-01, -4.42456637e-01,  2.35041057e+00,\n",
              "        -1.82929991e+00,  2.68994980e-01,  2.63286133e+00,\n",
              "         1.29973450e+00, -1.07303619e+00],\n",
              "       [ 5.22267532e-01, -1.06875098e-01, -6.47157410e-01,\n",
              "        -2.81002626e-02, -5.47216346e-01,  5.47366256e-02,\n",
              "        -9.22810177e-01,  2.97018333e-01, -1.85535435e-01,\n",
              "        -3.30751288e-01, -1.01051841e+00,  5.33572761e-01,\n",
              "        -8.41591880e-01, -1.03863818e+00, -5.91370564e-01,\n",
              "        -4.78529164e-01,  7.65523623e-02],\n",
              "       [ 2.31051081e-01, -3.20246036e-01,  3.33606515e-01,\n",
              "        -1.92448965e+00, -5.47077623e-01, -1.39470679e+00,\n",
              "         1.71658188e-01, -8.49813810e-01, -3.35016786e-01,\n",
              "        -3.30973339e-01, -1.13057908e+00, -3.96647241e-02,\n",
              "        -7.76402925e-02, -1.46431122e-01,  1.57365298e+00,\n",
              "         2.06583169e+00, -9.92270817e-01],\n",
              "       [-6.44453936e-01, -8.78723589e-03, -1.04418142e-01,\n",
              "        -5.22952367e-01,  9.09313099e-01,  1.69871781e+00,\n",
              "        -1.21474566e+00,  1.75610763e+00, -9.44599440e-02,\n",
              "        -3.30471019e-01,  8.05783791e-01, -1.11896137e+00,\n",
              "        -5.71419421e-01, -1.13904175e+00,  2.36554102e-01,\n",
              "        -7.98738149e-01, -5.48945706e-01],\n",
              "       [-7.06431308e-01, -9.89748699e-03,  1.32801751e+00,\n",
              "        -1.37618450e+00, -5.39625628e-01,  6.60326170e+00,\n",
              "         3.83648904e+00, -1.69002919e+00, -2.97148140e-01,\n",
              "        -3.30858604e-01,  2.98396859e+00, -8.38653288e-01,\n",
              "         2.67323855e+00,  1.97331940e+00,  4.59022522e-01,\n",
              "        -5.16599725e-01, -7.19356677e-01],\n",
              "       [ 2.06934501e+00,  1.21207305e+00, -8.02529761e-01,\n",
              "        -3.23665850e+00,  3.52435672e-01, -1.01385115e+00,\n",
              "         5.82901521e-02, -7.18648977e-01, -3.35432838e-01,\n",
              "        -3.30974284e-01, -1.66414662e+00,  2.66798240e+00,\n",
              "        -1.79401751e+00,  3.67356718e-01,  2.40487403e+00,\n",
              "         4.02913212e+00, -1.04661333e+00],\n",
              "       [ 1.78305420e+00,  1.37711411e+00,  1.49916132e-01,\n",
              "        -5.15432158e-01, -5.46664013e-01, -1.17824927e+00,\n",
              "        -7.14148531e-01, -3.26597763e-01, -3.31802117e-01,\n",
              "        -3.30964935e-01, -1.10153207e+00, -3.25076660e-01,\n",
              "        -2.76899548e-01, -4.81278689e-01,  1.63951096e-01,\n",
              "         7.64810335e-01, -8.13461099e-01],\n",
              "       [ 1.33916856e+00,  1.74729366e+00,  9.32576683e-01,\n",
              "        -3.92248540e-01, -5.42436939e-01, -5.48056477e-01,\n",
              "         5.42853861e-01, -3.37319011e-01,  1.32217300e-01,\n",
              "        -3.30650739e-01,  9.93278185e-02,  2.18568209e+00,\n",
              "        -7.52734919e-01, -5.58159325e-01, -2.94368551e-01,\n",
              "         3.00978986e-04, -1.51329794e-01],\n",
              "       [ 4.98694935e-01,  7.35612793e-01,  6.35586964e-01,\n",
              "         6.54250049e-01,  3.16485533e-02, -1.64460866e-01,\n",
              "        -3.77661612e-01,  2.09671116e+00, -2.80318175e-01,\n",
              "        -3.30830305e-01,  6.43998628e-01,  5.19144703e-01,\n",
              "        -1.77373780e+00, -1.05512656e+00,  3.58224772e+00,\n",
              "        -7.01535374e-01, -9.40195936e-01],\n",
              "       [-1.70779933e+00, -1.82177506e+00,  6.30653686e-01,\n",
              "         7.72815870e-02, -5.41667522e-01, -7.94653656e-01,\n",
              "        -2.36091860e-01, -5.49058455e-01, -3.32324336e-01,\n",
              "        -3.30964704e-01, -2.78998649e-01,  7.31967045e-01,\n",
              "        -5.20673201e-02,  1.47627164e+00, -2.60634812e-01,\n",
              "         1.91408748e-01, -6.63483974e-01],\n",
              "       [-2.61293931e-01, -8.16561598e-01,  1.04338706e+00,\n",
              "        -1.15650705e+00, -5.41614180e-01,  2.60290747e+00,\n",
              "         1.30285401e+00, -7.54923071e-01, -2.82469107e-01,\n",
              "        -3.30793928e-01,  5.28076978e-01, -1.69820568e-01,\n",
              "         8.39421745e-01, -4.61928261e-01, -2.21762703e-01,\n",
              "        -2.37855717e-01, -5.14552169e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WoqA4rT14-Q",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "36c130fb-71c4-4b34-cbe1-f3df5c0810cd",
        "id": "YEyjhsg414-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9ea3e7b1-b73d-45b7-9a34-66611a514177",
        "id": "9WylvzXG14-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw9GTwCC14-f",
        "colab": {}
      },
      "source": [
        "average_acc_history_reduced = [np.mean([x[i] for x in all_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_reduced = [np.mean([x[i] for x in all_loss_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_acc_history_reduced = [np.mean([x[i] for x in all_val_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_loss_history_reduced = [np.mean([x[i] for x in all_val_loss_histories_reduced]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8895188b-f522-4eb4-8659-ce56735445b5",
        "id": "MdMC3doS14-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "outputId": "eb9ee4fc-b280-4af1-cb97-8a0c2555a7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_44\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_87 (Dense)             (None, 10)                180       \n",
            "_________________________________________________________________\n",
            "dense_88 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrcg4Bx625fC"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9_gKwYk25fK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCcaAgTi25fZ",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "06e7ca58-3fab-4daf-a42d-3772a5c83fc3",
        "id": "6H8nOl_X25fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_reduced, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_reduced, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1dc41bd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGNMiLNEYIwxIe7/AbttOMztNoJtAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMh2BG152967"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ed3aca19-82f3-4b4b-c0f7-a0087dc5964d",
        "id": "WNEq_n08297J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_reduced, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_reduced, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa1dc3e2240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIlBKqSCniUAppYKcJgKllApy/w+glI756qlOvAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5BQnjMi93Dgn"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "65094145-907e-42b8-f3f0-176724304e6d",
        "id": "yANP0XPF3Dg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_reduced, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_reduced, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "131/131 [==============================] - 2s 12ms/step - loss: 1.1000 - acc: 0.3359\n",
            "Epoch 2/30\n",
            "131/131 [==============================] - 0s 177us/step - loss: 1.0106 - acc: 0.4809\n",
            "Epoch 3/30\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9656 - acc: 0.5038\n",
            "Epoch 4/30\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9384 - acc: 0.5191\n",
            "Epoch 5/30\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9226 - acc: 0.5496\n",
            "Epoch 6/30\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9098 - acc: 0.5496\n",
            "Epoch 7/30\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9004 - acc: 0.5649\n",
            "Epoch 8/30\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8924 - acc: 0.5573\n",
            "Epoch 9/30\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8845 - acc: 0.5725\n",
            "Epoch 10/30\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.8794 - acc: 0.5802\n",
            "Epoch 11/30\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8721 - acc: 0.6031\n",
            "Epoch 12/30\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.8666 - acc: 0.5954\n",
            "Epoch 13/30\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.8619 - acc: 0.6260\n",
            "Epoch 14/30\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8572 - acc: 0.6183\n",
            "Epoch 15/30\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8537 - acc: 0.6260\n",
            "Epoch 16/30\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.8506 - acc: 0.6183\n",
            "Epoch 17/30\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8487 - acc: 0.6260\n",
            "Epoch 18/30\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.8423 - acc: 0.6107\n",
            "Epoch 19/30\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.8399 - acc: 0.6412\n",
            "Epoch 20/30\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.8342 - acc: 0.6260\n",
            "Epoch 21/30\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.8325 - acc: 0.6107\n",
            "Epoch 22/30\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8320 - acc: 0.6489\n",
            "Epoch 23/30\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8290 - acc: 0.6336\n",
            "Epoch 24/30\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8233 - acc: 0.6183\n",
            "Epoch 25/30\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8198 - acc: 0.6336\n",
            "Epoch 26/30\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8175 - acc: 0.6412\n",
            "Epoch 27/30\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8132 - acc: 0.6489\n",
            "Epoch 28/30\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.8100 - acc: 0.6489\n",
            "Epoch 29/30\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8084 - acc: 0.6565\n",
            "Epoch 30/30\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.8048 - acc: 0.6565\n",
            "34/34 [==============================] - 1s 19ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9653f878-934d-40a6-bf45-f488901cc855",
        "id": "kQLgWQtS3DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2662e43e-489f-47db-e5e6-60d5a4e1e016",
        "id": "_c5TGT4H3DhR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17647058823529413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMbKL5Yw3DhZ"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "4b33d38a-446d-45a1-ec53-c2899fc9eb86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "7a988a3a-dfcd-41d1-ecdd-1b4f6932efe8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "dbf52f23-294e-42d9-d095-3f268e9ad774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "83587437-5d2c-4058-b218-6a707fe324c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "ea3b7832-2559-45c2-ed42-2f911f98d111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "6b490078-1b61-4674-e437-661cf4d8efc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "b826c4ee-0e28-4b12-9884-bb2190fb8971",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "1b97a4e4-bb5a-49cc-a975-e8f68c4025d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 3.9928 - acc: 0.1026 - val_loss: 2.6886 - val_acc: 0.2143\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 275us/step - loss: 3.6038 - acc: 0.1026 - val_loss: 2.4447 - val_acc: 0.2143\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 3.2548 - acc: 0.1111 - val_loss: 2.2219 - val_acc: 0.2143\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 2.9598 - acc: 0.1197 - val_loss: 2.0345 - val_acc: 0.2143\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 2.7026 - acc: 0.1282 - val_loss: 1.8655 - val_acc: 0.2857\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 2.4746 - acc: 0.1368 - val_loss: 1.7351 - val_acc: 0.2857\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 2.2608 - acc: 0.1453 - val_loss: 1.6131 - val_acc: 0.1429\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 2.0573 - acc: 0.1709 - val_loss: 1.5159 - val_acc: 0.1429\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 1.8916 - acc: 0.1880 - val_loss: 1.4390 - val_acc: 0.1429\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 1.7466 - acc: 0.2051 - val_loss: 1.3758 - val_acc: 0.2143\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 1.6274 - acc: 0.2051 - val_loss: 1.3291 - val_acc: 0.2143\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 1.5295 - acc: 0.2051 - val_loss: 1.2919 - val_acc: 0.2857\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.4492 - acc: 0.2137 - val_loss: 1.2619 - val_acc: 0.2857\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 1.3851 - acc: 0.2222 - val_loss: 1.2387 - val_acc: 0.2857\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 1.3303 - acc: 0.2650 - val_loss: 1.2208 - val_acc: 0.3571\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.2864 - acc: 0.3077 - val_loss: 1.2042 - val_acc: 0.3571\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 1.2486 - acc: 0.2991 - val_loss: 1.1911 - val_acc: 0.3571\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 1.2174 - acc: 0.3248 - val_loss: 1.1798 - val_acc: 0.3571\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 1.1914 - acc: 0.3504 - val_loss: 1.1694 - val_acc: 0.3571\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.1699 - acc: 0.3504 - val_loss: 1.1624 - val_acc: 0.3571\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 1.1504 - acc: 0.3504 - val_loss: 1.1554 - val_acc: 0.3571\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 1.1350 - acc: 0.3675 - val_loss: 1.1472 - val_acc: 0.4286\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 1.1184 - acc: 0.3846 - val_loss: 1.1394 - val_acc: 0.4286\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 1.1050 - acc: 0.4017 - val_loss: 1.1355 - val_acc: 0.4286\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 1.0931 - acc: 0.4188 - val_loss: 1.1287 - val_acc: 0.5000\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 1.0809 - acc: 0.4359 - val_loss: 1.1232 - val_acc: 0.5000\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 1.0700 - acc: 0.4359 - val_loss: 1.1177 - val_acc: 0.5000\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 1.0616 - acc: 0.4444 - val_loss: 1.1130 - val_acc: 0.5000\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 341us/step - loss: 1.0515 - acc: 0.4615 - val_loss: 1.1067 - val_acc: 0.5000\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 1.0455 - acc: 0.4530 - val_loss: 1.1023 - val_acc: 0.5000\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.0365 - acc: 0.4530 - val_loss: 1.0991 - val_acc: 0.5000\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 1.0294 - acc: 0.4530 - val_loss: 1.0930 - val_acc: 0.5000\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 1.0231 - acc: 0.4701 - val_loss: 1.0889 - val_acc: 0.5000\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.0172 - acc: 0.4701 - val_loss: 1.0863 - val_acc: 0.5000\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 282us/step - loss: 1.0119 - acc: 0.4786 - val_loss: 1.0827 - val_acc: 0.5000\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.0061 - acc: 0.4872 - val_loss: 1.0824 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 1.0018 - acc: 0.4786 - val_loss: 1.0789 - val_acc: 0.5000\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 0.9971 - acc: 0.4786 - val_loss: 1.0733 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9927 - acc: 0.4872 - val_loss: 1.0713 - val_acc: 0.5000\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.9893 - acc: 0.4957 - val_loss: 1.0672 - val_acc: 0.5000\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.9856 - acc: 0.4872 - val_loss: 1.0667 - val_acc: 0.5000\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9828 - acc: 0.4872 - val_loss: 1.0652 - val_acc: 0.5000\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.9791 - acc: 0.4957 - val_loss: 1.0634 - val_acc: 0.5000\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9764 - acc: 0.5043 - val_loss: 1.0624 - val_acc: 0.5000\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9752 - acc: 0.5043 - val_loss: 1.0622 - val_acc: 0.5000\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9730 - acc: 0.5043 - val_loss: 1.0611 - val_acc: 0.5000\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9719 - acc: 0.5043 - val_loss: 1.0590 - val_acc: 0.4286\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9710 - acc: 0.5043 - val_loss: 1.0595 - val_acc: 0.4286\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9692 - acc: 0.5043 - val_loss: 1.0577 - val_acc: 0.4286\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9677 - acc: 0.5043 - val_loss: 1.0576 - val_acc: 0.4286\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9670 - acc: 0.4957 - val_loss: 1.0574 - val_acc: 0.4286\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9658 - acc: 0.5043 - val_loss: 1.0572 - val_acc: 0.4286\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9643 - acc: 0.4957 - val_loss: 1.0564 - val_acc: 0.4286\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 0.9636 - acc: 0.4957 - val_loss: 1.0567 - val_acc: 0.4286\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 0.9633 - acc: 0.4957 - val_loss: 1.0560 - val_acc: 0.4286\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9623 - acc: 0.4957 - val_loss: 1.0566 - val_acc: 0.4286\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9610 - acc: 0.4957 - val_loss: 1.0541 - val_acc: 0.4286\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9603 - acc: 0.4957 - val_loss: 1.0529 - val_acc: 0.4286\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9598 - acc: 0.4957 - val_loss: 1.0515 - val_acc: 0.4286\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9599 - acc: 0.4957 - val_loss: 1.0501 - val_acc: 0.4286\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9594 - acc: 0.4957 - val_loss: 1.0490 - val_acc: 0.4286\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9593 - acc: 0.4957 - val_loss: 1.0496 - val_acc: 0.4286\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9588 - acc: 0.4957 - val_loss: 1.0476 - val_acc: 0.4286\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9582 - acc: 0.4957 - val_loss: 1.0463 - val_acc: 0.4286\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9571 - acc: 0.4957 - val_loss: 1.0453 - val_acc: 0.4286\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9570 - acc: 0.4957 - val_loss: 1.0450 - val_acc: 0.4286\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.9567 - acc: 0.4957 - val_loss: 1.0453 - val_acc: 0.4286\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9561 - acc: 0.4957 - val_loss: 1.0445 - val_acc: 0.4286\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9560 - acc: 0.4957 - val_loss: 1.0450 - val_acc: 0.4286\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9558 - acc: 0.4957 - val_loss: 1.0461 - val_acc: 0.4286\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9555 - acc: 0.4957 - val_loss: 1.0443 - val_acc: 0.4286\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9553 - acc: 0.4957 - val_loss: 1.0457 - val_acc: 0.4286\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9549 - acc: 0.5043 - val_loss: 1.0458 - val_acc: 0.4286\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9542 - acc: 0.5043 - val_loss: 1.0451 - val_acc: 0.4286\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.9539 - acc: 0.5043 - val_loss: 1.0458 - val_acc: 0.4286\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9538 - acc: 0.5043 - val_loss: 1.0464 - val_acc: 0.4286\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9531 - acc: 0.5128 - val_loss: 1.0465 - val_acc: 0.4286\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9530 - acc: 0.5043 - val_loss: 1.0468 - val_acc: 0.4286\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.9529 - acc: 0.5128 - val_loss: 1.0471 - val_acc: 0.4286\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 0.9524 - acc: 0.5043 - val_loss: 1.0460 - val_acc: 0.4286\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9523 - acc: 0.5043 - val_loss: 1.0452 - val_acc: 0.4286\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9520 - acc: 0.5043 - val_loss: 1.0453 - val_acc: 0.4286\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9518 - acc: 0.5043 - val_loss: 1.0457 - val_acc: 0.4286\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9513 - acc: 0.5043 - val_loss: 1.0463 - val_acc: 0.4286\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.9516 - acc: 0.5043 - val_loss: 1.0456 - val_acc: 0.4286\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9508 - acc: 0.5043 - val_loss: 1.0453 - val_acc: 0.4286\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9509 - acc: 0.5128 - val_loss: 1.0457 - val_acc: 0.4286\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9505 - acc: 0.5128 - val_loss: 1.0448 - val_acc: 0.4286\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9502 - acc: 0.5128 - val_loss: 1.0442 - val_acc: 0.4286\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.9504 - acc: 0.5043 - val_loss: 1.0438 - val_acc: 0.4286\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 0.9501 - acc: 0.5128 - val_loss: 1.0450 - val_acc: 0.4286\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.9499 - acc: 0.5043 - val_loss: 1.0455 - val_acc: 0.4286\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9493 - acc: 0.5128 - val_loss: 1.0452 - val_acc: 0.4286\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.9492 - acc: 0.5043 - val_loss: 1.0459 - val_acc: 0.4286\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9489 - acc: 0.5043 - val_loss: 1.0446 - val_acc: 0.4286\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 0.9489 - acc: 0.5214 - val_loss: 1.0450 - val_acc: 0.4286\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.9485 - acc: 0.5128 - val_loss: 1.0450 - val_acc: 0.4286\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 217us/step - loss: 0.9482 - acc: 0.5214 - val_loss: 1.0454 - val_acc: 0.4286\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.9488 - acc: 0.5128 - val_loss: 1.0440 - val_acc: 0.4286\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 247us/step - loss: 0.9477 - acc: 0.5128 - val_loss: 1.0429 - val_acc: 0.4286\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 1ms/step - loss: 3.0529 - acc: 0.3559 - val_loss: 2.2874 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 2.6614 - acc: 0.3814 - val_loss: 2.0969 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 2.3586 - acc: 0.3898 - val_loss: 1.9614 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 2.1346 - acc: 0.3729 - val_loss: 1.8702 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.9678 - acc: 0.4068 - val_loss: 1.7903 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.8356 - acc: 0.4237 - val_loss: 1.7166 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.7436 - acc: 0.4237 - val_loss: 1.6640 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.6736 - acc: 0.4322 - val_loss: 1.6125 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.6139 - acc: 0.4322 - val_loss: 1.5616 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.5609 - acc: 0.4407 - val_loss: 1.5117 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.5115 - acc: 0.4407 - val_loss: 1.4739 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.4645 - acc: 0.4407 - val_loss: 1.4339 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.4269 - acc: 0.4576 - val_loss: 1.3977 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.3892 - acc: 0.4746 - val_loss: 1.3676 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.3540 - acc: 0.4746 - val_loss: 1.3324 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3211 - acc: 0.4831 - val_loss: 1.3046 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.2915 - acc: 0.4831 - val_loss: 1.2766 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.2676 - acc: 0.4915 - val_loss: 1.2516 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.2422 - acc: 0.5000 - val_loss: 1.2278 - val_acc: 0.2308\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.2146 - acc: 0.5000 - val_loss: 1.2035 - val_acc: 0.2308\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1946 - acc: 0.5085 - val_loss: 1.1791 - val_acc: 0.2308\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1728 - acc: 0.4831 - val_loss: 1.1608 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1508 - acc: 0.4915 - val_loss: 1.1383 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.1313 - acc: 0.5085 - val_loss: 1.1192 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.1119 - acc: 0.4915 - val_loss: 1.1034 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0930 - acc: 0.4915 - val_loss: 1.0897 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0767 - acc: 0.5169 - val_loss: 1.0765 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0617 - acc: 0.5169 - val_loss: 1.0669 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.0507 - acc: 0.5254 - val_loss: 1.0570 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0381 - acc: 0.5339 - val_loss: 1.0491 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0291 - acc: 0.5339 - val_loss: 1.0435 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 150us/step - loss: 1.0200 - acc: 0.5424 - val_loss: 1.0384 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 264us/step - loss: 1.0106 - acc: 0.5254 - val_loss: 1.0350 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0039 - acc: 0.5424 - val_loss: 1.0283 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9971 - acc: 0.5424 - val_loss: 1.0255 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9916 - acc: 0.5508 - val_loss: 1.0223 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9865 - acc: 0.5678 - val_loss: 1.0197 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9808 - acc: 0.5678 - val_loss: 1.0186 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9756 - acc: 0.5763 - val_loss: 1.0168 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9711 - acc: 0.5763 - val_loss: 1.0153 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9674 - acc: 0.5932 - val_loss: 1.0145 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9630 - acc: 0.5932 - val_loss: 1.0122 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9588 - acc: 0.6017 - val_loss: 1.0119 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9557 - acc: 0.5932 - val_loss: 1.0111 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9526 - acc: 0.5932 - val_loss: 1.0115 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9492 - acc: 0.5763 - val_loss: 1.0113 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9465 - acc: 0.5847 - val_loss: 1.0120 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9438 - acc: 0.5847 - val_loss: 1.0121 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9418 - acc: 0.5932 - val_loss: 1.0132 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9390 - acc: 0.6102 - val_loss: 1.0124 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9368 - acc: 0.6186 - val_loss: 1.0122 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9351 - acc: 0.6186 - val_loss: 1.0124 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9336 - acc: 0.6102 - val_loss: 1.0117 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9316 - acc: 0.6102 - val_loss: 1.0109 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9305 - acc: 0.6017 - val_loss: 1.0126 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9275 - acc: 0.6102 - val_loss: 1.0109 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9258 - acc: 0.6102 - val_loss: 1.0109 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9248 - acc: 0.6102 - val_loss: 1.0104 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9225 - acc: 0.6102 - val_loss: 1.0109 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9216 - acc: 0.6102 - val_loss: 1.0118 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9197 - acc: 0.6102 - val_loss: 1.0100 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9181 - acc: 0.6102 - val_loss: 1.0091 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9179 - acc: 0.6186 - val_loss: 1.0085 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9159 - acc: 0.6186 - val_loss: 1.0085 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9151 - acc: 0.6186 - val_loss: 1.0077 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9139 - acc: 0.6271 - val_loss: 1.0077 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9129 - acc: 0.6271 - val_loss: 1.0069 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9117 - acc: 0.6356 - val_loss: 1.0066 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9101 - acc: 0.6356 - val_loss: 1.0053 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9098 - acc: 0.6356 - val_loss: 1.0055 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9097 - acc: 0.6356 - val_loss: 1.0051 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9082 - acc: 0.6271 - val_loss: 1.0051 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9063 - acc: 0.6356 - val_loss: 1.0067 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9063 - acc: 0.6356 - val_loss: 1.0067 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9056 - acc: 0.6441 - val_loss: 1.0058 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9048 - acc: 0.6356 - val_loss: 1.0055 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9035 - acc: 0.6525 - val_loss: 1.0070 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9029 - acc: 0.6356 - val_loss: 1.0079 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9025 - acc: 0.6525 - val_loss: 1.0091 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.9014 - acc: 0.6356 - val_loss: 1.0094 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9019 - acc: 0.6610 - val_loss: 1.0103 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8992 - acc: 0.6525 - val_loss: 1.0104 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8993 - acc: 0.6271 - val_loss: 1.0099 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9005 - acc: 0.6271 - val_loss: 1.0114 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 337us/step - loss: 0.8986 - acc: 0.6441 - val_loss: 1.0108 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8972 - acc: 0.6441 - val_loss: 1.0100 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8973 - acc: 0.6356 - val_loss: 1.0115 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8969 - acc: 0.6271 - val_loss: 1.0113 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8956 - acc: 0.6356 - val_loss: 1.0121 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8950 - acc: 0.6356 - val_loss: 1.0139 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8947 - acc: 0.6441 - val_loss: 1.0149 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8938 - acc: 0.6441 - val_loss: 1.0152 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8932 - acc: 0.6441 - val_loss: 1.0142 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8931 - acc: 0.6441 - val_loss: 1.0155 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8931 - acc: 0.6441 - val_loss: 1.0148 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8924 - acc: 0.6441 - val_loss: 1.0173 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8913 - acc: 0.6441 - val_loss: 1.0176 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8912 - acc: 0.6525 - val_loss: 1.0172 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8900 - acc: 0.6356 - val_loss: 1.0175 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8904 - acc: 0.6356 - val_loss: 1.0174 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.1281 - acc: 0.5424 - val_loss: 2.4983 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.9543 - acc: 0.5254 - val_loss: 2.3413 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.8311 - acc: 0.5085 - val_loss: 2.2257 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.7382 - acc: 0.5085 - val_loss: 2.1087 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.6638 - acc: 0.4915 - val_loss: 2.0284 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.5991 - acc: 0.5000 - val_loss: 1.9626 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.5487 - acc: 0.5085 - val_loss: 1.8964 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.4949 - acc: 0.5000 - val_loss: 1.8576 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.4532 - acc: 0.5169 - val_loss: 1.8173 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4113 - acc: 0.5169 - val_loss: 1.7811 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3805 - acc: 0.5169 - val_loss: 1.7460 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.3469 - acc: 0.5085 - val_loss: 1.7236 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.3191 - acc: 0.5085 - val_loss: 1.6967 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.2951 - acc: 0.5169 - val_loss: 1.6750 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.2714 - acc: 0.5085 - val_loss: 1.6585 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2527 - acc: 0.5000 - val_loss: 1.6368 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2296 - acc: 0.5085 - val_loss: 1.6217 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2135 - acc: 0.5254 - val_loss: 1.6050 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.1944 - acc: 0.5254 - val_loss: 1.5901 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1797 - acc: 0.5169 - val_loss: 1.5772 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.1680 - acc: 0.5339 - val_loss: 1.5618 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.1506 - acc: 0.5339 - val_loss: 1.5506 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1387 - acc: 0.5339 - val_loss: 1.5420 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1288 - acc: 0.5424 - val_loss: 1.5339 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1152 - acc: 0.5339 - val_loss: 1.5273 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1082 - acc: 0.5254 - val_loss: 1.5194 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1004 - acc: 0.5424 - val_loss: 1.5183 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0883 - acc: 0.5424 - val_loss: 1.5169 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0780 - acc: 0.5339 - val_loss: 1.5127 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0706 - acc: 0.5339 - val_loss: 1.5103 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0631 - acc: 0.5508 - val_loss: 1.5072 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0527 - acc: 0.5508 - val_loss: 1.5068 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0482 - acc: 0.5424 - val_loss: 1.5038 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0403 - acc: 0.5424 - val_loss: 1.4968 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0342 - acc: 0.5508 - val_loss: 1.4931 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0310 - acc: 0.5593 - val_loss: 1.4892 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0236 - acc: 0.5508 - val_loss: 1.4839 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0210 - acc: 0.5508 - val_loss: 1.4786 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0155 - acc: 0.5593 - val_loss: 1.4740 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0103 - acc: 0.5508 - val_loss: 1.4693 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0079 - acc: 0.5593 - val_loss: 1.4662 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.0049 - acc: 0.5593 - val_loss: 1.4626 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0001 - acc: 0.5593 - val_loss: 1.4587 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9980 - acc: 0.5508 - val_loss: 1.4548 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9934 - acc: 0.5678 - val_loss: 1.4486 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9898 - acc: 0.5593 - val_loss: 1.4445 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9878 - acc: 0.5678 - val_loss: 1.4398 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9873 - acc: 0.5593 - val_loss: 1.4352 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9802 - acc: 0.5593 - val_loss: 1.4334 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9783 - acc: 0.5678 - val_loss: 1.4287 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9766 - acc: 0.5508 - val_loss: 1.4231 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9737 - acc: 0.5593 - val_loss: 1.4188 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9704 - acc: 0.5593 - val_loss: 1.4164 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9674 - acc: 0.5593 - val_loss: 1.4110 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9642 - acc: 0.5593 - val_loss: 1.4062 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9622 - acc: 0.5593 - val_loss: 1.4041 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9599 - acc: 0.5593 - val_loss: 1.3989 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9573 - acc: 0.5508 - val_loss: 1.3956 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9558 - acc: 0.5593 - val_loss: 1.3930 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9534 - acc: 0.5593 - val_loss: 1.3891 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9510 - acc: 0.5508 - val_loss: 1.3882 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9489 - acc: 0.5593 - val_loss: 1.3836 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9468 - acc: 0.5593 - val_loss: 1.3814 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9436 - acc: 0.5593 - val_loss: 1.3768 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9425 - acc: 0.5593 - val_loss: 1.3729 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9405 - acc: 0.5593 - val_loss: 1.3701 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.9396 - acc: 0.5847 - val_loss: 1.3673 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9371 - acc: 0.5763 - val_loss: 1.3661 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9345 - acc: 0.5847 - val_loss: 1.3620 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9345 - acc: 0.5847 - val_loss: 1.3586 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9315 - acc: 0.5932 - val_loss: 1.3566 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9297 - acc: 0.5932 - val_loss: 1.3548 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9295 - acc: 0.5932 - val_loss: 1.3518 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9280 - acc: 0.5932 - val_loss: 1.3514 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9268 - acc: 0.5932 - val_loss: 1.3485 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9237 - acc: 0.5932 - val_loss: 1.3457 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9230 - acc: 0.5932 - val_loss: 1.3436 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9213 - acc: 0.5932 - val_loss: 1.3409 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9216 - acc: 0.5932 - val_loss: 1.3395 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9185 - acc: 0.5847 - val_loss: 1.3383 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9184 - acc: 0.5932 - val_loss: 1.3355 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9165 - acc: 0.5847 - val_loss: 1.3333 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9154 - acc: 0.5847 - val_loss: 1.3327 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9151 - acc: 0.5847 - val_loss: 1.3290 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9121 - acc: 0.5847 - val_loss: 1.3282 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9123 - acc: 0.5932 - val_loss: 1.3258 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9106 - acc: 0.5847 - val_loss: 1.3270 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9091 - acc: 0.5847 - val_loss: 1.3247 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9092 - acc: 0.5847 - val_loss: 1.3258 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9065 - acc: 0.5847 - val_loss: 1.3214 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9057 - acc: 0.5847 - val_loss: 1.3206 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9055 - acc: 0.5763 - val_loss: 1.3176 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9036 - acc: 0.5932 - val_loss: 1.3185 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9028 - acc: 0.5847 - val_loss: 1.3147 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9013 - acc: 0.5932 - val_loss: 1.3150 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9015 - acc: 0.5847 - val_loss: 1.3138 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8986 - acc: 0.6017 - val_loss: 1.3126 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 271us/step - loss: 0.8983 - acc: 0.5932 - val_loss: 1.3124 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.8968 - acc: 0.6017 - val_loss: 1.3112 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8966 - acc: 0.5763 - val_loss: 1.3103 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.5342 - acc: 0.4153 - val_loss: 1.0527 - val_acc: 0.6154\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.4699 - acc: 0.4322 - val_loss: 1.0242 - val_acc: 0.6154\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.4196 - acc: 0.4661 - val_loss: 1.0059 - val_acc: 0.6154\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.3783 - acc: 0.4831 - val_loss: 0.9907 - val_acc: 0.6154\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.3420 - acc: 0.5000 - val_loss: 0.9807 - val_acc: 0.6154\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.3125 - acc: 0.5000 - val_loss: 0.9741 - val_acc: 0.6923\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.2872 - acc: 0.5000 - val_loss: 0.9700 - val_acc: 0.6923\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.2640 - acc: 0.5000 - val_loss: 0.9653 - val_acc: 0.6923\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2440 - acc: 0.4915 - val_loss: 0.9636 - val_acc: 0.6923\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.2247 - acc: 0.4831 - val_loss: 0.9614 - val_acc: 0.6154\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.2094 - acc: 0.4661 - val_loss: 0.9620 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.1930 - acc: 0.4831 - val_loss: 0.9594 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1801 - acc: 0.4746 - val_loss: 0.9575 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1673 - acc: 0.4661 - val_loss: 0.9592 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.1551 - acc: 0.4746 - val_loss: 0.9585 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1441 - acc: 0.4576 - val_loss: 0.9589 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.1340 - acc: 0.4746 - val_loss: 0.9596 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.1244 - acc: 0.5085 - val_loss: 0.9604 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.1161 - acc: 0.5169 - val_loss: 0.9600 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1083 - acc: 0.5508 - val_loss: 0.9598 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1005 - acc: 0.5508 - val_loss: 0.9616 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0941 - acc: 0.5508 - val_loss: 0.9612 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0871 - acc: 0.5593 - val_loss: 0.9626 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0818 - acc: 0.5763 - val_loss: 0.9627 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0758 - acc: 0.5678 - val_loss: 0.9625 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0705 - acc: 0.5678 - val_loss: 0.9632 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0657 - acc: 0.5593 - val_loss: 0.9643 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0610 - acc: 0.5593 - val_loss: 0.9649 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0568 - acc: 0.5678 - val_loss: 0.9651 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0526 - acc: 0.5678 - val_loss: 0.9659 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0481 - acc: 0.5763 - val_loss: 0.9661 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0447 - acc: 0.5763 - val_loss: 0.9672 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0406 - acc: 0.5763 - val_loss: 0.9682 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0376 - acc: 0.5763 - val_loss: 0.9679 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0338 - acc: 0.5847 - val_loss: 0.9689 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0305 - acc: 0.5763 - val_loss: 0.9695 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0276 - acc: 0.5932 - val_loss: 0.9703 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0242 - acc: 0.5932 - val_loss: 0.9704 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0216 - acc: 0.5932 - val_loss: 0.9711 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0185 - acc: 0.5932 - val_loss: 0.9717 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.0161 - acc: 0.5932 - val_loss: 0.9712 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.0136 - acc: 0.5932 - val_loss: 0.9697 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0115 - acc: 0.5932 - val_loss: 0.9701 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0091 - acc: 0.5932 - val_loss: 0.9719 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0069 - acc: 0.5932 - val_loss: 0.9697 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0053 - acc: 0.5932 - val_loss: 0.9708 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0030 - acc: 0.5932 - val_loss: 0.9721 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0007 - acc: 0.5932 - val_loss: 0.9714 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9988 - acc: 0.5932 - val_loss: 0.9713 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9973 - acc: 0.5932 - val_loss: 0.9713 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9954 - acc: 0.5932 - val_loss: 0.9711 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9939 - acc: 0.5932 - val_loss: 0.9711 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9919 - acc: 0.5932 - val_loss: 0.9716 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9909 - acc: 0.5932 - val_loss: 0.9718 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9892 - acc: 0.5932 - val_loss: 0.9695 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9879 - acc: 0.5932 - val_loss: 0.9699 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9862 - acc: 0.5932 - val_loss: 0.9704 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9847 - acc: 0.5932 - val_loss: 0.9699 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9835 - acc: 0.5932 - val_loss: 0.9703 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9819 - acc: 0.5932 - val_loss: 0.9692 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9809 - acc: 0.5932 - val_loss: 0.9693 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.9798 - acc: 0.5932 - val_loss: 0.9696 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9784 - acc: 0.5932 - val_loss: 0.9694 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9771 - acc: 0.6017 - val_loss: 0.9689 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9760 - acc: 0.6017 - val_loss: 0.9690 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9751 - acc: 0.6017 - val_loss: 0.9686 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9735 - acc: 0.5932 - val_loss: 0.9690 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9728 - acc: 0.5932 - val_loss: 0.9699 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9716 - acc: 0.5932 - val_loss: 0.9664 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9707 - acc: 0.5932 - val_loss: 0.9667 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9696 - acc: 0.5763 - val_loss: 0.9664 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9689 - acc: 0.5763 - val_loss: 0.9653 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9677 - acc: 0.5847 - val_loss: 0.9678 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9671 - acc: 0.5763 - val_loss: 0.9659 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9657 - acc: 0.5763 - val_loss: 0.9658 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9644 - acc: 0.5847 - val_loss: 0.9656 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9637 - acc: 0.5847 - val_loss: 0.9651 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9629 - acc: 0.5847 - val_loss: 0.9652 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9624 - acc: 0.5847 - val_loss: 0.9667 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9611 - acc: 0.5763 - val_loss: 0.9631 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9603 - acc: 0.5678 - val_loss: 0.9630 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9595 - acc: 0.5763 - val_loss: 0.9630 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9590 - acc: 0.5678 - val_loss: 0.9635 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9577 - acc: 0.5678 - val_loss: 0.9622 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9569 - acc: 0.5847 - val_loss: 0.9623 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9561 - acc: 0.5763 - val_loss: 0.9620 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9552 - acc: 0.5847 - val_loss: 0.9623 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9544 - acc: 0.5847 - val_loss: 0.9623 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9539 - acc: 0.5763 - val_loss: 0.9615 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9531 - acc: 0.5678 - val_loss: 0.9621 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9524 - acc: 0.5763 - val_loss: 0.9603 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9523 - acc: 0.5763 - val_loss: 0.9609 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9513 - acc: 0.5847 - val_loss: 0.9591 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9503 - acc: 0.5763 - val_loss: 0.9587 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9499 - acc: 0.5763 - val_loss: 0.9585 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9492 - acc: 0.5763 - val_loss: 0.9590 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9485 - acc: 0.5678 - val_loss: 0.9590 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9481 - acc: 0.5763 - val_loss: 0.9582 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9471 - acc: 0.5763 - val_loss: 0.9592 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9466 - acc: 0.5763 - val_loss: 0.9571 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.0766 - acc: 0.4153 - val_loss: 2.3092 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.9533 - acc: 0.3898 - val_loss: 2.1490 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.8490 - acc: 0.3814 - val_loss: 2.0235 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.7680 - acc: 0.3898 - val_loss: 1.9163 - val_acc: 0.1538\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.7005 - acc: 0.3898 - val_loss: 1.8301 - val_acc: 0.1538\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.6434 - acc: 0.4153 - val_loss: 1.7638 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.5946 - acc: 0.4322 - val_loss: 1.7004 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.5530 - acc: 0.4237 - val_loss: 1.6474 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.5153 - acc: 0.4322 - val_loss: 1.6061 - val_acc: 0.1538\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.4818 - acc: 0.4237 - val_loss: 1.5644 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.4535 - acc: 0.4153 - val_loss: 1.5273 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.4288 - acc: 0.4153 - val_loss: 1.5034 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.4060 - acc: 0.4153 - val_loss: 1.4754 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.3856 - acc: 0.3983 - val_loss: 1.4466 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.3651 - acc: 0.3814 - val_loss: 1.4269 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.3487 - acc: 0.3898 - val_loss: 1.4051 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.3316 - acc: 0.3729 - val_loss: 1.3911 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3176 - acc: 0.3814 - val_loss: 1.3726 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.3016 - acc: 0.3814 - val_loss: 1.3636 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.2880 - acc: 0.3729 - val_loss: 1.3536 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.2765 - acc: 0.3814 - val_loss: 1.3380 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.2642 - acc: 0.3814 - val_loss: 1.3232 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.2528 - acc: 0.3983 - val_loss: 1.3166 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.2453 - acc: 0.3898 - val_loss: 1.3082 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 1.2323 - acc: 0.3729 - val_loss: 1.2992 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2231 - acc: 0.3814 - val_loss: 1.2918 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2142 - acc: 0.3898 - val_loss: 1.2831 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.2058 - acc: 0.3814 - val_loss: 1.2777 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1970 - acc: 0.3814 - val_loss: 1.2695 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.1898 - acc: 0.3983 - val_loss: 1.2647 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1825 - acc: 0.3983 - val_loss: 1.2566 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1754 - acc: 0.3983 - val_loss: 1.2508 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1686 - acc: 0.4153 - val_loss: 1.2473 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1619 - acc: 0.3898 - val_loss: 1.2416 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1570 - acc: 0.3983 - val_loss: 1.2400 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1500 - acc: 0.4068 - val_loss: 1.2346 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1455 - acc: 0.3983 - val_loss: 1.2318 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1395 - acc: 0.4237 - val_loss: 1.2293 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1343 - acc: 0.4322 - val_loss: 1.2219 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1304 - acc: 0.4237 - val_loss: 1.2175 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.1244 - acc: 0.4407 - val_loss: 1.2181 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1213 - acc: 0.4322 - val_loss: 1.2108 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1171 - acc: 0.4407 - val_loss: 1.2056 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1131 - acc: 0.4322 - val_loss: 1.2052 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.1096 - acc: 0.4492 - val_loss: 1.1963 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1048 - acc: 0.4407 - val_loss: 1.1926 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1019 - acc: 0.4492 - val_loss: 1.1900 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0980 - acc: 0.4407 - val_loss: 1.1848 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0956 - acc: 0.4576 - val_loss: 1.1803 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0916 - acc: 0.4407 - val_loss: 1.1754 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0887 - acc: 0.4576 - val_loss: 1.1724 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0855 - acc: 0.4661 - val_loss: 1.1689 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0831 - acc: 0.4576 - val_loss: 1.1668 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0796 - acc: 0.4661 - val_loss: 1.1637 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0776 - acc: 0.4746 - val_loss: 1.1611 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0757 - acc: 0.4492 - val_loss: 1.1614 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0721 - acc: 0.4831 - val_loss: 1.1560 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0702 - acc: 0.4746 - val_loss: 1.1567 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0679 - acc: 0.4746 - val_loss: 1.1579 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0652 - acc: 0.4746 - val_loss: 1.1544 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0630 - acc: 0.4746 - val_loss: 1.1494 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0612 - acc: 0.4746 - val_loss: 1.1498 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0586 - acc: 0.4576 - val_loss: 1.1466 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0561 - acc: 0.4831 - val_loss: 1.1442 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0558 - acc: 0.4746 - val_loss: 1.1436 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0554 - acc: 0.4915 - val_loss: 1.1417 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0510 - acc: 0.4831 - val_loss: 1.1399 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0502 - acc: 0.4831 - val_loss: 1.1393 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0478 - acc: 0.4831 - val_loss: 1.1392 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0472 - acc: 0.4831 - val_loss: 1.1367 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0454 - acc: 0.4831 - val_loss: 1.1404 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0443 - acc: 0.4831 - val_loss: 1.1358 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0421 - acc: 0.4661 - val_loss: 1.1342 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0406 - acc: 0.4831 - val_loss: 1.1324 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0397 - acc: 0.4831 - val_loss: 1.1311 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0390 - acc: 0.4831 - val_loss: 1.1325 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0380 - acc: 0.4831 - val_loss: 1.1318 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0359 - acc: 0.4831 - val_loss: 1.1297 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0349 - acc: 0.4831 - val_loss: 1.1306 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 1.0329 - acc: 0.4831 - val_loss: 1.1282 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.0316 - acc: 0.4831 - val_loss: 1.1276 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0311 - acc: 0.4831 - val_loss: 1.1268 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0291 - acc: 0.5000 - val_loss: 1.1261 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0279 - acc: 0.5000 - val_loss: 1.1255 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0266 - acc: 0.5000 - val_loss: 1.1249 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 1.0261 - acc: 0.4915 - val_loss: 1.1257 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 289us/step - loss: 1.0252 - acc: 0.4915 - val_loss: 1.1252 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0247 - acc: 0.4831 - val_loss: 1.1235 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.0233 - acc: 0.4831 - val_loss: 1.1187 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.0213 - acc: 0.4915 - val_loss: 1.1195 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0210 - acc: 0.4746 - val_loss: 1.1211 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0200 - acc: 0.4746 - val_loss: 1.1172 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0197 - acc: 0.4831 - val_loss: 1.1182 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0182 - acc: 0.4746 - val_loss: 1.1182 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0170 - acc: 0.4746 - val_loss: 1.1181 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0167 - acc: 0.4746 - val_loss: 1.1165 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0155 - acc: 0.4831 - val_loss: 1.1159 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0150 - acc: 0.4915 - val_loss: 1.1159 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0143 - acc: 0.4746 - val_loss: 1.1166 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0128 - acc: 0.4915 - val_loss: 1.1157 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.9320 - acc: 0.2712 - val_loss: 3.5120 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 2.7610 - acc: 0.2797 - val_loss: 3.3188 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 2.5995 - acc: 0.2881 - val_loss: 3.1434 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 2.4369 - acc: 0.2966 - val_loss: 2.9879 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 2.2896 - acc: 0.3136 - val_loss: 2.8229 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 2.1564 - acc: 0.3220 - val_loss: 2.6796 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 2.0341 - acc: 0.3220 - val_loss: 2.5514 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.9257 - acc: 0.3390 - val_loss: 2.4169 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.8272 - acc: 0.3559 - val_loss: 2.3017 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.7336 - acc: 0.3644 - val_loss: 2.2016 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.6510 - acc: 0.3814 - val_loss: 2.1006 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.5822 - acc: 0.3814 - val_loss: 2.0113 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.5168 - acc: 0.3729 - val_loss: 1.9266 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.4632 - acc: 0.3729 - val_loss: 1.8576 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.4099 - acc: 0.3898 - val_loss: 1.7840 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.3635 - acc: 0.4068 - val_loss: 1.7174 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.3248 - acc: 0.4068 - val_loss: 1.6553 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.2904 - acc: 0.4153 - val_loss: 1.5979 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2589 - acc: 0.4407 - val_loss: 1.5481 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2320 - acc: 0.4576 - val_loss: 1.4935 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.2089 - acc: 0.4746 - val_loss: 1.4475 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.1871 - acc: 0.4915 - val_loss: 1.4088 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1688 - acc: 0.4915 - val_loss: 1.3700 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1515 - acc: 0.4746 - val_loss: 1.3336 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1360 - acc: 0.4915 - val_loss: 1.3029 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1234 - acc: 0.5000 - val_loss: 1.2696 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.1109 - acc: 0.5000 - val_loss: 1.2420 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0996 - acc: 0.5169 - val_loss: 1.2158 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0891 - acc: 0.5169 - val_loss: 1.1896 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0793 - acc: 0.5085 - val_loss: 1.1658 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0716 - acc: 0.5085 - val_loss: 1.1465 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0637 - acc: 0.5169 - val_loss: 1.1287 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0570 - acc: 0.5085 - val_loss: 1.1109 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0512 - acc: 0.5085 - val_loss: 1.0965 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0448 - acc: 0.5000 - val_loss: 1.0852 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0394 - acc: 0.5000 - val_loss: 1.0731 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0339 - acc: 0.5000 - val_loss: 1.0588 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0288 - acc: 0.5000 - val_loss: 1.0512 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0243 - acc: 0.5000 - val_loss: 1.0399 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0205 - acc: 0.5085 - val_loss: 1.0295 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0161 - acc: 0.5085 - val_loss: 1.0209 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0130 - acc: 0.5085 - val_loss: 1.0099 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0086 - acc: 0.5085 - val_loss: 1.0023 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0059 - acc: 0.5085 - val_loss: 0.9945 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0026 - acc: 0.5085 - val_loss: 0.9877 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9995 - acc: 0.5085 - val_loss: 0.9820 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9973 - acc: 0.5169 - val_loss: 0.9754 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9950 - acc: 0.5169 - val_loss: 0.9716 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9928 - acc: 0.5254 - val_loss: 0.9669 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9908 - acc: 0.5169 - val_loss: 0.9640 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9887 - acc: 0.5254 - val_loss: 0.9602 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9867 - acc: 0.5254 - val_loss: 0.9568 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9851 - acc: 0.5339 - val_loss: 0.9544 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9842 - acc: 0.5339 - val_loss: 0.9518 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9812 - acc: 0.5254 - val_loss: 0.9499 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9799 - acc: 0.5254 - val_loss: 0.9476 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9792 - acc: 0.5339 - val_loss: 0.9457 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9772 - acc: 0.5339 - val_loss: 0.9442 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9754 - acc: 0.5254 - val_loss: 0.9420 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9745 - acc: 0.5424 - val_loss: 0.9406 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9732 - acc: 0.5339 - val_loss: 0.9393 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9720 - acc: 0.5254 - val_loss: 0.9381 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9704 - acc: 0.5339 - val_loss: 0.9369 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9699 - acc: 0.5424 - val_loss: 0.9355 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9684 - acc: 0.5339 - val_loss: 0.9359 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9677 - acc: 0.5424 - val_loss: 0.9350 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9670 - acc: 0.5339 - val_loss: 0.9345 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9656 - acc: 0.5339 - val_loss: 0.9331 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9646 - acc: 0.5339 - val_loss: 0.9326 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9646 - acc: 0.5339 - val_loss: 0.9334 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9627 - acc: 0.5339 - val_loss: 0.9320 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9625 - acc: 0.5424 - val_loss: 0.9309 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9613 - acc: 0.5339 - val_loss: 0.9312 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9610 - acc: 0.5254 - val_loss: 0.9311 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.9597 - acc: 0.5339 - val_loss: 0.9295 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9588 - acc: 0.5254 - val_loss: 0.9308 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9584 - acc: 0.5254 - val_loss: 0.9300 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9575 - acc: 0.5254 - val_loss: 0.9306 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9565 - acc: 0.5254 - val_loss: 0.9302 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9561 - acc: 0.5254 - val_loss: 0.9307 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9551 - acc: 0.5254 - val_loss: 0.9295 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9556 - acc: 0.5339 - val_loss: 0.9293 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9541 - acc: 0.5339 - val_loss: 0.9293 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9539 - acc: 0.5424 - val_loss: 0.9294 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9529 - acc: 0.5424 - val_loss: 0.9293 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9522 - acc: 0.5339 - val_loss: 0.9289 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9522 - acc: 0.5424 - val_loss: 0.9286 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9515 - acc: 0.5424 - val_loss: 0.9285 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9503 - acc: 0.5508 - val_loss: 0.9287 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9502 - acc: 0.5339 - val_loss: 0.9284 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9493 - acc: 0.5424 - val_loss: 0.9289 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9485 - acc: 0.5424 - val_loss: 0.9290 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9483 - acc: 0.5339 - val_loss: 0.9294 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9475 - acc: 0.5339 - val_loss: 0.9302 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9471 - acc: 0.5339 - val_loss: 0.9301 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9462 - acc: 0.5254 - val_loss: 0.9307 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9458 - acc: 0.5254 - val_loss: 0.9308 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9455 - acc: 0.5254 - val_loss: 0.9308 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9448 - acc: 0.5254 - val_loss: 0.9314 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9440 - acc: 0.5169 - val_loss: 0.9310 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.3389 - acc: 0.2881 - val_loss: 1.7641 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 2.0490 - acc: 0.3644 - val_loss: 1.5917 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.8405 - acc: 0.3644 - val_loss: 1.4629 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.6965 - acc: 0.3898 - val_loss: 1.3727 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.5866 - acc: 0.3983 - val_loss: 1.3007 - val_acc: 0.1538\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.4933 - acc: 0.3814 - val_loss: 1.2472 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.4157 - acc: 0.3814 - val_loss: 1.2025 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.3510 - acc: 0.4068 - val_loss: 1.1626 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.2994 - acc: 0.4237 - val_loss: 1.1323 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.2594 - acc: 0.3898 - val_loss: 1.1061 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.2257 - acc: 0.3983 - val_loss: 1.0853 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.1991 - acc: 0.3983 - val_loss: 1.0664 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.1758 - acc: 0.4068 - val_loss: 1.0504 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1564 - acc: 0.4237 - val_loss: 1.0340 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1421 - acc: 0.4153 - val_loss: 1.0235 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.1281 - acc: 0.4322 - val_loss: 1.0112 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.1160 - acc: 0.4492 - val_loss: 1.0013 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.1067 - acc: 0.4322 - val_loss: 0.9919 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0978 - acc: 0.4576 - val_loss: 0.9832 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0899 - acc: 0.4576 - val_loss: 0.9760 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0815 - acc: 0.4661 - val_loss: 0.9684 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.0744 - acc: 0.4746 - val_loss: 0.9630 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0695 - acc: 0.4831 - val_loss: 0.9573 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.0632 - acc: 0.4831 - val_loss: 0.9528 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0572 - acc: 0.5000 - val_loss: 0.9483 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.0524 - acc: 0.4831 - val_loss: 0.9450 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 1.0479 - acc: 0.4915 - val_loss: 0.9420 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0447 - acc: 0.5000 - val_loss: 0.9394 - val_acc: 0.3077\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0416 - acc: 0.5169 - val_loss: 0.9367 - val_acc: 0.3077\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0375 - acc: 0.5424 - val_loss: 0.9351 - val_acc: 0.3077\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0341 - acc: 0.5508 - val_loss: 0.9324 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0300 - acc: 0.5254 - val_loss: 0.9307 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0280 - acc: 0.5339 - val_loss: 0.9291 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0250 - acc: 0.5339 - val_loss: 0.9271 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0222 - acc: 0.5339 - val_loss: 0.9260 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0213 - acc: 0.5424 - val_loss: 0.9254 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0181 - acc: 0.5339 - val_loss: 0.9248 - val_acc: 0.3077\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.0153 - acc: 0.5508 - val_loss: 0.9243 - val_acc: 0.3077\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 1.0143 - acc: 0.5508 - val_loss: 0.9236 - val_acc: 0.3077\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0106 - acc: 0.5508 - val_loss: 0.9219 - val_acc: 0.3077\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0091 - acc: 0.5339 - val_loss: 0.9217 - val_acc: 0.2308\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0078 - acc: 0.5424 - val_loss: 0.9207 - val_acc: 0.2308\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0057 - acc: 0.5424 - val_loss: 0.9211 - val_acc: 0.2308\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0030 - acc: 0.5508 - val_loss: 0.9208 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0023 - acc: 0.5508 - val_loss: 0.9195 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9995 - acc: 0.5593 - val_loss: 0.9201 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9977 - acc: 0.5593 - val_loss: 0.9199 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9969 - acc: 0.5678 - val_loss: 0.9185 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9963 - acc: 0.5508 - val_loss: 0.9195 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9947 - acc: 0.5593 - val_loss: 0.9186 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 277us/step - loss: 0.9927 - acc: 0.5678 - val_loss: 0.9184 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9905 - acc: 0.5678 - val_loss: 0.9178 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9899 - acc: 0.5763 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9882 - acc: 0.5847 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9872 - acc: 0.5847 - val_loss: 0.9172 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9867 - acc: 0.5763 - val_loss: 0.9173 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9844 - acc: 0.5763 - val_loss: 0.9166 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9837 - acc: 0.5932 - val_loss: 0.9167 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9828 - acc: 0.5932 - val_loss: 0.9168 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9817 - acc: 0.5847 - val_loss: 0.9164 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9811 - acc: 0.5932 - val_loss: 0.9156 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9803 - acc: 0.5763 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9790 - acc: 0.5847 - val_loss: 0.9166 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9779 - acc: 0.5847 - val_loss: 0.9182 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9767 - acc: 0.5763 - val_loss: 0.9182 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9756 - acc: 0.5763 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9744 - acc: 0.5847 - val_loss: 0.9179 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9734 - acc: 0.5847 - val_loss: 0.9177 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9731 - acc: 0.5847 - val_loss: 0.9172 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9719 - acc: 0.5847 - val_loss: 0.9178 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9715 - acc: 0.5763 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9694 - acc: 0.5763 - val_loss: 0.9171 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9689 - acc: 0.5678 - val_loss: 0.9166 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9691 - acc: 0.5763 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9672 - acc: 0.5678 - val_loss: 0.9163 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9661 - acc: 0.5678 - val_loss: 0.9162 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9652 - acc: 0.5847 - val_loss: 0.9166 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9644 - acc: 0.5847 - val_loss: 0.9168 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9636 - acc: 0.5847 - val_loss: 0.9173 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9628 - acc: 0.5763 - val_loss: 0.9172 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9620 - acc: 0.5847 - val_loss: 0.9170 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9622 - acc: 0.5763 - val_loss: 0.9168 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9602 - acc: 0.5847 - val_loss: 0.9176 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9606 - acc: 0.5763 - val_loss: 0.9169 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9604 - acc: 0.5763 - val_loss: 0.9172 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9579 - acc: 0.5847 - val_loss: 0.9166 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9574 - acc: 0.5847 - val_loss: 0.9161 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9570 - acc: 0.5847 - val_loss: 0.9163 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9562 - acc: 0.5763 - val_loss: 0.9159 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9553 - acc: 0.5678 - val_loss: 0.9159 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9560 - acc: 0.5763 - val_loss: 0.9155 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9541 - acc: 0.5763 - val_loss: 0.9151 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.9529 - acc: 0.5847 - val_loss: 0.9147 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9519 - acc: 0.5678 - val_loss: 0.9148 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9512 - acc: 0.5763 - val_loss: 0.9140 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9508 - acc: 0.5763 - val_loss: 0.9146 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9502 - acc: 0.5678 - val_loss: 0.9140 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9492 - acc: 0.5763 - val_loss: 0.9140 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9499 - acc: 0.5678 - val_loss: 0.9139 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9482 - acc: 0.5678 - val_loss: 0.9136 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.3084 - acc: 0.2034 - val_loss: 1.9242 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 2.1267 - acc: 0.2119 - val_loss: 1.7714 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.9793 - acc: 0.2373 - val_loss: 1.6461 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.8605 - acc: 0.2458 - val_loss: 1.5349 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.7610 - acc: 0.2373 - val_loss: 1.4452 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.6792 - acc: 0.2034 - val_loss: 1.3668 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.6081 - acc: 0.2119 - val_loss: 1.3092 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.5539 - acc: 0.2119 - val_loss: 1.2635 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.5029 - acc: 0.2288 - val_loss: 1.2207 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4607 - acc: 0.2373 - val_loss: 1.1881 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.4222 - acc: 0.2458 - val_loss: 1.1591 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.3882 - acc: 0.2458 - val_loss: 1.1331 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.3575 - acc: 0.2542 - val_loss: 1.1084 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.3268 - acc: 0.2712 - val_loss: 1.0875 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.3000 - acc: 0.2966 - val_loss: 1.0670 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2784 - acc: 0.3305 - val_loss: 1.0529 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.2584 - acc: 0.3390 - val_loss: 1.0407 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.2408 - acc: 0.3390 - val_loss: 1.0311 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2243 - acc: 0.3475 - val_loss: 1.0220 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.2092 - acc: 0.3559 - val_loss: 1.0117 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1942 - acc: 0.3559 - val_loss: 1.0037 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1812 - acc: 0.3559 - val_loss: 0.9941 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1658 - acc: 0.3475 - val_loss: 0.9872 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.1551 - acc: 0.3475 - val_loss: 0.9787 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1424 - acc: 0.3559 - val_loss: 0.9724 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 285us/step - loss: 1.1312 - acc: 0.3644 - val_loss: 0.9660 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1202 - acc: 0.3729 - val_loss: 0.9592 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.1092 - acc: 0.3814 - val_loss: 0.9530 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.1005 - acc: 0.3814 - val_loss: 0.9474 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0907 - acc: 0.3814 - val_loss: 0.9417 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0829 - acc: 0.3814 - val_loss: 0.9383 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0731 - acc: 0.3814 - val_loss: 0.9350 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0645 - acc: 0.3814 - val_loss: 0.9308 - val_acc: 0.3077\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0578 - acc: 0.3898 - val_loss: 0.9280 - val_acc: 0.3077\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0495 - acc: 0.4068 - val_loss: 0.9249 - val_acc: 0.3077\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0424 - acc: 0.4068 - val_loss: 0.9224 - val_acc: 0.3077\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0356 - acc: 0.4068 - val_loss: 0.9204 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0283 - acc: 0.4322 - val_loss: 0.9184 - val_acc: 0.3077\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0242 - acc: 0.4322 - val_loss: 0.9162 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0184 - acc: 0.4237 - val_loss: 0.9140 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0126 - acc: 0.4237 - val_loss: 0.9129 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0087 - acc: 0.4407 - val_loss: 0.9118 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0038 - acc: 0.4407 - val_loss: 0.9107 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9998 - acc: 0.4407 - val_loss: 0.9099 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9966 - acc: 0.4492 - val_loss: 0.9086 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.9933 - acc: 0.4492 - val_loss: 0.9076 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9900 - acc: 0.4407 - val_loss: 0.9067 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9873 - acc: 0.4407 - val_loss: 0.9057 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9837 - acc: 0.4407 - val_loss: 0.9045 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9808 - acc: 0.4407 - val_loss: 0.9038 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9787 - acc: 0.4322 - val_loss: 0.9030 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9765 - acc: 0.4237 - val_loss: 0.9024 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9737 - acc: 0.4322 - val_loss: 0.9037 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9710 - acc: 0.4492 - val_loss: 0.9036 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9691 - acc: 0.4237 - val_loss: 0.9034 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9658 - acc: 0.4407 - val_loss: 0.9058 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9641 - acc: 0.4576 - val_loss: 0.9060 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 0.9626 - acc: 0.4576 - val_loss: 0.9068 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9605 - acc: 0.4746 - val_loss: 0.9074 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9583 - acc: 0.4661 - val_loss: 0.9091 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9557 - acc: 0.4746 - val_loss: 0.9086 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9545 - acc: 0.4915 - val_loss: 0.9095 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9531 - acc: 0.4831 - val_loss: 0.9113 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 0.9511 - acc: 0.4915 - val_loss: 0.9124 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9492 - acc: 0.5085 - val_loss: 0.9155 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9479 - acc: 0.5169 - val_loss: 0.9171 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9463 - acc: 0.5169 - val_loss: 0.9186 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9452 - acc: 0.5085 - val_loss: 0.9213 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9434 - acc: 0.5169 - val_loss: 0.9229 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9427 - acc: 0.5254 - val_loss: 0.9246 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9405 - acc: 0.5254 - val_loss: 0.9258 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9398 - acc: 0.5254 - val_loss: 0.9258 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9396 - acc: 0.5254 - val_loss: 0.9262 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9379 - acc: 0.5169 - val_loss: 0.9287 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9373 - acc: 0.5254 - val_loss: 0.9298 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9356 - acc: 0.5254 - val_loss: 0.9301 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9353 - acc: 0.5169 - val_loss: 0.9306 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9347 - acc: 0.5169 - val_loss: 0.9316 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9337 - acc: 0.5254 - val_loss: 0.9326 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9328 - acc: 0.5254 - val_loss: 0.9339 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9321 - acc: 0.5254 - val_loss: 0.9355 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9320 - acc: 0.5254 - val_loss: 0.9360 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9310 - acc: 0.5169 - val_loss: 0.9386 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9300 - acc: 0.5169 - val_loss: 0.9393 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9302 - acc: 0.5254 - val_loss: 0.9401 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9297 - acc: 0.5339 - val_loss: 0.9401 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9284 - acc: 0.5339 - val_loss: 0.9423 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9283 - acc: 0.5339 - val_loss: 0.9424 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9277 - acc: 0.5339 - val_loss: 0.9443 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9271 - acc: 0.5424 - val_loss: 0.9431 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9270 - acc: 0.5424 - val_loss: 0.9445 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9261 - acc: 0.5424 - val_loss: 0.9466 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9258 - acc: 0.5424 - val_loss: 0.9478 - val_acc: 0.6154\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9252 - acc: 0.5424 - val_loss: 0.9478 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9262 - acc: 0.5424 - val_loss: 0.9487 - val_acc: 0.6154\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9241 - acc: 0.5508 - val_loss: 0.9508 - val_acc: 0.6154\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9244 - acc: 0.5508 - val_loss: 0.9517 - val_acc: 0.6154\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9229 - acc: 0.5508 - val_loss: 0.9518 - val_acc: 0.6154\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9231 - acc: 0.5508 - val_loss: 0.9514 - val_acc: 0.6154\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9231 - acc: 0.5508 - val_loss: 0.9538 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 2.2084 - acc: 0.3475 - val_loss: 2.4610 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.9857 - acc: 0.3644 - val_loss: 2.2683 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.8066 - acc: 0.3644 - val_loss: 2.1012 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.6694 - acc: 0.3814 - val_loss: 1.9720 - val_acc: 0.1538\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.5593 - acc: 0.4068 - val_loss: 1.8708 - val_acc: 0.1538\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.4706 - acc: 0.4068 - val_loss: 1.7830 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.3975 - acc: 0.4237 - val_loss: 1.7035 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.3352 - acc: 0.4237 - val_loss: 1.6370 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2845 - acc: 0.4322 - val_loss: 1.5763 - val_acc: 0.1538\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.2419 - acc: 0.4407 - val_loss: 1.5254 - val_acc: 0.1538\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.2046 - acc: 0.4576 - val_loss: 1.4767 - val_acc: 0.0769\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1721 - acc: 0.4492 - val_loss: 1.4348 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1460 - acc: 0.4407 - val_loss: 1.3982 - val_acc: 0.1538\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1239 - acc: 0.4746 - val_loss: 1.3685 - val_acc: 0.1538\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1043 - acc: 0.4746 - val_loss: 1.3408 - val_acc: 0.1538\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0868 - acc: 0.4831 - val_loss: 1.3166 - val_acc: 0.1538\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0720 - acc: 0.4831 - val_loss: 1.2973 - val_acc: 0.2308\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0587 - acc: 0.5169 - val_loss: 1.2777 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0485 - acc: 0.5254 - val_loss: 1.2623 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 1.0384 - acc: 0.5254 - val_loss: 1.2489 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0296 - acc: 0.5169 - val_loss: 1.2342 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.0218 - acc: 0.5085 - val_loss: 1.2245 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0152 - acc: 0.5169 - val_loss: 1.2123 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0095 - acc: 0.5339 - val_loss: 1.2038 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0048 - acc: 0.5254 - val_loss: 1.1959 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0017 - acc: 0.5339 - val_loss: 1.1875 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9949 - acc: 0.5508 - val_loss: 1.1799 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9915 - acc: 0.5508 - val_loss: 1.1741 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 267us/step - loss: 0.9888 - acc: 0.5508 - val_loss: 1.1689 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9846 - acc: 0.5593 - val_loss: 1.1626 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9815 - acc: 0.5508 - val_loss: 1.1589 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9778 - acc: 0.5508 - val_loss: 1.1538 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9760 - acc: 0.5508 - val_loss: 1.1499 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9723 - acc: 0.5508 - val_loss: 1.1455 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9702 - acc: 0.5508 - val_loss: 1.1419 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9688 - acc: 0.5508 - val_loss: 1.1397 - val_acc: 0.6154\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9665 - acc: 0.5508 - val_loss: 1.1370 - val_acc: 0.6154\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9649 - acc: 0.5424 - val_loss: 1.1345 - val_acc: 0.6154\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9626 - acc: 0.5508 - val_loss: 1.1320 - val_acc: 0.6154\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9608 - acc: 0.5339 - val_loss: 1.1293 - val_acc: 0.6154\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9599 - acc: 0.5169 - val_loss: 1.1271 - val_acc: 0.6154\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9579 - acc: 0.5254 - val_loss: 1.1241 - val_acc: 0.6154\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9567 - acc: 0.5254 - val_loss: 1.1237 - val_acc: 0.6154\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9549 - acc: 0.5254 - val_loss: 1.1208 - val_acc: 0.6154\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9538 - acc: 0.5339 - val_loss: 1.1185 - val_acc: 0.6154\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9527 - acc: 0.5254 - val_loss: 1.1181 - val_acc: 0.6154\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9514 - acc: 0.5254 - val_loss: 1.1169 - val_acc: 0.6154\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.9499 - acc: 0.5339 - val_loss: 1.1162 - val_acc: 0.6154\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9489 - acc: 0.5254 - val_loss: 1.1146 - val_acc: 0.6154\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.9477 - acc: 0.5424 - val_loss: 1.1135 - val_acc: 0.6154\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9465 - acc: 0.5339 - val_loss: 1.1110 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9455 - acc: 0.5339 - val_loss: 1.1110 - val_acc: 0.6154\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9447 - acc: 0.5339 - val_loss: 1.1085 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9438 - acc: 0.5339 - val_loss: 1.1086 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9429 - acc: 0.5339 - val_loss: 1.1080 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9429 - acc: 0.5593 - val_loss: 1.1062 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9411 - acc: 0.5339 - val_loss: 1.1063 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9398 - acc: 0.5424 - val_loss: 1.1055 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9393 - acc: 0.5424 - val_loss: 1.1048 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9383 - acc: 0.5424 - val_loss: 1.1036 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9378 - acc: 0.5593 - val_loss: 1.1037 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9374 - acc: 0.5508 - val_loss: 1.1047 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9362 - acc: 0.5593 - val_loss: 1.1042 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9355 - acc: 0.5593 - val_loss: 1.1028 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9350 - acc: 0.5424 - val_loss: 1.1021 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9342 - acc: 0.5508 - val_loss: 1.1014 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9336 - acc: 0.5508 - val_loss: 1.1006 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9335 - acc: 0.5508 - val_loss: 1.1005 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9329 - acc: 0.5763 - val_loss: 1.0995 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9322 - acc: 0.5678 - val_loss: 1.0980 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9315 - acc: 0.5678 - val_loss: 1.0992 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9315 - acc: 0.5678 - val_loss: 1.0993 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9313 - acc: 0.5593 - val_loss: 1.0992 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9299 - acc: 0.5593 - val_loss: 1.0998 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9296 - acc: 0.5508 - val_loss: 1.1008 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9289 - acc: 0.5424 - val_loss: 1.0999 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9287 - acc: 0.5593 - val_loss: 1.0999 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9286 - acc: 0.5593 - val_loss: 1.1001 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9278 - acc: 0.5593 - val_loss: 1.1007 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9275 - acc: 0.5763 - val_loss: 1.1005 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9271 - acc: 0.5593 - val_loss: 1.1003 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9266 - acc: 0.5678 - val_loss: 1.1008 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9257 - acc: 0.5763 - val_loss: 1.1001 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9258 - acc: 0.5508 - val_loss: 1.0993 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9253 - acc: 0.5678 - val_loss: 1.0998 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9253 - acc: 0.5678 - val_loss: 1.1006 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9257 - acc: 0.5593 - val_loss: 1.0978 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9249 - acc: 0.5508 - val_loss: 1.0994 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9246 - acc: 0.5678 - val_loss: 1.0995 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9242 - acc: 0.5593 - val_loss: 1.0978 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9235 - acc: 0.5763 - val_loss: 1.0994 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9230 - acc: 0.5593 - val_loss: 1.0987 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9231 - acc: 0.5593 - val_loss: 1.0986 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9224 - acc: 0.5678 - val_loss: 1.0980 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9220 - acc: 0.5763 - val_loss: 1.0993 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9222 - acc: 0.5678 - val_loss: 1.1006 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9215 - acc: 0.5678 - val_loss: 1.0990 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9212 - acc: 0.5508 - val_loss: 1.0988 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9211 - acc: 0.5508 - val_loss: 1.1002 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9204 - acc: 0.5678 - val_loss: 1.1000 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 2.5497 - acc: 0.1525 - val_loss: 2.6801 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 2.2075 - acc: 0.2881 - val_loss: 2.4849 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.9842 - acc: 0.3220 - val_loss: 2.3295 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.8283 - acc: 0.3475 - val_loss: 2.1992 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.7099 - acc: 0.3559 - val_loss: 2.0972 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.6199 - acc: 0.3644 - val_loss: 2.0051 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.5412 - acc: 0.3729 - val_loss: 1.9231 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.4765 - acc: 0.3729 - val_loss: 1.8580 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.4222 - acc: 0.3559 - val_loss: 1.8012 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.3749 - acc: 0.3644 - val_loss: 1.7484 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.3335 - acc: 0.3644 - val_loss: 1.7056 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 1.2967 - acc: 0.3729 - val_loss: 1.6636 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 265us/step - loss: 1.2649 - acc: 0.3559 - val_loss: 1.6277 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2354 - acc: 0.3814 - val_loss: 1.5966 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2123 - acc: 0.3814 - val_loss: 1.5692 - val_acc: 0.2308\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1898 - acc: 0.3983 - val_loss: 1.5403 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1713 - acc: 0.4322 - val_loss: 1.5217 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.1540 - acc: 0.4407 - val_loss: 1.5023 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.1398 - acc: 0.4322 - val_loss: 1.4842 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.1272 - acc: 0.4237 - val_loss: 1.4712 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1156 - acc: 0.4407 - val_loss: 1.4566 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.1073 - acc: 0.4237 - val_loss: 1.4443 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0972 - acc: 0.4407 - val_loss: 1.4347 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.0885 - acc: 0.4322 - val_loss: 1.4244 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0800 - acc: 0.4322 - val_loss: 1.4164 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0741 - acc: 0.4322 - val_loss: 1.4081 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0686 - acc: 0.4492 - val_loss: 1.4006 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.0627 - acc: 0.4322 - val_loss: 1.3944 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0583 - acc: 0.4407 - val_loss: 1.3885 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0556 - acc: 0.4576 - val_loss: 1.3831 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0500 - acc: 0.4492 - val_loss: 1.3772 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0472 - acc: 0.4576 - val_loss: 1.3722 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0427 - acc: 0.4492 - val_loss: 1.3676 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0385 - acc: 0.4746 - val_loss: 1.3624 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0354 - acc: 0.4661 - val_loss: 1.3590 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0323 - acc: 0.4492 - val_loss: 1.3549 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0307 - acc: 0.4492 - val_loss: 1.3517 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0271 - acc: 0.4661 - val_loss: 1.3483 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0233 - acc: 0.4576 - val_loss: 1.3439 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0220 - acc: 0.4831 - val_loss: 1.3411 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.0194 - acc: 0.4915 - val_loss: 1.3382 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0173 - acc: 0.4661 - val_loss: 1.3346 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.0150 - acc: 0.4576 - val_loss: 1.3323 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0138 - acc: 0.4915 - val_loss: 1.3299 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0103 - acc: 0.4746 - val_loss: 1.3268 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0093 - acc: 0.5254 - val_loss: 1.3233 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0065 - acc: 0.4915 - val_loss: 1.3211 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0057 - acc: 0.5085 - val_loss: 1.3189 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0034 - acc: 0.5000 - val_loss: 1.3157 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0020 - acc: 0.4915 - val_loss: 1.3126 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0006 - acc: 0.5085 - val_loss: 1.3105 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9987 - acc: 0.4915 - val_loss: 1.3087 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9974 - acc: 0.5169 - val_loss: 1.3072 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9973 - acc: 0.5000 - val_loss: 1.3056 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9956 - acc: 0.5085 - val_loss: 1.3045 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9931 - acc: 0.5339 - val_loss: 1.3026 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9927 - acc: 0.5339 - val_loss: 1.3011 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9913 - acc: 0.5254 - val_loss: 1.3002 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9892 - acc: 0.4915 - val_loss: 1.2989 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9887 - acc: 0.5169 - val_loss: 1.2980 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9875 - acc: 0.5000 - val_loss: 1.2961 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9860 - acc: 0.5085 - val_loss: 1.2948 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9865 - acc: 0.5254 - val_loss: 1.2933 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9847 - acc: 0.5254 - val_loss: 1.2935 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9829 - acc: 0.5339 - val_loss: 1.2927 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9831 - acc: 0.5339 - val_loss: 1.2915 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9818 - acc: 0.5424 - val_loss: 1.2918 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.9807 - acc: 0.5254 - val_loss: 1.2902 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9795 - acc: 0.5339 - val_loss: 1.2892 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9800 - acc: 0.5169 - val_loss: 1.2880 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9766 - acc: 0.5254 - val_loss: 1.2875 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9764 - acc: 0.5254 - val_loss: 1.2862 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9756 - acc: 0.5000 - val_loss: 1.2855 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9752 - acc: 0.5085 - val_loss: 1.2849 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9732 - acc: 0.5169 - val_loss: 1.2840 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9735 - acc: 0.5169 - val_loss: 1.2838 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9733 - acc: 0.5254 - val_loss: 1.2823 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9715 - acc: 0.5000 - val_loss: 1.2820 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9719 - acc: 0.5169 - val_loss: 1.2817 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9707 - acc: 0.5000 - val_loss: 1.2807 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9692 - acc: 0.5085 - val_loss: 1.2798 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9695 - acc: 0.4915 - val_loss: 1.2787 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9669 - acc: 0.4915 - val_loss: 1.2785 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9664 - acc: 0.5085 - val_loss: 1.2786 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9660 - acc: 0.4915 - val_loss: 1.2779 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9656 - acc: 0.4915 - val_loss: 1.2777 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9648 - acc: 0.4915 - val_loss: 1.2773 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9638 - acc: 0.4915 - val_loss: 1.2759 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9630 - acc: 0.4915 - val_loss: 1.2749 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9625 - acc: 0.4915 - val_loss: 1.2745 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9627 - acc: 0.5169 - val_loss: 1.2729 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9626 - acc: 0.4831 - val_loss: 1.2719 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9605 - acc: 0.4915 - val_loss: 1.2730 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9590 - acc: 0.4831 - val_loss: 1.2714 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9593 - acc: 0.4831 - val_loss: 1.2723 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9594 - acc: 0.4746 - val_loss: 1.2711 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9575 - acc: 0.4831 - val_loss: 1.2699 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9572 - acc: 0.5169 - val_loss: 1.2692 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9562 - acc: 0.4915 - val_loss: 1.2691 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9559 - acc: 0.5254 - val_loss: 1.2676 - val_acc: 0.4615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "ae0a8cbb-4d6e-45f6-c8d8-d153c829340b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "53e29178-74be-4d8b-a96d-2fb248c280ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "9ef80376-ccd1-42ae-f56f-82d141d1e6e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "df19d794-b288-4984-cc23-35744d96a2f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7eff9230b278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnCUCY4wJOEsExhgTcP8fr06og850PR8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "829b4c02-c2eb-4290-d97a-776a04ef99f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7eff91e43cf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkQcI55xzEf0/h7eW0mF1IUgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "c08b57b0-76b6-4d8b-f4d7-d07ca89a05df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 0s 4ms/step - loss: 2.7938 - acc: 0.2137\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 2.4296 - acc: 0.2214\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 2.1555 - acc: 0.2214\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.9364 - acc: 0.2290\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.7579 - acc: 0.2443\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.6101 - acc: 0.2901\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.5001 - acc: 0.3282\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.4195 - acc: 0.3435\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.3570 - acc: 0.3435\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.3130 - acc: 0.3893\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.2772 - acc: 0.4351\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.2506 - acc: 0.4580\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 1.2292 - acc: 0.4809\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.2129 - acc: 0.5344\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.1970 - acc: 0.5420\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.1806 - acc: 0.5725\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.1696 - acc: 0.5649\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.1582 - acc: 0.5802\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.1481 - acc: 0.5878\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.1401 - acc: 0.5802\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.1321 - acc: 0.5802\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.1237 - acc: 0.5802\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.1180 - acc: 0.5802\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.1108 - acc: 0.5954\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.1076 - acc: 0.5878\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 1.1005 - acc: 0.6031\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 1.0943 - acc: 0.5954\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 1.0889 - acc: 0.5954\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.0844 - acc: 0.5954\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 1.0798 - acc: 0.6107\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.0752 - acc: 0.6031\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0717 - acc: 0.6107\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0669 - acc: 0.6183\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0635 - acc: 0.6260\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0618 - acc: 0.6107\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.0576 - acc: 0.5954\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0532 - acc: 0.6183\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0516 - acc: 0.6031\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0477 - acc: 0.6031\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0464 - acc: 0.6031\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0420 - acc: 0.5954\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0401 - acc: 0.5802\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0389 - acc: 0.5802\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0377 - acc: 0.5802\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0324 - acc: 0.5878\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0304 - acc: 0.5878\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0288 - acc: 0.5954\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.0265 - acc: 0.5878\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.0247 - acc: 0.5878\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 1.0238 - acc: 0.6031\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0208 - acc: 0.6031\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.0191 - acc: 0.5954\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0174 - acc: 0.6031\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0159 - acc: 0.5954\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0152 - acc: 0.5954\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0132 - acc: 0.5878\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0132 - acc: 0.5878\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0102 - acc: 0.5878\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0105 - acc: 0.5802\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0085 - acc: 0.5649\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 1.0058 - acc: 0.5802\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.0049 - acc: 0.6107\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0035 - acc: 0.6031\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.0028 - acc: 0.5954\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0015 - acc: 0.6107\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0005 - acc: 0.5954\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9988 - acc: 0.6031\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9982 - acc: 0.5878\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9960 - acc: 0.5954\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9965 - acc: 0.5878\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 259us/step - loss: 0.9944 - acc: 0.5954\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9930 - acc: 0.5725\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9920 - acc: 0.5802\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9912 - acc: 0.5725\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9899 - acc: 0.5802\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9894 - acc: 0.5725\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.9873 - acc: 0.5878\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9862 - acc: 0.5954\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9853 - acc: 0.5878\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9846 - acc: 0.5878\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9839 - acc: 0.5954\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9824 - acc: 0.6031\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9823 - acc: 0.5954\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9810 - acc: 0.5878\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9806 - acc: 0.5802\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9788 - acc: 0.5725\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9784 - acc: 0.5725\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9779 - acc: 0.5802\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9767 - acc: 0.5802\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9762 - acc: 0.5725\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9756 - acc: 0.5802\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9740 - acc: 0.5878\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9736 - acc: 0.5725\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9730 - acc: 0.5802\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.9721 - acc: 0.5878\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9714 - acc: 0.5878\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9711 - acc: 0.5954\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9696 - acc: 0.5802\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9701 - acc: 0.5802\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.9681 - acc: 0.5954\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "7dcac688-6857-4a62-9826-b8ffa1995485",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "a230ba8f-6f28-45ef-a2c3-dab2e1923d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17647058823529413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "4ffcc8d1-2314-4609-b752-31a293db5ee8"
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "017ff249-4ac4-4794-d63a-a7d6a62c64ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dvJzYLTzmiQa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7088f06a-157b-4fff-9dbf-2868d7acd58b",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c9b97d7a-d087-4fc7-a6bd-fa52ad7e06c0",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "01513750-27b2-4775-92ed-b52f46ecefb5",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "27b38fe9-76b8-4ffe-dcdc-ccdc08255b6a",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 1.3600 - acc: 0.3932 - val_loss: 1.3911 - val_acc: 0.4286\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 1.2576 - acc: 0.4103 - val_loss: 1.2856 - val_acc: 0.4286\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.1627 - acc: 0.4103 - val_loss: 1.1837 - val_acc: 0.4286\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 1.0784 - acc: 0.6581 - val_loss: 1.0957 - val_acc: 0.8571\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 1.0007 - acc: 0.8547 - val_loss: 1.0164 - val_acc: 0.8571\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.9295 - acc: 0.8547 - val_loss: 0.9381 - val_acc: 0.8571\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.8646 - acc: 0.8547 - val_loss: 0.8695 - val_acc: 0.8571\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.8071 - acc: 0.8632 - val_loss: 0.8085 - val_acc: 0.8571\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.7558 - acc: 0.8547 - val_loss: 0.7521 - val_acc: 0.8571\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.7075 - acc: 0.8547 - val_loss: 0.7022 - val_acc: 0.8571\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.6644 - acc: 0.8547 - val_loss: 0.6567 - val_acc: 0.8571\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.6263 - acc: 0.8547 - val_loss: 0.6158 - val_acc: 0.8571\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.5910 - acc: 0.8547 - val_loss: 0.5796 - val_acc: 0.8571\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.5598 - acc: 0.8547 - val_loss: 0.5472 - val_acc: 0.8571\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.5309 - acc: 0.8632 - val_loss: 0.5164 - val_acc: 0.8571\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.5063 - acc: 0.8632 - val_loss: 0.4913 - val_acc: 0.8571\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.4863 - acc: 0.8718 - val_loss: 0.4695 - val_acc: 0.8571\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.4688 - acc: 0.8718 - val_loss: 0.4510 - val_acc: 0.8571\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.4533 - acc: 0.8718 - val_loss: 0.4362 - val_acc: 0.8571\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.4393 - acc: 0.8718 - val_loss: 0.4233 - val_acc: 0.8571\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.4271 - acc: 0.8718 - val_loss: 0.4129 - val_acc: 0.8571\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 231us/step - loss: 0.4164 - acc: 0.8718 - val_loss: 0.4033 - val_acc: 0.8571\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.4064 - acc: 0.8718 - val_loss: 0.3945 - val_acc: 0.8571\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 258us/step - loss: 0.3974 - acc: 0.8718 - val_loss: 0.3862 - val_acc: 0.8571\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.3889 - acc: 0.8718 - val_loss: 0.3784 - val_acc: 0.8571\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.3810 - acc: 0.8718 - val_loss: 0.3711 - val_acc: 0.8571\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.3735 - acc: 0.8718 - val_loss: 0.3643 - val_acc: 0.8571\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 155us/step - loss: 0.3664 - acc: 0.8718 - val_loss: 0.3578 - val_acc: 0.8571\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.3599 - acc: 0.8718 - val_loss: 0.3517 - val_acc: 0.8571\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.3538 - acc: 0.8718 - val_loss: 0.3459 - val_acc: 0.8571\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.3480 - acc: 0.8718 - val_loss: 0.3404 - val_acc: 0.8571\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.3425 - acc: 0.8718 - val_loss: 0.3351 - val_acc: 0.8571\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 156us/step - loss: 0.3373 - acc: 0.8718 - val_loss: 0.3302 - val_acc: 0.8571\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.3323 - acc: 0.8718 - val_loss: 0.3255 - val_acc: 0.8571\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.3276 - acc: 0.8718 - val_loss: 0.3210 - val_acc: 0.8571\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 232us/step - loss: 0.3232 - acc: 0.8718 - val_loss: 0.3168 - val_acc: 0.8571\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.3189 - acc: 0.8718 - val_loss: 0.3128 - val_acc: 0.8571\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.3148 - acc: 0.8718 - val_loss: 0.3088 - val_acc: 0.8571\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.3108 - acc: 0.8718 - val_loss: 0.3051 - val_acc: 0.8571\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.3071 - acc: 0.8718 - val_loss: 0.3015 - val_acc: 0.8571\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.3036 - acc: 0.8718 - val_loss: 0.2981 - val_acc: 0.8571\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.3003 - acc: 0.8718 - val_loss: 0.2949 - val_acc: 0.8571\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.2971 - acc: 0.8718 - val_loss: 0.2917 - val_acc: 0.8571\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.2940 - acc: 0.8718 - val_loss: 0.2887 - val_acc: 0.8571\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.2911 - acc: 0.8718 - val_loss: 0.2859 - val_acc: 0.8571\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.2883 - acc: 0.8718 - val_loss: 0.2831 - val_acc: 0.8571\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.2855 - acc: 0.8718 - val_loss: 0.2804 - val_acc: 0.8571\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 244us/step - loss: 0.2829 - acc: 0.8718 - val_loss: 0.2777 - val_acc: 0.8571\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.2804 - acc: 0.8718 - val_loss: 0.2752 - val_acc: 0.8571\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.2780 - acc: 0.8718 - val_loss: 0.2729 - val_acc: 0.8571\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.2756 - acc: 0.8718 - val_loss: 0.2705 - val_acc: 0.8571\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.2733 - acc: 0.8718 - val_loss: 0.2682 - val_acc: 0.8571\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.2712 - acc: 0.8718 - val_loss: 0.2661 - val_acc: 0.8571\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.2690 - acc: 0.8718 - val_loss: 0.2639 - val_acc: 0.8571\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.2670 - acc: 0.8718 - val_loss: 0.2619 - val_acc: 0.8571\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.2651 - acc: 0.8718 - val_loss: 0.2599 - val_acc: 0.8571\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.2632 - acc: 0.8718 - val_loss: 0.2580 - val_acc: 0.8571\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.2614 - acc: 0.8718 - val_loss: 0.2562 - val_acc: 0.8571\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 225us/step - loss: 0.2596 - acc: 0.8718 - val_loss: 0.2544 - val_acc: 0.8571\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 264us/step - loss: 0.2579 - acc: 0.8718 - val_loss: 0.2526 - val_acc: 0.8571\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.2563 - acc: 0.8718 - val_loss: 0.2509 - val_acc: 0.8571\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.2546 - acc: 0.8718 - val_loss: 0.2492 - val_acc: 0.8571\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.2531 - acc: 0.8718 - val_loss: 0.2476 - val_acc: 0.8571\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.2516 - acc: 0.8718 - val_loss: 0.2460 - val_acc: 0.8571\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 0.2501 - acc: 0.8718 - val_loss: 0.2445 - val_acc: 0.8571\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.2486 - acc: 0.8718 - val_loss: 0.2430 - val_acc: 0.8571\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.2472 - acc: 0.8718 - val_loss: 0.2415 - val_acc: 0.8571\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.2458 - acc: 0.8718 - val_loss: 0.2401 - val_acc: 0.8571\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.2445 - acc: 0.8718 - val_loss: 0.2386 - val_acc: 0.8571\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.2431 - acc: 0.8718 - val_loss: 0.2373 - val_acc: 0.8571\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.2419 - acc: 0.8718 - val_loss: 0.2359 - val_acc: 0.8571\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.2406 - acc: 0.8718 - val_loss: 0.2346 - val_acc: 0.8571\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 300us/step - loss: 0.2394 - acc: 0.8718 - val_loss: 0.2333 - val_acc: 0.8571\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.2382 - acc: 0.8718 - val_loss: 0.2320 - val_acc: 0.8571\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.2369 - acc: 0.8718 - val_loss: 0.2308 - val_acc: 0.8571\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.2358 - acc: 0.8803 - val_loss: 0.2296 - val_acc: 0.8571\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.2346 - acc: 0.8803 - val_loss: 0.2283 - val_acc: 0.8571\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.2335 - acc: 0.8803 - val_loss: 0.2272 - val_acc: 0.8571\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 0.2324 - acc: 0.8803 - val_loss: 0.2260 - val_acc: 0.8571\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.2313 - acc: 0.8803 - val_loss: 0.2249 - val_acc: 0.8571\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 0.2302 - acc: 0.8803 - val_loss: 0.2237 - val_acc: 0.8571\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 251us/step - loss: 0.2292 - acc: 0.8803 - val_loss: 0.2226 - val_acc: 0.8571\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.2282 - acc: 0.8803 - val_loss: 0.2215 - val_acc: 0.8571\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.2271 - acc: 0.9060 - val_loss: 0.2205 - val_acc: 0.8571\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.2261 - acc: 0.9060 - val_loss: 0.2194 - val_acc: 0.8571\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.2251 - acc: 0.9060 - val_loss: 0.2184 - val_acc: 0.8571\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.2242 - acc: 0.9060 - val_loss: 0.2174 - val_acc: 0.8571\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.2232 - acc: 0.9060 - val_loss: 0.2164 - val_acc: 0.8571\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.2223 - acc: 0.9060 - val_loss: 0.2154 - val_acc: 0.8571\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.2214 - acc: 0.9060 - val_loss: 0.2144 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.2205 - acc: 0.9060 - val_loss: 0.2135 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.2196 - acc: 0.9145 - val_loss: 0.2125 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.2187 - acc: 0.9145 - val_loss: 0.2115 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.2178 - acc: 0.9145 - val_loss: 0.2106 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.2169 - acc: 0.9231 - val_loss: 0.2097 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 0.2161 - acc: 0.9231 - val_loss: 0.2088 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.2152 - acc: 0.9316 - val_loss: 0.2078 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 226us/step - loss: 0.2143 - acc: 0.9487 - val_loss: 0.2069 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.2135 - acc: 0.9487 - val_loss: 0.2061 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.2127 - acc: 0.9487 - val_loss: 0.2052 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 0.8467 - acc: 0.8220 - val_loss: 0.8993 - val_acc: 0.8462\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8166 - acc: 0.8390 - val_loss: 0.8605 - val_acc: 0.8462\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7890 - acc: 0.8390 - val_loss: 0.8263 - val_acc: 0.8462\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7626 - acc: 0.8390 - val_loss: 0.7901 - val_acc: 0.8462\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7381 - acc: 0.8390 - val_loss: 0.7596 - val_acc: 0.8462\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7162 - acc: 0.8390 - val_loss: 0.7308 - val_acc: 0.8462\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6956 - acc: 0.8390 - val_loss: 0.7065 - val_acc: 0.8462\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6768 - acc: 0.8475 - val_loss: 0.6831 - val_acc: 0.8462\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6598 - acc: 0.8475 - val_loss: 0.6610 - val_acc: 0.8462\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6440 - acc: 0.8475 - val_loss: 0.6427 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6291 - acc: 0.8475 - val_loss: 0.6240 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6158 - acc: 0.8475 - val_loss: 0.6093 - val_acc: 0.8462\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6041 - acc: 0.8475 - val_loss: 0.5940 - val_acc: 0.8462\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5932 - acc: 0.8475 - val_loss: 0.5814 - val_acc: 0.8462\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.5824 - acc: 0.8475 - val_loss: 0.5697 - val_acc: 0.8462\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.5731 - acc: 0.8475 - val_loss: 0.5592 - val_acc: 0.8462\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.5651 - acc: 0.8475 - val_loss: 0.5491 - val_acc: 0.8462\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5571 - acc: 0.8475 - val_loss: 0.5394 - val_acc: 0.8462\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.5499 - acc: 0.8559 - val_loss: 0.5311 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.5432 - acc: 0.8559 - val_loss: 0.5231 - val_acc: 0.8462\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5369 - acc: 0.8559 - val_loss: 0.5153 - val_acc: 0.8462\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5311 - acc: 0.8559 - val_loss: 0.5086 - val_acc: 0.8462\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.5260 - acc: 0.8644 - val_loss: 0.5026 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.5204 - acc: 0.9322 - val_loss: 0.4962 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5158 - acc: 0.9576 - val_loss: 0.4906 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.5112 - acc: 0.9576 - val_loss: 0.4855 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.5069 - acc: 0.9576 - val_loss: 0.4808 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5027 - acc: 0.9661 - val_loss: 0.4761 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4986 - acc: 0.9661 - val_loss: 0.4716 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4947 - acc: 0.9746 - val_loss: 0.4672 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4909 - acc: 0.9746 - val_loss: 0.4632 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.4874 - acc: 0.9746 - val_loss: 0.4593 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4840 - acc: 0.9661 - val_loss: 0.4554 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4808 - acc: 0.9661 - val_loss: 0.4520 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4776 - acc: 0.9661 - val_loss: 0.4484 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4746 - acc: 0.9576 - val_loss: 0.4451 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4716 - acc: 0.9576 - val_loss: 0.4417 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4687 - acc: 0.9661 - val_loss: 0.4389 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.4657 - acc: 0.9576 - val_loss: 0.4355 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4630 - acc: 0.9492 - val_loss: 0.4325 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4602 - acc: 0.9576 - val_loss: 0.4295 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4575 - acc: 0.9576 - val_loss: 0.4268 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4550 - acc: 0.9576 - val_loss: 0.4239 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.4524 - acc: 0.9576 - val_loss: 0.4214 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.4497 - acc: 0.9576 - val_loss: 0.4185 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4473 - acc: 0.9576 - val_loss: 0.4158 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.4450 - acc: 0.9576 - val_loss: 0.4133 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4424 - acc: 0.9661 - val_loss: 0.4107 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4401 - acc: 0.9661 - val_loss: 0.4083 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4377 - acc: 0.9661 - val_loss: 0.4059 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4353 - acc: 0.9661 - val_loss: 0.4033 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4331 - acc: 0.9661 - val_loss: 0.4010 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4308 - acc: 0.9746 - val_loss: 0.3988 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4286 - acc: 0.9661 - val_loss: 0.3964 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.4264 - acc: 0.9746 - val_loss: 0.3941 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.4242 - acc: 0.9746 - val_loss: 0.3919 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4221 - acc: 0.9746 - val_loss: 0.3897 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4199 - acc: 0.9746 - val_loss: 0.3874 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4177 - acc: 0.9746 - val_loss: 0.3854 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4157 - acc: 0.9746 - val_loss: 0.3833 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.4136 - acc: 0.9746 - val_loss: 0.3812 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4116 - acc: 0.9746 - val_loss: 0.3792 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.4097 - acc: 0.9746 - val_loss: 0.3773 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4076 - acc: 0.9746 - val_loss: 0.3751 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4057 - acc: 0.9746 - val_loss: 0.3732 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4039 - acc: 0.9746 - val_loss: 0.3710 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4018 - acc: 0.9746 - val_loss: 0.3690 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3999 - acc: 0.9831 - val_loss: 0.3673 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3980 - acc: 0.9831 - val_loss: 0.3655 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3962 - acc: 0.9831 - val_loss: 0.3634 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3943 - acc: 0.9831 - val_loss: 0.3616 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3925 - acc: 0.9746 - val_loss: 0.3595 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3906 - acc: 0.9831 - val_loss: 0.3576 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3888 - acc: 0.9831 - val_loss: 0.3559 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.3870 - acc: 0.9831 - val_loss: 0.3540 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3851 - acc: 0.9831 - val_loss: 0.3523 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3834 - acc: 0.9831 - val_loss: 0.3504 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.3818 - acc: 0.9831 - val_loss: 0.3487 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3799 - acc: 0.9831 - val_loss: 0.3470 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3782 - acc: 0.9831 - val_loss: 0.3451 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.3766 - acc: 0.9831 - val_loss: 0.3434 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3748 - acc: 0.9831 - val_loss: 0.3415 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3731 - acc: 0.9831 - val_loss: 0.3399 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3715 - acc: 0.9915 - val_loss: 0.3382 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.3699 - acc: 0.9915 - val_loss: 0.3365 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3682 - acc: 0.9915 - val_loss: 0.3349 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3666 - acc: 0.9915 - val_loss: 0.3332 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.3651 - acc: 0.9915 - val_loss: 0.3315 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3637 - acc: 0.9915 - val_loss: 0.3300 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3619 - acc: 0.9915 - val_loss: 0.3281 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3604 - acc: 0.9915 - val_loss: 0.3266 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3590 - acc: 0.9915 - val_loss: 0.3249 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.3574 - acc: 0.9915 - val_loss: 0.3233 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3558 - acc: 0.9915 - val_loss: 0.3219 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.3543 - acc: 0.9915 - val_loss: 0.3203 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3528 - acc: 0.9915 - val_loss: 0.3187 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3514 - acc: 0.9915 - val_loss: 0.3172 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3499 - acc: 0.9915 - val_loss: 0.3158 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3484 - acc: 0.9915 - val_loss: 0.3141 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3469 - acc: 0.9915 - val_loss: 0.3126 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 2.6866 - acc: 0.1186 - val_loss: 2.2365 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 2.4708 - acc: 0.1186 - val_loss: 2.0652 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 2.2774 - acc: 0.1186 - val_loss: 1.9179 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 2.1065 - acc: 0.1186 - val_loss: 1.7846 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.9545 - acc: 0.1186 - val_loss: 1.6685 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.8189 - acc: 0.1186 - val_loss: 1.5617 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 1.6987 - acc: 0.1271 - val_loss: 1.4702 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.5927 - acc: 0.1271 - val_loss: 1.3864 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.4974 - acc: 0.1356 - val_loss: 1.3132 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.4116 - acc: 0.1356 - val_loss: 1.2476 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.3351 - acc: 0.1356 - val_loss: 1.1875 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.2646 - acc: 0.1356 - val_loss: 1.1320 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.2019 - acc: 0.1356 - val_loss: 1.0831 - val_acc: 0.1538\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1442 - acc: 0.1356 - val_loss: 1.0383 - val_acc: 0.1538\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0916 - acc: 0.1356 - val_loss: 0.9977 - val_acc: 0.1538\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0434 - acc: 0.1525 - val_loss: 0.9597 - val_acc: 0.2308\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9995 - acc: 0.1864 - val_loss: 0.9244 - val_acc: 0.2308\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9586 - acc: 0.2542 - val_loss: 0.8929 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9205 - acc: 0.4237 - val_loss: 0.8628 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8854 - acc: 0.5254 - val_loss: 0.8348 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8524 - acc: 0.5424 - val_loss: 0.8091 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8216 - acc: 0.5508 - val_loss: 0.7847 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7928 - acc: 0.5932 - val_loss: 0.7613 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7656 - acc: 0.6102 - val_loss: 0.7401 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7401 - acc: 0.6441 - val_loss: 0.7199 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7158 - acc: 0.7034 - val_loss: 0.7002 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.6927 - acc: 0.7881 - val_loss: 0.6818 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6711 - acc: 0.8729 - val_loss: 0.6642 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6504 - acc: 0.9237 - val_loss: 0.6476 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6307 - acc: 0.9492 - val_loss: 0.6317 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6120 - acc: 0.9576 - val_loss: 0.6165 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5940 - acc: 0.9661 - val_loss: 0.6019 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5770 - acc: 0.9831 - val_loss: 0.5880 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5605 - acc: 0.9915 - val_loss: 0.5747 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5450 - acc: 0.9915 - val_loss: 0.5618 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.5297 - acc: 0.9915 - val_loss: 0.5493 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5153 - acc: 0.9915 - val_loss: 0.5373 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5014 - acc: 1.0000 - val_loss: 0.5260 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4881 - acc: 1.0000 - val_loss: 0.5148 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4752 - acc: 1.0000 - val_loss: 0.5042 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.4627 - acc: 1.0000 - val_loss: 0.4938 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4507 - acc: 1.0000 - val_loss: 0.4839 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4392 - acc: 1.0000 - val_loss: 0.4741 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4281 - acc: 1.0000 - val_loss: 0.4648 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.4174 - acc: 1.0000 - val_loss: 0.4558 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.4070 - acc: 1.0000 - val_loss: 0.4469 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3970 - acc: 1.0000 - val_loss: 0.4385 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3872 - acc: 1.0000 - val_loss: 0.4302 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.3779 - acc: 1.0000 - val_loss: 0.4222 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.3689 - acc: 1.0000 - val_loss: 0.4147 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3602 - acc: 1.0000 - val_loss: 0.4071 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.3516 - acc: 1.0000 - val_loss: 0.3999 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3435 - acc: 1.0000 - val_loss: 0.3928 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3355 - acc: 1.0000 - val_loss: 0.3859 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3279 - acc: 1.0000 - val_loss: 0.3791 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.3204 - acc: 1.0000 - val_loss: 0.3724 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3132 - acc: 1.0000 - val_loss: 0.3659 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3063 - acc: 1.0000 - val_loss: 0.3597 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2995 - acc: 1.0000 - val_loss: 0.3537 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.2930 - acc: 1.0000 - val_loss: 0.3478 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.2867 - acc: 1.0000 - val_loss: 0.3421 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2806 - acc: 1.0000 - val_loss: 0.3365 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2747 - acc: 1.0000 - val_loss: 0.3312 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2689 - acc: 1.0000 - val_loss: 0.3259 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2633 - acc: 1.0000 - val_loss: 0.3207 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.2579 - acc: 1.0000 - val_loss: 0.3159 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.2527 - acc: 1.0000 - val_loss: 0.3111 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2476 - acc: 1.0000 - val_loss: 0.3063 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2426 - acc: 1.0000 - val_loss: 0.3018 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2379 - acc: 1.0000 - val_loss: 0.2974 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.2332 - acc: 1.0000 - val_loss: 0.2931 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2288 - acc: 1.0000 - val_loss: 0.2890 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2245 - acc: 1.0000 - val_loss: 0.2849 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2203 - acc: 1.0000 - val_loss: 0.2809 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2161 - acc: 1.0000 - val_loss: 0.2771 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.2121 - acc: 1.0000 - val_loss: 0.2733 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.2083 - acc: 1.0000 - val_loss: 0.2697 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.2046 - acc: 1.0000 - val_loss: 0.2662 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2009 - acc: 1.0000 - val_loss: 0.2627 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.1974 - acc: 1.0000 - val_loss: 0.2594 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1940 - acc: 1.0000 - val_loss: 0.2561 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1907 - acc: 1.0000 - val_loss: 0.2530 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1874 - acc: 1.0000 - val_loss: 0.2499 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1843 - acc: 1.0000 - val_loss: 0.2469 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1813 - acc: 1.0000 - val_loss: 0.2440 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1784 - acc: 1.0000 - val_loss: 0.2412 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1755 - acc: 1.0000 - val_loss: 0.2384 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1727 - acc: 1.0000 - val_loss: 0.2357 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1700 - acc: 1.0000 - val_loss: 0.2330 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1673 - acc: 1.0000 - val_loss: 0.2305 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1648 - acc: 1.0000 - val_loss: 0.2280 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1623 - acc: 1.0000 - val_loss: 0.2256 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1599 - acc: 1.0000 - val_loss: 0.2232 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.1575 - acc: 1.0000 - val_loss: 0.2209 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1552 - acc: 1.0000 - val_loss: 0.2186 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1530 - acc: 1.0000 - val_loss: 0.2164 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1508 - acc: 1.0000 - val_loss: 0.2143 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1486 - acc: 1.0000 - val_loss: 0.2122 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1466 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1445 - acc: 1.0000 - val_loss: 0.2081 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.0828 - acc: 0.3898 - val_loss: 1.0740 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0242 - acc: 0.4068 - val_loss: 1.0156 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9719 - acc: 0.4492 - val_loss: 0.9639 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9229 - acc: 0.5169 - val_loss: 0.9127 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8789 - acc: 0.5254 - val_loss: 0.8704 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8392 - acc: 0.5254 - val_loss: 0.8319 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8031 - acc: 0.5254 - val_loss: 0.7968 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7700 - acc: 0.6186 - val_loss: 0.7651 - val_acc: 0.6154\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7391 - acc: 0.7627 - val_loss: 0.7352 - val_acc: 0.8462\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.7107 - acc: 0.8559 - val_loss: 0.7075 - val_acc: 0.9231\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.6841 - acc: 0.9068 - val_loss: 0.6814 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6593 - acc: 0.9407 - val_loss: 0.6576 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6361 - acc: 0.9661 - val_loss: 0.6346 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.6137 - acc: 0.9746 - val_loss: 0.6135 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5927 - acc: 0.9831 - val_loss: 0.5935 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5726 - acc: 0.9831 - val_loss: 0.5745 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.5537 - acc: 0.9831 - val_loss: 0.5564 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.5354 - acc: 0.9831 - val_loss: 0.5392 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5179 - acc: 0.9831 - val_loss: 0.5229 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.5014 - acc: 0.9831 - val_loss: 0.5076 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.4853 - acc: 0.9831 - val_loss: 0.4932 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4700 - acc: 0.9831 - val_loss: 0.4795 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4554 - acc: 0.9831 - val_loss: 0.4664 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.4414 - acc: 0.9831 - val_loss: 0.4539 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.4281 - acc: 0.9831 - val_loss: 0.4419 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.4153 - acc: 0.9831 - val_loss: 0.4306 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4032 - acc: 0.9831 - val_loss: 0.4198 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.3916 - acc: 0.9831 - val_loss: 0.4096 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3804 - acc: 0.9831 - val_loss: 0.3998 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3698 - acc: 0.9915 - val_loss: 0.3905 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3595 - acc: 0.9915 - val_loss: 0.3817 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3496 - acc: 0.9915 - val_loss: 0.3732 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3402 - acc: 0.9915 - val_loss: 0.3651 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3312 - acc: 0.9915 - val_loss: 0.3572 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3226 - acc: 0.9915 - val_loss: 0.3497 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3143 - acc: 0.9915 - val_loss: 0.3425 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.3064 - acc: 0.9915 - val_loss: 0.3356 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2987 - acc: 0.9915 - val_loss: 0.3289 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2914 - acc: 0.9915 - val_loss: 0.3225 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2844 - acc: 0.9915 - val_loss: 0.3163 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.2777 - acc: 0.9915 - val_loss: 0.3104 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2713 - acc: 0.9915 - val_loss: 0.3046 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2651 - acc: 0.9915 - val_loss: 0.2993 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2592 - acc: 0.9915 - val_loss: 0.2939 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2535 - acc: 0.9915 - val_loss: 0.2889 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2480 - acc: 0.9915 - val_loss: 0.2840 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2428 - acc: 0.9915 - val_loss: 0.2792 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2378 - acc: 0.9915 - val_loss: 0.2747 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.2329 - acc: 0.9915 - val_loss: 0.2703 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2283 - acc: 0.9915 - val_loss: 0.2660 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.2238 - acc: 0.9915 - val_loss: 0.2619 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2195 - acc: 0.9915 - val_loss: 0.2579 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2153 - acc: 0.9915 - val_loss: 0.2541 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.2113 - acc: 0.9915 - val_loss: 0.2504 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2075 - acc: 0.9915 - val_loss: 0.2468 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.2037 - acc: 0.9915 - val_loss: 0.2433 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2002 - acc: 0.9915 - val_loss: 0.2399 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1967 - acc: 0.9915 - val_loss: 0.2366 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1933 - acc: 0.9915 - val_loss: 0.2334 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1901 - acc: 0.9915 - val_loss: 0.2303 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1869 - acc: 0.9915 - val_loss: 0.2273 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1838 - acc: 0.9915 - val_loss: 0.2244 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.1809 - acc: 0.9915 - val_loss: 0.2215 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1780 - acc: 0.9915 - val_loss: 0.2188 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1752 - acc: 0.9915 - val_loss: 0.2161 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1725 - acc: 0.9915 - val_loss: 0.2135 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1699 - acc: 0.9915 - val_loss: 0.2109 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.1673 - acc: 0.9915 - val_loss: 0.2084 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1648 - acc: 0.9915 - val_loss: 0.2060 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1624 - acc: 0.9915 - val_loss: 0.2036 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.1601 - acc: 0.9915 - val_loss: 0.2013 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1578 - acc: 0.9915 - val_loss: 0.1991 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.1556 - acc: 0.9915 - val_loss: 0.1969 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1535 - acc: 0.9915 - val_loss: 0.1948 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.1514 - acc: 0.9915 - val_loss: 0.1927 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.1494 - acc: 0.9915 - val_loss: 0.1906 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1474 - acc: 0.9915 - val_loss: 0.1886 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1455 - acc: 0.9915 - val_loss: 0.1866 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1436 - acc: 0.9915 - val_loss: 0.1847 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1418 - acc: 0.9915 - val_loss: 0.1829 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1400 - acc: 0.9915 - val_loss: 0.1810 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.1383 - acc: 0.9915 - val_loss: 0.1792 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.1366 - acc: 0.9915 - val_loss: 0.1775 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1350 - acc: 0.9915 - val_loss: 0.1758 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1334 - acc: 0.9915 - val_loss: 0.1741 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.1318 - acc: 0.9915 - val_loss: 0.1725 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1303 - acc: 0.9915 - val_loss: 0.1709 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1288 - acc: 0.9915 - val_loss: 0.1693 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1273 - acc: 0.9915 - val_loss: 0.1678 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1259 - acc: 0.9915 - val_loss: 0.1663 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.1246 - acc: 0.9915 - val_loss: 0.1648 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1232 - acc: 0.9915 - val_loss: 0.1633 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.1219 - acc: 0.9915 - val_loss: 0.1620 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1206 - acc: 0.9915 - val_loss: 0.1605 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1193 - acc: 0.9915 - val_loss: 0.1592 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1181 - acc: 0.9915 - val_loss: 0.1578 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1169 - acc: 0.9915 - val_loss: 0.1565 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1157 - acc: 0.9915 - val_loss: 0.1552 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.1145 - acc: 0.9915 - val_loss: 0.1540 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1134 - acc: 0.9915 - val_loss: 0.1527 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 3.1299 - acc: 0.0339 - val_loss: 2.9084 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 2.8007 - acc: 0.0424 - val_loss: 2.6042 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 2.4972 - acc: 0.0508 - val_loss: 2.3288 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 2.2164 - acc: 0.0508 - val_loss: 2.0686 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.9626 - acc: 0.0763 - val_loss: 1.8361 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.7350 - acc: 0.0763 - val_loss: 1.6328 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.5365 - acc: 0.0763 - val_loss: 1.4527 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.3660 - acc: 0.0678 - val_loss: 1.3000 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.2249 - acc: 0.1441 - val_loss: 1.1798 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1127 - acc: 0.4237 - val_loss: 1.0832 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0227 - acc: 0.4407 - val_loss: 1.0071 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9512 - acc: 0.4322 - val_loss: 0.9446 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8935 - acc: 0.4407 - val_loss: 0.8943 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8450 - acc: 0.4492 - val_loss: 0.8506 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8039 - acc: 0.4831 - val_loss: 0.8130 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7682 - acc: 0.5169 - val_loss: 0.7797 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.7366 - acc: 0.5678 - val_loss: 0.7502 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7083 - acc: 0.5847 - val_loss: 0.7232 - val_acc: 0.6923\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.6826 - acc: 0.6441 - val_loss: 0.6991 - val_acc: 0.7692\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 284us/step - loss: 0.6589 - acc: 0.7542 - val_loss: 0.6763 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 304us/step - loss: 0.6370 - acc: 0.8729 - val_loss: 0.6556 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 264us/step - loss: 0.6166 - acc: 0.9068 - val_loss: 0.6363 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.5974 - acc: 0.9492 - val_loss: 0.6183 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.5794 - acc: 0.9492 - val_loss: 0.6011 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.5624 - acc: 0.9492 - val_loss: 0.5847 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5463 - acc: 0.9492 - val_loss: 0.5692 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.5310 - acc: 0.9492 - val_loss: 0.5542 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.5165 - acc: 0.9492 - val_loss: 0.5401 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 278us/step - loss: 0.5026 - acc: 0.9492 - val_loss: 0.5267 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.4892 - acc: 0.9492 - val_loss: 0.5137 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4764 - acc: 0.9576 - val_loss: 0.5012 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4640 - acc: 0.9576 - val_loss: 0.4889 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4522 - acc: 0.9576 - val_loss: 0.4772 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4407 - acc: 0.9661 - val_loss: 0.4660 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4298 - acc: 0.9661 - val_loss: 0.4552 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4192 - acc: 0.9746 - val_loss: 0.4447 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4088 - acc: 0.9746 - val_loss: 0.4344 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3989 - acc: 0.9746 - val_loss: 0.4246 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3893 - acc: 0.9746 - val_loss: 0.4149 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3801 - acc: 0.9746 - val_loss: 0.4055 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3712 - acc: 0.9746 - val_loss: 0.3967 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.3627 - acc: 0.9746 - val_loss: 0.3878 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.3544 - acc: 0.9831 - val_loss: 0.3794 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3464 - acc: 0.9831 - val_loss: 0.3710 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.3387 - acc: 0.9831 - val_loss: 0.3630 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3312 - acc: 0.9831 - val_loss: 0.3553 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3239 - acc: 0.9831 - val_loss: 0.3478 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3170 - acc: 0.9831 - val_loss: 0.3404 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.3101 - acc: 0.9915 - val_loss: 0.3333 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3036 - acc: 0.9915 - val_loss: 0.3264 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2973 - acc: 0.9915 - val_loss: 0.3197 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2912 - acc: 0.9915 - val_loss: 0.3132 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2853 - acc: 0.9915 - val_loss: 0.3071 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2797 - acc: 0.9915 - val_loss: 0.3010 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2739 - acc: 0.9915 - val_loss: 0.2949 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2685 - acc: 0.9915 - val_loss: 0.2890 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2634 - acc: 0.9915 - val_loss: 0.2833 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2584 - acc: 0.9915 - val_loss: 0.2779 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.2534 - acc: 0.9915 - val_loss: 0.2725 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.2486 - acc: 0.9915 - val_loss: 0.2674 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2440 - acc: 0.9915 - val_loss: 0.2622 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2396 - acc: 0.9915 - val_loss: 0.2573 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.2352 - acc: 0.9915 - val_loss: 0.2526 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2310 - acc: 0.9915 - val_loss: 0.2479 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2269 - acc: 0.9915 - val_loss: 0.2434 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2229 - acc: 0.9915 - val_loss: 0.2389 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2191 - acc: 0.9915 - val_loss: 0.2347 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.2154 - acc: 0.9915 - val_loss: 0.2304 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.2117 - acc: 0.9915 - val_loss: 0.2263 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2082 - acc: 0.9915 - val_loss: 0.2224 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2048 - acc: 0.9915 - val_loss: 0.2186 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2015 - acc: 0.9915 - val_loss: 0.2148 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1982 - acc: 0.9915 - val_loss: 0.2110 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1951 - acc: 0.9915 - val_loss: 0.2075 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1921 - acc: 0.9915 - val_loss: 0.2039 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1891 - acc: 0.9915 - val_loss: 0.2007 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1862 - acc: 0.9915 - val_loss: 0.1972 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1834 - acc: 0.9915 - val_loss: 0.1941 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1806 - acc: 0.9915 - val_loss: 0.1908 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1780 - acc: 0.9915 - val_loss: 0.1878 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.1754 - acc: 0.9915 - val_loss: 0.1848 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.1729 - acc: 0.9915 - val_loss: 0.1820 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1704 - acc: 0.9915 - val_loss: 0.1791 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1680 - acc: 0.9915 - val_loss: 0.1763 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1657 - acc: 0.9915 - val_loss: 0.1735 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1634 - acc: 0.9915 - val_loss: 0.1710 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1612 - acc: 0.9915 - val_loss: 0.1683 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1591 - acc: 0.9915 - val_loss: 0.1659 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1570 - acc: 0.9915 - val_loss: 0.1635 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1549 - acc: 0.9915 - val_loss: 0.1611 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1530 - acc: 0.9915 - val_loss: 0.1587 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1510 - acc: 0.9915 - val_loss: 0.1564 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1491 - acc: 0.9915 - val_loss: 0.1543 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1473 - acc: 0.9915 - val_loss: 0.1521 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1455 - acc: 0.9915 - val_loss: 0.1499 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1438 - acc: 0.9915 - val_loss: 0.1479 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1421 - acc: 0.9915 - val_loss: 0.1459 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1404 - acc: 0.9915 - val_loss: 0.1439 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1388 - acc: 0.9915 - val_loss: 0.1420 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1372 - acc: 0.9915 - val_loss: 0.1401 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 1.4757 - acc: 0.3898 - val_loss: 1.2234 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.3629 - acc: 0.3898 - val_loss: 1.1521 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.2618 - acc: 0.4068 - val_loss: 1.0945 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 1.1752 - acc: 0.4407 - val_loss: 1.0427 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1009 - acc: 0.4407 - val_loss: 1.0018 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0365 - acc: 0.4407 - val_loss: 0.9664 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9806 - acc: 0.4407 - val_loss: 0.9359 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9306 - acc: 0.4661 - val_loss: 0.9081 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8857 - acc: 0.4831 - val_loss: 0.8828 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8452 - acc: 0.4915 - val_loss: 0.8609 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8094 - acc: 0.4915 - val_loss: 0.8418 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7778 - acc: 0.5085 - val_loss: 0.8232 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7492 - acc: 0.5169 - val_loss: 0.8069 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.7227 - acc: 0.5339 - val_loss: 0.7902 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6986 - acc: 0.6017 - val_loss: 0.7754 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6766 - acc: 0.6441 - val_loss: 0.7608 - val_acc: 0.6923\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6563 - acc: 0.7203 - val_loss: 0.7471 - val_acc: 0.6923\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6377 - acc: 0.7881 - val_loss: 0.7347 - val_acc: 0.6923\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6204 - acc: 0.7966 - val_loss: 0.7228 - val_acc: 0.6923\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6044 - acc: 0.8051 - val_loss: 0.7112 - val_acc: 0.6923\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5894 - acc: 0.8136 - val_loss: 0.7001 - val_acc: 0.6923\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5755 - acc: 0.8220 - val_loss: 0.6898 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5622 - acc: 0.8475 - val_loss: 0.6790 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.5498 - acc: 0.8475 - val_loss: 0.6680 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5380 - acc: 0.8559 - val_loss: 0.6573 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5269 - acc: 0.8559 - val_loss: 0.6469 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.5166 - acc: 0.8644 - val_loss: 0.6373 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.5067 - acc: 0.8729 - val_loss: 0.6279 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4974 - acc: 0.8729 - val_loss: 0.6190 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4883 - acc: 0.8729 - val_loss: 0.6096 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4794 - acc: 0.8729 - val_loss: 0.6010 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4709 - acc: 0.8729 - val_loss: 0.5929 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4627 - acc: 0.8729 - val_loss: 0.5844 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.4548 - acc: 0.8729 - val_loss: 0.5765 - val_acc: 0.7692\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4474 - acc: 0.8729 - val_loss: 0.5687 - val_acc: 0.7692\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.4404 - acc: 0.8729 - val_loss: 0.5613 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4337 - acc: 0.8729 - val_loss: 0.5543 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4273 - acc: 0.8729 - val_loss: 0.5476 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4212 - acc: 0.8729 - val_loss: 0.5410 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4152 - acc: 0.8729 - val_loss: 0.5344 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4096 - acc: 0.8729 - val_loss: 0.5283 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4043 - acc: 0.8729 - val_loss: 0.5221 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3991 - acc: 0.8729 - val_loss: 0.5164 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3943 - acc: 0.8729 - val_loss: 0.5108 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.3896 - acc: 0.8729 - val_loss: 0.5054 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3851 - acc: 0.8729 - val_loss: 0.5002 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.3809 - acc: 0.8729 - val_loss: 0.4952 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3767 - acc: 0.8729 - val_loss: 0.4901 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.3727 - acc: 0.8729 - val_loss: 0.4853 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 280us/step - loss: 0.3689 - acc: 0.8729 - val_loss: 0.4808 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.3653 - acc: 0.8729 - val_loss: 0.4762 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3618 - acc: 0.8729 - val_loss: 0.4719 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3584 - acc: 0.8729 - val_loss: 0.4677 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.3552 - acc: 0.8729 - val_loss: 0.4636 - val_acc: 0.8462\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.3521 - acc: 0.8729 - val_loss: 0.4596 - val_acc: 0.8462\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3489 - acc: 0.8729 - val_loss: 0.4557 - val_acc: 0.8462\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3459 - acc: 0.8729 - val_loss: 0.4519 - val_acc: 0.8462\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.3429 - acc: 0.8729 - val_loss: 0.4482 - val_acc: 0.8462\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3400 - acc: 0.8729 - val_loss: 0.4446 - val_acc: 0.8462\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3372 - acc: 0.8729 - val_loss: 0.4411 - val_acc: 0.8462\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3343 - acc: 0.8729 - val_loss: 0.4374 - val_acc: 0.8462\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3314 - acc: 0.8729 - val_loss: 0.4340 - val_acc: 0.8462\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.3286 - acc: 0.8729 - val_loss: 0.4305 - val_acc: 0.8462\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3258 - acc: 0.8729 - val_loss: 0.4272 - val_acc: 0.8462\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3231 - acc: 0.8729 - val_loss: 0.4239 - val_acc: 0.8462\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3205 - acc: 0.8729 - val_loss: 0.4208 - val_acc: 0.8462\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3180 - acc: 0.8729 - val_loss: 0.4178 - val_acc: 0.8462\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3155 - acc: 0.8729 - val_loss: 0.4148 - val_acc: 0.8462\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3131 - acc: 0.8729 - val_loss: 0.4118 - val_acc: 0.8462\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3107 - acc: 0.8729 - val_loss: 0.4090 - val_acc: 0.8462\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.3085 - acc: 0.8729 - val_loss: 0.4061 - val_acc: 0.8462\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3062 - acc: 0.8729 - val_loss: 0.4033 - val_acc: 0.8462\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3040 - acc: 0.8729 - val_loss: 0.4006 - val_acc: 0.8462\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.3019 - acc: 0.8729 - val_loss: 0.3980 - val_acc: 0.8462\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2998 - acc: 0.8729 - val_loss: 0.3954 - val_acc: 0.8462\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2978 - acc: 0.8729 - val_loss: 0.3927 - val_acc: 0.8462\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.2958 - acc: 0.8729 - val_loss: 0.3903 - val_acc: 0.8462\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2939 - acc: 0.8729 - val_loss: 0.3878 - val_acc: 0.8462\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2920 - acc: 0.8729 - val_loss: 0.3854 - val_acc: 0.8462\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2902 - acc: 0.8729 - val_loss: 0.3831 - val_acc: 0.8462\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2885 - acc: 0.8729 - val_loss: 0.3808 - val_acc: 0.8462\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2867 - acc: 0.8729 - val_loss: 0.3786 - val_acc: 0.8462\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2850 - acc: 0.8729 - val_loss: 0.3764 - val_acc: 0.8462\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2834 - acc: 0.8729 - val_loss: 0.3743 - val_acc: 0.8462\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2818 - acc: 0.8729 - val_loss: 0.3722 - val_acc: 0.8462\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2802 - acc: 0.8729 - val_loss: 0.3701 - val_acc: 0.8462\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.2787 - acc: 0.8729 - val_loss: 0.3681 - val_acc: 0.8462\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2772 - acc: 0.8729 - val_loss: 0.3661 - val_acc: 0.8462\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2757 - acc: 0.8729 - val_loss: 0.3642 - val_acc: 0.8462\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2743 - acc: 0.8729 - val_loss: 0.3623 - val_acc: 0.8462\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2728 - acc: 0.8729 - val_loss: 0.3604 - val_acc: 0.8462\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2715 - acc: 0.8729 - val_loss: 0.3586 - val_acc: 0.8462\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.2701 - acc: 0.8729 - val_loss: 0.3568 - val_acc: 0.8462\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2688 - acc: 0.8729 - val_loss: 0.3551 - val_acc: 0.8462\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2675 - acc: 0.8729 - val_loss: 0.3533 - val_acc: 0.8462\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.2662 - acc: 0.8729 - val_loss: 0.3516 - val_acc: 0.8462\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.2650 - acc: 0.8729 - val_loss: 0.3499 - val_acc: 0.8462\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.2638 - acc: 0.8729 - val_loss: 0.3483 - val_acc: 0.8462\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.2626 - acc: 0.8729 - val_loss: 0.3467 - val_acc: 0.8462\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.2614 - acc: 0.8729 - val_loss: 0.3451 - val_acc: 0.8462\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 1.8890 - acc: 0.0932 - val_loss: 1.9249 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.7576 - acc: 0.1017 - val_loss: 1.7985 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.6400 - acc: 0.1017 - val_loss: 1.6852 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.5387 - acc: 0.1017 - val_loss: 1.5847 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.4502 - acc: 0.1102 - val_loss: 1.5017 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.3733 - acc: 0.1186 - val_loss: 1.4255 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.3042 - acc: 0.1356 - val_loss: 1.3548 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.2410 - acc: 0.1441 - val_loss: 1.2913 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1839 - acc: 0.1525 - val_loss: 1.2336 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1318 - acc: 0.1780 - val_loss: 1.1794 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0846 - acc: 0.1949 - val_loss: 1.1338 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0417 - acc: 0.2712 - val_loss: 1.0921 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0028 - acc: 0.3644 - val_loss: 1.0537 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9672 - acc: 0.4237 - val_loss: 1.0185 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9339 - acc: 0.4492 - val_loss: 0.9847 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9031 - acc: 0.4746 - val_loss: 0.9534 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.8751 - acc: 0.5000 - val_loss: 0.9245 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8489 - acc: 0.5424 - val_loss: 0.8982 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.8248 - acc: 0.5593 - val_loss: 0.8727 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8023 - acc: 0.5932 - val_loss: 0.8496 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7809 - acc: 0.6102 - val_loss: 0.8277 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7614 - acc: 0.6186 - val_loss: 0.8079 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7432 - acc: 0.6271 - val_loss: 0.7886 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7260 - acc: 0.6441 - val_loss: 0.7710 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7100 - acc: 0.6441 - val_loss: 0.7544 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.6948 - acc: 0.6441 - val_loss: 0.7386 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6803 - acc: 0.6610 - val_loss: 0.7234 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6668 - acc: 0.6864 - val_loss: 0.7087 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6540 - acc: 0.6864 - val_loss: 0.6953 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6417 - acc: 0.7034 - val_loss: 0.6823 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6301 - acc: 0.7034 - val_loss: 0.6699 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6189 - acc: 0.7034 - val_loss: 0.6578 - val_acc: 0.7692\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6081 - acc: 0.7203 - val_loss: 0.6461 - val_acc: 0.7692\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5978 - acc: 0.7288 - val_loss: 0.6345 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 292us/step - loss: 0.5878 - acc: 0.7712 - val_loss: 0.6235 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5783 - acc: 0.8051 - val_loss: 0.6129 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5689 - acc: 0.8390 - val_loss: 0.6026 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5599 - acc: 0.8644 - val_loss: 0.5924 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.5510 - acc: 0.9068 - val_loss: 0.5828 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5425 - acc: 0.9153 - val_loss: 0.5734 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5342 - acc: 0.9576 - val_loss: 0.5641 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.5261 - acc: 0.9576 - val_loss: 0.5553 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5181 - acc: 0.9576 - val_loss: 0.5466 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.5105 - acc: 0.9576 - val_loss: 0.5381 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5030 - acc: 0.9576 - val_loss: 0.5300 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4957 - acc: 0.9576 - val_loss: 0.5218 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4885 - acc: 0.9576 - val_loss: 0.5139 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4814 - acc: 0.9661 - val_loss: 0.5062 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.4744 - acc: 0.9661 - val_loss: 0.4985 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4676 - acc: 0.9661 - val_loss: 0.4909 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4608 - acc: 0.9661 - val_loss: 0.4836 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.4543 - acc: 0.9661 - val_loss: 0.4762 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4477 - acc: 0.9661 - val_loss: 0.4691 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4412 - acc: 0.9661 - val_loss: 0.4622 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4349 - acc: 0.9661 - val_loss: 0.4554 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4286 - acc: 0.9661 - val_loss: 0.4485 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4224 - acc: 0.9746 - val_loss: 0.4418 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.4162 - acc: 0.9746 - val_loss: 0.4351 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4101 - acc: 0.9746 - val_loss: 0.4286 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4040 - acc: 0.9746 - val_loss: 0.4222 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3980 - acc: 0.9746 - val_loss: 0.4157 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3921 - acc: 0.9746 - val_loss: 0.4093 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3863 - acc: 0.9831 - val_loss: 0.4031 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3805 - acc: 0.9831 - val_loss: 0.3969 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3748 - acc: 0.9831 - val_loss: 0.3909 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3691 - acc: 0.9831 - val_loss: 0.3850 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3635 - acc: 0.9831 - val_loss: 0.3790 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.3579 - acc: 0.9831 - val_loss: 0.3732 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3524 - acc: 0.9831 - val_loss: 0.3675 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3469 - acc: 0.9831 - val_loss: 0.3618 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3415 - acc: 0.9831 - val_loss: 0.3559 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3361 - acc: 0.9831 - val_loss: 0.3503 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.3308 - acc: 0.9831 - val_loss: 0.3448 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3255 - acc: 0.9831 - val_loss: 0.3395 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.3203 - acc: 0.9831 - val_loss: 0.3342 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3151 - acc: 0.9831 - val_loss: 0.3286 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3100 - acc: 0.9831 - val_loss: 0.3235 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.3049 - acc: 0.9831 - val_loss: 0.3183 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.2999 - acc: 0.9831 - val_loss: 0.3132 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.2950 - acc: 0.9831 - val_loss: 0.3081 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.2901 - acc: 0.9831 - val_loss: 0.3031 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.2852 - acc: 0.9831 - val_loss: 0.2982 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.2805 - acc: 0.9831 - val_loss: 0.2934 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.2758 - acc: 0.9831 - val_loss: 0.2887 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2712 - acc: 0.9831 - val_loss: 0.2839 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2667 - acc: 0.9831 - val_loss: 0.2794 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.2623 - acc: 0.9831 - val_loss: 0.2749 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2578 - acc: 0.9915 - val_loss: 0.2705 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2535 - acc: 0.9915 - val_loss: 0.2661 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.2492 - acc: 0.9915 - val_loss: 0.2618 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2450 - acc: 0.9915 - val_loss: 0.2576 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2408 - acc: 0.9915 - val_loss: 0.2534 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2367 - acc: 0.9915 - val_loss: 0.2494 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.2327 - acc: 0.9915 - val_loss: 0.2454 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2288 - acc: 0.9915 - val_loss: 0.2415 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2249 - acc: 1.0000 - val_loss: 0.2377 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.2211 - acc: 1.0000 - val_loss: 0.2341 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2174 - acc: 1.0000 - val_loss: 0.2305 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.2138 - acc: 1.0000 - val_loss: 0.2269 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2102 - acc: 1.0000 - val_loss: 0.2234 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 3.2484 - acc: 0.0085 - val_loss: 3.9549 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 3.0018 - acc: 0.0169 - val_loss: 3.6467 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 2.7847 - acc: 0.0169 - val_loss: 3.3740 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 2.5900 - acc: 0.0169 - val_loss: 3.1508 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.4187 - acc: 0.0169 - val_loss: 2.9314 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 2.2647 - acc: 0.0254 - val_loss: 2.7445 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 2.1278 - acc: 0.0254 - val_loss: 2.5749 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.0061 - acc: 0.0254 - val_loss: 2.4234 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.8949 - acc: 0.0169 - val_loss: 2.2844 - val_acc: 0.0000e+00\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.7915 - acc: 0.0169 - val_loss: 2.1409 - val_acc: 0.0000e+00\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.6927 - acc: 0.0169 - val_loss: 2.0192 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.6033 - acc: 0.0169 - val_loss: 1.9042 - val_acc: 0.0000e+00\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.5228 - acc: 0.0169 - val_loss: 1.8004 - val_acc: 0.0000e+00\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.4495 - acc: 0.0169 - val_loss: 1.7089 - val_acc: 0.0000e+00\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.3840 - acc: 0.0169 - val_loss: 1.6224 - val_acc: 0.0000e+00\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.3250 - acc: 0.0169 - val_loss: 1.5469 - val_acc: 0.0000e+00\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.2721 - acc: 0.0424 - val_loss: 1.4773 - val_acc: 0.0000e+00\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.2246 - acc: 0.0932 - val_loss: 1.4151 - val_acc: 0.0769\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1820 - acc: 0.1271 - val_loss: 1.3573 - val_acc: 0.1538\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.1433 - acc: 0.1356 - val_loss: 1.3050 - val_acc: 0.1538\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1082 - acc: 0.1610 - val_loss: 1.2569 - val_acc: 0.1538\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0754 - acc: 0.1780 - val_loss: 1.2115 - val_acc: 0.1538\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.0452 - acc: 0.1949 - val_loss: 1.1693 - val_acc: 0.1538\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0174 - acc: 0.2288 - val_loss: 1.1285 - val_acc: 0.1538\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9912 - acc: 0.2966 - val_loss: 1.0916 - val_acc: 0.2308\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9667 - acc: 0.3559 - val_loss: 1.0566 - val_acc: 0.2308\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9438 - acc: 0.3814 - val_loss: 1.0225 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9218 - acc: 0.4153 - val_loss: 0.9920 - val_acc: 0.3077\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9011 - acc: 0.4237 - val_loss: 0.9634 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8814 - acc: 0.4661 - val_loss: 0.9352 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.8626 - acc: 0.4915 - val_loss: 0.9091 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.8445 - acc: 0.5085 - val_loss: 0.8836 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8274 - acc: 0.5254 - val_loss: 0.8600 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8109 - acc: 0.5254 - val_loss: 0.8374 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7952 - acc: 0.5254 - val_loss: 0.8162 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7801 - acc: 0.5254 - val_loss: 0.7960 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7655 - acc: 0.5254 - val_loss: 0.7770 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7516 - acc: 0.5254 - val_loss: 0.7588 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7380 - acc: 0.5763 - val_loss: 0.7415 - val_acc: 0.6154\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7250 - acc: 0.5932 - val_loss: 0.7251 - val_acc: 0.6154\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7125 - acc: 0.5932 - val_loss: 0.7097 - val_acc: 0.6154\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7003 - acc: 0.6017 - val_loss: 0.6949 - val_acc: 0.6154\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6886 - acc: 0.6186 - val_loss: 0.6812 - val_acc: 0.6154\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6773 - acc: 0.6271 - val_loss: 0.6678 - val_acc: 0.6154\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6664 - acc: 0.6525 - val_loss: 0.6550 - val_acc: 0.6154\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.6558 - acc: 0.6780 - val_loss: 0.6429 - val_acc: 0.6154\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6456 - acc: 0.6949 - val_loss: 0.6312 - val_acc: 0.6154\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6357 - acc: 0.7034 - val_loss: 0.6199 - val_acc: 0.6154\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6260 - acc: 0.7288 - val_loss: 0.6090 - val_acc: 0.6154\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6166 - acc: 0.8051 - val_loss: 0.5986 - val_acc: 0.7692\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6074 - acc: 0.8475 - val_loss: 0.5884 - val_acc: 0.7692\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5986 - acc: 0.9237 - val_loss: 0.5786 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5898 - acc: 0.9661 - val_loss: 0.5690 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.5814 - acc: 0.9831 - val_loss: 0.5599 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.5731 - acc: 0.9831 - val_loss: 0.5509 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.5650 - acc: 0.9915 - val_loss: 0.5422 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.5570 - acc: 0.9915 - val_loss: 0.5337 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5492 - acc: 0.9915 - val_loss: 0.5255 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5417 - acc: 0.9915 - val_loss: 0.5175 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.5342 - acc: 0.9915 - val_loss: 0.5095 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5268 - acc: 0.9915 - val_loss: 0.5018 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5196 - acc: 0.9915 - val_loss: 0.4941 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5125 - acc: 0.9915 - val_loss: 0.4867 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5055 - acc: 0.9915 - val_loss: 0.4792 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4986 - acc: 0.9915 - val_loss: 0.4720 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4918 - acc: 0.9915 - val_loss: 0.4647 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4850 - acc: 0.9915 - val_loss: 0.4576 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.4785 - acc: 0.9915 - val_loss: 0.4507 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4719 - acc: 0.9915 - val_loss: 0.4437 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4654 - acc: 0.9915 - val_loss: 0.4368 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4591 - acc: 1.0000 - val_loss: 0.4301 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4528 - acc: 1.0000 - val_loss: 0.4233 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4465 - acc: 1.0000 - val_loss: 0.4168 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4404 - acc: 1.0000 - val_loss: 0.4102 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4342 - acc: 1.0000 - val_loss: 0.4037 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4282 - acc: 1.0000 - val_loss: 0.3972 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4222 - acc: 1.0000 - val_loss: 0.3907 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.4162 - acc: 1.0000 - val_loss: 0.3844 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4103 - acc: 1.0000 - val_loss: 0.3781 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.4044 - acc: 1.0000 - val_loss: 0.3717 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3985 - acc: 1.0000 - val_loss: 0.3657 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3928 - acc: 1.0000 - val_loss: 0.3594 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3870 - acc: 1.0000 - val_loss: 0.3534 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3814 - acc: 1.0000 - val_loss: 0.3474 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3758 - acc: 1.0000 - val_loss: 0.3414 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3702 - acc: 1.0000 - val_loss: 0.3355 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3647 - acc: 1.0000 - val_loss: 0.3296 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 301us/step - loss: 0.3592 - acc: 1.0000 - val_loss: 0.3239 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.3537 - acc: 1.0000 - val_loss: 0.3181 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.3483 - acc: 1.0000 - val_loss: 0.3123 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3430 - acc: 1.0000 - val_loss: 0.3066 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3376 - acc: 1.0000 - val_loss: 0.3011 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3324 - acc: 1.0000 - val_loss: 0.2957 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3272 - acc: 1.0000 - val_loss: 0.2902 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3221 - acc: 1.0000 - val_loss: 0.2847 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3170 - acc: 1.0000 - val_loss: 0.2795 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3120 - acc: 1.0000 - val_loss: 0.2743 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3071 - acc: 1.0000 - val_loss: 0.2692 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3022 - acc: 1.0000 - val_loss: 0.2642 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2973 - acc: 1.0000 - val_loss: 0.2590 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 1.7642 - acc: 0.2119 - val_loss: 1.7316 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.6493 - acc: 0.2712 - val_loss: 1.6188 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.5389 - acc: 0.2881 - val_loss: 1.5123 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.4381 - acc: 0.3051 - val_loss: 1.4147 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.3471 - acc: 0.3305 - val_loss: 1.3284 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.2691 - acc: 0.3559 - val_loss: 1.2534 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.2005 - acc: 0.3559 - val_loss: 1.1863 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.1412 - acc: 0.3644 - val_loss: 1.1346 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0933 - acc: 0.3898 - val_loss: 1.0921 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0544 - acc: 0.4322 - val_loss: 1.0585 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0230 - acc: 0.5254 - val_loss: 1.0302 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9983 - acc: 0.5678 - val_loss: 1.0102 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9793 - acc: 0.6017 - val_loss: 0.9929 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9651 - acc: 0.6186 - val_loss: 0.9814 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9538 - acc: 0.6356 - val_loss: 0.9711 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9435 - acc: 0.6695 - val_loss: 0.9614 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9343 - acc: 0.6695 - val_loss: 0.9533 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9260 - acc: 0.6780 - val_loss: 0.9460 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9184 - acc: 0.6949 - val_loss: 0.9392 - val_acc: 0.6923\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.9111 - acc: 0.7288 - val_loss: 0.9327 - val_acc: 0.6923\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9045 - acc: 0.7542 - val_loss: 0.9262 - val_acc: 0.7692\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8983 - acc: 0.7627 - val_loss: 0.9198 - val_acc: 0.7692\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.8923 - acc: 0.7627 - val_loss: 0.9137 - val_acc: 0.7692\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8867 - acc: 0.7627 - val_loss: 0.9079 - val_acc: 0.7692\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8812 - acc: 0.7881 - val_loss: 0.9023 - val_acc: 0.7692\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.8759 - acc: 0.7881 - val_loss: 0.8967 - val_acc: 0.7692\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.8708 - acc: 0.7881 - val_loss: 0.8914 - val_acc: 0.7692\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.8656 - acc: 0.7966 - val_loss: 0.8860 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8603 - acc: 0.8220 - val_loss: 0.8805 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8550 - acc: 0.8220 - val_loss: 0.8750 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8498 - acc: 0.8136 - val_loss: 0.8694 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8446 - acc: 0.8390 - val_loss: 0.8638 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8395 - acc: 0.8475 - val_loss: 0.8586 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8343 - acc: 0.8644 - val_loss: 0.8531 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8293 - acc: 0.8729 - val_loss: 0.8476 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8243 - acc: 0.8729 - val_loss: 0.8424 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8194 - acc: 0.8729 - val_loss: 0.8366 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8142 - acc: 0.8729 - val_loss: 0.8307 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8091 - acc: 0.8729 - val_loss: 0.8250 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8041 - acc: 0.8814 - val_loss: 0.8190 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.7988 - acc: 0.8814 - val_loss: 0.8132 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7938 - acc: 0.8814 - val_loss: 0.8074 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.7889 - acc: 0.8814 - val_loss: 0.8017 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7840 - acc: 0.8814 - val_loss: 0.7963 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7790 - acc: 0.8814 - val_loss: 0.7907 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.7738 - acc: 0.8814 - val_loss: 0.7851 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.7687 - acc: 0.8898 - val_loss: 0.7795 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7634 - acc: 0.8898 - val_loss: 0.7739 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.7575 - acc: 0.8898 - val_loss: 0.7681 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7514 - acc: 0.8898 - val_loss: 0.7622 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7453 - acc: 0.8898 - val_loss: 0.7565 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7384 - acc: 0.8983 - val_loss: 0.7504 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7308 - acc: 0.8983 - val_loss: 0.7438 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7228 - acc: 0.8983 - val_loss: 0.7359 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7139 - acc: 0.9068 - val_loss: 0.7280 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7045 - acc: 0.9068 - val_loss: 0.7198 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.6943 - acc: 0.9068 - val_loss: 0.7120 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6844 - acc: 0.9068 - val_loss: 0.7040 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6743 - acc: 0.9068 - val_loss: 0.6961 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6638 - acc: 0.9068 - val_loss: 0.6882 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6535 - acc: 0.9068 - val_loss: 0.6777 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6423 - acc: 0.9068 - val_loss: 0.6662 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6297 - acc: 0.9068 - val_loss: 0.6511 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.6165 - acc: 0.9068 - val_loss: 0.6365 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.6033 - acc: 0.9068 - val_loss: 0.6221 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.5904 - acc: 0.9068 - val_loss: 0.6061 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5781 - acc: 0.9068 - val_loss: 0.5903 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.5663 - acc: 0.9068 - val_loss: 0.5747 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5545 - acc: 0.9068 - val_loss: 0.5598 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5429 - acc: 0.9068 - val_loss: 0.5446 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.5314 - acc: 0.9068 - val_loss: 0.5271 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5210 - acc: 0.9068 - val_loss: 0.5103 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5107 - acc: 0.9068 - val_loss: 0.4946 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.5009 - acc: 0.9068 - val_loss: 0.4799 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4905 - acc: 0.9068 - val_loss: 0.4648 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.4804 - acc: 0.9153 - val_loss: 0.4497 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4705 - acc: 0.9322 - val_loss: 0.4366 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4611 - acc: 0.9322 - val_loss: 0.4236 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4523 - acc: 0.9322 - val_loss: 0.4115 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.4436 - acc: 0.9322 - val_loss: 0.3997 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4350 - acc: 0.9915 - val_loss: 0.3883 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4267 - acc: 0.9915 - val_loss: 0.3778 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.4189 - acc: 0.9915 - val_loss: 0.3676 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4112 - acc: 0.9915 - val_loss: 0.3580 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4039 - acc: 0.9915 - val_loss: 0.3490 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3968 - acc: 0.9915 - val_loss: 0.3403 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3900 - acc: 0.9831 - val_loss: 0.3320 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.3835 - acc: 0.9831 - val_loss: 0.3243 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3769 - acc: 0.9831 - val_loss: 0.3163 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.3705 - acc: 0.9831 - val_loss: 0.3091 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3642 - acc: 0.9831 - val_loss: 0.3019 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.3581 - acc: 0.9831 - val_loss: 0.2951 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 267us/step - loss: 0.3523 - acc: 0.9831 - val_loss: 0.2887 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3468 - acc: 0.9831 - val_loss: 0.2825 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.3412 - acc: 0.9831 - val_loss: 0.2764 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3358 - acc: 0.9915 - val_loss: 0.2706 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3306 - acc: 0.9915 - val_loss: 0.2651 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3255 - acc: 0.9915 - val_loss: 0.2598 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.3207 - acc: 0.9915 - val_loss: 0.2547 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3159 - acc: 0.9915 - val_loss: 0.2496 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 1.3575 - acc: 0.2034 - val_loss: 1.1170 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1963 - acc: 0.2034 - val_loss: 0.9997 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0703 - acc: 0.2797 - val_loss: 0.9169 - val_acc: 0.6154\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9770 - acc: 0.5932 - val_loss: 0.8557 - val_acc: 0.6154\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9073 - acc: 0.6017 - val_loss: 0.8106 - val_acc: 0.6154\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 287us/step - loss: 0.8555 - acc: 0.6017 - val_loss: 0.7737 - val_acc: 0.6154\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8156 - acc: 0.6186 - val_loss: 0.7462 - val_acc: 0.6154\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7823 - acc: 0.6186 - val_loss: 0.7222 - val_acc: 0.6154\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7539 - acc: 0.6186 - val_loss: 0.7016 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7301 - acc: 0.6186 - val_loss: 0.6843 - val_acc: 0.6154\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7090 - acc: 0.8051 - val_loss: 0.6692 - val_acc: 0.9231\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6902 - acc: 0.8729 - val_loss: 0.6559 - val_acc: 0.9231\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6744 - acc: 0.9153 - val_loss: 0.6439 - val_acc: 0.9231\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6601 - acc: 0.9322 - val_loss: 0.6331 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.6470 - acc: 0.9492 - val_loss: 0.6232 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6349 - acc: 0.9746 - val_loss: 0.6141 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.6243 - acc: 0.9746 - val_loss: 0.6057 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6144 - acc: 0.9915 - val_loss: 0.5979 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.6053 - acc: 1.0000 - val_loss: 0.5905 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5969 - acc: 1.0000 - val_loss: 0.5835 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.5890 - acc: 1.0000 - val_loss: 0.5769 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5818 - acc: 0.9915 - val_loss: 0.5707 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.5747 - acc: 0.9831 - val_loss: 0.5648 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.5681 - acc: 0.9831 - val_loss: 0.5590 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5618 - acc: 0.9831 - val_loss: 0.5535 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.5559 - acc: 0.9831 - val_loss: 0.5480 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.5503 - acc: 0.9831 - val_loss: 0.5429 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.5448 - acc: 0.9831 - val_loss: 0.5378 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.5395 - acc: 0.9831 - val_loss: 0.5329 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.5344 - acc: 0.9831 - val_loss: 0.5281 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.5295 - acc: 0.9746 - val_loss: 0.5234 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.5248 - acc: 0.9746 - val_loss: 0.5190 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.5201 - acc: 0.9831 - val_loss: 0.5145 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.5156 - acc: 0.9746 - val_loss: 0.5102 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.5113 - acc: 0.9831 - val_loss: 0.5060 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5070 - acc: 0.9831 - val_loss: 0.5018 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.5028 - acc: 0.9831 - val_loss: 0.4977 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.4987 - acc: 0.9831 - val_loss: 0.4937 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.4947 - acc: 0.9831 - val_loss: 0.4897 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 303us/step - loss: 0.4909 - acc: 0.9831 - val_loss: 0.4859 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4871 - acc: 0.9831 - val_loss: 0.4821 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.4832 - acc: 0.9831 - val_loss: 0.4783 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4795 - acc: 0.9831 - val_loss: 0.4747 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.4759 - acc: 0.9831 - val_loss: 0.4711 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.4723 - acc: 0.9831 - val_loss: 0.4675 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4688 - acc: 0.9831 - val_loss: 0.4640 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4654 - acc: 0.9831 - val_loss: 0.4606 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4620 - acc: 0.9831 - val_loss: 0.4571 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4586 - acc: 0.9831 - val_loss: 0.4538 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.4553 - acc: 0.9831 - val_loss: 0.4505 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4520 - acc: 0.9831 - val_loss: 0.4472 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.4487 - acc: 0.9831 - val_loss: 0.4440 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4456 - acc: 0.9831 - val_loss: 0.4408 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4424 - acc: 0.9831 - val_loss: 0.4376 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4393 - acc: 0.9915 - val_loss: 0.4345 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4363 - acc: 0.9915 - val_loss: 0.4314 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4332 - acc: 0.9915 - val_loss: 0.4283 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.4302 - acc: 0.9915 - val_loss: 0.4253 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4273 - acc: 0.9915 - val_loss: 0.4223 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4243 - acc: 0.9915 - val_loss: 0.4193 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4214 - acc: 0.9915 - val_loss: 0.4163 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4185 - acc: 0.9915 - val_loss: 0.4134 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.4156 - acc: 0.9915 - val_loss: 0.4105 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4128 - acc: 0.9915 - val_loss: 0.4076 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4100 - acc: 0.9915 - val_loss: 0.4048 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4072 - acc: 0.9915 - val_loss: 0.4020 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4044 - acc: 0.9915 - val_loss: 0.3992 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4017 - acc: 0.9915 - val_loss: 0.3965 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.3989 - acc: 0.9915 - val_loss: 0.3937 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3962 - acc: 0.9915 - val_loss: 0.3909 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.3935 - acc: 0.9915 - val_loss: 0.3882 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3908 - acc: 0.9915 - val_loss: 0.3856 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3882 - acc: 0.9915 - val_loss: 0.3830 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3856 - acc: 0.9915 - val_loss: 0.3803 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3830 - acc: 0.9915 - val_loss: 0.3778 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3805 - acc: 0.9915 - val_loss: 0.3752 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3779 - acc: 0.9915 - val_loss: 0.3727 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3755 - acc: 0.9915 - val_loss: 0.3703 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3730 - acc: 0.9915 - val_loss: 0.3678 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.3705 - acc: 0.9915 - val_loss: 0.3653 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.3681 - acc: 0.9915 - val_loss: 0.3629 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.3657 - acc: 0.9915 - val_loss: 0.3605 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3633 - acc: 0.9915 - val_loss: 0.3582 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.3609 - acc: 0.9915 - val_loss: 0.3558 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.3585 - acc: 0.9915 - val_loss: 0.3535 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.3561 - acc: 0.9915 - val_loss: 0.3511 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3538 - acc: 0.9915 - val_loss: 0.3488 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3515 - acc: 0.9915 - val_loss: 0.3466 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3492 - acc: 0.9915 - val_loss: 0.3443 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3470 - acc: 0.9915 - val_loss: 0.3421 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3447 - acc: 0.9915 - val_loss: 0.3399 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3425 - acc: 0.9915 - val_loss: 0.3376 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3403 - acc: 0.9915 - val_loss: 0.3355 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.3381 - acc: 0.9915 - val_loss: 0.3333 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.3359 - acc: 0.9915 - val_loss: 0.3312 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.3337 - acc: 0.9915 - val_loss: 0.3291 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3316 - acc: 0.9915 - val_loss: 0.3270 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3294 - acc: 0.9915 - val_loss: 0.3249 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.3273 - acc: 0.9915 - val_loss: 0.3228 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3252 - acc: 0.9915 - val_loss: 0.3207 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "0bb6ca63-c156-4457-c636-721a1d4a3aac"
      },
      "source": [
        "val_data"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.46815712,  2.41613978],\n",
              "       [-2.04213808, -1.20055142],\n",
              "       [-2.26598031, -2.51797746],\n",
              "       [-1.62561816, -1.97201226],\n",
              "       [-2.12949607, -1.76064808],\n",
              "       [-1.32853889, -2.05415965],\n",
              "       [-1.86366758, -1.96105974],\n",
              "       [ 3.1426414 ,  0.23838493],\n",
              "       [ 2.76856724,  0.87507987],\n",
              "       [ 3.33006485,  0.80105572],\n",
              "       [ 3.99255757,  0.1366972 ],\n",
              "       [ 2.79798337,  1.20174003],\n",
              "       [ 1.67824072,  0.59967329]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1fba1779-faf6-4673-af53-3f619b2d23b7",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e9e6eb9b-8068-451e-d87b-fbff3cacc5be",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "23af790b-9d59-4601-86c5-62eeeb9b400a",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15b38709-cefd-414f-a38e-b0d5d556f07e",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7eff8c8a1908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "222b059e-b3ae-42f0-9d60-aced64c53b83",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7eff8c80ecc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhLn/ByBsrTq+4oF9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "408b770f-d0ce-4bc6-d527-75247c007ad4",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 1.1533 - acc: 0.4504\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 1.0981 - acc: 0.4656\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 1.0497 - acc: 0.4733\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 1.0067 - acc: 0.4733\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9679 - acc: 0.4809\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.9340 - acc: 0.4885\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9014 - acc: 0.4962\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8720 - acc: 0.5496\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8448 - acc: 0.6870\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8199 - acc: 0.6947\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.7979 - acc: 0.7328\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.7762 - acc: 0.7634\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.7563 - acc: 0.7710\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.7373 - acc: 0.7786\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.7192 - acc: 0.7786\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.7018 - acc: 0.7786\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.6851 - acc: 0.7863\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.6695 - acc: 0.7863\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.6546 - acc: 0.7939\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.6397 - acc: 0.8092\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6259 - acc: 0.8092\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 232us/step - loss: 0.6132 - acc: 0.8168\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.6006 - acc: 0.8168\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5886 - acc: 0.8168\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.5770 - acc: 0.8168\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.5660 - acc: 0.8168\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.5555 - acc: 0.8244\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5454 - acc: 0.8397\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.5353 - acc: 0.8397\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.5255 - acc: 0.8397\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.5161 - acc: 0.8397\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.5070 - acc: 0.8397\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 239us/step - loss: 0.4986 - acc: 0.8397\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.4905 - acc: 0.8397\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.4823 - acc: 0.8397\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 239us/step - loss: 0.4744 - acc: 0.8397\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4666 - acc: 0.8397\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4590 - acc: 0.8397\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.4515 - acc: 0.8550\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 236us/step - loss: 0.4447 - acc: 0.8550\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.4380 - acc: 0.8550\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.4315 - acc: 0.8550\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 228us/step - loss: 0.4253 - acc: 0.8550\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.4195 - acc: 0.8550\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 245us/step - loss: 0.4137 - acc: 0.8550\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.4082 - acc: 0.8626\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.4029 - acc: 0.8626\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.3979 - acc: 0.8626\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.3928 - acc: 0.8626\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.3881 - acc: 0.8702\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.3835 - acc: 0.8702\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.3790 - acc: 0.8702\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.3746 - acc: 0.8702\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.3704 - acc: 0.8702\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.3664 - acc: 0.8702\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.3624 - acc: 0.8702\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3584 - acc: 0.8702\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.3543 - acc: 0.8702\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3505 - acc: 0.8702\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.3469 - acc: 0.8702\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.3433 - acc: 0.8702\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.3399 - acc: 0.8702\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.3365 - acc: 0.8702\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 224us/step - loss: 0.3330 - acc: 0.8702\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.3297 - acc: 0.8702\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3264 - acc: 0.8702\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.3233 - acc: 0.8702\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.3203 - acc: 0.8702\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.3173 - acc: 0.8702\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.3144 - acc: 0.8702\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.3115 - acc: 0.8702\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.3086 - acc: 0.8702\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3058 - acc: 0.8702\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.3031 - acc: 0.8702\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3005 - acc: 0.8702\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.2981 - acc: 0.8702\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.2957 - acc: 0.8702\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.2934 - acc: 0.8702\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.2911 - acc: 0.8702\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.2889 - acc: 0.8702\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.2867 - acc: 0.8702\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2846 - acc: 0.8626\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2825 - acc: 0.8626\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.2805 - acc: 0.8626\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.2785 - acc: 0.8626\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.2766 - acc: 0.8626\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.2748 - acc: 0.8626\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.2730 - acc: 0.8626\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.2711 - acc: 0.8626\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.2694 - acc: 0.8626\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.2677 - acc: 0.8626\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.2660 - acc: 0.8626\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.2643 - acc: 0.8626\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.2627 - acc: 0.8626\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.2612 - acc: 0.8626\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.2596 - acc: 0.8626\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.2581 - acc: 0.8626\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.2565 - acc: 0.8626\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 0.2550 - acc: 0.8626\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.2536 - acc: 0.8626\n",
            "34/34 [==============================] - 0s 10ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c6331f93-6632-4428-b760-2a59dfeffe6e",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ae26c9e5-e0ca-4dad-e126-a7cf355a548c",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14705882352941177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQV6AwSwrmDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
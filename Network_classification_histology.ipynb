{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "29cb5985-6cd2-4a49-a92c-f26ad5c4c892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e7e1ad79-dafa-4ae4-b02c-9c30773916d8"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "e02cf884-9c1b-4d45-f0c0-21fa577d62a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "79c3ebb8-ee05-4d64-97a9-ced53f8661b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(6, activation='relu', input_shape=(9,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.4)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "c5b9aba7-c690-41e3-a7ac-181d484cd181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "c0e342aa-871f-4e9d-debf-f74c33e2d5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
            "  20  22  23  24  25  27  28  29  30  31  32  33  34  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  74  75  76  77  78  79  80\n",
            "  81  83  84  85  86  87  88  89  90  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 107 108 109 110 111 113 114 115 116 117 119 120 121\n",
            " 122 123 124 125 126 127 128 129] TEST: [  3  13  21  26  35  50  54  68  73  82  91 106 112 118 130]\n",
            "TRAIN: [  0   1   3   4   5   6   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  71  72  73  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98\n",
            " 100 101 102 103 104 105 106 107 109 110 111 112 113 115 116 117 118 120\n",
            " 121 122 124 125 126 127 128 129 130] TEST: [  2   7  18  43  47  61  70  74  93  99 108 114 119 123]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  50  51  52  53  54  56  57  58\n",
            "  59  60  61  62  63  65  66  67  68  69  70  72  73  74  76  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  91  92  93  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119\n",
            " 120 121 122 123 124 125 127 128 130] TEST: [  6  10  31  49  55  64  71  75  86  94  95 116 126 129]\n",
            "TRAIN: [  0   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  29  30  31  33  34  35  36  37  38  39\n",
            "  40  41  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  68  70  71  73  74  75  76  77  78  79\n",
            "  80  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 105 106 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 122 123 124 125 126 127 129 130] TEST: [  1   4  28  32  42  67  69  72  81 103 104 110 121 128]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  20\n",
            "  21  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
            "  42  43  44  45  46  47  48  49  50  51  53  54  55  57  58  59  60  61\n",
            "  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  80\n",
            "  81  82  83  84  85  86  87  90  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  0  14  19  22  37  41  52  56  79  88  89 107 120]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  25  26  27  28  29  30  31  32  34  35  36  37  38  39\n",
            "  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  60\n",
            "  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 114 115 116 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  9  15  24  33  40  51  59  62  83  97  98 113 117]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  18  19  20\n",
            "  21  22  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  61  62  64  65  66  67  68  69  70  71  72  73  74  75  76  79  81\n",
            "  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 123 124 125 126 127 128 129 130] TEST: [  8  16  17  23  25  60  63  77  78  80  90 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37\n",
            "  39  40  41  42  43  44  45  47  49  50  51  52  53  54  55  56  58  59\n",
            "  60  61  62  63  64  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119 120 121 122 123 124 126 128 129 130] TEST: [ 12  29  38  46  48  57  65  84 101 105 125 127]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
            "  19  21  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  40  41  42  43  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  88  89  90  91  93  94  95  96  97  98\n",
            "  99 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 125 126 127 128 129 130] TEST: [ 11  20  30  39  44  45  66  87  92 100 102 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  28  29  30  31  32  33  35  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78\n",
            "  79  80  81  82  83  84  86  87  88  89  90  91  92  93  94  95  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 112 113 114 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130] TEST: [  5  27  34  36  53  58  76  85  96 109 111 115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "9e49d930-47eb-425f-f971-e51dde4cd5e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "1089bd3c-c1e8-4cae-e617-6f072646386e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 20\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 116 samples, validate on 15 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 1.4316 - acc: 0.3621 - val_loss: 1.3475 - val_acc: 0.5333\n",
            "Epoch 2/20\n",
            "116/116 [==============================] - 0s 177us/step - loss: 1.1452 - acc: 0.4655 - val_loss: 1.2664 - val_acc: 0.5333\n",
            "Epoch 3/20\n",
            "116/116 [==============================] - 0s 169us/step - loss: 1.0297 - acc: 0.5690 - val_loss: 1.3354 - val_acc: 0.4000\n",
            "Epoch 4/20\n",
            "116/116 [==============================] - 0s 181us/step - loss: 0.9736 - acc: 0.5862 - val_loss: 1.0905 - val_acc: 0.5333\n",
            "Epoch 5/20\n",
            "116/116 [==============================] - 0s 219us/step - loss: 0.9747 - acc: 0.5948 - val_loss: 1.4627 - val_acc: 0.4667\n",
            "Epoch 6/20\n",
            "116/116 [==============================] - 0s 211us/step - loss: 0.9318 - acc: 0.6121 - val_loss: 1.3258 - val_acc: 0.4667\n",
            "Epoch 7/20\n",
            "116/116 [==============================] - 0s 211us/step - loss: 0.9140 - acc: 0.6466 - val_loss: 1.2775 - val_acc: 0.4667\n",
            "Epoch 8/20\n",
            "116/116 [==============================] - 0s 224us/step - loss: 0.9027 - acc: 0.6466 - val_loss: 1.2761 - val_acc: 0.4000\n",
            "Epoch 9/20\n",
            "116/116 [==============================] - 0s 204us/step - loss: 0.8902 - acc: 0.6552 - val_loss: 1.3305 - val_acc: 0.4667\n",
            "Epoch 10/20\n",
            "116/116 [==============================] - 0s 188us/step - loss: 0.8799 - acc: 0.6638 - val_loss: 1.3506 - val_acc: 0.4667\n",
            "Epoch 11/20\n",
            "116/116 [==============================] - 0s 209us/step - loss: 0.8713 - acc: 0.6638 - val_loss: 1.3567 - val_acc: 0.4667\n",
            "Epoch 12/20\n",
            "116/116 [==============================] - 0s 198us/step - loss: 0.8632 - acc: 0.6638 - val_loss: 1.5441 - val_acc: 0.4667\n",
            "Epoch 13/20\n",
            "116/116 [==============================] - 0s 200us/step - loss: 0.8658 - acc: 0.6638 - val_loss: 1.3738 - val_acc: 0.4667\n",
            "Epoch 14/20\n",
            "116/116 [==============================] - 0s 225us/step - loss: 0.8638 - acc: 0.6638 - val_loss: 1.4659 - val_acc: 0.4667\n",
            "Epoch 15/20\n",
            "116/116 [==============================] - 0s 165us/step - loss: 0.8506 - acc: 0.6724 - val_loss: 1.2594 - val_acc: 0.4000\n",
            "Epoch 16/20\n",
            "116/116 [==============================] - 0s 165us/step - loss: 0.8600 - acc: 0.6638 - val_loss: 1.3798 - val_acc: 0.4667\n",
            "Epoch 17/20\n",
            "116/116 [==============================] - 0s 177us/step - loss: 0.8495 - acc: 0.6810 - val_loss: 1.4540 - val_acc: 0.4667\n",
            "Epoch 18/20\n",
            "116/116 [==============================] - 0s 195us/step - loss: 0.8447 - acc: 0.6724 - val_loss: 1.4802 - val_acc: 0.4667\n",
            "Epoch 19/20\n",
            "116/116 [==============================] - 0s 179us/step - loss: 0.8424 - acc: 0.6810 - val_loss: 1.5393 - val_acc: 0.4667\n",
            "Epoch 20/20\n",
            "116/116 [==============================] - 0s 207us/step - loss: 0.8394 - acc: 0.6638 - val_loss: 1.5302 - val_acc: 0.4667\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 0s 1ms/step - loss: 1.3449 - acc: 0.4103 - val_loss: 1.0647 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 0s 195us/step - loss: 1.0528 - acc: 0.5470 - val_loss: 1.0783 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.9917 - acc: 0.6154 - val_loss: 1.0849 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9553 - acc: 0.6068 - val_loss: 1.0940 - val_acc: 0.4286\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9340 - acc: 0.6239 - val_loss: 1.0888 - val_acc: 0.4286\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9178 - acc: 0.6068 - val_loss: 1.0842 - val_acc: 0.4286\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9131 - acc: 0.5983 - val_loss: 1.0794 - val_acc: 0.4286\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.8966 - acc: 0.5983 - val_loss: 1.0807 - val_acc: 0.4286\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.8937 - acc: 0.6068 - val_loss: 1.0795 - val_acc: 0.4286\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.8997 - acc: 0.5983 - val_loss: 1.0776 - val_acc: 0.4286\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.8812 - acc: 0.6068 - val_loss: 1.0802 - val_acc: 0.3571\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.8773 - acc: 0.6068 - val_loss: 1.0870 - val_acc: 0.3571\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.8789 - acc: 0.5983 - val_loss: 1.0806 - val_acc: 0.4286\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 0s 234us/step - loss: 0.8712 - acc: 0.6154 - val_loss: 1.0864 - val_acc: 0.4286\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.8735 - acc: 0.6154 - val_loss: 1.0827 - val_acc: 0.4286\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.8691 - acc: 0.6239 - val_loss: 1.0838 - val_acc: 0.4286\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.8659 - acc: 0.6154 - val_loss: 1.0887 - val_acc: 0.4286\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.8632 - acc: 0.6154 - val_loss: 1.0856 - val_acc: 0.4286\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.8608 - acc: 0.6325 - val_loss: 1.0811 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.8571 - acc: 0.6496 - val_loss: 1.0909 - val_acc: 0.4286\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 0s 2ms/step - loss: 1.1851 - acc: 0.4359 - val_loss: 1.1192 - val_acc: 0.5714\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.0811 - acc: 0.4530 - val_loss: 1.0959 - val_acc: 0.5714\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.0237 - acc: 0.4786 - val_loss: 1.0578 - val_acc: 0.4286\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.0038 - acc: 0.4872 - val_loss: 1.0420 - val_acc: 0.2857\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9911 - acc: 0.4957 - val_loss: 1.0325 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9834 - acc: 0.4872 - val_loss: 1.0192 - val_acc: 0.4286\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9735 - acc: 0.5470 - val_loss: 1.0076 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9592 - acc: 0.5726 - val_loss: 0.9955 - val_acc: 0.4286\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9580 - acc: 0.5556 - val_loss: 0.9910 - val_acc: 0.5714\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9522 - acc: 0.5556 - val_loss: 0.9814 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9407 - acc: 0.5983 - val_loss: 0.9763 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9342 - acc: 0.6068 - val_loss: 0.9814 - val_acc: 0.5714\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 0s 280us/step - loss: 0.9334 - acc: 0.5641 - val_loss: 0.9730 - val_acc: 0.5714\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.9332 - acc: 0.5556 - val_loss: 0.9725 - val_acc: 0.5714\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.9299 - acc: 0.5385 - val_loss: 0.9700 - val_acc: 0.5714\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 0s 250us/step - loss: 0.9290 - acc: 0.5470 - val_loss: 0.9747 - val_acc: 0.5714\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 0s 260us/step - loss: 0.9289 - acc: 0.5556 - val_loss: 0.9586 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.9204 - acc: 0.5385 - val_loss: 0.9615 - val_acc: 0.5714\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.9245 - acc: 0.5556 - val_loss: 0.9593 - val_acc: 0.5714\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.9304 - acc: 0.5726 - val_loss: 0.9647 - val_acc: 0.5714\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 0s 2ms/step - loss: 1.8784 - acc: 0.4188 - val_loss: 1.0009 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 0s 171us/step - loss: 1.2218 - acc: 0.3761 - val_loss: 1.0662 - val_acc: 0.5714\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 0s 168us/step - loss: 1.0849 - acc: 0.4274 - val_loss: 1.0613 - val_acc: 0.4286\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.0377 - acc: 0.4188 - val_loss: 1.0628 - val_acc: 0.4286\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.0166 - acc: 0.4872 - val_loss: 1.1317 - val_acc: 0.4286\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 0s 205us/step - loss: 1.0072 - acc: 0.4786 - val_loss: 1.0953 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9921 - acc: 0.4615 - val_loss: 1.1001 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.9792 - acc: 0.4701 - val_loss: 1.0764 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9720 - acc: 0.5043 - val_loss: 1.0779 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9619 - acc: 0.4872 - val_loss: 1.0266 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.9681 - acc: 0.5299 - val_loss: 1.0748 - val_acc: 0.4286\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9570 - acc: 0.4957 - val_loss: 1.0556 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9483 - acc: 0.5214 - val_loss: 1.0818 - val_acc: 0.4286\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9500 - acc: 0.4957 - val_loss: 1.0402 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9393 - acc: 0.5214 - val_loss: 1.0662 - val_acc: 0.4286\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9408 - acc: 0.5299 - val_loss: 1.0400 - val_acc: 0.4286\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9418 - acc: 0.5470 - val_loss: 1.0462 - val_acc: 0.4286\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9385 - acc: 0.5556 - val_loss: 1.0543 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9349 - acc: 0.5385 - val_loss: 1.0540 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.9254 - acc: 0.5470 - val_loss: 1.0310 - val_acc: 0.4286\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.9056 - acc: 0.3898 - val_loss: 1.3764 - val_acc: 0.3077\n",
            "Epoch 2/20\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.2234 - acc: 0.4661 - val_loss: 1.4252 - val_acc: 0.3846\n",
            "Epoch 3/20\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0889 - acc: 0.4661 - val_loss: 1.4849 - val_acc: 0.4615\n",
            "Epoch 4/20\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0301 - acc: 0.5000 - val_loss: 1.5300 - val_acc: 0.3846\n",
            "Epoch 5/20\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9942 - acc: 0.5000 - val_loss: 1.5605 - val_acc: 0.3846\n",
            "Epoch 6/20\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9790 - acc: 0.5508 - val_loss: 1.5184 - val_acc: 0.4615\n",
            "Epoch 7/20\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9671 - acc: 0.5085 - val_loss: 1.5216 - val_acc: 0.4615\n",
            "Epoch 8/20\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9528 - acc: 0.5254 - val_loss: 1.5081 - val_acc: 0.5385\n",
            "Epoch 9/20\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9490 - acc: 0.5085 - val_loss: 1.5084 - val_acc: 0.5385\n",
            "Epoch 10/20\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9427 - acc: 0.5339 - val_loss: 1.5240 - val_acc: 0.4615\n",
            "Epoch 11/20\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9254 - acc: 0.5339 - val_loss: 1.5697 - val_acc: 0.3846\n",
            "Epoch 12/20\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9373 - acc: 0.5424 - val_loss: 1.5449 - val_acc: 0.3846\n",
            "Epoch 13/20\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9205 - acc: 0.5424 - val_loss: 1.5407 - val_acc: 0.4615\n",
            "Epoch 14/20\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9208 - acc: 0.5424 - val_loss: 1.5415 - val_acc: 0.3846\n",
            "Epoch 15/20\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9174 - acc: 0.5763 - val_loss: 1.5616 - val_acc: 0.3846\n",
            "Epoch 16/20\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9186 - acc: 0.5678 - val_loss: 1.5360 - val_acc: 0.3846\n",
            "Epoch 17/20\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9131 - acc: 0.5678 - val_loss: 1.5590 - val_acc: 0.5385\n",
            "Epoch 18/20\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9074 - acc: 0.5678 - val_loss: 1.5426 - val_acc: 0.6154\n",
            "Epoch 19/20\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9161 - acc: 0.5254 - val_loss: 1.5475 - val_acc: 0.5385\n",
            "Epoch 20/20\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9005 - acc: 0.5508 - val_loss: 1.5988 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.6065 - acc: 0.2627 - val_loss: 1.5753 - val_acc: 0.1538\n",
            "Epoch 2/20\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1752 - acc: 0.4322 - val_loss: 1.2776 - val_acc: 0.1538\n",
            "Epoch 3/20\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.0321 - acc: 0.4661 - val_loss: 1.1878 - val_acc: 0.3077\n",
            "Epoch 4/20\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9898 - acc: 0.4915 - val_loss: 1.1509 - val_acc: 0.3077\n",
            "Epoch 5/20\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9668 - acc: 0.5169 - val_loss: 1.1319 - val_acc: 0.3846\n",
            "Epoch 6/20\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9433 - acc: 0.5424 - val_loss: 1.1425 - val_acc: 0.4615\n",
            "Epoch 7/20\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9369 - acc: 0.5169 - val_loss: 1.1170 - val_acc: 0.4615\n",
            "Epoch 8/20\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9171 - acc: 0.5339 - val_loss: 1.0911 - val_acc: 0.5385\n",
            "Epoch 9/20\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9104 - acc: 0.5339 - val_loss: 1.0941 - val_acc: 0.4615\n",
            "Epoch 10/20\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.8974 - acc: 0.5593 - val_loss: 1.1070 - val_acc: 0.4615\n",
            "Epoch 11/20\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8914 - acc: 0.5508 - val_loss: 1.1091 - val_acc: 0.4615\n",
            "Epoch 12/20\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8837 - acc: 0.5508 - val_loss: 1.1257 - val_acc: 0.4615\n",
            "Epoch 13/20\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.8743 - acc: 0.5593 - val_loss: 1.1130 - val_acc: 0.4615\n",
            "Epoch 14/20\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8713 - acc: 0.5847 - val_loss: 1.1176 - val_acc: 0.3077\n",
            "Epoch 15/20\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8608 - acc: 0.5763 - val_loss: 1.1523 - val_acc: 0.3077\n",
            "Epoch 16/20\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8610 - acc: 0.5847 - val_loss: 1.1467 - val_acc: 0.3077\n",
            "Epoch 17/20\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8512 - acc: 0.6186 - val_loss: 1.1488 - val_acc: 0.3077\n",
            "Epoch 18/20\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8480 - acc: 0.5932 - val_loss: 1.1597 - val_acc: 0.3077\n",
            "Epoch 19/20\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8415 - acc: 0.6102 - val_loss: 1.1746 - val_acc: 0.3077\n",
            "Epoch 20/20\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8378 - acc: 0.6271 - val_loss: 1.1743 - val_acc: 0.3077\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 1.9524 - acc: 0.3782 - val_loss: 1.7573 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 169us/step - loss: 1.2888 - acc: 0.3866 - val_loss: 1.4910 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 165us/step - loss: 1.1266 - acc: 0.3950 - val_loss: 1.3241 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 1.0533 - acc: 0.4202 - val_loss: 1.2119 - val_acc: 0.5833\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 175us/step - loss: 1.0113 - acc: 0.4790 - val_loss: 1.1500 - val_acc: 0.5833\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 200us/step - loss: 0.9875 - acc: 0.5462 - val_loss: 1.1234 - val_acc: 0.5833\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 184us/step - loss: 0.9805 - acc: 0.5294 - val_loss: 1.1256 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 159us/step - loss: 0.9630 - acc: 0.5210 - val_loss: 1.1059 - val_acc: 0.5833\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 156us/step - loss: 0.9455 - acc: 0.5126 - val_loss: 1.0982 - val_acc: 0.5833\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 218us/step - loss: 0.9332 - acc: 0.4958 - val_loss: 1.0746 - val_acc: 0.5833\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 262us/step - loss: 0.9352 - acc: 0.5210 - val_loss: 1.0639 - val_acc: 0.5833\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 168us/step - loss: 0.9205 - acc: 0.5294 - val_loss: 1.0309 - val_acc: 0.6667\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 176us/step - loss: 0.9280 - acc: 0.5210 - val_loss: 1.0223 - val_acc: 0.6667\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 173us/step - loss: 0.9133 - acc: 0.5546 - val_loss: 1.0144 - val_acc: 0.6667\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 185us/step - loss: 0.9055 - acc: 0.5378 - val_loss: 1.0264 - val_acc: 0.6667\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 193us/step - loss: 0.8988 - acc: 0.5630 - val_loss: 1.0113 - val_acc: 0.6667\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 183us/step - loss: 0.8972 - acc: 0.5462 - val_loss: 0.9903 - val_acc: 0.6667\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 222us/step - loss: 0.8931 - acc: 0.5378 - val_loss: 0.9824 - val_acc: 0.6667\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 178us/step - loss: 0.8838 - acc: 0.5378 - val_loss: 0.9748 - val_acc: 0.6667\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 214us/step - loss: 0.8866 - acc: 0.5798 - val_loss: 0.9664 - val_acc: 0.6667\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 2.7443 - acc: 0.2185 - val_loss: 1.8755 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 200us/step - loss: 1.6358 - acc: 0.3361 - val_loss: 1.2532 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 195us/step - loss: 1.2784 - acc: 0.4118 - val_loss: 1.0271 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 186us/step - loss: 1.1392 - acc: 0.4454 - val_loss: 0.9229 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 185us/step - loss: 1.0693 - acc: 0.4958 - val_loss: 0.8754 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 220us/step - loss: 1.0295 - acc: 0.5042 - val_loss: 0.8449 - val_acc: 0.5833\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 190us/step - loss: 1.0015 - acc: 0.5294 - val_loss: 0.8287 - val_acc: 0.6667\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 194us/step - loss: 0.9840 - acc: 0.5126 - val_loss: 0.8186 - val_acc: 0.6667\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 174us/step - loss: 0.9699 - acc: 0.5462 - val_loss: 0.8114 - val_acc: 0.6667\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.9563 - acc: 0.5462 - val_loss: 0.8109 - val_acc: 0.6667\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 164us/step - loss: 0.9517 - acc: 0.5714 - val_loss: 0.8109 - val_acc: 0.6667\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 168us/step - loss: 0.9420 - acc: 0.5714 - val_loss: 0.8077 - val_acc: 0.7500\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 168us/step - loss: 0.9402 - acc: 0.5630 - val_loss: 0.8078 - val_acc: 0.7500\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 168us/step - loss: 0.9312 - acc: 0.5798 - val_loss: 0.8073 - val_acc: 0.7500\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.9291 - acc: 0.5798 - val_loss: 0.8069 - val_acc: 0.8333\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 156us/step - loss: 0.9256 - acc: 0.5798 - val_loss: 0.8038 - val_acc: 0.8333\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 194us/step - loss: 0.9231 - acc: 0.5714 - val_loss: 0.8021 - val_acc: 0.7500\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 211us/step - loss: 0.9173 - acc: 0.5630 - val_loss: 0.8139 - val_acc: 0.7500\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 206us/step - loss: 0.9210 - acc: 0.5798 - val_loss: 0.8079 - val_acc: 0.7500\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 198us/step - loss: 0.9135 - acc: 0.5714 - val_loss: 0.8053 - val_acc: 0.6667\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 2.2317 - acc: 0.3193 - val_loss: 1.7644 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 195us/step - loss: 1.5455 - acc: 0.4202 - val_loss: 1.3450 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 195us/step - loss: 1.2565 - acc: 0.4874 - val_loss: 1.1598 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 193us/step - loss: 1.1464 - acc: 0.5126 - val_loss: 1.1051 - val_acc: 0.5833\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 186us/step - loss: 1.0996 - acc: 0.5462 - val_loss: 1.0369 - val_acc: 0.5833\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 224us/step - loss: 1.0445 - acc: 0.5378 - val_loss: 1.0015 - val_acc: 0.5833\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 173us/step - loss: 1.0171 - acc: 0.5714 - val_loss: 0.9830 - val_acc: 0.5833\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 197us/step - loss: 0.9904 - acc: 0.5294 - val_loss: 0.9962 - val_acc: 0.5833\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 173us/step - loss: 0.9679 - acc: 0.5798 - val_loss: 1.0060 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 223us/step - loss: 0.9471 - acc: 0.6050 - val_loss: 0.9893 - val_acc: 0.4167\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 206us/step - loss: 0.9425 - acc: 0.5462 - val_loss: 0.9855 - val_acc: 0.4167\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 174us/step - loss: 0.9220 - acc: 0.5714 - val_loss: 0.9892 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 187us/step - loss: 0.9105 - acc: 0.5714 - val_loss: 1.0005 - val_acc: 0.4167\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 178us/step - loss: 0.9096 - acc: 0.5714 - val_loss: 1.0004 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 194us/step - loss: 0.8963 - acc: 0.5798 - val_loss: 0.9995 - val_acc: 0.4167\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 161us/step - loss: 0.8907 - acc: 0.5966 - val_loss: 1.0046 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 159us/step - loss: 0.8999 - acc: 0.5882 - val_loss: 0.9938 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 161us/step - loss: 0.8808 - acc: 0.5798 - val_loss: 1.0183 - val_acc: 0.4167\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 182us/step - loss: 0.8803 - acc: 0.5630 - val_loss: 1.0125 - val_acc: 0.3333\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 198us/step - loss: 0.8687 - acc: 0.6050 - val_loss: 1.0276 - val_acc: 0.5000\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 1.8093 - acc: 0.3866 - val_loss: 0.9081 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 209us/step - loss: 1.3194 - acc: 0.3697 - val_loss: 0.8578 - val_acc: 0.5833\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 180us/step - loss: 1.1781 - acc: 0.3866 - val_loss: 0.8532 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 188us/step - loss: 1.1114 - acc: 0.4202 - val_loss: 0.8743 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 216us/step - loss: 1.0724 - acc: 0.4874 - val_loss: 0.8928 - val_acc: 0.5833\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 176us/step - loss: 1.0515 - acc: 0.4790 - val_loss: 0.9302 - val_acc: 0.5833\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 183us/step - loss: 1.0295 - acc: 0.5294 - val_loss: 0.9491 - val_acc: 0.5833\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 174us/step - loss: 1.0112 - acc: 0.5378 - val_loss: 0.9605 - val_acc: 0.5833\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 168us/step - loss: 1.0078 - acc: 0.5294 - val_loss: 0.9740 - val_acc: 0.5833\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 176us/step - loss: 0.9978 - acc: 0.5378 - val_loss: 0.9842 - val_acc: 0.5833\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 0.9887 - acc: 0.5630 - val_loss: 0.9927 - val_acc: 0.5833\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 161us/step - loss: 0.9855 - acc: 0.5546 - val_loss: 1.0004 - val_acc: 0.5833\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 169us/step - loss: 0.9738 - acc: 0.5714 - val_loss: 1.0027 - val_acc: 0.5833\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 169us/step - loss: 0.9666 - acc: 0.5714 - val_loss: 0.9869 - val_acc: 0.5833\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 216us/step - loss: 0.9592 - acc: 0.5714 - val_loss: 0.9894 - val_acc: 0.5833\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 186us/step - loss: 0.9533 - acc: 0.5798 - val_loss: 0.9900 - val_acc: 0.5833\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 189us/step - loss: 0.9444 - acc: 0.5798 - val_loss: 0.9867 - val_acc: 0.5833\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 168us/step - loss: 0.9402 - acc: 0.5798 - val_loss: 0.9899 - val_acc: 0.5833\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 171us/step - loss: 0.9359 - acc: 0.6050 - val_loss: 0.9898 - val_acc: 0.5833\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 180us/step - loss: 0.9385 - acc: 0.5546 - val_loss: 0.9931 - val_acc: 0.5833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "6c7f4661-3cc0-4aef-a3c4-406f2866d070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "b8e3174f-d031-4cfd-b045-2bd45fa4b445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "9ac5c017-c194-4ca2-a61e-e88b09e91e58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "4c6044f0-0a43-41f3-eac9-bc757ac1c7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'bo', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'b', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb8511a6048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZgU1dn38e/NIjiAMAJuLDMYNxxA\nllEwaMBIDOIWfVyDCRAJj8Q8xtfIK29MNDEhyyOiwRAMJooRJFHULEajMWLQxA0QUMGVRQGBAWQd\nVJb7/ePUQDNM9/TQ010z07/PddXV1VWnqu+u6am7zznVp8zdERGR/NUo7gBERCReSgQiInlOiUBE\nJM8pEYiI5DklAhGRPKdEICKS55QIpFaZWWMz22pmnWuzbJzM7Bgzq/XrrM1skJktS3j+tpmdnk7Z\nA3it35rZ9w50+xT7/YmZTa3t/UpuNYk7AImXmW1NeFoAfArsip7/t7tPr8n+3H0X0LK2y+YDdz++\nNvZjZiOBK919YMK+R9bGvqVhUiLIc+6+50QcfeMc6e7PJCtvZk3cfWcuYhOR3FDTkKQUVf3/aGYz\nzGwLcKWZnWpmL5nZRjP7yMwmmlnTqHwTM3MzK46eT4vWP2lmW8zsRTPrUtOy0fqzzewdM9tkZneZ\n2b/NbHiSuNOJ8b/N7D0z+9jMJiZs29jM7jCz9Wa2BBic4vjcZGZ/qLRskplNiOZHmtni6P28H31b\nT7avFWY2MJovMLMHotjeBPpUKvt9M1sS7fdNMzs/Wt4d+BVwetTsti7h2P4wYfuro/e+3sz+ZGZH\npnNsqmNmF0bxbDSzZ83s+IR13zOzVWa22czeSniv/cxsXrR8jZndlu7rSS1xd02acHeAZcCgSst+\nAnwGnEf44nAwcDLQl1CjPBp4B/h2VL4J4EBx9HwasA4oBZoCfwSmHUDZw4AtwAXRuuuBHcDwJO8l\nnRj/DLQGioENFe8d+DbwJtARaAvMDv8qVb7O0cBWoEXCvtcCpdHz86IyBnwR2A70iNYNApYl7GsF\nMDCaHw88BxQCRcCiSmUvBY6M/iZfjWI4PFo3EniuUpzTgB9G82dFMfYEmgO/Bp5N59hU8f5/AkyN\n5rtGcXwx+ht9D3g7mi8BlgNHRGW7AEdH868CV0TzrYC+cf8v5NukGoGk4wV3/6u773b37e7+qru/\n7O473X0JMAUYkGL7me4+x913ANMJJ6Calj0XmO/uf47W3UFIGlVKM8afufsmd19GOOlWvNalwB3u\nvsLd1wM/T/E6S4A3CAkK4EvAx+4+J1r/V3df4sGzwD+BKjuEK7kU+Im7f+zuywnf8hNf9yF3/yj6\nmzxISOKlaewXYCjwW3ef7+6fAGOBAWbWMaFMsmOTyuXAX9z92ehv9HNCMukL7CQknZKoeXFpdOwg\nJPRjzaytu29x95fTfB9SS5QIJB0fJj4xsxPM7G9mttrMNgO3Au1SbL86Yb6c1B3EycoelRiHuzvh\nG3SV0owxrdcifJNN5UHgimj+q9HzijjONbOXzWyDmW0kfBtPdawqHJkqBjMbbmYLoiaYjcAJae4X\nwvvbsz933wx8DHRIKFOTv1my/e4m/I06uPvbwHcJf4e1UVPjEVHREcCJwNtm9oqZDUnzfUgtUSKQ\ndFS+dPI3hG/Bx7j7IcDNhKaPbPqI0FQDgJkZ+564Ksskxo+ATgnPq7u89SFgkJl1INQMHoxiPBiY\nCfyM0GzTBng6zThWJ4vBzI4GJgOjgbbRft9K2G91l7quIjQ3VeyvFaEJamUacdVkv40If7OVAO4+\nzd37E5qFGhOOC+7+trtfTmj+ux14xMyaZxiL1IASgRyIVsAmYJuZdQX+Owev+TjQ28zOM7MmwHeA\n9lmK8SHgOjPrYGZtgRtTFXb31cALwFTgbXd/N1rVDDgIKAN2mdm5wJk1iOF7ZtbGwu8svp2wriXh\nZF9GyInfJNQIKqwBOlZ0jldhBnCVmfUws2aEE/Lz7p60hlWDmM83s4HRa48h9Ou8bGZdzeyM6PW2\nR9Nuwhv4mpm1i2oQm6L3tjvDWKQGlAjkQHwXGEb4J/8NoVM3q9x9DXAZMAFYD3wOeI3wu4fajnEy\noS3/dUJH5sw0tnmQ0Pm7p1nI3TcC/wd4jNDhejEhoaXjFkLNZBnwJPD7hP0uBO4CXonKHA8ktqv/\nA3gXWGNmiU08Fdv/ndBE81i0fWdCv0FG3P1NwjGfTEhSg4Hzo/6CZsD/Evp1VhNqIDdFmw4BFlu4\nKm08cJm7f5ZpPJI+C02tIvWLmTUmNEVc7O7Pxx2PSH2mGoHUG2Y2OGoqaQb8gHC1ySsxhyVS7ykR\nSH1yGrCE0OzwZeBCd0/WNCQiaVLTkIhInlONQEQkz9W7QefatWvnxcXFcYchIlKvzJ07d527V3nJ\ndb1LBMXFxcyZMyfuMERE6hUzS/oLeTUNiYjkOSUCEZE8p0QgIpLn6l0fgYjk3o4dO1ixYgWffPJJ\n3KFINZo3b07Hjh1p2jTZUFP7UyIQkWqtWLGCVq1aUVxcTBj4Veoid2f9+vWsWLGCLl26VL9BJC+a\nhqZPh+JiaNQoPE6v0e3YReSTTz6hbdu2SgJ1nJnRtm3bGtfcGnyNYPp0GDUKysvD8+XLw3OAoRmP\ntyiSP5QE6ocD+TtlrUZgZvea2VozeyPJ+tZm9tfoLktvmtmIbMRx0017k0CF8vKwXEREsts0NJUw\nHnky1wCL3P0kYCBwu5kdVNtBfPBBzZaLSN2zceNGfv3rXx/QtkOGDGHjxo0py9x8880888wzB7T/\nyoqLi1m3LunttOukrCUCd59NuBlH0iJAq+iWgy2jsjtrO47OSW4ymGy5iGSutvvlUiWCnTtTnzae\neOIJ2rRpk7LMrbfeyqBBgw44vvouzs7iXwFdCTcXeR34TnSruv2Y2Sgzm2Nmc8rKymr0IuPGQUHB\nvssKCsJyEal9Ff1yy5eD+95+uUySwdixY3n//ffp2bMnY8aM4bnnnuP000/n/PPP58QTTwTgK1/5\nCn369KGkpIQpU6bs2bbiG/qyZcvo2rUr3/zmNykpKeGss85i+/btAAwfPpyZM2fuKX/LLbfQu3dv\nunfvzltvvQVAWVkZX/rSlygpKWHkyJEUFRVV+81/woQJdOvWjW7dunHnnXcCsG3bNs455xxOOukk\nunXrxh//+Mc97/HEE0+kR48e3HDDDQd+sA6Eu2dtAoqBN5Ksuxi4g3DD7WOApcAh1e2zT58+XlPT\nprkXFbmbhcdp02q8C5G8tmjRorTLFhW5hxSw71RUdOCvv3TpUi8pKdnzfNasWV5QUOBLlizZs2z9\n+vXu7l5eXu4lJSW+bt26KJ4iLysr86VLl3rjxo39tddec3f3Sy65xB944AF3dx82bJg//PDDe8pP\nnDjR3d0nTZrkV111lbu7X3PNNf7Tn/7U3d2ffPJJB7ysrKyK9x9eb86cOd6tWzffunWrb9myxU88\n8USfN2+ez5w500eOHLmn/MaNG33dunV+3HHH+e7du93d/eOPPz7wg+VV/72AOZ7kvBpnjWAE8GgU\n43tRIjihmm0OyNChsGwZ7N4dHnW1kEj25Kpf7pRTTtnnWvmJEydy0kkn0a9fPz788EPefffd/bbp\n0qULPXv2BKBPnz4sW7asyn1fdNFF+5V54YUXuPzyywEYPHgwhYWFKeN74YUXuPDCC2nRogUtW7bk\noosu4vnnn6d79+784x//4MYbb+T555+ndevWtG7dmubNm3PVVVfx6KOPUlC5GSPL4kwEHwBnApjZ\n4YQbcC+JMR4RqQW56pdr0aLFnvnnnnuOZ555hhdffJEFCxbQq1evKq+lb9as2Z75xo0bJ+1fqCiX\nqsyBOu6445g3bx7du3fn+9//PrfeeitNmjThlVde4eKLL+bxxx9n8OBU19nUvmxePjoDeBE43sxW\nmNlVZna1mV0dFfkx8Hkzex34J3Cju9evrnYR2U82+uVatWrFli1bkq7ftGkThYWFFBQU8NZbb/HS\nSy8d+Isl0b9/fx566CEAnn76aT7++OOU5U8//XT+9Kc/UV5ezrZt23jsscc4/fTTWbVqFQUFBVx5\n5ZWMGTOGefPmsXXrVjZt2sSQIUO44447WLBgQa3Hn0rWflDm7ldUs34VcFa2Xl9E4lHR9HrTTaE5\nqHPnkAQyaZJt27Yt/fv3p1u3bpx99tmcc845+6wfPHgwd999N127duX444+nX79+GbyDqt1yyy1c\nccUVPPDAA5x66qkcccQRtGrVKmn53r17M3z4cE455RQARo4cSa9evXjqqacYM2YMjRo1omnTpkye\nPJktW7ZwwQUX8Mknn+DuTJgwodbjT6Xe3bO4tLTUdWMakdxavHgxXbt2jTuMWH366ac0btyYJk2a\n8OKLLzJ69Gjmz58fd1hVqurvZWZz3b20qvINfogJEZHa8MEHH3DppZeye/duDjroIO655564Q6o1\nSgQiImk49thjee211+IOIyvyYvRRERFJTolARCTPKRGIiOQ5JQIRkTynRCAiDVLLli0BWLVqFRdf\nfHGVZQYOHEh1l6PfeeedlCfc1CSdYa3T8cMf/pDx48dnvJ/aoEQgIg3aUUcdtWdk0QNRORGkM6x1\nfaNEICJ13tixY5k0adKe5xXfprdu3cqZZ565Z8joP//5z/ttu2zZMrp16wbA9u3bufzyy+natSsX\nXnjhnmGoAUaPHk1paSklJSXccsstQBjIbtWqVZxxxhmcccYZwL43nqlqmOlUw10nM3/+fPr160eP\nHj248MIL9wxfMXHixD1DU1cMePevf/2Lnj170rNnT3r16pVy6I106XcEIlIj110Htf2D2p49ITqP\nVumyyy7juuuu45prrgHgoYce4qmnnqJ58+Y89thjHHLIIaxbt45+/fpx/vnnJ71v7+TJkykoKGDx\n4sUsXLiQ3r1771k3btw4Dj30UHbt2sWZZ57JwoULufbaa5kwYQKzZs2iXbt2++xr7ty53Hfffbz8\n8su4O3379mXAgAEUFhby7rvvMmPGDO655x4uvfRSHnnkEa688sqk7+/rX/86d911FwMGDODmm2/m\nRz/6EXfeeSc///nPWbp0Kc2aNdvTHDV+/HgmTZpE//792bp1K82bN0/3MCelGoGI1Hm9evVi7dq1\nrFq1igULFlBYWEinTp1wd773ve/Ro0cPBg0axMqVK1mzZk3S/cyePXvPCblHjx706NFjz7qHHnqI\n3r1706tXL958800WLVqUMqZkw0xD+sNdQxgwb+PGjQwYMACAYcOGMXv27D0xDh06lGnTptGkSfje\n3r9/f66//nomTpzIxo0b9yzPhGoEIlIjqb65Z9Mll1zCzJkzWb16NZdddhkA06dPp6ysjLlz59K0\naVOKi4urHH66OkuXLmX8+PG8+uqrFBYWMnz48APaT4XKw11X1zSUzN/+9jdmz57NX//6V8aNG8fr\nr7/O2LFjOeecc3jiiSfo378/Tz31FCeckNmtXFQjEJF64bLLLuMPf/gDM2fO5JJLLgHCt+nDDjuM\npk2bMmvWLJYvX55yH1/4whd48MEHAXjjjTdYuHAhAJs3b6ZFixa0bt2aNWvW8OSTT+7ZJtkQ2MmG\nma6p1q1bU1hYuKc28cADDzBgwAB2797Nhx9+yBlnnMEvfvELNm3axNatW3n//ffp3r07N954Iyef\nfPKeW2lmQjUCEakXSkpK2LJlCx06dODII48EYOjQoZx33nl0796d0tLSar8Zjx49mhEjRtC1a1e6\ndu1Knz59ADjppJPo1asXJ5xwAp06daJ///57thk1ahSDBw/mqKOOYtasWXuWJxtmOlUzUDL3338/\nV199NeXl5Rx99NHcd9997Nq1iyuvvJJNmzbh7lx77bW0adOGH/zgB8yaNYtGjRpRUlLC2WefXePX\nq0zDUItItTQMdf1S02Go1TQkIpLnlAhERPKcEoGIpKW+NSPnqwP5OykRiEi1mjdvzvr165UM6jh3\nZ/369TX+kZmuGhKRanXs2JEVK1ZQVlYWdyhSjebNm9OxY8cabaNEICLVatq0KV26dIk7DMkSNQ2J\niOQ5JQIRkTynRCAikueUCERE8pwSgYhInlMiEBHJc0oEIiJ5TolARCTPKRGIiOQ5JQIRkTynRCAi\nkueUCERE8lzWEoGZ3Wtma83sjRRlBprZfDN708z+la1YREQkuWzWCKYCg5OtNLM2wK+B8929BLgk\ni7GIiEgSWUsE7j4b2JCiyFeBR939g6j82mzFIiIiycXZR3AcUGhmz5nZXDP7erKCZjbKzOaY2Rzd\nGENEpHbFmQiaAH2Ac4AvAz8ws+OqKujuU9y91N1L27dvn8sYRUQavDjvULYCWO/u24BtZjYbOAl4\nJ8aYRETyTpw1gj8Dp5lZEzMrAPoCi2OMR0QkL2WtRmBmM4CBQDszWwHcAjQFcPe73X2xmf0dWAjs\nBn7r7kkvNRURkezIWiJw9yvSKHMbcFu2YhARkerpl8UiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS\n55QIRETynBKBiEieUyIQEclzSgQiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETynBKBiEie\nUyIQEclzSgQiInlOiUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETynBKBiEieUyIQEclzSgQiInlO\niUBEJM8pEYiI5DklAhGRPKdEICKS55QIRETynBKBiEieUyIQEclzSgQiInlOiUBEJM9lLRGY2b1m\nttbM3qim3MlmttPMLs5WLCIiklw2awRTgcGpCphZY+AXwNNZjENERFLIWiJw99nAhmqK/Q/wCLA2\nW3GIiEhqsfURmFkH4EJgchplR5nZHDObU1ZWlv3gRETySJydxXcCN7r77uoKuvsUdy9199L27dvn\nIDQRkfzRJMbXLgX+YGYA7YAhZrbT3f8UY0wiInknrRqBmX3OzJpF8wPN7Foza5PJC7t7F3cvdvdi\nYCbwrWwngY8+yubeRUTqp3Sbhh4BdpnZMcAUoBPwYKoNzGwG8CJwvJmtMLOrzOxqM7s6o4gP0PTp\n0KkTvPNOHK8uIlJ3pds0tNvdd5rZhcBd7n6Xmb2WagN3vyLdINx9eLplD9SgQdCkCUyYAHffne1X\nExGpP9KtEewwsyuAYcDj0bKm2QkpOw4/HIYNg6lTYa0uVhUR2SPdRDACOBUY5+5LzawL8ED2wsqO\n66+Hzz6DX/0q7khEROqOtBKBuy9y92vdfYaZFQKt3P0XWY6t1h1/PJx/PkyaBNu2xR2NiEjdkO5V\nQ8+Z2SFmdigwD7jHzCZkN7TsGDMGNmwITUQiIpJ+01Brd98MXAT83t37AoOyF1b29O8Pp54aOo13\n7Yo7GhGR+KWbCJqY2ZHApeztLK63brgBliyBRx+NOxIRkfilmwhuBZ4C3nf3V83saODd7IWVXRdc\nAMccA7fdBu5xRyMiEq90O4sfdvce7j46er7E3f8ru6FlT+PG8N3vwquvwuzZcUcjIhKvdDuLO5rZ\nY9GNZtaa2SNm1jHbwWXTsGHQvj2MHx93JCIi8Uq3aeg+4C/AUdH012hZvXXwwfDtb8Pjj8OiRXFH\nIyISn3QTQXt3v8/dd0bTVKDejwf9rW+FhHD77XFHIiISn3QTwXozu9LMGkfTlcD6bAaWC+3awYgR\nMG2aRiYVkfyVbiL4BuHS0dXAR8DFwPAsxZRT118PO3fCxIlxRyIiEo90rxpa7u7nu3t7dz/M3b8C\n1NurhhJ97nNw0UVhRNItW+KORkQk9zK5VeX1tRZFzG64ATZuhN/9Lu5IRERyL5NEYLUWRcz69oXT\nT4c77oAdO+KORkQktzJJBA3qN7ljxsAHH8DDD8cdiYhIbqVMBGa2xcw2VzFtIfyeoME45xw44QQN\nOyEi+SdlInD3Vu5+SBVTK3dP9zaX9UKjRqGvYP58ePbZuKMREcmdTJqGGpyhQ8MtLW+7bd/l06dD\ncXFIFsXF4bmISEOhRJCgeXO49lp46ilYuDAsmz4dRo2C5ctDk9Hy5eG5koGINBRKBJWMHg0tWuwd\njO6mm6C8fN8y5eVhuYhIQ6BEUElhIYwcCTNmwIcfhiuJqpJsuYhIfaNEUIXrrgvNQBMnQufOVZdJ\ntlxEpL5RIqhCcTFccgn85jehCaigYN/1BQUwblwsoYmI1DolgiTGjAljD23cCFOmQFERmIXHKVPC\nFUYiIg1Bg/otQG3q3Ru++EX45S/Dje514heRhko1ghTGjIGVK0PHsYhIQ6VEkMKXvwzduoVLSTXs\nhIg0VEoEKZiFYSfeeCP8yExEpCFSIqjGFVdAhw77DzshItJQKBFU46CD4DvfCQPRzZsXdzQiIrVP\niSANo0ZBq1aqFYhIw6REkIbWrUMyePhh+Nvf4o5GRKR2ZS0RmNm9ZrbWzN5Isn6omS00s9fN7D9m\ndlK2YqkNN94IPXrAuefCzTfDrl1xRyQiUjuyWSOYCgxOsX4pMMDduwM/BqZkMZaMtW8P//43jBgB\nP/5xuKPZ+vVxRyUikrmsJQJ3nw1sSLH+P+7+cfT0JaBjtmKpLQcfDL/7XRhiYtYs6NMH5syJOyoR\nkczUlT6Cq4Ank600s1FmNsfM5pSVleUwrKpigW9+E154IfzIrH9/+O1vYw1JRCQjsScCMzuDkAhu\nTFbG3ae4e6m7l7Zv3z53waVw8skwdy4MHBgSw1VXwfbtcUclIlJzsSYCM+sB/Ba4wN3rXYt7u3bw\nxBPw/e/DvffCaafB0qVxRyUiUjOxJQIz6ww8CnzN3d+JK45MNW4cOo//8hd4//3Qb/Bk0kYuEZG6\nJ5uXj84AXgSON7MVZnaVmV1tZldHRW4G2gK/NrP5Zlavu13POy80FXXuHK4o+tGPYPfuuKMSEame\neT0bVrO0tNTn1OFLdcrLYfRo+P3v4eyzYdo0OPTQuKMSkXxnZnPdvbSqdbF3Fjc0BQUwdSpMngzP\nPBOail57Le6oRESSUyLIAjO4+mp4/nnYuRNOPRXuuy/uqEREqqZEkEV9+4YRS087Db7xjTBeUXl5\n3FGJiOxLiSDL2rcPN7X5f/8P7rkHTjoJZs+OOyoRkb2UCHKgcWP46U/DPQ1274YBA+B//ge2bo07\nMhERJYKcOuMMWLgQrr0WJk0Ko5k++2zcUYlIvlMiyLEWLeCXvwzNQ02awJlnho7lzZvjjkxE8pUS\nQUxOOw3mz4fvfjeMZtqtGzz9dHZea9GicD+Fvn1h7FhYvDg7ryMi9ZMSQYwKCmD8ePjPf0JN4ctf\nDoPXbdyY+b4//jj8lqFvXygpgdtvD6Oljh8PJ54Ylk+eHMqJSH5TIqgD+vULPzobOzb8GK1btwO7\nJebOnWEQvMsugyOOgG99K4yIOmECrFwJr7wSHm+/PSz/1rdCuUsvDa+3c2etvzURqQc0xEQd8+qr\n4S5ob74JX/sa3Hln9UNUvPkm3H8/PPAArF4dRkUdOhSGDYOePcMP3CpzD01T998P06fDunUhKQwd\nCsOHh2QkIg1HqiEmlAhyYPp0uOkm+OCDMCjduHHhhJvMp5/CT34CP/tZ+B3C3XfDBRfsW2bDBpgx\nI5zIX301dDyfc044iQ8ZAgcdlH58n30WahL33w+PPx5qBr17h31dcUVILBKP8nJYsCAMaLhoERx9\ndLgZUu/e0KxZ3NFJutxhyxbYtClcGLJ5877z6T6/7row2vGBUCKI0fTp+/+iuKAgdBCnSgYQmotG\njAgngssvhzvuCCeEqVPDsNeffRZ+oDZ8OHz1q3DYYZnHW1YWEszUqeH1mzaFc88NtYshQ8JzyY6t\nW8Mxnzcv/J3nzQsd+xWj2LZqFU4mEJJAaWlICv37h2FM6sg9mxq0XbvCCXnDhppPu3ZVv/9WraB1\nazjkkL1T4vNBg8L/4YFQIohRcTEsX77/8qIiWLas+u0/+wx+/vNQQ9ixIyxr1w6uvHJv00+2LFwY\nagnTpsHataGJqqQkvKcuXfadOnQIP5yT9GzatP9J/+23wzdHgCOPDN/6+/TZ+9ihQ2j6e/FF+Pe/\nw0UGc+fu/Vwcdxx8/vN7k8Pxx0Mj9QKmVF4evvyUlYXm0armE59v3Lj3b1SVQw4J/ydVTYWF0KbN\n/if3ivmWLbP791IiiFGjRlV/cMxqdr+C118PA9cNGBCGt65J00+mduwIw2Q8+ii89164C9vKlfu+\nr6ZNQ7NX5QRRkTQOO6zqvoqGzD2cQJYtC9OSJeHkP3duOI4VOnbce7KvOPEfeWR6r7F9O8yZE5JC\nRXJYH93r79BDQ02hf/+QIE4+OdQktm0LNYstW0ItpPJ8Vcu2bAknzQ4doGvXvdPRR9edWuKOHeG9\nr1u39zFxWr9+/5N7srG/mjQJX7jat9/3sV275Cf6Nm3qzrGoihJBjDKtEdRVn30W+jyWLt1/WrYs\n1CASFRSEY9GpUziZdOy4d6p4XlhYe8li1669//hr14bHDRtCHK1bh3/axMdDDql5jcY9vEbFiT5x\nqjgOlU80RUX7fsvv3bt2mvQSY3rnnZAUKhLDW2+FdWapv81WVlAQmipatgyPBx8cPsurVu0t07Qp\nHHPMvsmha9dQG2nR4sDfx2efhWNbMVWcyJOd4NetC7WsZFq2hLZtwwk98eSe7Hnr1g3vi4sSQYwy\n6SOoz7Zt23tCTEwQK1aEafXq/U9KBx+8b2KoPH/44aHTLPHknjifuGz9+pqd9GBv+2zlJFEx36IF\nfPTRvif8bdv23UdhYUh4yaZDDqn5sczUunXw0kvhogKz8D4TT/CJ8xWPLVokT4ybN4fksnjx3umt\nt8KtWhPbwTt33psYTjghJIedO6s+sVeer+gLqUrFSb3iG3q7dvs+r2qdOtaVCGJX06uG8sGOHSEZ\nrFgRmpkqEkTi85Ur97Z/J2MWquXt24dv1hXf6KqaP/TQ0JSyaVNo6018TGfZzp0hISQ7yRcVhfX5\n6tNPQ5NXRWJITBLbt1e9TevW4URdcfJOfEy2rHnz3L6vhkKJQOql3bvDN8SKBLFmTfhGnXhyb9s2\ntOdmm3tortA3y5rbvRs+/DA0WTVrtvfEfuihdbtNvaFJlQhy8C8kcmAaNQon/MMOC23pcTJTEjhQ\njRqF2lJRUdyRSDK6uExEJBE/FFIAAApDSURBVM8pEYiI5DklAhGRPKdEICKS55QIRETynBJBPTB9\nerhOvVGj8Dh9etwRiUhDostH67jKv0xevjw8B/0oTURqh2oEddxNN+0/Xk15eVguIlIblAjquA8+\nqNlyEZGaUiKo4zp3rtlyEZGaUiKo48aNC6OVJiooCMtFRGqDEkEdN3RoGLK6qCiMd1NUVPMhrHXV\nkYikoquG6oGhQw/8CiFddSQi1VGNoIHTVUciUp2sJQIzu9fM1prZG0nWm5lNNLP3zGyhmcU80HDD\npKuORKQ62awRTAUGp1h/NnBsNI0CJmcxlrylq45EpDpZSwTuPhvYkKLIBcDvPXgJaGNmR2Yrnnyl\nq45EpDpx9hF0AD5MeL4iWrYfMxtlZnPMbE5ZWVlOgmsoauOqI9CVRyINWb24asjdpwBTINyzOOZw\n6p1MrjoCXXkk0tDFWSNYCXRKeN4xWiZ1jK48EmnY4kwEfwG+Hl091A/Y5O4fxRiPJFEbVx6paUmk\n7spa05CZzQAGAu3MbAVwC9AUwN3vBp4AhgDvAeXAiGzFIpnp3Dk0B1W1PB1qWhKp28y9fjW5l5aW\n+pw5c+IOI69UPpFDuPIo3U7n4uKqE0lRESxbVltRikgqZjbX3UurWqdfFku1Mr3ySD9qE6nblAgk\nLUOHhm/vu3eHx5o06dTGj9rUxyCSPUoEknWZ/qitomlq+XJw39vHoGQgUjuUCCTrMm1aqo3LV1Wj\nEElOncVS5zVqFGoClZmFpqrqZNrZLdIQqLNY6rVM+xhUoxBJTYlA6rxM+xgyvWqpNvoolEikLlMi\nkDov0z6GuGsU6uyWuk6JQOqFTC5fjbtGoaYpqeuUCKTBi7tGoaYpqeuUCCQvxFmjaAhNU0okDZsS\ngUg1Mq1R1PemKSWSPODu9Wrq06ePi9Q306a5FxW5m4XHadPS37aoyD2cgvediorS296s6u3NcvP6\n06a5FxTsu21BQc2OQSbHrza2bwiAOZ7kvBr7ib2mkxKB5JtMT6T5nkhqIxE1BKkSgZqGROq4uJum\n4u4sz7Rpqy5ctVXnm8aSZYi6OqlGIFJzmTSN1PcaSabb14UaSW00baGmIRHJRH1OJPV9+9pq2lIi\nEJFYxZlIMt0+7hpJpomkQqpEoD4CEcm6TH7HkWkfSdw/KIy7jyUdSgQiUudlkkgy3T7Tzva4O+vT\noUQgIpJC3DWSTBNJOnRjGhGROm769HC56wcfhJrAuHE1rxWlujFNk9oIUkREsmfo0OzeTU9NQyIi\neU6JQEQkzykRiIjkOSUCEZE8p0QgIpLn6t3lo2ZWBiyPO44k2gHr4g4ihboeH9T9GBVfZhRfZjKJ\nr8jd21e1ot4lgrrMzOYku063Lqjr8UHdj1HxZUbxZSZb8alpSEQkzykRiIjkOSWC2jUl7gCqUdfj\ng7ofo+LLjOLLTFbiUx+BiEieU41ARCTPKRGIiOQ5JYIaMrNOZjbLzBaZ2Ztm9p0qygw0s01mNj+a\nbs5xjMvM7PXotfcbs9uCiWb2npktNLPeOYzt+ITjMt/MNpvZdZXK5Pz4mdm9ZrbWzN5IWHaomf3D\nzN6NHguTbDssKvOumQ3LYXy3mdlb0d/wMTNrk2TblJ+HLMb3QzNbmfB3HJJk28Fm9nb0eRybw/j+\nmBDbMjObn2TbrB6/ZOeUnH7+kt3DUlOSmzzDkUDvaL4V8A5wYqUyA4HHY4xxGdAuxfohwJOAAf2A\nl2OKszGwmvBDl1iPH/AFoDfwRsKy/wXGRvNjgV9Usd2hwJLosTCaL8xRfGcBTaL5X1QVXzqfhyzG\n90PghjQ+A+8DRwMHAQsq/z9lK75K628Hbo7j+CU7p+Ty86caQQ25+0fuPi+a3wIsBjrEG1WNXQD8\n3oOXgDZmdmQMcZwJvO/usf9S3N1nAxsqLb4AuD+avx/4ShWbfhn4h7tvcPePgX8Ag3MRn7s/7e47\no6cvAR1r+3XTleT4peMU4D13X+LunwF/IBz3WpUqPjMz4FJgRm2/bjpSnFNy9vlTIsiAmRUDvYCX\nq1h9qpktMLMnzawkp4GBA0+b2VwzG1XF+g7AhwnPVxBPMruc5P98cR6/Coe7+0fR/Grg8CrK1JVj\n+Q1CLa8q1X0esunbUdPVvUmaNurC8TsdWOPu7yZZn7PjV+mckrPPnxLBATKzlsAjwHXuvrnS6nmE\n5o6TgLuAP+U4vNPcvTdwNnCNmX0hx69fLTM7CDgfeLiK1XEfv/14qIfXyWutzewmYCcwPUmRuD4P\nk4HPAT2BjwjNL3XRFaSuDeTk+KU6p2T786dEcADMrCnhDzbd3R+tvN7dN7v71mj+CaCpmbXLVXzu\nvjJ6XAs8Rqh+J1oJdEp43jFalktnA/PcfU3lFXEfvwRrKprMose1VZSJ9Via2XDgXGBodLLYTxqf\nh6xw9zXuvsvddwP3JHnduI9fE+Ai4I/JyuTi+CU5p+Ts86dEUENRe+LvgMXuPiFJmSOicpjZKYTj\nvD5H8bUws1YV84QOxTcqFfsL8PXo6qF+wKaEKmiuJP0WFufxq+QvQMVVGMOAP1dR5ingLDMrjJo+\nzoqWZZ2ZDQb+L3C+u5cnKZPO5yFb8SX2O12Y5HVfBY41sy5RLfFywnHPlUHAW+6+oqqVuTh+Kc4p\nufv8ZasnvKFOwGmEKtpCYH40DQGuBq6OynwbeJNwBcRLwOdzGN/R0esuiGK4KVqeGJ8BkwhXa7wO\nlOb4GLYgnNhbJyyL9fgRktJHwA5CO+tVQFvgn8C7wDPAoVHZUuC3Cdt+A3gvmkbkML73CO3DFZ/D\nu6OyRwFPpPo85Ci+B6LP10LCSe3IyvFFz4cQrpR5P5fxRcunVnzuEsrm9PilOKfk7POnISZERPKc\nmoZERPKcEoGISJ5TIhARyXNKBCIieU6JQEQkzykRiETMbJftOzJqrY2EaWbFiSNfitQlTeIOQKQO\n2e7uPeMOQiTXVCMQqUY0Hv3/RmPSv2Jmx0TLi83s2WhQtX+aWedo+eEW7g+wIJo+H+2qsZndE405\n/7SZHRyVvzYai36hmf0hprcpeUyJQGSvgys1DV2WsG6Tu3cHfgXcGS27C7jf3XsQBnybGC2fCPzL\nw6B5vQm/SAU4Fpjk7iXARuC/ouVjgV7Rfq7O1psTSUa/LBaJmNlWd29ZxfJlwBfdfUk0ONhqd29r\nZusIwybsiJZ/5O7tzKwM6Ojunybso5gwbvyx0fMbgabu/hMz+zuwlTDK6p88GnBPJFdUIxBJjyeZ\nr4lPE+Z3sbeP7hzC2E+9gVejETFFckaJQCQ9lyU8vhjN/4cwWibAUOD5aP6fwGgAM2tsZq2T7dTM\nGgGd3H0WcCPQGtivViKSTfrmIbLXwbbvDcz/7u4Vl5AWmtlCwrf6K6Jl/wPcZ2ZjgDJgRLT8O8AU\nM7uK8M1/NGHky6o0BqZFycKAie6+sdbekUga1EcgUo2oj6DU3dfFHYtINqhpSEQkz6lGICKS51Qj\nEBHJc0oEIiJ5TolARCTPKRGIiOQ5JQIRkTz3/wHmKLC/ZkJkVgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "980fe197-dbf2-4224-ee2f-19b923512040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb8514e9e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nflmCMMYY45clCGOMMX79P/1CyRG35dRoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "71b7d601-3890-434e-b655-506c1fc78a90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.7668 - acc: 0.3511\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.3724 - acc: 0.4351\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.2067 - acc: 0.4580\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.1108 - acc: 0.5115\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.0630 - acc: 0.5191\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0310 - acc: 0.5420\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9993 - acc: 0.5344\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9810 - acc: 0.5267\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9631 - acc: 0.5344\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9556 - acc: 0.5573\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 0s 222us/step - loss: 0.9397 - acc: 0.5649\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9327 - acc: 0.5420\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9262 - acc: 0.5878\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9172 - acc: 0.5573\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9103 - acc: 0.5802\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9135 - acc: 0.5573\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.8953 - acc: 0.5878\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9091 - acc: 0.5802\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.8896 - acc: 0.5954\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8897 - acc: 0.5802\n",
            "34/34 [==============================] - 0s 4ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "311e1179-f2a8-421b-e415-71b0468617d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "06c3c06b-4b67-4c8a-98e3-da7f26ee15a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2647058823529412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}
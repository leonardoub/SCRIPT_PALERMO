{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "14591ebb-421a-4391-d73a-aaf574ab0c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "a2093a71-5287-4a8a-83e5-199fedfbc7e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "ad5ee96e-283b-4065-9e1d-3207df6d3a83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "ccee2803-f658-4dca-cce8-88b7ffd4b29d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "f3d3806b-9d82-4d09-a157-db800481ef57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "76724a74-9eb3-4eaa-e0b7-e76b933ec430",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "ad629af1-98ff-4957-ebb2-2387f75bbcaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "3e925f49-a20a-4c23-c48a-ae4ee91ea521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 2.3096 - acc: 0.2222 - val_loss: 2.1010 - val_acc: 0.2857\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 2.1328 - acc: 0.2479 - val_loss: 2.0231 - val_acc: 0.2857\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.9829 - acc: 0.2479 - val_loss: 1.9630 - val_acc: 0.2857\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 1.8542 - acc: 0.2564 - val_loss: 1.9163 - val_acc: 0.2857\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 1.7446 - acc: 0.2906 - val_loss: 1.8820 - val_acc: 0.2857\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 1.6479 - acc: 0.3248 - val_loss: 1.8530 - val_acc: 0.2857\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.5652 - acc: 0.3419 - val_loss: 1.8255 - val_acc: 0.2857\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.4920 - acc: 0.3846 - val_loss: 1.8077 - val_acc: 0.2857\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.4299 - acc: 0.3675 - val_loss: 1.7934 - val_acc: 0.2857\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 1.3747 - acc: 0.3761 - val_loss: 1.7743 - val_acc: 0.2857\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 1.3258 - acc: 0.4017 - val_loss: 1.7521 - val_acc: 0.3571\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.2835 - acc: 0.4103 - val_loss: 1.7353 - val_acc: 0.4286\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 1.2461 - acc: 0.4188 - val_loss: 1.7144 - val_acc: 0.4286\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 1.2144 - acc: 0.4188 - val_loss: 1.7012 - val_acc: 0.4286\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 1.1861 - acc: 0.4188 - val_loss: 1.6852 - val_acc: 0.4286\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 1.1603 - acc: 0.4274 - val_loss: 1.6764 - val_acc: 0.4286\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 1.1367 - acc: 0.4103 - val_loss: 1.6611 - val_acc: 0.4286\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.1158 - acc: 0.4359 - val_loss: 1.6464 - val_acc: 0.4286\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 1.0973 - acc: 0.4359 - val_loss: 1.6363 - val_acc: 0.4286\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 1.0808 - acc: 0.4530 - val_loss: 1.6241 - val_acc: 0.4286\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 1.0658 - acc: 0.4701 - val_loss: 1.6057 - val_acc: 0.4286\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 253us/step - loss: 1.0521 - acc: 0.4701 - val_loss: 1.5970 - val_acc: 0.4286\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 1.0401 - acc: 0.4872 - val_loss: 1.5877 - val_acc: 0.4286\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 1.0284 - acc: 0.4872 - val_loss: 1.5802 - val_acc: 0.4286\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 1.0192 - acc: 0.4872 - val_loss: 1.5749 - val_acc: 0.4286\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 1.0103 - acc: 0.5043 - val_loss: 1.5682 - val_acc: 0.4286\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 1.0019 - acc: 0.5128 - val_loss: 1.5566 - val_acc: 0.4286\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.9940 - acc: 0.5128 - val_loss: 1.5539 - val_acc: 0.4286\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.9868 - acc: 0.5214 - val_loss: 1.5442 - val_acc: 0.4286\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 273us/step - loss: 0.9804 - acc: 0.5299 - val_loss: 1.5400 - val_acc: 0.4286\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9747 - acc: 0.5299 - val_loss: 1.5371 - val_acc: 0.3571\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.9687 - acc: 0.5470 - val_loss: 1.5301 - val_acc: 0.3571\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9635 - acc: 0.5470 - val_loss: 1.5264 - val_acc: 0.3571\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9586 - acc: 0.5385 - val_loss: 1.5235 - val_acc: 0.3571\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9540 - acc: 0.5470 - val_loss: 1.5193 - val_acc: 0.2857\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9495 - acc: 0.5641 - val_loss: 1.5178 - val_acc: 0.2857\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9450 - acc: 0.5726 - val_loss: 1.5090 - val_acc: 0.2857\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9414 - acc: 0.5726 - val_loss: 1.5061 - val_acc: 0.2143\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9376 - acc: 0.5726 - val_loss: 1.5032 - val_acc: 0.2143\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.9348 - acc: 0.5897 - val_loss: 1.5023 - val_acc: 0.2143\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9316 - acc: 0.5983 - val_loss: 1.5005 - val_acc: 0.2143\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.9286 - acc: 0.5983 - val_loss: 1.4976 - val_acc: 0.2143\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.9257 - acc: 0.6068 - val_loss: 1.4911 - val_acc: 0.2143\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9234 - acc: 0.5897 - val_loss: 1.4948 - val_acc: 0.2143\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9206 - acc: 0.5983 - val_loss: 1.4903 - val_acc: 0.2143\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.9185 - acc: 0.5983 - val_loss: 1.4859 - val_acc: 0.2143\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.9160 - acc: 0.5983 - val_loss: 1.4884 - val_acc: 0.2143\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 248us/step - loss: 0.9134 - acc: 0.5897 - val_loss: 1.4883 - val_acc: 0.2143\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9111 - acc: 0.5983 - val_loss: 1.4868 - val_acc: 0.2143\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9098 - acc: 0.5983 - val_loss: 1.4853 - val_acc: 0.2857\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.9076 - acc: 0.5983 - val_loss: 1.4845 - val_acc: 0.2857\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.9061 - acc: 0.6068 - val_loss: 1.4825 - val_acc: 0.2857\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9045 - acc: 0.5983 - val_loss: 1.4820 - val_acc: 0.2857\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9025 - acc: 0.6068 - val_loss: 1.4835 - val_acc: 0.2857\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9009 - acc: 0.6068 - val_loss: 1.4835 - val_acc: 0.2857\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9001 - acc: 0.6068 - val_loss: 1.4836 - val_acc: 0.2857\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.8980 - acc: 0.6239 - val_loss: 1.4829 - val_acc: 0.2857\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.8963 - acc: 0.6239 - val_loss: 1.4834 - val_acc: 0.2857\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.8955 - acc: 0.6068 - val_loss: 1.4784 - val_acc: 0.2857\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.8942 - acc: 0.6068 - val_loss: 1.4789 - val_acc: 0.2857\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.8926 - acc: 0.6068 - val_loss: 1.4798 - val_acc: 0.2857\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.8916 - acc: 0.6154 - val_loss: 1.4806 - val_acc: 0.2857\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.8904 - acc: 0.6068 - val_loss: 1.4776 - val_acc: 0.2857\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.8893 - acc: 0.6154 - val_loss: 1.4764 - val_acc: 0.2857\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 247us/step - loss: 0.8879 - acc: 0.6068 - val_loss: 1.4730 - val_acc: 0.2857\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.8872 - acc: 0.6068 - val_loss: 1.4750 - val_acc: 0.2857\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 241us/step - loss: 0.8858 - acc: 0.6154 - val_loss: 1.4728 - val_acc: 0.2857\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.8850 - acc: 0.6154 - val_loss: 1.4749 - val_acc: 0.2857\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.8839 - acc: 0.6154 - val_loss: 1.4737 - val_acc: 0.2857\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.8834 - acc: 0.6068 - val_loss: 1.4748 - val_acc: 0.2857\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.8824 - acc: 0.6154 - val_loss: 1.4725 - val_acc: 0.2857\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.8812 - acc: 0.6154 - val_loss: 1.4713 - val_acc: 0.2857\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.8803 - acc: 0.6154 - val_loss: 1.4704 - val_acc: 0.2857\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.8790 - acc: 0.6068 - val_loss: 1.4702 - val_acc: 0.2857\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.8782 - acc: 0.6154 - val_loss: 1.4718 - val_acc: 0.2857\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.8772 - acc: 0.6154 - val_loss: 1.4697 - val_acc: 0.2857\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.8766 - acc: 0.6154 - val_loss: 1.4682 - val_acc: 0.2857\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.8762 - acc: 0.6154 - val_loss: 1.4676 - val_acc: 0.2857\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.8751 - acc: 0.6154 - val_loss: 1.4656 - val_acc: 0.2857\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.8747 - acc: 0.6154 - val_loss: 1.4658 - val_acc: 0.2857\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.8738 - acc: 0.6154 - val_loss: 1.4669 - val_acc: 0.2857\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.8732 - acc: 0.6154 - val_loss: 1.4634 - val_acc: 0.2857\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 250us/step - loss: 0.8725 - acc: 0.6154 - val_loss: 1.4637 - val_acc: 0.2857\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 241us/step - loss: 0.8715 - acc: 0.6154 - val_loss: 1.4634 - val_acc: 0.2857\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.8713 - acc: 0.6154 - val_loss: 1.4645 - val_acc: 0.2857\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.8701 - acc: 0.6154 - val_loss: 1.4663 - val_acc: 0.2857\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.8698 - acc: 0.6154 - val_loss: 1.4640 - val_acc: 0.2857\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 157us/step - loss: 0.8692 - acc: 0.6154 - val_loss: 1.4662 - val_acc: 0.2857\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.8686 - acc: 0.6154 - val_loss: 1.4677 - val_acc: 0.2857\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.8682 - acc: 0.6154 - val_loss: 1.4674 - val_acc: 0.2857\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.8671 - acc: 0.6154 - val_loss: 1.4674 - val_acc: 0.2857\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.8668 - acc: 0.6154 - val_loss: 1.4667 - val_acc: 0.2857\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.8665 - acc: 0.6154 - val_loss: 1.4664 - val_acc: 0.2857\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.8657 - acc: 0.6154 - val_loss: 1.4662 - val_acc: 0.2857\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.8652 - acc: 0.6154 - val_loss: 1.4650 - val_acc: 0.2857\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.8643 - acc: 0.6154 - val_loss: 1.4654 - val_acc: 0.2857\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.8643 - acc: 0.6154 - val_loss: 1.4666 - val_acc: 0.2857\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.8635 - acc: 0.6154 - val_loss: 1.4627 - val_acc: 0.2857\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.8631 - acc: 0.6154 - val_loss: 1.4609 - val_acc: 0.2857\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.8626 - acc: 0.6154 - val_loss: 1.4636 - val_acc: 0.2857\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 1ms/step - loss: 2.3896 - acc: 0.2966 - val_loss: 1.8987 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 2.1670 - acc: 0.2966 - val_loss: 1.7906 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.9649 - acc: 0.2881 - val_loss: 1.7118 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.7938 - acc: 0.2966 - val_loss: 1.6331 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.6545 - acc: 0.3051 - val_loss: 1.5842 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.5383 - acc: 0.3305 - val_loss: 1.5469 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.4421 - acc: 0.3305 - val_loss: 1.5240 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.3650 - acc: 0.3983 - val_loss: 1.5165 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.3079 - acc: 0.4237 - val_loss: 1.5131 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.2625 - acc: 0.4322 - val_loss: 1.5169 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.2307 - acc: 0.4576 - val_loss: 1.5192 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2003 - acc: 0.5254 - val_loss: 1.5289 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1772 - acc: 0.5508 - val_loss: 1.5436 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1584 - acc: 0.5593 - val_loss: 1.5603 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1383 - acc: 0.5508 - val_loss: 1.5753 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1267 - acc: 0.5508 - val_loss: 1.5885 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1133 - acc: 0.5508 - val_loss: 1.5835 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1022 - acc: 0.5508 - val_loss: 1.5844 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0920 - acc: 0.5932 - val_loss: 1.5949 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0828 - acc: 0.6017 - val_loss: 1.6014 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0765 - acc: 0.5932 - val_loss: 1.5970 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0666 - acc: 0.5932 - val_loss: 1.6002 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0604 - acc: 0.5847 - val_loss: 1.6090 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0531 - acc: 0.5932 - val_loss: 1.5989 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0463 - acc: 0.5932 - val_loss: 1.6057 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0413 - acc: 0.5932 - val_loss: 1.6031 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0343 - acc: 0.5932 - val_loss: 1.6003 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0313 - acc: 0.5932 - val_loss: 1.5946 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0248 - acc: 0.5847 - val_loss: 1.5983 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0206 - acc: 0.5932 - val_loss: 1.6019 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0186 - acc: 0.5932 - val_loss: 1.6010 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 1.0127 - acc: 0.5932 - val_loss: 1.5986 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0093 - acc: 0.5932 - val_loss: 1.5972 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 1.0080 - acc: 0.5932 - val_loss: 1.5925 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0027 - acc: 0.5932 - val_loss: 1.5978 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 279us/step - loss: 0.9988 - acc: 0.5932 - val_loss: 1.5991 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9964 - acc: 0.5932 - val_loss: 1.5999 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9927 - acc: 0.5932 - val_loss: 1.6036 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9893 - acc: 0.6017 - val_loss: 1.5976 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9864 - acc: 0.6017 - val_loss: 1.5944 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9848 - acc: 0.6017 - val_loss: 1.5906 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9797 - acc: 0.6102 - val_loss: 1.5931 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9770 - acc: 0.6102 - val_loss: 1.5931 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9753 - acc: 0.6102 - val_loss: 1.5934 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9728 - acc: 0.6102 - val_loss: 1.5934 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9712 - acc: 0.6102 - val_loss: 1.5988 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9698 - acc: 0.6102 - val_loss: 1.5997 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9665 - acc: 0.6102 - val_loss: 1.5969 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 269us/step - loss: 0.9653 - acc: 0.6017 - val_loss: 1.5993 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9620 - acc: 0.6017 - val_loss: 1.5998 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 293us/step - loss: 0.9601 - acc: 0.6102 - val_loss: 1.5991 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9586 - acc: 0.6102 - val_loss: 1.6032 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 277us/step - loss: 0.9573 - acc: 0.6102 - val_loss: 1.6018 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9556 - acc: 0.5932 - val_loss: 1.6029 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9533 - acc: 0.5932 - val_loss: 1.6037 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9535 - acc: 0.6017 - val_loss: 1.6033 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9517 - acc: 0.5847 - val_loss: 1.6039 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9494 - acc: 0.5847 - val_loss: 1.6026 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9484 - acc: 0.5847 - val_loss: 1.6030 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9464 - acc: 0.5932 - val_loss: 1.6033 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9463 - acc: 0.6017 - val_loss: 1.5997 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9431 - acc: 0.5847 - val_loss: 1.6007 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9430 - acc: 0.5932 - val_loss: 1.6014 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9407 - acc: 0.5932 - val_loss: 1.6004 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9400 - acc: 0.5932 - val_loss: 1.6003 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9384 - acc: 0.5932 - val_loss: 1.6010 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9365 - acc: 0.5932 - val_loss: 1.6019 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9354 - acc: 0.5932 - val_loss: 1.6014 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9365 - acc: 0.5932 - val_loss: 1.6019 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9341 - acc: 0.5932 - val_loss: 1.6025 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9323 - acc: 0.5932 - val_loss: 1.6012 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9317 - acc: 0.5932 - val_loss: 1.6054 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9305 - acc: 0.5932 - val_loss: 1.6009 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9288 - acc: 0.5932 - val_loss: 1.6023 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9275 - acc: 0.5932 - val_loss: 1.6023 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9279 - acc: 0.5932 - val_loss: 1.6023 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9250 - acc: 0.5932 - val_loss: 1.6037 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9246 - acc: 0.5932 - val_loss: 1.6038 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9231 - acc: 0.5932 - val_loss: 1.6017 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9231 - acc: 0.5932 - val_loss: 1.6018 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9210 - acc: 0.5932 - val_loss: 1.6042 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9200 - acc: 0.5932 - val_loss: 1.6039 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9212 - acc: 0.5847 - val_loss: 1.6028 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9190 - acc: 0.5932 - val_loss: 1.6016 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9179 - acc: 0.5932 - val_loss: 1.6021 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9174 - acc: 0.5932 - val_loss: 1.6012 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9155 - acc: 0.5932 - val_loss: 1.6021 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9153 - acc: 0.5932 - val_loss: 1.6009 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9146 - acc: 0.5932 - val_loss: 1.5969 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9125 - acc: 0.5932 - val_loss: 1.6017 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9121 - acc: 0.5932 - val_loss: 1.5955 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9117 - acc: 0.5932 - val_loss: 1.5954 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9118 - acc: 0.5932 - val_loss: 1.5925 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9098 - acc: 0.5932 - val_loss: 1.5938 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9092 - acc: 0.5932 - val_loss: 1.5935 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9087 - acc: 0.6017 - val_loss: 1.5943 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9078 - acc: 0.6017 - val_loss: 1.5967 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9067 - acc: 0.6017 - val_loss: 1.5958 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9064 - acc: 0.6017 - val_loss: 1.5942 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9059 - acc: 0.6017 - val_loss: 1.5906 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.8060 - acc: 0.3644 - val_loss: 2.3879 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 2.5637 - acc: 0.3644 - val_loss: 2.2563 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 2.3582 - acc: 0.3559 - val_loss: 2.1373 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.1788 - acc: 0.3559 - val_loss: 2.0258 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 2.0161 - acc: 0.3475 - val_loss: 1.9284 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.8790 - acc: 0.3475 - val_loss: 1.8419 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.7570 - acc: 0.3475 - val_loss: 1.7675 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.6553 - acc: 0.3559 - val_loss: 1.6986 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.5641 - acc: 0.3644 - val_loss: 1.6376 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.4867 - acc: 0.3814 - val_loss: 1.5832 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.4186 - acc: 0.3814 - val_loss: 1.5358 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.3609 - acc: 0.3983 - val_loss: 1.4922 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.3103 - acc: 0.3983 - val_loss: 1.4551 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.2653 - acc: 0.3983 - val_loss: 1.4192 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.2267 - acc: 0.3898 - val_loss: 1.3891 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.1947 - acc: 0.3983 - val_loss: 1.3606 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.1657 - acc: 0.4153 - val_loss: 1.3356 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1427 - acc: 0.4322 - val_loss: 1.3117 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.1221 - acc: 0.4237 - val_loss: 1.2934 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 1.1035 - acc: 0.4322 - val_loss: 1.2778 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0876 - acc: 0.4407 - val_loss: 1.2616 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0735 - acc: 0.4492 - val_loss: 1.2480 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0621 - acc: 0.4576 - val_loss: 1.2353 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0515 - acc: 0.4661 - val_loss: 1.2266 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0435 - acc: 0.4576 - val_loss: 1.2174 - val_acc: 0.2308\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0355 - acc: 0.4661 - val_loss: 1.2111 - val_acc: 0.2308\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0285 - acc: 0.4746 - val_loss: 1.2025 - val_acc: 0.2308\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0223 - acc: 0.4661 - val_loss: 1.1971 - val_acc: 0.2308\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0175 - acc: 0.4746 - val_loss: 1.1925 - val_acc: 0.2308\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0123 - acc: 0.5000 - val_loss: 1.1876 - val_acc: 0.2308\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0085 - acc: 0.4831 - val_loss: 1.1846 - val_acc: 0.3077\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0047 - acc: 0.4915 - val_loss: 1.1807 - val_acc: 0.3077\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0009 - acc: 0.4915 - val_loss: 1.1781 - val_acc: 0.3077\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9981 - acc: 0.4831 - val_loss: 1.1773 - val_acc: 0.3077\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9957 - acc: 0.4915 - val_loss: 1.1738 - val_acc: 0.3077\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9919 - acc: 0.4831 - val_loss: 1.1728 - val_acc: 0.3077\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9895 - acc: 0.4915 - val_loss: 1.1735 - val_acc: 0.3077\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9872 - acc: 0.5000 - val_loss: 1.1730 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9848 - acc: 0.5085 - val_loss: 1.1709 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9826 - acc: 0.5085 - val_loss: 1.1712 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9804 - acc: 0.5169 - val_loss: 1.1707 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9782 - acc: 0.5169 - val_loss: 1.1706 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9766 - acc: 0.5169 - val_loss: 1.1711 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9745 - acc: 0.5254 - val_loss: 1.1720 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9725 - acc: 0.5254 - val_loss: 1.1727 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9712 - acc: 0.5169 - val_loss: 1.1746 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9692 - acc: 0.5254 - val_loss: 1.1741 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9677 - acc: 0.5339 - val_loss: 1.1747 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9666 - acc: 0.5254 - val_loss: 1.1765 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9644 - acc: 0.5254 - val_loss: 1.1788 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9633 - acc: 0.5254 - val_loss: 1.1794 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9620 - acc: 0.5424 - val_loss: 1.1811 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9601 - acc: 0.5424 - val_loss: 1.1840 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9575 - acc: 0.5424 - val_loss: 1.1861 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9558 - acc: 0.5424 - val_loss: 1.1878 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9550 - acc: 0.5424 - val_loss: 1.1893 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.9529 - acc: 0.5339 - val_loss: 1.1908 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9507 - acc: 0.5424 - val_loss: 1.1925 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.9496 - acc: 0.5339 - val_loss: 1.1945 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9487 - acc: 0.5339 - val_loss: 1.1959 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9474 - acc: 0.5339 - val_loss: 1.1980 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9454 - acc: 0.5339 - val_loss: 1.1995 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9438 - acc: 0.5339 - val_loss: 1.2028 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9419 - acc: 0.5339 - val_loss: 1.2034 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9409 - acc: 0.5339 - val_loss: 1.2057 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9396 - acc: 0.5254 - val_loss: 1.2072 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9380 - acc: 0.5339 - val_loss: 1.2088 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9378 - acc: 0.5339 - val_loss: 1.2107 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9362 - acc: 0.5424 - val_loss: 1.2127 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9353 - acc: 0.5424 - val_loss: 1.2136 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9346 - acc: 0.5339 - val_loss: 1.2148 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9331 - acc: 0.5424 - val_loss: 1.2169 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9320 - acc: 0.5254 - val_loss: 1.2186 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9316 - acc: 0.5339 - val_loss: 1.2194 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9299 - acc: 0.5254 - val_loss: 1.2219 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9293 - acc: 0.5339 - val_loss: 1.2237 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9283 - acc: 0.5508 - val_loss: 1.2251 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9269 - acc: 0.5339 - val_loss: 1.2273 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9266 - acc: 0.5424 - val_loss: 1.2290 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9255 - acc: 0.5424 - val_loss: 1.2310 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9243 - acc: 0.5339 - val_loss: 1.2322 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9233 - acc: 0.5339 - val_loss: 1.2348 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9230 - acc: 0.5339 - val_loss: 1.2357 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9220 - acc: 0.5593 - val_loss: 1.2362 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9213 - acc: 0.5424 - val_loss: 1.2382 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9202 - acc: 0.5424 - val_loss: 1.2408 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9195 - acc: 0.5424 - val_loss: 1.2423 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9188 - acc: 0.5508 - val_loss: 1.2427 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9185 - acc: 0.5508 - val_loss: 1.2451 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9176 - acc: 0.5508 - val_loss: 1.2459 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9167 - acc: 0.5508 - val_loss: 1.2475 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9159 - acc: 0.5424 - val_loss: 1.2487 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9162 - acc: 0.5424 - val_loss: 1.2503 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9147 - acc: 0.5508 - val_loss: 1.2515 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9149 - acc: 0.5508 - val_loss: 1.2529 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9134 - acc: 0.5508 - val_loss: 1.2545 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9128 - acc: 0.5508 - val_loss: 1.2558 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9131 - acc: 0.5508 - val_loss: 1.2572 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9118 - acc: 0.5508 - val_loss: 1.2588 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9113 - acc: 0.5508 - val_loss: 1.2609 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 3.5876 - acc: 0.3051 - val_loss: 3.5501 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 3.2456 - acc: 0.3051 - val_loss: 3.1464 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 2.8699 - acc: 0.3136 - val_loss: 2.7817 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 2.5234 - acc: 0.3136 - val_loss: 2.4778 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.2381 - acc: 0.3305 - val_loss: 2.1781 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 2.0112 - acc: 0.3559 - val_loss: 1.9447 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.8348 - acc: 0.3983 - val_loss: 1.7615 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.7066 - acc: 0.4068 - val_loss: 1.6217 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.6172 - acc: 0.3898 - val_loss: 1.5299 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.5494 - acc: 0.3898 - val_loss: 1.4561 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.4987 - acc: 0.3814 - val_loss: 1.3960 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.4533 - acc: 0.3983 - val_loss: 1.3544 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4182 - acc: 0.4068 - val_loss: 1.3065 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.3896 - acc: 0.4153 - val_loss: 1.2834 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.3642 - acc: 0.4153 - val_loss: 1.2630 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.3435 - acc: 0.4237 - val_loss: 1.2430 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.3240 - acc: 0.4322 - val_loss: 1.2277 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.3069 - acc: 0.4322 - val_loss: 1.2136 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.2904 - acc: 0.4407 - val_loss: 1.1995 - val_acc: 0.2308\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2755 - acc: 0.4492 - val_loss: 1.1929 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.2619 - acc: 0.4407 - val_loss: 1.1828 - val_acc: 0.2308\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.2470 - acc: 0.4407 - val_loss: 1.1757 - val_acc: 0.2308\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.2355 - acc: 0.4407 - val_loss: 1.1613 - val_acc: 0.2308\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2224 - acc: 0.4407 - val_loss: 1.1548 - val_acc: 0.2308\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.2128 - acc: 0.4407 - val_loss: 1.1486 - val_acc: 0.2308\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.2006 - acc: 0.4492 - val_loss: 1.1438 - val_acc: 0.2308\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1892 - acc: 0.4492 - val_loss: 1.1354 - val_acc: 0.2308\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.1803 - acc: 0.4492 - val_loss: 1.1284 - val_acc: 0.2308\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1717 - acc: 0.4407 - val_loss: 1.1263 - val_acc: 0.2308\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1615 - acc: 0.4322 - val_loss: 1.1168 - val_acc: 0.2308\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1553 - acc: 0.4153 - val_loss: 1.1149 - val_acc: 0.2308\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1485 - acc: 0.4153 - val_loss: 1.1077 - val_acc: 0.2308\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.1404 - acc: 0.4237 - val_loss: 1.1013 - val_acc: 0.2308\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.1326 - acc: 0.4322 - val_loss: 1.0963 - val_acc: 0.2308\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.1257 - acc: 0.4407 - val_loss: 1.0911 - val_acc: 0.2308\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.1202 - acc: 0.4322 - val_loss: 1.0883 - val_acc: 0.2308\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1142 - acc: 0.4322 - val_loss: 1.0830 - val_acc: 0.2308\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1088 - acc: 0.4492 - val_loss: 1.0747 - val_acc: 0.2308\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1027 - acc: 0.4322 - val_loss: 1.0717 - val_acc: 0.2308\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0973 - acc: 0.4322 - val_loss: 1.0702 - val_acc: 0.2308\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0929 - acc: 0.4407 - val_loss: 1.0667 - val_acc: 0.2308\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0869 - acc: 0.4407 - val_loss: 1.0629 - val_acc: 0.2308\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.0823 - acc: 0.4492 - val_loss: 1.0598 - val_acc: 0.2308\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0790 - acc: 0.4576 - val_loss: 1.0549 - val_acc: 0.2308\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0741 - acc: 0.4576 - val_loss: 1.0503 - val_acc: 0.3077\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0706 - acc: 0.4407 - val_loss: 1.0491 - val_acc: 0.2308\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0673 - acc: 0.4407 - val_loss: 1.0478 - val_acc: 0.3077\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0628 - acc: 0.4576 - val_loss: 1.0425 - val_acc: 0.3077\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0586 - acc: 0.4492 - val_loss: 1.0412 - val_acc: 0.2308\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0563 - acc: 0.4492 - val_loss: 1.0369 - val_acc: 0.3077\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0534 - acc: 0.4492 - val_loss: 1.0337 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 1.0501 - acc: 0.4746 - val_loss: 1.0290 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0464 - acc: 0.4746 - val_loss: 1.0305 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0448 - acc: 0.4661 - val_loss: 1.0276 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0423 - acc: 0.4661 - val_loss: 1.0235 - val_acc: 0.3077\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.0385 - acc: 0.4576 - val_loss: 1.0261 - val_acc: 0.3077\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0375 - acc: 0.4661 - val_loss: 1.0249 - val_acc: 0.3077\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0349 - acc: 0.4661 - val_loss: 1.0221 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0340 - acc: 0.4661 - val_loss: 1.0237 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0304 - acc: 0.4661 - val_loss: 1.0213 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0290 - acc: 0.4576 - val_loss: 1.0210 - val_acc: 0.3077\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.0266 - acc: 0.4576 - val_loss: 1.0187 - val_acc: 0.3077\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0252 - acc: 0.4661 - val_loss: 1.0179 - val_acc: 0.3077\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0242 - acc: 0.4661 - val_loss: 1.0155 - val_acc: 0.3077\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.0222 - acc: 0.4576 - val_loss: 1.0158 - val_acc: 0.3077\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0214 - acc: 0.4661 - val_loss: 1.0146 - val_acc: 0.3077\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0192 - acc: 0.4661 - val_loss: 1.0129 - val_acc: 0.3077\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0175 - acc: 0.4746 - val_loss: 1.0123 - val_acc: 0.3077\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0158 - acc: 0.4661 - val_loss: 1.0128 - val_acc: 0.3077\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0142 - acc: 0.4661 - val_loss: 1.0072 - val_acc: 0.3077\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0127 - acc: 0.4661 - val_loss: 1.0079 - val_acc: 0.3077\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0113 - acc: 0.4746 - val_loss: 1.0075 - val_acc: 0.3077\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0109 - acc: 0.4746 - val_loss: 1.0082 - val_acc: 0.3077\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0087 - acc: 0.4746 - val_loss: 1.0086 - val_acc: 0.3077\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0081 - acc: 0.4746 - val_loss: 1.0087 - val_acc: 0.3077\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0078 - acc: 0.4831 - val_loss: 1.0087 - val_acc: 0.3077\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0063 - acc: 0.4831 - val_loss: 1.0048 - val_acc: 0.3077\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 1.0040 - acc: 0.4831 - val_loss: 1.0087 - val_acc: 0.3077\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0030 - acc: 0.4831 - val_loss: 1.0073 - val_acc: 0.3077\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0023 - acc: 0.4831 - val_loss: 1.0079 - val_acc: 0.3077\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0007 - acc: 0.4831 - val_loss: 1.0051 - val_acc: 0.3077\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0011 - acc: 0.4746 - val_loss: 1.0082 - val_acc: 0.3077\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9987 - acc: 0.4831 - val_loss: 1.0042 - val_acc: 0.3077\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9971 - acc: 0.4831 - val_loss: 1.0020 - val_acc: 0.3077\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9968 - acc: 0.4831 - val_loss: 1.0013 - val_acc: 0.3077\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9954 - acc: 0.4831 - val_loss: 1.0034 - val_acc: 0.3077\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9946 - acc: 0.4831 - val_loss: 1.0031 - val_acc: 0.3077\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9943 - acc: 0.4831 - val_loss: 1.0023 - val_acc: 0.3077\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9927 - acc: 0.4831 - val_loss: 1.0020 - val_acc: 0.3077\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9928 - acc: 0.4915 - val_loss: 1.0017 - val_acc: 0.3077\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9922 - acc: 0.4831 - val_loss: 1.0022 - val_acc: 0.3077\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9900 - acc: 0.4915 - val_loss: 1.0016 - val_acc: 0.3077\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9899 - acc: 0.4915 - val_loss: 0.9998 - val_acc: 0.3077\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9897 - acc: 0.4746 - val_loss: 0.9988 - val_acc: 0.3077\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9883 - acc: 0.4915 - val_loss: 0.9978 - val_acc: 0.3077\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9873 - acc: 0.4831 - val_loss: 0.9993 - val_acc: 0.3077\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9866 - acc: 0.4746 - val_loss: 0.9995 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9861 - acc: 0.4831 - val_loss: 0.9979 - val_acc: 0.3077\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9851 - acc: 0.4746 - val_loss: 0.9984 - val_acc: 0.3077\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9846 - acc: 0.4746 - val_loss: 0.9979 - val_acc: 0.3077\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.8511 - acc: 0.1780 - val_loss: 1.5713 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.6885 - acc: 0.1864 - val_loss: 1.4398 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.5532 - acc: 0.1780 - val_loss: 1.3404 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.4419 - acc: 0.1949 - val_loss: 1.2616 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.3510 - acc: 0.2542 - val_loss: 1.1951 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.2765 - acc: 0.2712 - val_loss: 1.1465 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2147 - acc: 0.2712 - val_loss: 1.1093 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1655 - acc: 0.2797 - val_loss: 1.0788 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.1259 - acc: 0.2712 - val_loss: 1.0533 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0945 - acc: 0.2712 - val_loss: 1.0363 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0686 - acc: 0.2966 - val_loss: 1.0223 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0502 - acc: 0.3305 - val_loss: 1.0131 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0343 - acc: 0.3814 - val_loss: 1.0028 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0227 - acc: 0.3983 - val_loss: 0.9975 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0139 - acc: 0.4068 - val_loss: 0.9925 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0049 - acc: 0.4322 - val_loss: 0.9877 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9982 - acc: 0.4492 - val_loss: 0.9849 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9941 - acc: 0.4407 - val_loss: 0.9820 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9887 - acc: 0.4407 - val_loss: 0.9818 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9840 - acc: 0.4746 - val_loss: 0.9795 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9821 - acc: 0.4746 - val_loss: 0.9799 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9780 - acc: 0.4661 - val_loss: 0.9758 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9776 - acc: 0.4746 - val_loss: 0.9771 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9738 - acc: 0.4576 - val_loss: 0.9770 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9718 - acc: 0.4746 - val_loss: 0.9751 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9703 - acc: 0.4831 - val_loss: 0.9749 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.9690 - acc: 0.4831 - val_loss: 0.9735 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9683 - acc: 0.4746 - val_loss: 0.9740 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9664 - acc: 0.4831 - val_loss: 0.9746 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9668 - acc: 0.4831 - val_loss: 0.9754 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9646 - acc: 0.4915 - val_loss: 0.9755 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9641 - acc: 0.4831 - val_loss: 0.9769 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9633 - acc: 0.4831 - val_loss: 0.9775 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9630 - acc: 0.4746 - val_loss: 0.9791 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9618 - acc: 0.4746 - val_loss: 0.9786 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9606 - acc: 0.4915 - val_loss: 0.9794 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9607 - acc: 0.4831 - val_loss: 0.9782 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9611 - acc: 0.5000 - val_loss: 0.9778 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9605 - acc: 0.4831 - val_loss: 0.9792 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9599 - acc: 0.5000 - val_loss: 0.9800 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9591 - acc: 0.5000 - val_loss: 0.9802 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9590 - acc: 0.5000 - val_loss: 0.9807 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9576 - acc: 0.5000 - val_loss: 0.9823 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9586 - acc: 0.5000 - val_loss: 0.9808 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9573 - acc: 0.5085 - val_loss: 0.9812 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9573 - acc: 0.5169 - val_loss: 0.9810 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9565 - acc: 0.5000 - val_loss: 0.9803 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9559 - acc: 0.5169 - val_loss: 0.9805 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9576 - acc: 0.5169 - val_loss: 0.9821 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9559 - acc: 0.5169 - val_loss: 0.9824 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9554 - acc: 0.5085 - val_loss: 0.9828 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9558 - acc: 0.5085 - val_loss: 0.9822 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9547 - acc: 0.5169 - val_loss: 0.9826 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9550 - acc: 0.5169 - val_loss: 0.9839 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9540 - acc: 0.5169 - val_loss: 0.9840 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9546 - acc: 0.5169 - val_loss: 0.9834 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9548 - acc: 0.5169 - val_loss: 0.9848 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9543 - acc: 0.5169 - val_loss: 0.9847 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9542 - acc: 0.5169 - val_loss: 0.9851 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9541 - acc: 0.5169 - val_loss: 0.9858 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9535 - acc: 0.5085 - val_loss: 0.9843 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9534 - acc: 0.5169 - val_loss: 0.9839 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9530 - acc: 0.5169 - val_loss: 0.9843 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9529 - acc: 0.5085 - val_loss: 0.9858 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9531 - acc: 0.5169 - val_loss: 0.9849 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9532 - acc: 0.5169 - val_loss: 0.9858 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9524 - acc: 0.5169 - val_loss: 0.9846 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9524 - acc: 0.5085 - val_loss: 0.9857 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9533 - acc: 0.5085 - val_loss: 0.9855 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9526 - acc: 0.5169 - val_loss: 0.9860 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9523 - acc: 0.5169 - val_loss: 0.9872 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9527 - acc: 0.5169 - val_loss: 0.9851 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9518 - acc: 0.5169 - val_loss: 0.9881 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9519 - acc: 0.5169 - val_loss: 0.9883 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9516 - acc: 0.5169 - val_loss: 0.9888 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9512 - acc: 0.5169 - val_loss: 0.9873 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9528 - acc: 0.5085 - val_loss: 0.9876 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9511 - acc: 0.5169 - val_loss: 0.9884 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9510 - acc: 0.5169 - val_loss: 0.9874 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9509 - acc: 0.5085 - val_loss: 0.9876 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9504 - acc: 0.5169 - val_loss: 0.9903 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9506 - acc: 0.5169 - val_loss: 0.9879 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9505 - acc: 0.5169 - val_loss: 0.9897 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9510 - acc: 0.5169 - val_loss: 0.9892 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9508 - acc: 0.5169 - val_loss: 0.9903 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9502 - acc: 0.5169 - val_loss: 0.9898 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9501 - acc: 0.5085 - val_loss: 0.9895 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9497 - acc: 0.5169 - val_loss: 0.9888 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9503 - acc: 0.5169 - val_loss: 0.9891 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9502 - acc: 0.5169 - val_loss: 0.9899 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9500 - acc: 0.5169 - val_loss: 0.9909 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9503 - acc: 0.5169 - val_loss: 0.9913 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9492 - acc: 0.5169 - val_loss: 0.9898 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9502 - acc: 0.5169 - val_loss: 0.9892 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9492 - acc: 0.5169 - val_loss: 0.9892 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9492 - acc: 0.5085 - val_loss: 0.9903 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.9497 - acc: 0.5169 - val_loss: 0.9910 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9489 - acc: 0.5169 - val_loss: 0.9887 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9493 - acc: 0.5085 - val_loss: 0.9884 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9485 - acc: 0.5169 - val_loss: 0.9894 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.3566 - acc: 0.4661 - val_loss: 2.1104 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 2.1571 - acc: 0.4661 - val_loss: 1.9591 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.9837 - acc: 0.4661 - val_loss: 1.8210 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.8335 - acc: 0.4661 - val_loss: 1.7108 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.7071 - acc: 0.4661 - val_loss: 1.6069 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.5989 - acc: 0.4576 - val_loss: 1.5241 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.5079 - acc: 0.4576 - val_loss: 1.4498 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.4341 - acc: 0.4576 - val_loss: 1.3888 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.3698 - acc: 0.4661 - val_loss: 1.3358 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.3177 - acc: 0.4576 - val_loss: 1.2922 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.2747 - acc: 0.4407 - val_loss: 1.2540 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.2390 - acc: 0.4407 - val_loss: 1.2204 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.2108 - acc: 0.4661 - val_loss: 1.1948 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1887 - acc: 0.4661 - val_loss: 1.1678 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1675 - acc: 0.4746 - val_loss: 1.1485 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.1499 - acc: 0.4661 - val_loss: 1.1325 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1341 - acc: 0.4831 - val_loss: 1.1154 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1217 - acc: 0.4831 - val_loss: 1.1029 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1096 - acc: 0.4831 - val_loss: 1.0882 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0991 - acc: 0.4831 - val_loss: 1.0767 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0912 - acc: 0.4831 - val_loss: 1.0676 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0819 - acc: 0.4746 - val_loss: 1.0591 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0740 - acc: 0.4661 - val_loss: 1.0526 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0665 - acc: 0.4661 - val_loss: 1.0482 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0602 - acc: 0.4661 - val_loss: 1.0411 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0535 - acc: 0.4661 - val_loss: 1.0346 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0477 - acc: 0.4746 - val_loss: 1.0292 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0427 - acc: 0.4746 - val_loss: 1.0258 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0369 - acc: 0.4746 - val_loss: 1.0194 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0330 - acc: 0.4746 - val_loss: 1.0165 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0278 - acc: 0.4661 - val_loss: 1.0135 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0239 - acc: 0.4661 - val_loss: 1.0119 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0201 - acc: 0.4746 - val_loss: 1.0075 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0171 - acc: 0.4746 - val_loss: 1.0053 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0131 - acc: 0.4831 - val_loss: 1.0034 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0100 - acc: 0.4831 - val_loss: 1.0020 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0072 - acc: 0.4831 - val_loss: 0.9996 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0049 - acc: 0.4831 - val_loss: 0.9957 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0014 - acc: 0.4915 - val_loss: 0.9959 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9988 - acc: 0.4915 - val_loss: 0.9944 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9968 - acc: 0.4915 - val_loss: 0.9938 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9946 - acc: 0.4915 - val_loss: 0.9910 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9921 - acc: 0.5085 - val_loss: 0.9910 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9906 - acc: 0.5085 - val_loss: 0.9889 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9878 - acc: 0.5254 - val_loss: 0.9867 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9866 - acc: 0.5339 - val_loss: 0.9859 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9842 - acc: 0.5339 - val_loss: 0.9848 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9830 - acc: 0.5424 - val_loss: 0.9850 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9817 - acc: 0.5424 - val_loss: 0.9840 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9791 - acc: 0.5508 - val_loss: 0.9824 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9781 - acc: 0.5424 - val_loss: 0.9815 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9765 - acc: 0.5424 - val_loss: 0.9803 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9750 - acc: 0.5424 - val_loss: 0.9816 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9746 - acc: 0.5508 - val_loss: 0.9799 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9725 - acc: 0.5678 - val_loss: 0.9805 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9715 - acc: 0.5508 - val_loss: 0.9799 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9705 - acc: 0.5678 - val_loss: 0.9808 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9706 - acc: 0.5508 - val_loss: 0.9812 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9691 - acc: 0.5593 - val_loss: 0.9822 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9676 - acc: 0.5593 - val_loss: 0.9804 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9667 - acc: 0.5508 - val_loss: 0.9803 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9651 - acc: 0.5593 - val_loss: 0.9804 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9646 - acc: 0.5678 - val_loss: 0.9809 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9641 - acc: 0.5593 - val_loss: 0.9803 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9624 - acc: 0.5593 - val_loss: 0.9809 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9619 - acc: 0.5678 - val_loss: 0.9822 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9609 - acc: 0.5593 - val_loss: 0.9813 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9603 - acc: 0.5763 - val_loss: 0.9823 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9598 - acc: 0.5593 - val_loss: 0.9817 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9602 - acc: 0.5678 - val_loss: 0.9813 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9584 - acc: 0.5763 - val_loss: 0.9812 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 0.9577 - acc: 0.5678 - val_loss: 0.9811 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9568 - acc: 0.5678 - val_loss: 0.9815 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9560 - acc: 0.5763 - val_loss: 0.9820 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9559 - acc: 0.5763 - val_loss: 0.9815 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9558 - acc: 0.5678 - val_loss: 0.9819 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9547 - acc: 0.5678 - val_loss: 0.9817 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9549 - acc: 0.5678 - val_loss: 0.9822 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9538 - acc: 0.5678 - val_loss: 0.9822 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9535 - acc: 0.5678 - val_loss: 0.9802 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9534 - acc: 0.5678 - val_loss: 0.9810 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9522 - acc: 0.5678 - val_loss: 0.9807 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9523 - acc: 0.5847 - val_loss: 0.9814 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9517 - acc: 0.5763 - val_loss: 0.9816 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9513 - acc: 0.5763 - val_loss: 0.9805 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9505 - acc: 0.5763 - val_loss: 0.9809 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9498 - acc: 0.5763 - val_loss: 0.9817 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9498 - acc: 0.5763 - val_loss: 0.9816 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9488 - acc: 0.5763 - val_loss: 0.9817 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9487 - acc: 0.5678 - val_loss: 0.9816 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9489 - acc: 0.5763 - val_loss: 0.9829 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9478 - acc: 0.5763 - val_loss: 0.9821 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9475 - acc: 0.5763 - val_loss: 0.9816 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9483 - acc: 0.5763 - val_loss: 0.9807 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9467 - acc: 0.5678 - val_loss: 0.9836 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9464 - acc: 0.5678 - val_loss: 0.9822 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9461 - acc: 0.5678 - val_loss: 0.9824 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9454 - acc: 0.5763 - val_loss: 0.9836 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9448 - acc: 0.5763 - val_loss: 0.9827 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.9445 - acc: 0.5763 - val_loss: 0.9829 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.8922 - acc: 0.4237 - val_loss: 2.0608 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.7680 - acc: 0.4068 - val_loss: 2.0232 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.6655 - acc: 0.4153 - val_loss: 1.9864 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.5779 - acc: 0.4407 - val_loss: 1.9580 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.5048 - acc: 0.4407 - val_loss: 1.9296 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.4401 - acc: 0.4322 - val_loss: 1.9046 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3859 - acc: 0.4322 - val_loss: 1.8815 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.3420 - acc: 0.4407 - val_loss: 1.8636 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.3062 - acc: 0.4746 - val_loss: 1.8467 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.2763 - acc: 0.4831 - val_loss: 1.8292 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.2507 - acc: 0.4831 - val_loss: 1.8146 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2268 - acc: 0.5000 - val_loss: 1.7958 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2069 - acc: 0.5254 - val_loss: 1.7805 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1890 - acc: 0.5000 - val_loss: 1.7670 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.1733 - acc: 0.5085 - val_loss: 1.7524 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1599 - acc: 0.5085 - val_loss: 1.7412 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1478 - acc: 0.5000 - val_loss: 1.7214 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1366 - acc: 0.4915 - val_loss: 1.7106 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1252 - acc: 0.4831 - val_loss: 1.7007 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1151 - acc: 0.4915 - val_loss: 1.6908 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1060 - acc: 0.5000 - val_loss: 1.6782 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0982 - acc: 0.5000 - val_loss: 1.6711 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0913 - acc: 0.5169 - val_loss: 1.6585 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0855 - acc: 0.5254 - val_loss: 1.6490 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0804 - acc: 0.5169 - val_loss: 1.6403 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0765 - acc: 0.5085 - val_loss: 1.6279 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0719 - acc: 0.5169 - val_loss: 1.6186 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0676 - acc: 0.5169 - val_loss: 1.6115 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0637 - acc: 0.5000 - val_loss: 1.5970 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0606 - acc: 0.5000 - val_loss: 1.5881 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0572 - acc: 0.5169 - val_loss: 1.5781 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0543 - acc: 0.5085 - val_loss: 1.5697 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0515 - acc: 0.5254 - val_loss: 1.5603 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0479 - acc: 0.5169 - val_loss: 1.5523 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0457 - acc: 0.5339 - val_loss: 1.5452 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0432 - acc: 0.5254 - val_loss: 1.5350 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0409 - acc: 0.5169 - val_loss: 1.5280 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0385 - acc: 0.5254 - val_loss: 1.5194 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0359 - acc: 0.5254 - val_loss: 1.5103 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0340 - acc: 0.5339 - val_loss: 1.5026 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0322 - acc: 0.5339 - val_loss: 1.4959 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0306 - acc: 0.5424 - val_loss: 1.4873 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0277 - acc: 0.5339 - val_loss: 1.4820 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0258 - acc: 0.5424 - val_loss: 1.4741 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0244 - acc: 0.5339 - val_loss: 1.4686 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0220 - acc: 0.5339 - val_loss: 1.4601 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 1.0202 - acc: 0.5339 - val_loss: 1.4534 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.0182 - acc: 0.5254 - val_loss: 1.4471 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0170 - acc: 0.5254 - val_loss: 1.4406 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0149 - acc: 0.5254 - val_loss: 1.4348 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0135 - acc: 0.5254 - val_loss: 1.4271 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0118 - acc: 0.5254 - val_loss: 1.4217 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0100 - acc: 0.5169 - val_loss: 1.4145 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0086 - acc: 0.5169 - val_loss: 1.4082 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0071 - acc: 0.5169 - val_loss: 1.4029 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0058 - acc: 0.5169 - val_loss: 1.3966 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.0043 - acc: 0.5169 - val_loss: 1.3913 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0025 - acc: 0.5085 - val_loss: 1.3858 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0009 - acc: 0.5085 - val_loss: 1.3813 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9998 - acc: 0.5085 - val_loss: 1.3753 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9986 - acc: 0.5085 - val_loss: 1.3720 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9979 - acc: 0.4915 - val_loss: 1.3630 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9956 - acc: 0.5085 - val_loss: 1.3589 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9951 - acc: 0.5085 - val_loss: 1.3544 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9940 - acc: 0.4915 - val_loss: 1.3490 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9923 - acc: 0.5000 - val_loss: 1.3447 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9910 - acc: 0.4915 - val_loss: 1.3393 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9895 - acc: 0.4915 - val_loss: 1.3355 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9893 - acc: 0.4831 - val_loss: 1.3300 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9875 - acc: 0.4915 - val_loss: 1.3262 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9863 - acc: 0.4915 - val_loss: 1.3214 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9851 - acc: 0.4915 - val_loss: 1.3179 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9841 - acc: 0.4915 - val_loss: 1.3129 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9832 - acc: 0.5085 - val_loss: 1.3101 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9821 - acc: 0.5000 - val_loss: 1.3049 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9815 - acc: 0.5000 - val_loss: 1.3012 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9798 - acc: 0.5000 - val_loss: 1.2971 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9787 - acc: 0.4915 - val_loss: 1.2941 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9781 - acc: 0.5000 - val_loss: 1.2899 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9764 - acc: 0.5000 - val_loss: 1.2861 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9756 - acc: 0.4915 - val_loss: 1.2835 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9747 - acc: 0.5000 - val_loss: 1.2805 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9734 - acc: 0.5000 - val_loss: 1.2770 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9724 - acc: 0.4915 - val_loss: 1.2731 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9715 - acc: 0.4915 - val_loss: 1.2695 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9706 - acc: 0.4915 - val_loss: 1.2666 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9694 - acc: 0.5085 - val_loss: 1.2638 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9691 - acc: 0.5000 - val_loss: 1.2606 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9672 - acc: 0.5000 - val_loss: 1.2576 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9665 - acc: 0.5085 - val_loss: 1.2547 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9660 - acc: 0.5000 - val_loss: 1.2512 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9651 - acc: 0.5000 - val_loss: 1.2483 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9640 - acc: 0.5085 - val_loss: 1.2453 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9632 - acc: 0.5085 - val_loss: 1.2421 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9625 - acc: 0.5085 - val_loss: 1.2399 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9616 - acc: 0.5085 - val_loss: 1.2376 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.9606 - acc: 0.5085 - val_loss: 1.2352 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9599 - acc: 0.5000 - val_loss: 1.2297 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9587 - acc: 0.5085 - val_loss: 1.2281 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9584 - acc: 0.5085 - val_loss: 1.2251 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 1.5017 - acc: 0.2797 - val_loss: 1.4755 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.4491 - acc: 0.2881 - val_loss: 1.4129 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.4103 - acc: 0.3051 - val_loss: 1.3692 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.3783 - acc: 0.2881 - val_loss: 1.3329 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3456 - acc: 0.2797 - val_loss: 1.3032 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.3206 - acc: 0.2966 - val_loss: 1.2767 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.3025 - acc: 0.2881 - val_loss: 1.2503 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.2829 - acc: 0.2966 - val_loss: 1.2311 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2669 - acc: 0.3051 - val_loss: 1.2211 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.2532 - acc: 0.3220 - val_loss: 1.2029 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.2433 - acc: 0.3136 - val_loss: 1.1824 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.2300 - acc: 0.3136 - val_loss: 1.1698 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.2200 - acc: 0.3136 - val_loss: 1.1532 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.2109 - acc: 0.3136 - val_loss: 1.1478 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 1.2012 - acc: 0.3051 - val_loss: 1.1335 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1930 - acc: 0.3136 - val_loss: 1.1251 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.1854 - acc: 0.3136 - val_loss: 1.1155 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1760 - acc: 0.2966 - val_loss: 1.1065 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1701 - acc: 0.2966 - val_loss: 1.0975 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.1630 - acc: 0.2881 - val_loss: 1.0893 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1563 - acc: 0.2881 - val_loss: 1.0811 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.1500 - acc: 0.2881 - val_loss: 1.0739 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.1438 - acc: 0.2881 - val_loss: 1.0691 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.1391 - acc: 0.2881 - val_loss: 1.0588 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.1340 - acc: 0.2881 - val_loss: 1.0513 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1284 - acc: 0.2881 - val_loss: 1.0476 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.1232 - acc: 0.2881 - val_loss: 1.0421 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1198 - acc: 0.2881 - val_loss: 1.0395 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1125 - acc: 0.2881 - val_loss: 1.0393 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1087 - acc: 0.3051 - val_loss: 1.0343 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1043 - acc: 0.2966 - val_loss: 1.0264 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.1004 - acc: 0.2966 - val_loss: 1.0218 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0964 - acc: 0.2966 - val_loss: 1.0203 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0917 - acc: 0.3051 - val_loss: 1.0156 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0886 - acc: 0.3136 - val_loss: 1.0148 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0859 - acc: 0.3136 - val_loss: 1.0111 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0825 - acc: 0.3136 - val_loss: 1.0065 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0802 - acc: 0.3051 - val_loss: 1.0059 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0775 - acc: 0.3051 - val_loss: 1.0045 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0735 - acc: 0.3136 - val_loss: 1.0020 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0712 - acc: 0.2966 - val_loss: 1.0007 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0694 - acc: 0.2966 - val_loss: 0.9982 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0663 - acc: 0.3051 - val_loss: 0.9954 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0638 - acc: 0.3051 - val_loss: 0.9910 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0612 - acc: 0.3051 - val_loss: 0.9912 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0592 - acc: 0.3051 - val_loss: 0.9889 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0581 - acc: 0.3220 - val_loss: 0.9863 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0546 - acc: 0.3136 - val_loss: 0.9843 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0525 - acc: 0.3220 - val_loss: 0.9832 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0504 - acc: 0.3475 - val_loss: 0.9813 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0479 - acc: 0.3390 - val_loss: 0.9814 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0465 - acc: 0.3390 - val_loss: 0.9821 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0453 - acc: 0.3559 - val_loss: 0.9785 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0426 - acc: 0.3475 - val_loss: 0.9772 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0417 - acc: 0.3559 - val_loss: 0.9755 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0387 - acc: 0.3644 - val_loss: 0.9741 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0373 - acc: 0.3644 - val_loss: 0.9730 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0357 - acc: 0.3898 - val_loss: 0.9715 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0353 - acc: 0.3983 - val_loss: 0.9715 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0327 - acc: 0.3898 - val_loss: 0.9697 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0309 - acc: 0.3814 - val_loss: 0.9697 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0299 - acc: 0.3898 - val_loss: 0.9671 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0279 - acc: 0.3898 - val_loss: 0.9650 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0268 - acc: 0.3814 - val_loss: 0.9634 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0264 - acc: 0.3729 - val_loss: 0.9637 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0259 - acc: 0.3814 - val_loss: 0.9619 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0231 - acc: 0.3983 - val_loss: 0.9613 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0232 - acc: 0.3983 - val_loss: 0.9578 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0223 - acc: 0.3729 - val_loss: 0.9592 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0213 - acc: 0.3983 - val_loss: 0.9559 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0195 - acc: 0.3814 - val_loss: 0.9559 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0192 - acc: 0.3983 - val_loss: 0.9550 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0172 - acc: 0.3983 - val_loss: 0.9549 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0168 - acc: 0.3983 - val_loss: 0.9529 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0158 - acc: 0.3814 - val_loss: 0.9535 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0155 - acc: 0.3983 - val_loss: 0.9526 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0154 - acc: 0.3898 - val_loss: 0.9516 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0136 - acc: 0.3898 - val_loss: 0.9532 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0122 - acc: 0.3898 - val_loss: 0.9532 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0112 - acc: 0.3898 - val_loss: 0.9525 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0119 - acc: 0.3983 - val_loss: 0.9509 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0108 - acc: 0.4068 - val_loss: 0.9490 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0101 - acc: 0.4153 - val_loss: 0.9502 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 1.0086 - acc: 0.4237 - val_loss: 0.9487 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0089 - acc: 0.4068 - val_loss: 0.9475 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0077 - acc: 0.4068 - val_loss: 0.9461 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0067 - acc: 0.4153 - val_loss: 0.9454 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0067 - acc: 0.4237 - val_loss: 0.9427 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0061 - acc: 0.4237 - val_loss: 0.9420 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 1.0047 - acc: 0.4322 - val_loss: 0.9397 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0042 - acc: 0.4237 - val_loss: 0.9406 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0051 - acc: 0.4322 - val_loss: 0.9412 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0028 - acc: 0.4237 - val_loss: 0.9401 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0026 - acc: 0.4322 - val_loss: 0.9405 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0025 - acc: 0.4322 - val_loss: 0.9390 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0012 - acc: 0.4237 - val_loss: 0.9378 - val_acc: 0.3077\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0015 - acc: 0.4492 - val_loss: 0.9363 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0006 - acc: 0.4576 - val_loss: 0.9374 - val_acc: 0.3077\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9992 - acc: 0.4661 - val_loss: 0.9359 - val_acc: 0.3077\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9995 - acc: 0.4576 - val_loss: 0.9359 - val_acc: 0.3077\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 3.3496 - acc: 0.1780 - val_loss: 4.3026 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 2.9646 - acc: 0.1780 - val_loss: 3.9647 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 2.6458 - acc: 0.1864 - val_loss: 3.6656 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 2.3958 - acc: 0.2034 - val_loss: 3.4053 - val_acc: 0.1538\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 2.1923 - acc: 0.2203 - val_loss: 3.1837 - val_acc: 0.1538\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 2.0239 - acc: 0.2627 - val_loss: 2.9542 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.8806 - acc: 0.2966 - val_loss: 2.7526 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.7623 - acc: 0.3136 - val_loss: 2.5579 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.6644 - acc: 0.2712 - val_loss: 2.4080 - val_acc: 0.1538\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.5754 - acc: 0.2881 - val_loss: 2.2329 - val_acc: 0.1538\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.5010 - acc: 0.2797 - val_loss: 2.0989 - val_acc: 0.1538\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.4350 - acc: 0.2797 - val_loss: 1.9764 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3766 - acc: 0.2881 - val_loss: 1.8674 - val_acc: 0.1538\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.3265 - acc: 0.2966 - val_loss: 1.7557 - val_acc: 0.1538\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2825 - acc: 0.3051 - val_loss: 1.6581 - val_acc: 0.1538\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.2446 - acc: 0.3220 - val_loss: 1.5685 - val_acc: 0.1538\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.2078 - acc: 0.3305 - val_loss: 1.4948 - val_acc: 0.1538\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1759 - acc: 0.3390 - val_loss: 1.4219 - val_acc: 0.1538\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1487 - acc: 0.3390 - val_loss: 1.3628 - val_acc: 0.2308\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.1252 - acc: 0.3305 - val_loss: 1.3110 - val_acc: 0.2308\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1050 - acc: 0.3390 - val_loss: 1.2658 - val_acc: 0.2308\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0859 - acc: 0.3220 - val_loss: 1.2276 - val_acc: 0.2308\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0707 - acc: 0.3390 - val_loss: 1.1957 - val_acc: 0.1538\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0578 - acc: 0.3644 - val_loss: 1.1699 - val_acc: 0.2308\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0464 - acc: 0.3644 - val_loss: 1.1463 - val_acc: 0.2308\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0367 - acc: 0.3644 - val_loss: 1.1307 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0282 - acc: 0.3644 - val_loss: 1.1158 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0212 - acc: 0.3729 - val_loss: 1.1031 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0163 - acc: 0.3729 - val_loss: 1.0923 - val_acc: 0.3077\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0117 - acc: 0.3729 - val_loss: 1.0838 - val_acc: 0.3077\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0073 - acc: 0.3898 - val_loss: 1.0768 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0041 - acc: 0.3898 - val_loss: 1.0722 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0008 - acc: 0.3898 - val_loss: 1.0664 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9981 - acc: 0.4068 - val_loss: 1.0627 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9950 - acc: 0.3983 - val_loss: 1.0576 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9925 - acc: 0.3814 - val_loss: 1.0550 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9907 - acc: 0.3814 - val_loss: 1.0519 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9883 - acc: 0.3729 - val_loss: 1.0497 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9865 - acc: 0.3729 - val_loss: 1.0477 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9845 - acc: 0.3814 - val_loss: 1.0447 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9825 - acc: 0.3898 - val_loss: 1.0447 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9810 - acc: 0.3983 - val_loss: 1.0419 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9796 - acc: 0.4068 - val_loss: 1.0410 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9781 - acc: 0.4068 - val_loss: 1.0392 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9765 - acc: 0.4153 - val_loss: 1.0382 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9749 - acc: 0.4068 - val_loss: 1.0386 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9736 - acc: 0.4237 - val_loss: 1.0373 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9727 - acc: 0.4068 - val_loss: 1.0382 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9707 - acc: 0.4068 - val_loss: 1.0357 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9695 - acc: 0.4237 - val_loss: 1.0350 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9684 - acc: 0.4068 - val_loss: 1.0351 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9673 - acc: 0.4322 - val_loss: 1.0354 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9662 - acc: 0.4322 - val_loss: 1.0345 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9653 - acc: 0.4322 - val_loss: 1.0344 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9641 - acc: 0.4237 - val_loss: 1.0356 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9630 - acc: 0.4492 - val_loss: 1.0351 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9621 - acc: 0.4322 - val_loss: 1.0359 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9609 - acc: 0.4576 - val_loss: 1.0355 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9597 - acc: 0.4492 - val_loss: 1.0364 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9588 - acc: 0.4661 - val_loss: 1.0377 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9577 - acc: 0.4746 - val_loss: 1.0366 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9562 - acc: 0.4576 - val_loss: 1.0394 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9556 - acc: 0.4576 - val_loss: 1.0390 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9544 - acc: 0.4746 - val_loss: 1.0388 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9539 - acc: 0.4831 - val_loss: 1.0372 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9526 - acc: 0.4915 - val_loss: 1.0384 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9519 - acc: 0.4915 - val_loss: 1.0378 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9512 - acc: 0.5000 - val_loss: 1.0373 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9504 - acc: 0.4915 - val_loss: 1.0387 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9495 - acc: 0.5000 - val_loss: 1.0373 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9485 - acc: 0.4915 - val_loss: 1.0369 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9481 - acc: 0.4915 - val_loss: 1.0372 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9474 - acc: 0.4915 - val_loss: 1.0373 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9463 - acc: 0.4915 - val_loss: 1.0373 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9456 - acc: 0.5000 - val_loss: 1.0375 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9444 - acc: 0.4915 - val_loss: 1.0364 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9440 - acc: 0.4915 - val_loss: 1.0359 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9436 - acc: 0.5000 - val_loss: 1.0366 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9423 - acc: 0.4915 - val_loss: 1.0376 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9414 - acc: 0.4831 - val_loss: 1.0371 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9416 - acc: 0.4915 - val_loss: 1.0365 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9413 - acc: 0.4915 - val_loss: 1.0363 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9404 - acc: 0.4915 - val_loss: 1.0368 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9395 - acc: 0.5000 - val_loss: 1.0368 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9396 - acc: 0.5000 - val_loss: 1.0367 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9383 - acc: 0.5000 - val_loss: 1.0359 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9383 - acc: 0.5000 - val_loss: 1.0364 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9369 - acc: 0.5085 - val_loss: 1.0368 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9368 - acc: 0.5000 - val_loss: 1.0373 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9361 - acc: 0.5085 - val_loss: 1.0365 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9355 - acc: 0.5000 - val_loss: 1.0370 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9349 - acc: 0.5169 - val_loss: 1.0388 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9344 - acc: 0.5169 - val_loss: 1.0387 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9342 - acc: 0.5085 - val_loss: 1.0382 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9335 - acc: 0.5085 - val_loss: 1.0376 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9335 - acc: 0.5000 - val_loss: 1.0369 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.9322 - acc: 0.5000 - val_loss: 1.0380 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9327 - acc: 0.5000 - val_loss: 1.0379 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9307 - acc: 0.5000 - val_loss: 1.0385 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9308 - acc: 0.5000 - val_loss: 1.0391 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 4ms/step - loss: 1.5188 - acc: 0.3814 - val_loss: 1.2425 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.4514 - acc: 0.3644 - val_loss: 1.1982 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.3928 - acc: 0.3644 - val_loss: 1.1635 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.3399 - acc: 0.3644 - val_loss: 1.1321 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2966 - acc: 0.3814 - val_loss: 1.1067 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2607 - acc: 0.3814 - val_loss: 1.0862 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.2296 - acc: 0.3814 - val_loss: 1.0679 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2040 - acc: 0.3814 - val_loss: 1.0554 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 1.1822 - acc: 0.3814 - val_loss: 1.0438 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1642 - acc: 0.3898 - val_loss: 1.0340 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1475 - acc: 0.3814 - val_loss: 1.0254 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1334 - acc: 0.4153 - val_loss: 1.0183 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.1218 - acc: 0.4237 - val_loss: 1.0136 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1118 - acc: 0.4661 - val_loss: 1.0075 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1021 - acc: 0.4492 - val_loss: 1.0028 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.0951 - acc: 0.4831 - val_loss: 0.9991 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0877 - acc: 0.5085 - val_loss: 0.9958 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0816 - acc: 0.5085 - val_loss: 0.9920 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0760 - acc: 0.5169 - val_loss: 0.9893 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0718 - acc: 0.5254 - val_loss: 0.9861 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0669 - acc: 0.5085 - val_loss: 0.9841 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0619 - acc: 0.5085 - val_loss: 0.9824 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0582 - acc: 0.5000 - val_loss: 0.9785 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0540 - acc: 0.5000 - val_loss: 0.9768 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0505 - acc: 0.5000 - val_loss: 0.9749 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0474 - acc: 0.5085 - val_loss: 0.9733 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0450 - acc: 0.5169 - val_loss: 0.9718 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0410 - acc: 0.5169 - val_loss: 0.9704 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0388 - acc: 0.5169 - val_loss: 0.9687 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0362 - acc: 0.5254 - val_loss: 0.9678 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0340 - acc: 0.5254 - val_loss: 0.9665 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0317 - acc: 0.5424 - val_loss: 0.9657 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0294 - acc: 0.5339 - val_loss: 0.9644 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0271 - acc: 0.5339 - val_loss: 0.9639 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0250 - acc: 0.5339 - val_loss: 0.9637 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0232 - acc: 0.5339 - val_loss: 0.9626 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0214 - acc: 0.5424 - val_loss: 0.9615 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0199 - acc: 0.5424 - val_loss: 0.9607 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0182 - acc: 0.5508 - val_loss: 0.9603 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0169 - acc: 0.5508 - val_loss: 0.9599 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0150 - acc: 0.5424 - val_loss: 0.9581 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.0130 - acc: 0.5508 - val_loss: 0.9580 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0118 - acc: 0.5508 - val_loss: 0.9570 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0104 - acc: 0.5593 - val_loss: 0.9568 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0087 - acc: 0.5593 - val_loss: 0.9560 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0078 - acc: 0.5508 - val_loss: 0.9560 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0064 - acc: 0.5508 - val_loss: 0.9554 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0057 - acc: 0.5508 - val_loss: 0.9552 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0039 - acc: 0.5593 - val_loss: 0.9552 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0026 - acc: 0.5678 - val_loss: 0.9547 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0020 - acc: 0.5508 - val_loss: 0.9545 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0012 - acc: 0.5508 - val_loss: 0.9545 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0000 - acc: 0.5508 - val_loss: 0.9535 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9989 - acc: 0.5508 - val_loss: 0.9532 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9979 - acc: 0.5508 - val_loss: 0.9527 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9969 - acc: 0.5593 - val_loss: 0.9523 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9963 - acc: 0.5678 - val_loss: 0.9521 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9956 - acc: 0.5593 - val_loss: 0.9519 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9942 - acc: 0.5678 - val_loss: 0.9520 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9934 - acc: 0.5678 - val_loss: 0.9515 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9924 - acc: 0.5508 - val_loss: 0.9514 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9919 - acc: 0.5678 - val_loss: 0.9515 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9909 - acc: 0.5593 - val_loss: 0.9515 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9902 - acc: 0.5678 - val_loss: 0.9514 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9894 - acc: 0.5593 - val_loss: 0.9511 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9888 - acc: 0.5593 - val_loss: 0.9512 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9884 - acc: 0.5678 - val_loss: 0.9515 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9875 - acc: 0.5424 - val_loss: 0.9514 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9868 - acc: 0.5424 - val_loss: 0.9510 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9864 - acc: 0.5424 - val_loss: 0.9508 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9854 - acc: 0.5593 - val_loss: 0.9505 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9845 - acc: 0.5254 - val_loss: 0.9505 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9843 - acc: 0.5254 - val_loss: 0.9502 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9832 - acc: 0.5339 - val_loss: 0.9506 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9824 - acc: 0.5424 - val_loss: 0.9508 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9823 - acc: 0.5508 - val_loss: 0.9503 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9818 - acc: 0.5254 - val_loss: 0.9504 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9812 - acc: 0.5339 - val_loss: 0.9501 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9803 - acc: 0.5339 - val_loss: 0.9503 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9799 - acc: 0.5424 - val_loss: 0.9504 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9793 - acc: 0.5424 - val_loss: 0.9502 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9787 - acc: 0.5339 - val_loss: 0.9501 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9783 - acc: 0.5424 - val_loss: 0.9504 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9776 - acc: 0.5339 - val_loss: 0.9504 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9770 - acc: 0.5339 - val_loss: 0.9503 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9770 - acc: 0.5424 - val_loss: 0.9503 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9761 - acc: 0.5339 - val_loss: 0.9508 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9755 - acc: 0.5424 - val_loss: 0.9504 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9754 - acc: 0.5424 - val_loss: 0.9506 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9755 - acc: 0.5339 - val_loss: 0.9506 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9749 - acc: 0.5339 - val_loss: 0.9513 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9744 - acc: 0.5339 - val_loss: 0.9513 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9735 - acc: 0.5424 - val_loss: 0.9511 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9733 - acc: 0.5508 - val_loss: 0.9513 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9731 - acc: 0.5508 - val_loss: 0.9518 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9724 - acc: 0.5424 - val_loss: 0.9519 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9719 - acc: 0.5508 - val_loss: 0.9512 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9719 - acc: 0.5508 - val_loss: 0.9515 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9709 - acc: 0.5424 - val_loss: 0.9514 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9709 - acc: 0.5424 - val_loss: 0.9518 - val_acc: 0.4615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "acea0fc5-8e2f-4264-f551-a61002c889b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "ca0ec9e3-09cd-47f4-a432-a92bec5801e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "cd489bba-f38d-4143-e18a-d79bec609052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "d97947e4-5f63-492c-83a2-df242a763fd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4e6d6ab668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "8aa642fa-e91e-4063-9159-48be0df2c694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4e6d174208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwjnnHMheYJwzjkX0v8D2JI13Hp5QqoAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "c4d95b85-09a6-47c3-8dac-2780f4782f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 1.7723 - acc: 0.5115\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 1.6064 - acc: 0.5115\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.4724 - acc: 0.5115\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.3540 - acc: 0.4962\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.2570 - acc: 0.4962\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.1866 - acc: 0.4809\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1424 - acc: 0.4733\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.1102 - acc: 0.4809\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 1.0884 - acc: 0.4809\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0734 - acc: 0.4962\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 1.0588 - acc: 0.4885\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0468 - acc: 0.5191\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.0396 - acc: 0.5038\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0294 - acc: 0.5191\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 1.0224 - acc: 0.5191\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 1.0151 - acc: 0.5344\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.0112 - acc: 0.5267\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0057 - acc: 0.5344\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9999 - acc: 0.5344\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9965 - acc: 0.5496\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9919 - acc: 0.5496\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.9878 - acc: 0.5420\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9847 - acc: 0.5344\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9819 - acc: 0.5420\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9776 - acc: 0.5344\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9757 - acc: 0.5496\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9730 - acc: 0.5496\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9703 - acc: 0.5420\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.9678 - acc: 0.5649\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9653 - acc: 0.5725\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9635 - acc: 0.5802\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9605 - acc: 0.5649\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9598 - acc: 0.5649\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9577 - acc: 0.5649\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9561 - acc: 0.5725\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9539 - acc: 0.5725\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9528 - acc: 0.5725\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9503 - acc: 0.5725\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9492 - acc: 0.5725\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9481 - acc: 0.5725\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9461 - acc: 0.5725\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9448 - acc: 0.5725\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9439 - acc: 0.5649\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9427 - acc: 0.5725\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.9409 - acc: 0.5649\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9406 - acc: 0.5649\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9381 - acc: 0.5725\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9402 - acc: 0.5496\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9368 - acc: 0.5649\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9366 - acc: 0.5725\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9354 - acc: 0.5725\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.9344 - acc: 0.5725\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 221us/step - loss: 0.9338 - acc: 0.5649\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9321 - acc: 0.5802\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9305 - acc: 0.5649\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9309 - acc: 0.5573\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9308 - acc: 0.5725\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9287 - acc: 0.5725\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9288 - acc: 0.5725\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.9280 - acc: 0.5725\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9276 - acc: 0.5725\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9274 - acc: 0.5725\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 0.9270 - acc: 0.5725\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9272 - acc: 0.5802\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 251us/step - loss: 0.9268 - acc: 0.5725\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9253 - acc: 0.5725\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.9255 - acc: 0.5725\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9255 - acc: 0.5802\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9251 - acc: 0.5802\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9238 - acc: 0.5802\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9248 - acc: 0.5802\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9246 - acc: 0.5802\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9232 - acc: 0.5802\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9227 - acc: 0.5802\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9222 - acc: 0.5725\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9223 - acc: 0.5725\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9214 - acc: 0.5725\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9210 - acc: 0.5725\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9216 - acc: 0.5573\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9212 - acc: 0.5725\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9205 - acc: 0.5573\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9207 - acc: 0.5649\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9216 - acc: 0.5573\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.9208 - acc: 0.5573\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9199 - acc: 0.5573\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.9194 - acc: 0.5573\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9185 - acc: 0.5573\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 261us/step - loss: 0.9195 - acc: 0.5573\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9182 - acc: 0.5573\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 229us/step - loss: 0.9185 - acc: 0.5573\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 254us/step - loss: 0.9177 - acc: 0.5573\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9173 - acc: 0.5573\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9172 - acc: 0.5573\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9171 - acc: 0.5573\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9163 - acc: 0.5573\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9177 - acc: 0.5649\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9163 - acc: 0.5573\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9171 - acc: 0.5573\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9176 - acc: 0.5573\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "0cea7167-8534-461c-8a33-502add8d6a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "0fa85f59-795e-44cd-de92-c672c1ba3e81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23529411764705882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "7a48208c-d5a4-41ce-80b3-caf53a44c9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "5fc11aad-9b5e-487c-e4ac-715d66e7d04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3f0a268a-d66e-48ea-e47f-0c3ee53c1bc0",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fae0e5a7-69e3-4034-9c04-3066a26e225e",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d4b8fccc-9578-4161-d246-e44691881bda",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c99c8c93-8a32-4f83-f6d9-c2ed8847112a",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 1.1833 - acc: 0.3504 - val_loss: 1.0331 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 1.1132 - acc: 0.4530 - val_loss: 0.9781 - val_acc: 0.5714\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 1.0540 - acc: 0.4872 - val_loss: 0.9355 - val_acc: 0.6429\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.0042 - acc: 0.5385 - val_loss: 0.8986 - val_acc: 0.6429\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9621 - acc: 0.6068 - val_loss: 0.8679 - val_acc: 0.6429\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9263 - acc: 0.6325 - val_loss: 0.8418 - val_acc: 0.6429\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.8954 - acc: 0.6496 - val_loss: 0.8190 - val_acc: 0.7143\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.8687 - acc: 0.6496 - val_loss: 0.7997 - val_acc: 0.7143\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 0.8453 - acc: 0.6667 - val_loss: 0.7824 - val_acc: 0.7143\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.8236 - acc: 0.6752 - val_loss: 0.7661 - val_acc: 0.7143\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 213us/step - loss: 0.8019 - acc: 0.7179 - val_loss: 0.7481 - val_acc: 0.7143\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.7825 - acc: 0.7265 - val_loss: 0.7301 - val_acc: 0.7857\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.7650 - acc: 0.7265 - val_loss: 0.7143 - val_acc: 0.7857\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.7495 - acc: 0.7521 - val_loss: 0.7000 - val_acc: 0.7857\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.7351 - acc: 0.7863 - val_loss: 0.6868 - val_acc: 0.7857\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.7220 - acc: 0.8120 - val_loss: 0.6744 - val_acc: 0.7857\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.7100 - acc: 0.8120 - val_loss: 0.6631 - val_acc: 0.8571\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.6990 - acc: 0.8205 - val_loss: 0.6525 - val_acc: 0.8571\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.6886 - acc: 0.8205 - val_loss: 0.6427 - val_acc: 0.8571\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.6783 - acc: 0.8205 - val_loss: 0.6328 - val_acc: 0.8571\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.6682 - acc: 0.8205 - val_loss: 0.6235 - val_acc: 0.8571\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.6572 - acc: 0.8291 - val_loss: 0.6138 - val_acc: 0.8571\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.6467 - acc: 0.8376 - val_loss: 0.6050 - val_acc: 0.8571\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.6368 - acc: 0.8376 - val_loss: 0.5963 - val_acc: 0.8571\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.6270 - acc: 0.8376 - val_loss: 0.5883 - val_acc: 0.8571\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.6176 - acc: 0.8462 - val_loss: 0.5801 - val_acc: 0.8571\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.6088 - acc: 0.8632 - val_loss: 0.5727 - val_acc: 0.9286\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.6002 - acc: 0.8718 - val_loss: 0.5653 - val_acc: 0.9286\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.5923 - acc: 0.8803 - val_loss: 0.5582 - val_acc: 0.9286\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.5846 - acc: 0.8803 - val_loss: 0.5516 - val_acc: 0.9286\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.5772 - acc: 0.8889 - val_loss: 0.5451 - val_acc: 0.9286\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.5701 - acc: 0.8889 - val_loss: 0.5389 - val_acc: 0.9286\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.5634 - acc: 0.8974 - val_loss: 0.5328 - val_acc: 0.9286\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.5568 - acc: 0.8974 - val_loss: 0.5270 - val_acc: 0.9286\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.5505 - acc: 0.8974 - val_loss: 0.5213 - val_acc: 0.9286\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.5445 - acc: 0.8974 - val_loss: 0.5159 - val_acc: 0.9286\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.5389 - acc: 0.8974 - val_loss: 0.5107 - val_acc: 0.9286\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.5333 - acc: 0.9060 - val_loss: 0.5056 - val_acc: 0.9286\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.5280 - acc: 0.9060 - val_loss: 0.5007 - val_acc: 0.9286\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 237us/step - loss: 0.5228 - acc: 0.9145 - val_loss: 0.4958 - val_acc: 0.9286\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.5177 - acc: 0.9316 - val_loss: 0.4912 - val_acc: 0.9286\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.5128 - acc: 0.9316 - val_loss: 0.4866 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.5081 - acc: 0.9402 - val_loss: 0.4821 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.5035 - acc: 0.9402 - val_loss: 0.4779 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.4990 - acc: 0.9402 - val_loss: 0.4736 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.4946 - acc: 0.9487 - val_loss: 0.4695 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.4903 - acc: 0.9487 - val_loss: 0.4655 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.4861 - acc: 0.9487 - val_loss: 0.4616 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 226us/step - loss: 0.4820 - acc: 0.9487 - val_loss: 0.4577 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.4780 - acc: 0.9487 - val_loss: 0.4538 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.4741 - acc: 0.9487 - val_loss: 0.4501 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.4704 - acc: 0.9487 - val_loss: 0.4465 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.4666 - acc: 0.9487 - val_loss: 0.4430 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.4630 - acc: 0.9487 - val_loss: 0.4395 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.4595 - acc: 0.9487 - val_loss: 0.4361 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.4560 - acc: 0.9487 - val_loss: 0.4328 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.4526 - acc: 0.9487 - val_loss: 0.4296 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.4492 - acc: 0.9487 - val_loss: 0.4265 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.4459 - acc: 0.9487 - val_loss: 0.4235 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.4426 - acc: 0.9487 - val_loss: 0.4205 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.4394 - acc: 0.9573 - val_loss: 0.4175 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.4362 - acc: 0.9573 - val_loss: 0.4145 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.4331 - acc: 0.9573 - val_loss: 0.4117 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.4301 - acc: 0.9573 - val_loss: 0.4088 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.4270 - acc: 0.9573 - val_loss: 0.4060 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 244us/step - loss: 0.4240 - acc: 0.9573 - val_loss: 0.4032 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.4210 - acc: 0.9573 - val_loss: 0.4004 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.4181 - acc: 0.9573 - val_loss: 0.3977 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.4153 - acc: 0.9573 - val_loss: 0.3951 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.4124 - acc: 0.9573 - val_loss: 0.3925 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.4096 - acc: 0.9573 - val_loss: 0.3899 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 236us/step - loss: 0.4069 - acc: 0.9573 - val_loss: 0.3873 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.4041 - acc: 0.9573 - val_loss: 0.3848 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.4014 - acc: 0.9573 - val_loss: 0.3822 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.3988 - acc: 0.9573 - val_loss: 0.3798 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.3961 - acc: 0.9573 - val_loss: 0.3773 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.3936 - acc: 0.9573 - val_loss: 0.3749 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.3911 - acc: 0.9573 - val_loss: 0.3725 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.3886 - acc: 0.9573 - val_loss: 0.3702 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.3861 - acc: 0.9573 - val_loss: 0.3679 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.3837 - acc: 0.9573 - val_loss: 0.3657 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.3813 - acc: 0.9573 - val_loss: 0.3634 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.3790 - acc: 0.9573 - val_loss: 0.3612 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.3767 - acc: 0.9573 - val_loss: 0.3590 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.3744 - acc: 0.9573 - val_loss: 0.3568 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.3721 - acc: 0.9573 - val_loss: 0.3547 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.3700 - acc: 0.9573 - val_loss: 0.3525 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.3678 - acc: 0.9573 - val_loss: 0.3504 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.3657 - acc: 0.9573 - val_loss: 0.3484 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.3635 - acc: 0.9573 - val_loss: 0.3463 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.3614 - acc: 0.9573 - val_loss: 0.3443 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.3593 - acc: 0.9573 - val_loss: 0.3423 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.3573 - acc: 0.9573 - val_loss: 0.3403 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.3552 - acc: 0.9573 - val_loss: 0.3383 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 213us/step - loss: 0.3532 - acc: 0.9573 - val_loss: 0.3363 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.3512 - acc: 0.9658 - val_loss: 0.3344 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.3492 - acc: 0.9658 - val_loss: 0.3325 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 0.3473 - acc: 0.9658 - val_loss: 0.3306 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.3453 - acc: 0.9658 - val_loss: 0.3287 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.3434 - acc: 0.9658 - val_loss: 0.3268 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 0.7549 - acc: 0.8220 - val_loss: 0.8013 - val_acc: 0.7692\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7236 - acc: 0.8475 - val_loss: 0.7717 - val_acc: 0.7692\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6932 - acc: 0.8729 - val_loss: 0.7434 - val_acc: 0.8462\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6649 - acc: 0.8814 - val_loss: 0.7163 - val_acc: 0.8462\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6383 - acc: 0.8814 - val_loss: 0.6909 - val_acc: 0.8462\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6129 - acc: 0.8814 - val_loss: 0.6666 - val_acc: 0.8462\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5892 - acc: 0.8898 - val_loss: 0.6434 - val_acc: 0.8462\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5669 - acc: 0.9068 - val_loss: 0.6214 - val_acc: 0.8462\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5461 - acc: 0.9237 - val_loss: 0.6004 - val_acc: 0.8462\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5263 - acc: 0.9237 - val_loss: 0.5808 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5076 - acc: 0.9237 - val_loss: 0.5620 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4901 - acc: 0.9237 - val_loss: 0.5448 - val_acc: 0.8462\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4734 - acc: 0.9322 - val_loss: 0.5281 - val_acc: 0.8462\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.4577 - acc: 0.9322 - val_loss: 0.5123 - val_acc: 0.8462\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4428 - acc: 0.9407 - val_loss: 0.4972 - val_acc: 0.8462\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4286 - acc: 0.9576 - val_loss: 0.4830 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4154 - acc: 0.9576 - val_loss: 0.4694 - val_acc: 0.9231\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4028 - acc: 0.9661 - val_loss: 0.4566 - val_acc: 0.9231\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3910 - acc: 0.9661 - val_loss: 0.4445 - val_acc: 0.9231\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3798 - acc: 0.9661 - val_loss: 0.4328 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3691 - acc: 0.9661 - val_loss: 0.4219 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3589 - acc: 0.9661 - val_loss: 0.4112 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3491 - acc: 0.9661 - val_loss: 0.4011 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3399 - acc: 0.9661 - val_loss: 0.3914 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3311 - acc: 0.9746 - val_loss: 0.3821 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3227 - acc: 0.9746 - val_loss: 0.3730 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3147 - acc: 0.9746 - val_loss: 0.3643 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3071 - acc: 0.9746 - val_loss: 0.3559 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2997 - acc: 0.9746 - val_loss: 0.3478 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.2928 - acc: 0.9746 - val_loss: 0.3402 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.2860 - acc: 0.9746 - val_loss: 0.3328 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2794 - acc: 0.9746 - val_loss: 0.3255 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2733 - acc: 0.9746 - val_loss: 0.3187 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2673 - acc: 0.9746 - val_loss: 0.3121 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.2616 - acc: 0.9746 - val_loss: 0.3058 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2560 - acc: 0.9746 - val_loss: 0.2997 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2507 - acc: 0.9746 - val_loss: 0.2939 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.2456 - acc: 0.9746 - val_loss: 0.2881 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2407 - acc: 0.9746 - val_loss: 0.2827 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.2359 - acc: 0.9746 - val_loss: 0.2776 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2314 - acc: 0.9746 - val_loss: 0.2728 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2270 - acc: 0.9746 - val_loss: 0.2681 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2227 - acc: 0.9746 - val_loss: 0.2637 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2186 - acc: 0.9746 - val_loss: 0.2593 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2146 - acc: 0.9746 - val_loss: 0.2551 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2107 - acc: 0.9831 - val_loss: 0.2510 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2070 - acc: 0.9831 - val_loss: 0.2471 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2035 - acc: 0.9831 - val_loss: 0.2433 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2000 - acc: 0.9831 - val_loss: 0.2396 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1966 - acc: 0.9831 - val_loss: 0.2360 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1934 - acc: 0.9831 - val_loss: 0.2325 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1902 - acc: 0.9831 - val_loss: 0.2292 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1871 - acc: 0.9831 - val_loss: 0.2259 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1842 - acc: 0.9831 - val_loss: 0.2228 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1813 - acc: 0.9831 - val_loss: 0.2197 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1785 - acc: 0.9831 - val_loss: 0.2168 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1758 - acc: 0.9831 - val_loss: 0.2139 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1732 - acc: 0.9831 - val_loss: 0.2111 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.1706 - acc: 0.9831 - val_loss: 0.2084 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1682 - acc: 0.9831 - val_loss: 0.2057 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1657 - acc: 0.9831 - val_loss: 0.2032 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1634 - acc: 0.9831 - val_loss: 0.2007 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1611 - acc: 0.9831 - val_loss: 0.1982 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1589 - acc: 0.9831 - val_loss: 0.1959 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1567 - acc: 0.9831 - val_loss: 0.1936 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1546 - acc: 0.9831 - val_loss: 0.1914 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1526 - acc: 0.9831 - val_loss: 0.1892 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.1506 - acc: 0.9831 - val_loss: 0.1871 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1486 - acc: 0.9831 - val_loss: 0.1850 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1468 - acc: 0.9831 - val_loss: 0.1830 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1449 - acc: 0.9831 - val_loss: 0.1811 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1431 - acc: 0.9831 - val_loss: 0.1792 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.1413 - acc: 0.9831 - val_loss: 0.1774 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.1396 - acc: 0.9831 - val_loss: 0.1756 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1379 - acc: 0.9831 - val_loss: 0.1738 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1363 - acc: 0.9831 - val_loss: 0.1721 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.1347 - acc: 0.9831 - val_loss: 0.1704 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.1332 - acc: 0.9831 - val_loss: 0.1688 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1316 - acc: 0.9831 - val_loss: 0.1672 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.1301 - acc: 0.9831 - val_loss: 0.1656 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.1286 - acc: 0.9831 - val_loss: 0.1640 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1272 - acc: 0.9831 - val_loss: 0.1625 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1258 - acc: 0.9831 - val_loss: 0.1610 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1244 - acc: 0.9831 - val_loss: 0.1595 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1231 - acc: 0.9831 - val_loss: 0.1581 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1218 - acc: 0.9831 - val_loss: 0.1567 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1205 - acc: 0.9831 - val_loss: 0.1554 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1193 - acc: 0.9831 - val_loss: 0.1541 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.1180 - acc: 0.9831 - val_loss: 0.1528 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1169 - acc: 0.9831 - val_loss: 0.1515 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.1157 - acc: 0.9831 - val_loss: 0.1502 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.1145 - acc: 0.9831 - val_loss: 0.1490 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1134 - acc: 0.9831 - val_loss: 0.1478 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.1123 - acc: 0.9831 - val_loss: 0.1466 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1112 - acc: 0.9831 - val_loss: 0.1454 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1102 - acc: 0.9831 - val_loss: 0.1443 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1091 - acc: 0.9831 - val_loss: 0.1432 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.1081 - acc: 0.9831 - val_loss: 0.1421 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1071 - acc: 0.9831 - val_loss: 0.1410 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1062 - acc: 0.9831 - val_loss: 0.1399 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.4822 - acc: 0.2034 - val_loss: 1.1684 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.2722 - acc: 0.2119 - val_loss: 1.0183 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0951 - acc: 0.2288 - val_loss: 0.8977 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9490 - acc: 0.2881 - val_loss: 0.7982 - val_acc: 0.6923\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8300 - acc: 0.7119 - val_loss: 0.7189 - val_acc: 0.7692\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7349 - acc: 0.7881 - val_loss: 0.6560 - val_acc: 0.7692\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6619 - acc: 0.8305 - val_loss: 0.6115 - val_acc: 0.7692\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6085 - acc: 0.8559 - val_loss: 0.5752 - val_acc: 0.7692\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5649 - acc: 0.8644 - val_loss: 0.5446 - val_acc: 0.7692\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5267 - acc: 0.8898 - val_loss: 0.5163 - val_acc: 0.7692\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.4937 - acc: 0.8898 - val_loss: 0.4914 - val_acc: 0.7692\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4644 - acc: 0.9068 - val_loss: 0.4691 - val_acc: 0.7692\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4380 - acc: 0.9153 - val_loss: 0.4494 - val_acc: 0.8462\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4145 - acc: 0.9237 - val_loss: 0.4313 - val_acc: 0.8462\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3931 - acc: 0.9237 - val_loss: 0.4148 - val_acc: 0.8462\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3737 - acc: 0.9237 - val_loss: 0.3995 - val_acc: 0.8462\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3562 - acc: 0.9237 - val_loss: 0.3854 - val_acc: 0.8462\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3399 - acc: 0.9237 - val_loss: 0.3722 - val_acc: 0.8462\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.3250 - acc: 0.9237 - val_loss: 0.3597 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3114 - acc: 0.9322 - val_loss: 0.3484 - val_acc: 0.8462\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2986 - acc: 0.9322 - val_loss: 0.3377 - val_acc: 0.8462\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.2870 - acc: 0.9576 - val_loss: 0.3276 - val_acc: 0.8462\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2763 - acc: 0.9661 - val_loss: 0.3182 - val_acc: 0.8462\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.2661 - acc: 0.9661 - val_loss: 0.3094 - val_acc: 0.8462\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 259us/step - loss: 0.2566 - acc: 0.9661 - val_loss: 0.3011 - val_acc: 0.8462\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2478 - acc: 0.9661 - val_loss: 0.2934 - val_acc: 0.8462\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.2396 - acc: 0.9661 - val_loss: 0.2861 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2318 - acc: 0.9661 - val_loss: 0.2790 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2246 - acc: 0.9661 - val_loss: 0.2726 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2177 - acc: 0.9661 - val_loss: 0.2664 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.2114 - acc: 0.9661 - val_loss: 0.2605 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2053 - acc: 0.9831 - val_loss: 0.2550 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1997 - acc: 0.9831 - val_loss: 0.2496 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1943 - acc: 0.9831 - val_loss: 0.2446 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.1892 - acc: 0.9831 - val_loss: 0.2399 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1843 - acc: 0.9831 - val_loss: 0.2353 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.1797 - acc: 0.9831 - val_loss: 0.2310 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1753 - acc: 0.9915 - val_loss: 0.2270 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1712 - acc: 1.0000 - val_loss: 0.2229 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1672 - acc: 1.0000 - val_loss: 0.2192 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1634 - acc: 1.0000 - val_loss: 0.2156 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.1598 - acc: 1.0000 - val_loss: 0.2122 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1563 - acc: 1.0000 - val_loss: 0.2088 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1530 - acc: 1.0000 - val_loss: 0.2055 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1498 - acc: 1.0000 - val_loss: 0.2025 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1468 - acc: 1.0000 - val_loss: 0.1995 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1440 - acc: 1.0000 - val_loss: 0.1967 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.1412 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1385 - acc: 1.0000 - val_loss: 0.1914 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1360 - acc: 1.0000 - val_loss: 0.1889 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.1336 - acc: 1.0000 - val_loss: 0.1864 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1312 - acc: 1.0000 - val_loss: 0.1841 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1289 - acc: 1.0000 - val_loss: 0.1818 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1267 - acc: 1.0000 - val_loss: 0.1796 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1246 - acc: 1.0000 - val_loss: 0.1775 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1226 - acc: 1.0000 - val_loss: 0.1755 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1207 - acc: 1.0000 - val_loss: 0.1736 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1188 - acc: 1.0000 - val_loss: 0.1716 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1169 - acc: 1.0000 - val_loss: 0.1698 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1152 - acc: 1.0000 - val_loss: 0.1680 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.1135 - acc: 1.0000 - val_loss: 0.1663 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1118 - acc: 1.0000 - val_loss: 0.1646 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.1102 - acc: 1.0000 - val_loss: 0.1630 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1086 - acc: 1.0000 - val_loss: 0.1614 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1072 - acc: 1.0000 - val_loss: 0.1598 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1057 - acc: 1.0000 - val_loss: 0.1584 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1043 - acc: 1.0000 - val_loss: 0.1569 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1029 - acc: 1.0000 - val_loss: 0.1555 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1015 - acc: 1.0000 - val_loss: 0.1541 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1002 - acc: 1.0000 - val_loss: 0.1528 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.0990 - acc: 1.0000 - val_loss: 0.1515 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.0977 - acc: 1.0000 - val_loss: 0.1502 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.0966 - acc: 1.0000 - val_loss: 0.1490 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.0954 - acc: 1.0000 - val_loss: 0.1478 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.0943 - acc: 1.0000 - val_loss: 0.1466 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.0932 - acc: 1.0000 - val_loss: 0.1455 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.0921 - acc: 1.0000 - val_loss: 0.1444 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.0910 - acc: 1.0000 - val_loss: 0.1433 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.0900 - acc: 1.0000 - val_loss: 0.1422 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.0890 - acc: 1.0000 - val_loss: 0.1412 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.0880 - acc: 1.0000 - val_loss: 0.1402 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.0871 - acc: 1.0000 - val_loss: 0.1392 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.0862 - acc: 1.0000 - val_loss: 0.1383 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.0853 - acc: 1.0000 - val_loss: 0.1373 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.0844 - acc: 1.0000 - val_loss: 0.1364 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.0835 - acc: 1.0000 - val_loss: 0.1355 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.0827 - acc: 1.0000 - val_loss: 0.1347 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.0818 - acc: 1.0000 - val_loss: 0.1338 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.0810 - acc: 1.0000 - val_loss: 0.1330 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.0803 - acc: 1.0000 - val_loss: 0.1321 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.0795 - acc: 1.0000 - val_loss: 0.1313 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.0788 - acc: 1.0000 - val_loss: 0.1306 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.0780 - acc: 1.0000 - val_loss: 0.1298 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.0773 - acc: 1.0000 - val_loss: 0.1290 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.0766 - acc: 1.0000 - val_loss: 0.1283 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.0759 - acc: 1.0000 - val_loss: 0.1276 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.0752 - acc: 1.0000 - val_loss: 0.1269 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.0745 - acc: 1.0000 - val_loss: 0.1262 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.0739 - acc: 1.0000 - val_loss: 0.1256 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.0733 - acc: 1.0000 - val_loss: 0.1249 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.5798 - acc: 0.0085 - val_loss: 1.4478 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.5090 - acc: 0.0254 - val_loss: 1.3961 - val_acc: 0.0769\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.4451 - acc: 0.0254 - val_loss: 1.3499 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3889 - acc: 0.0254 - val_loss: 1.3117 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.3399 - acc: 0.0508 - val_loss: 1.2763 - val_acc: 0.0769\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.2966 - acc: 0.0593 - val_loss: 1.2448 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.2577 - acc: 0.0593 - val_loss: 1.2145 - val_acc: 0.0769\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.2237 - acc: 0.0593 - val_loss: 1.1862 - val_acc: 0.0769\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.1920 - acc: 0.0847 - val_loss: 1.1595 - val_acc: 0.0769\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 1.1623 - acc: 0.0847 - val_loss: 1.1342 - val_acc: 0.0769\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1347 - acc: 0.1186 - val_loss: 1.1106 - val_acc: 0.0769\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.1078 - acc: 0.1356 - val_loss: 1.0884 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0823 - acc: 0.1864 - val_loss: 1.0672 - val_acc: 0.1538\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0579 - acc: 0.2288 - val_loss: 1.0464 - val_acc: 0.1538\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0344 - acc: 0.3136 - val_loss: 1.0269 - val_acc: 0.2308\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0117 - acc: 0.4407 - val_loss: 1.0083 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9901 - acc: 0.6017 - val_loss: 0.9905 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9700 - acc: 0.6780 - val_loss: 0.9741 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9514 - acc: 0.7542 - val_loss: 0.9584 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9337 - acc: 0.7966 - val_loss: 0.9434 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9164 - acc: 0.8559 - val_loss: 0.9287 - val_acc: 0.7692\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9005 - acc: 0.8814 - val_loss: 0.9155 - val_acc: 0.8462\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8845 - acc: 0.9237 - val_loss: 0.9015 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8694 - acc: 0.9322 - val_loss: 0.8881 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.8545 - acc: 0.9322 - val_loss: 0.8754 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8398 - acc: 0.9407 - val_loss: 0.8629 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8258 - acc: 0.9661 - val_loss: 0.8515 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8123 - acc: 0.9746 - val_loss: 0.8403 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.7997 - acc: 0.9746 - val_loss: 0.8298 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7875 - acc: 0.9746 - val_loss: 0.8195 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7757 - acc: 0.9746 - val_loss: 0.8096 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7643 - acc: 0.9746 - val_loss: 0.8001 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7534 - acc: 0.9746 - val_loss: 0.7908 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7426 - acc: 0.9746 - val_loss: 0.7815 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7320 - acc: 0.9746 - val_loss: 0.7723 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7216 - acc: 0.9746 - val_loss: 0.7633 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7116 - acc: 0.9746 - val_loss: 0.7543 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7019 - acc: 0.9746 - val_loss: 0.7456 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6920 - acc: 0.9746 - val_loss: 0.7372 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.6826 - acc: 0.9746 - val_loss: 0.7286 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.6733 - acc: 0.9746 - val_loss: 0.7206 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.6643 - acc: 0.9746 - val_loss: 0.7126 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6555 - acc: 0.9746 - val_loss: 0.7049 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6470 - acc: 0.9746 - val_loss: 0.6972 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.6386 - acc: 0.9746 - val_loss: 0.6899 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6304 - acc: 0.9746 - val_loss: 0.6826 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6223 - acc: 0.9746 - val_loss: 0.6756 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.6146 - acc: 0.9746 - val_loss: 0.6687 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6067 - acc: 0.9746 - val_loss: 0.6621 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5991 - acc: 0.9831 - val_loss: 0.6553 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5916 - acc: 0.9831 - val_loss: 0.6489 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5844 - acc: 0.9831 - val_loss: 0.6424 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5774 - acc: 0.9831 - val_loss: 0.6363 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5705 - acc: 0.9831 - val_loss: 0.6302 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5638 - acc: 0.9831 - val_loss: 0.6242 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5572 - acc: 0.9831 - val_loss: 0.6185 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.5509 - acc: 0.9831 - val_loss: 0.6127 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5446 - acc: 0.9831 - val_loss: 0.6071 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5384 - acc: 0.9915 - val_loss: 0.6017 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5325 - acc: 0.9915 - val_loss: 0.5964 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5267 - acc: 0.9915 - val_loss: 0.5912 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5210 - acc: 0.9915 - val_loss: 0.5861 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.5155 - acc: 0.9915 - val_loss: 0.5810 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5099 - acc: 0.9915 - val_loss: 0.5762 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5046 - acc: 0.9915 - val_loss: 0.5713 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4993 - acc: 0.9915 - val_loss: 0.5665 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4942 - acc: 0.9915 - val_loss: 0.5615 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4892 - acc: 0.9915 - val_loss: 0.5568 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4842 - acc: 0.9915 - val_loss: 0.5521 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4794 - acc: 0.9915 - val_loss: 0.5475 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4747 - acc: 0.9915 - val_loss: 0.5430 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4700 - acc: 0.9915 - val_loss: 0.5385 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4655 - acc: 0.9915 - val_loss: 0.5342 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4610 - acc: 0.9915 - val_loss: 0.5300 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.4567 - acc: 0.9915 - val_loss: 0.5258 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.4524 - acc: 0.9915 - val_loss: 0.5216 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4482 - acc: 0.9915 - val_loss: 0.5175 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4441 - acc: 0.9915 - val_loss: 0.5135 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.4401 - acc: 0.9915 - val_loss: 0.5096 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4360 - acc: 0.9915 - val_loss: 0.5058 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4322 - acc: 0.9915 - val_loss: 0.5020 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 278us/step - loss: 0.4283 - acc: 0.9915 - val_loss: 0.4983 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4246 - acc: 0.9915 - val_loss: 0.4947 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4209 - acc: 0.9915 - val_loss: 0.4912 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4173 - acc: 0.9915 - val_loss: 0.4877 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4138 - acc: 0.9915 - val_loss: 0.4842 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4104 - acc: 0.9915 - val_loss: 0.4807 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4069 - acc: 0.9915 - val_loss: 0.4774 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4035 - acc: 0.9915 - val_loss: 0.4741 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4003 - acc: 0.9915 - val_loss: 0.4709 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3970 - acc: 0.9915 - val_loss: 0.4677 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3938 - acc: 0.9915 - val_loss: 0.4645 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3906 - acc: 0.9915 - val_loss: 0.4613 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3876 - acc: 0.9915 - val_loss: 0.4583 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3845 - acc: 0.9915 - val_loss: 0.4553 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.3815 - acc: 0.9915 - val_loss: 0.4524 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3786 - acc: 0.9915 - val_loss: 0.4495 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.3757 - acc: 0.9915 - val_loss: 0.4467 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3729 - acc: 0.9915 - val_loss: 0.4438 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3701 - acc: 0.9915 - val_loss: 0.4411 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 0.4263 - acc: 0.8559 - val_loss: 0.4521 - val_acc: 0.8462\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4043 - acc: 0.8644 - val_loss: 0.4268 - val_acc: 0.8462\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3850 - acc: 0.8644 - val_loss: 0.4058 - val_acc: 0.8462\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3678 - acc: 0.8729 - val_loss: 0.3867 - val_acc: 0.8462\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.3522 - acc: 0.8729 - val_loss: 0.3683 - val_acc: 0.8462\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.3378 - acc: 0.8814 - val_loss: 0.3536 - val_acc: 0.8462\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3248 - acc: 0.8814 - val_loss: 0.3375 - val_acc: 0.8462\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3129 - acc: 0.8983 - val_loss: 0.3253 - val_acc: 0.8462\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3023 - acc: 0.8983 - val_loss: 0.3128 - val_acc: 0.8462\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2924 - acc: 0.9322 - val_loss: 0.3026 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.2830 - acc: 0.9322 - val_loss: 0.2919 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2746 - acc: 0.9492 - val_loss: 0.2822 - val_acc: 0.9231\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2666 - acc: 0.9492 - val_loss: 0.2732 - val_acc: 0.9231\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2593 - acc: 0.9492 - val_loss: 0.2652 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.2525 - acc: 0.9661 - val_loss: 0.2575 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2459 - acc: 0.9746 - val_loss: 0.2504 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2399 - acc: 0.9746 - val_loss: 0.2437 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2342 - acc: 0.9831 - val_loss: 0.2375 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2288 - acc: 0.9831 - val_loss: 0.2313 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.2237 - acc: 0.9831 - val_loss: 0.2258 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.2188 - acc: 0.9831 - val_loss: 0.2202 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.2141 - acc: 0.9831 - val_loss: 0.2151 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.2097 - acc: 0.9831 - val_loss: 0.2105 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2055 - acc: 0.9831 - val_loss: 0.2058 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.2014 - acc: 0.9831 - val_loss: 0.2013 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1976 - acc: 0.9831 - val_loss: 0.1971 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1938 - acc: 0.9831 - val_loss: 0.1930 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.1902 - acc: 0.9831 - val_loss: 0.1893 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.1869 - acc: 0.9831 - val_loss: 0.1854 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1835 - acc: 0.9831 - val_loss: 0.1819 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1803 - acc: 0.9831 - val_loss: 0.1785 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1773 - acc: 0.9831 - val_loss: 0.1752 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1742 - acc: 0.9831 - val_loss: 0.1719 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.1714 - acc: 0.9831 - val_loss: 0.1688 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.1686 - acc: 0.9831 - val_loss: 0.1658 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.1659 - acc: 0.9831 - val_loss: 0.1630 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1634 - acc: 0.9831 - val_loss: 0.1599 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1608 - acc: 0.9831 - val_loss: 0.1573 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1585 - acc: 0.9831 - val_loss: 0.1546 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1562 - acc: 0.9831 - val_loss: 0.1522 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.1540 - acc: 0.9831 - val_loss: 0.1498 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1517 - acc: 0.9831 - val_loss: 0.1473 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.1496 - acc: 0.9831 - val_loss: 0.1451 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1475 - acc: 0.9831 - val_loss: 0.1427 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1455 - acc: 0.9831 - val_loss: 0.1405 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1435 - acc: 0.9831 - val_loss: 0.1384 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1417 - acc: 0.9831 - val_loss: 0.1363 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1398 - acc: 0.9831 - val_loss: 0.1343 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1380 - acc: 0.9831 - val_loss: 0.1324 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1362 - acc: 0.9831 - val_loss: 0.1304 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1346 - acc: 0.9831 - val_loss: 0.1287 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1329 - acc: 0.9831 - val_loss: 0.1268 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1313 - acc: 0.9831 - val_loss: 0.1251 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1298 - acc: 0.9831 - val_loss: 0.1234 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.1282 - acc: 0.9831 - val_loss: 0.1218 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1268 - acc: 0.9831 - val_loss: 0.1201 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1253 - acc: 0.9831 - val_loss: 0.1187 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1239 - acc: 0.9831 - val_loss: 0.1171 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1225 - acc: 0.9831 - val_loss: 0.1157 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1212 - acc: 0.9831 - val_loss: 0.1142 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1199 - acc: 0.9831 - val_loss: 0.1128 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1186 - acc: 0.9831 - val_loss: 0.1114 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1173 - acc: 0.9831 - val_loss: 0.1100 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1161 - acc: 0.9831 - val_loss: 0.1087 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1149 - acc: 0.9831 - val_loss: 0.1074 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.1137 - acc: 0.9831 - val_loss: 0.1062 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.1126 - acc: 0.9831 - val_loss: 0.1050 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1115 - acc: 0.9831 - val_loss: 0.1039 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.1104 - acc: 0.9831 - val_loss: 0.1028 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1093 - acc: 0.9831 - val_loss: 0.1017 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1083 - acc: 0.9831 - val_loss: 0.1006 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1073 - acc: 0.9831 - val_loss: 0.0995 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1063 - acc: 0.9831 - val_loss: 0.0985 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1054 - acc: 0.9831 - val_loss: 0.0974 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1045 - acc: 0.9831 - val_loss: 0.0965 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1035 - acc: 0.9831 - val_loss: 0.0955 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1026 - acc: 0.9831 - val_loss: 0.0945 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.1018 - acc: 0.9831 - val_loss: 0.0936 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1009 - acc: 0.9831 - val_loss: 0.0927 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1001 - acc: 0.9831 - val_loss: 0.0919 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.0992 - acc: 0.9831 - val_loss: 0.0910 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.0984 - acc: 0.9831 - val_loss: 0.0901 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.0976 - acc: 0.9831 - val_loss: 0.0893 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.0968 - acc: 0.9831 - val_loss: 0.0885 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.0961 - acc: 0.9831 - val_loss: 0.0877 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.0953 - acc: 0.9831 - val_loss: 0.0869 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.0946 - acc: 0.9831 - val_loss: 0.0861 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.0938 - acc: 0.9831 - val_loss: 0.0854 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.0931 - acc: 0.9831 - val_loss: 0.0846 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.0924 - acc: 0.9831 - val_loss: 0.0838 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.0917 - acc: 0.9831 - val_loss: 0.0832 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.0911 - acc: 0.9831 - val_loss: 0.0824 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.0904 - acc: 0.9831 - val_loss: 0.0818 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.0897 - acc: 0.9831 - val_loss: 0.0811 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.0891 - acc: 0.9831 - val_loss: 0.0804 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.0885 - acc: 0.9831 - val_loss: 0.0798 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.0879 - acc: 0.9831 - val_loss: 0.0792 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.0873 - acc: 0.9831 - val_loss: 0.0785 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.0866 - acc: 0.9831 - val_loss: 0.0779 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.0861 - acc: 0.9831 - val_loss: 0.0773 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 1.3107 - acc: 0.4153 - val_loss: 1.2344 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2296 - acc: 0.4153 - val_loss: 1.1662 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.1545 - acc: 0.4153 - val_loss: 1.0984 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0830 - acc: 0.4153 - val_loss: 1.0377 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0183 - acc: 0.4153 - val_loss: 0.9829 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9602 - acc: 0.4153 - val_loss: 0.9320 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9070 - acc: 0.4153 - val_loss: 0.8865 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8577 - acc: 0.4322 - val_loss: 0.8445 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8130 - acc: 0.5508 - val_loss: 0.8053 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7719 - acc: 0.5763 - val_loss: 0.7704 - val_acc: 0.6923\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7343 - acc: 0.6102 - val_loss: 0.7370 - val_acc: 0.7692\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6993 - acc: 0.7542 - val_loss: 0.7067 - val_acc: 0.9231\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6672 - acc: 0.9153 - val_loss: 0.6783 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6374 - acc: 0.9661 - val_loss: 0.6521 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6096 - acc: 0.9661 - val_loss: 0.6273 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5833 - acc: 0.9831 - val_loss: 0.6039 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5590 - acc: 0.9831 - val_loss: 0.5822 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5361 - acc: 0.9915 - val_loss: 0.5614 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5148 - acc: 0.9915 - val_loss: 0.5418 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4944 - acc: 0.9915 - val_loss: 0.5232 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4754 - acc: 0.9915 - val_loss: 0.5058 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4573 - acc: 0.9915 - val_loss: 0.4889 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.4405 - acc: 0.9915 - val_loss: 0.4733 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4246 - acc: 0.9915 - val_loss: 0.4584 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4094 - acc: 0.9915 - val_loss: 0.4442 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3953 - acc: 0.9915 - val_loss: 0.4307 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3818 - acc: 0.9915 - val_loss: 0.4179 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3691 - acc: 0.9915 - val_loss: 0.4057 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.3570 - acc: 0.9915 - val_loss: 0.3940 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3455 - acc: 0.9915 - val_loss: 0.3830 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3347 - acc: 0.9915 - val_loss: 0.3723 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3244 - acc: 0.9915 - val_loss: 0.3623 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 280us/step - loss: 0.3146 - acc: 0.9915 - val_loss: 0.3526 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.3054 - acc: 0.9915 - val_loss: 0.3434 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.2965 - acc: 0.9915 - val_loss: 0.3345 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2881 - acc: 0.9915 - val_loss: 0.3260 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2800 - acc: 0.9915 - val_loss: 0.3179 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2723 - acc: 0.9915 - val_loss: 0.3100 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2650 - acc: 0.9915 - val_loss: 0.3025 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2581 - acc: 0.9915 - val_loss: 0.2953 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.2515 - acc: 0.9915 - val_loss: 0.2884 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2451 - acc: 0.9915 - val_loss: 0.2817 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2391 - acc: 0.9915 - val_loss: 0.2754 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2333 - acc: 0.9915 - val_loss: 0.2693 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2277 - acc: 0.9915 - val_loss: 0.2634 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2224 - acc: 0.9915 - val_loss: 0.2577 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2173 - acc: 0.9915 - val_loss: 0.2523 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.2124 - acc: 0.9915 - val_loss: 0.2471 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.2078 - acc: 0.9915 - val_loss: 0.2421 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.2033 - acc: 0.9915 - val_loss: 0.2372 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.1990 - acc: 0.9915 - val_loss: 0.2326 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1949 - acc: 0.9915 - val_loss: 0.2281 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1909 - acc: 0.9915 - val_loss: 0.2236 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.1870 - acc: 0.9915 - val_loss: 0.2193 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.1834 - acc: 0.9915 - val_loss: 0.2153 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1798 - acc: 0.9915 - val_loss: 0.2113 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1764 - acc: 0.9915 - val_loss: 0.2075 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1732 - acc: 0.9915 - val_loss: 0.2038 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.1700 - acc: 0.9915 - val_loss: 0.2003 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1670 - acc: 0.9915 - val_loss: 0.1968 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1641 - acc: 0.9915 - val_loss: 0.1935 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1612 - acc: 0.9915 - val_loss: 0.1902 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1585 - acc: 0.9915 - val_loss: 0.1872 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1558 - acc: 0.9915 - val_loss: 0.1841 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1533 - acc: 0.9915 - val_loss: 0.1812 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1508 - acc: 0.9915 - val_loss: 0.1783 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1484 - acc: 0.9915 - val_loss: 0.1755 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.1461 - acc: 0.9915 - val_loss: 0.1729 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1439 - acc: 0.9915 - val_loss: 0.1703 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1417 - acc: 0.9915 - val_loss: 0.1677 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1396 - acc: 0.9915 - val_loss: 0.1652 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1376 - acc: 0.9915 - val_loss: 0.1628 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1356 - acc: 0.9915 - val_loss: 0.1605 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1337 - acc: 0.9915 - val_loss: 0.1582 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1319 - acc: 0.9915 - val_loss: 0.1561 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1301 - acc: 0.9915 - val_loss: 0.1538 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1283 - acc: 0.9915 - val_loss: 0.1518 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1266 - acc: 0.9915 - val_loss: 0.1497 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1250 - acc: 0.9915 - val_loss: 0.1477 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1234 - acc: 0.9915 - val_loss: 0.1457 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1218 - acc: 0.9915 - val_loss: 0.1439 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.1203 - acc: 0.9915 - val_loss: 0.1420 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1188 - acc: 0.9915 - val_loss: 0.1402 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1174 - acc: 0.9915 - val_loss: 0.1384 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1160 - acc: 0.9915 - val_loss: 0.1367 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1146 - acc: 0.9915 - val_loss: 0.1350 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1133 - acc: 0.9915 - val_loss: 0.1334 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.1120 - acc: 0.9915 - val_loss: 0.1318 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1107 - acc: 0.9915 - val_loss: 0.1302 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1095 - acc: 0.9915 - val_loss: 0.1287 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1083 - acc: 0.9915 - val_loss: 0.1271 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1072 - acc: 0.9915 - val_loss: 0.1257 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1060 - acc: 0.9915 - val_loss: 0.1243 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1049 - acc: 0.9915 - val_loss: 0.1229 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.1038 - acc: 0.9915 - val_loss: 0.1215 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.1028 - acc: 0.9915 - val_loss: 0.1202 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1017 - acc: 0.9915 - val_loss: 0.1189 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.1007 - acc: 0.9915 - val_loss: 0.1176 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.0997 - acc: 0.9915 - val_loss: 0.1164 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.0988 - acc: 0.9915 - val_loss: 0.1151 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 0.9404 - acc: 0.5254 - val_loss: 1.0433 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9147 - acc: 0.5339 - val_loss: 1.0141 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8909 - acc: 0.5339 - val_loss: 0.9872 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8688 - acc: 0.5424 - val_loss: 0.9622 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8487 - acc: 0.5424 - val_loss: 0.9393 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8297 - acc: 0.5424 - val_loss: 0.9176 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8120 - acc: 0.5424 - val_loss: 0.8972 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7955 - acc: 0.5424 - val_loss: 0.8783 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7803 - acc: 0.5424 - val_loss: 0.8607 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7658 - acc: 0.5424 - val_loss: 0.8443 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.7524 - acc: 0.5508 - val_loss: 0.8290 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.7399 - acc: 0.5763 - val_loss: 0.8145 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7279 - acc: 0.5763 - val_loss: 0.8007 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7163 - acc: 0.5847 - val_loss: 0.7875 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7052 - acc: 0.6102 - val_loss: 0.7750 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6946 - acc: 0.6525 - val_loss: 0.7631 - val_acc: 0.6923\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.6843 - acc: 0.6780 - val_loss: 0.7518 - val_acc: 0.6923\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6740 - acc: 0.7119 - val_loss: 0.7405 - val_acc: 0.7692\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 269us/step - loss: 0.6634 - acc: 0.7203 - val_loss: 0.7289 - val_acc: 0.7692\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6530 - acc: 0.7627 - val_loss: 0.7171 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6425 - acc: 0.7881 - val_loss: 0.7053 - val_acc: 0.7692\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6314 - acc: 0.8136 - val_loss: 0.6930 - val_acc: 0.7692\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6198 - acc: 0.8136 - val_loss: 0.6807 - val_acc: 0.7692\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6081 - acc: 0.8390 - val_loss: 0.6688 - val_acc: 0.7692\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5962 - acc: 0.8559 - val_loss: 0.6571 - val_acc: 0.7692\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5839 - acc: 0.8814 - val_loss: 0.6439 - val_acc: 0.7692\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5719 - acc: 0.8814 - val_loss: 0.6303 - val_acc: 0.7692\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5604 - acc: 0.8983 - val_loss: 0.6170 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.5491 - acc: 0.9068 - val_loss: 0.6038 - val_acc: 0.8462\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.5379 - acc: 0.9068 - val_loss: 0.5910 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.5266 - acc: 0.9153 - val_loss: 0.5791 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.5153 - acc: 0.9237 - val_loss: 0.5673 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.5040 - acc: 0.9322 - val_loss: 0.5556 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4929 - acc: 0.9492 - val_loss: 0.5441 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4818 - acc: 0.9492 - val_loss: 0.5330 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.4712 - acc: 0.9492 - val_loss: 0.5227 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4609 - acc: 0.9492 - val_loss: 0.5129 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.4510 - acc: 0.9492 - val_loss: 0.5030 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4414 - acc: 0.9492 - val_loss: 0.4940 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4320 - acc: 0.9492 - val_loss: 0.4843 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4229 - acc: 0.9576 - val_loss: 0.4746 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4135 - acc: 0.9576 - val_loss: 0.4649 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4040 - acc: 0.9661 - val_loss: 0.4554 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3951 - acc: 0.9746 - val_loss: 0.4465 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3863 - acc: 0.9831 - val_loss: 0.4378 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3778 - acc: 0.9915 - val_loss: 0.4296 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3697 - acc: 0.9915 - val_loss: 0.4216 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.3617 - acc: 0.9915 - val_loss: 0.4135 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3538 - acc: 0.9915 - val_loss: 0.4047 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3460 - acc: 0.9915 - val_loss: 0.3961 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3383 - acc: 0.9915 - val_loss: 0.3875 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3306 - acc: 0.9915 - val_loss: 0.3791 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3232 - acc: 0.9915 - val_loss: 0.3710 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.3160 - acc: 0.9915 - val_loss: 0.3632 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3091 - acc: 0.9915 - val_loss: 0.3557 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3024 - acc: 0.9915 - val_loss: 0.3483 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2959 - acc: 0.9915 - val_loss: 0.3413 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2895 - acc: 0.9915 - val_loss: 0.3344 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2835 - acc: 0.9915 - val_loss: 0.3277 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.2776 - acc: 0.9915 - val_loss: 0.3211 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2719 - acc: 0.9915 - val_loss: 0.3150 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2663 - acc: 0.9915 - val_loss: 0.3088 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2608 - acc: 1.0000 - val_loss: 0.3028 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2556 - acc: 1.0000 - val_loss: 0.2972 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2506 - acc: 1.0000 - val_loss: 0.2917 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.2457 - acc: 1.0000 - val_loss: 0.2862 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2408 - acc: 1.0000 - val_loss: 0.2807 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2361 - acc: 1.0000 - val_loss: 0.2758 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.2316 - acc: 1.0000 - val_loss: 0.2708 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.2272 - acc: 1.0000 - val_loss: 0.2660 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.2229 - acc: 1.0000 - val_loss: 0.2614 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.2187 - acc: 1.0000 - val_loss: 0.2569 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2147 - acc: 1.0000 - val_loss: 0.2524 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2108 - acc: 1.0000 - val_loss: 0.2482 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2070 - acc: 1.0000 - val_loss: 0.2441 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.2033 - acc: 1.0000 - val_loss: 0.2401 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1997 - acc: 1.0000 - val_loss: 0.2360 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1962 - acc: 1.0000 - val_loss: 0.2324 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1928 - acc: 1.0000 - val_loss: 0.2286 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1896 - acc: 1.0000 - val_loss: 0.2250 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1864 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1834 - acc: 1.0000 - val_loss: 0.2183 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1804 - acc: 1.0000 - val_loss: 0.2151 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1775 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1746 - acc: 1.0000 - val_loss: 0.2088 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1719 - acc: 1.0000 - val_loss: 0.2058 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1692 - acc: 1.0000 - val_loss: 0.2029 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1666 - acc: 1.0000 - val_loss: 0.2000 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1641 - acc: 1.0000 - val_loss: 0.1973 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.1616 - acc: 1.0000 - val_loss: 0.1947 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1592 - acc: 1.0000 - val_loss: 0.1920 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1569 - acc: 0.9915 - val_loss: 0.1894 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1546 - acc: 0.9915 - val_loss: 0.1869 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1524 - acc: 0.9915 - val_loss: 0.1845 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1502 - acc: 0.9915 - val_loss: 0.1821 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1481 - acc: 0.9915 - val_loss: 0.1799 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1461 - acc: 0.9915 - val_loss: 0.1776 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1441 - acc: 0.9915 - val_loss: 0.1754 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1421 - acc: 0.9915 - val_loss: 0.1732 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1402 - acc: 0.9915 - val_loss: 0.1712 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 2.4908 - acc: 0.1271 - val_loss: 2.4241 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 2.2999 - acc: 0.1271 - val_loss: 2.2141 - val_acc: 0.0769\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 2.1253 - acc: 0.1356 - val_loss: 2.0351 - val_acc: 0.0769\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.9750 - acc: 0.1356 - val_loss: 1.8692 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.8440 - acc: 0.1441 - val_loss: 1.7306 - val_acc: 0.0769\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.7283 - acc: 0.1525 - val_loss: 1.6017 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.6256 - acc: 0.1695 - val_loss: 1.4975 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.5357 - acc: 0.1780 - val_loss: 1.4068 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.4547 - acc: 0.2034 - val_loss: 1.3296 - val_acc: 0.1538\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.3810 - acc: 0.2288 - val_loss: 1.2560 - val_acc: 0.1538\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.3151 - acc: 0.2288 - val_loss: 1.1916 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.2562 - acc: 0.2542 - val_loss: 1.1352 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.2021 - acc: 0.2881 - val_loss: 1.0821 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.1513 - acc: 0.3305 - val_loss: 1.0373 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.1062 - acc: 0.3644 - val_loss: 0.9954 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 1.0650 - acc: 0.4661 - val_loss: 0.9584 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 1.0284 - acc: 0.5254 - val_loss: 0.9240 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9965 - acc: 0.6102 - val_loss: 0.8929 - val_acc: 0.6923\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9668 - acc: 0.6610 - val_loss: 0.8636 - val_acc: 0.7692\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9402 - acc: 0.6780 - val_loss: 0.8362 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9160 - acc: 0.7034 - val_loss: 0.8131 - val_acc: 0.7692\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8940 - acc: 0.7288 - val_loss: 0.7939 - val_acc: 0.7692\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8735 - acc: 0.7458 - val_loss: 0.7762 - val_acc: 0.7692\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8541 - acc: 0.7458 - val_loss: 0.7589 - val_acc: 0.8462\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8363 - acc: 0.7542 - val_loss: 0.7431 - val_acc: 0.8462\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8192 - acc: 0.7627 - val_loss: 0.7274 - val_acc: 0.8462\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8032 - acc: 0.7712 - val_loss: 0.7126 - val_acc: 0.8462\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7880 - acc: 0.7712 - val_loss: 0.6986 - val_acc: 0.8462\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7741 - acc: 0.7797 - val_loss: 0.6850 - val_acc: 0.8462\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7609 - acc: 0.7797 - val_loss: 0.6723 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7487 - acc: 0.7797 - val_loss: 0.6599 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7371 - acc: 0.7797 - val_loss: 0.6484 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7264 - acc: 0.7881 - val_loss: 0.6374 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7160 - acc: 0.7881 - val_loss: 0.6275 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7061 - acc: 0.7966 - val_loss: 0.6176 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6967 - acc: 0.7966 - val_loss: 0.6089 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6876 - acc: 0.7966 - val_loss: 0.5996 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6789 - acc: 0.7966 - val_loss: 0.5913 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6704 - acc: 0.7966 - val_loss: 0.5828 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6622 - acc: 0.7966 - val_loss: 0.5747 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.6544 - acc: 0.8051 - val_loss: 0.5668 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6465 - acc: 0.8051 - val_loss: 0.5590 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.6392 - acc: 0.8051 - val_loss: 0.5515 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.6322 - acc: 0.8051 - val_loss: 0.5443 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6255 - acc: 0.8051 - val_loss: 0.5376 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6189 - acc: 0.8051 - val_loss: 0.5312 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6127 - acc: 0.8051 - val_loss: 0.5248 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6068 - acc: 0.8051 - val_loss: 0.5191 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6012 - acc: 0.8136 - val_loss: 0.5134 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.5956 - acc: 0.8136 - val_loss: 0.5076 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.5904 - acc: 0.8136 - val_loss: 0.5020 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5852 - acc: 0.8136 - val_loss: 0.4969 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.5802 - acc: 0.8220 - val_loss: 0.4916 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.5753 - acc: 0.8220 - val_loss: 0.4870 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 314us/step - loss: 0.5705 - acc: 0.8220 - val_loss: 0.4821 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.5658 - acc: 0.8220 - val_loss: 0.4781 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 343us/step - loss: 0.5615 - acc: 0.8220 - val_loss: 0.4739 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.5572 - acc: 0.8220 - val_loss: 0.4703 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.5531 - acc: 0.8220 - val_loss: 0.4660 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5491 - acc: 0.8220 - val_loss: 0.4623 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5453 - acc: 0.8220 - val_loss: 0.4590 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5417 - acc: 0.8220 - val_loss: 0.4554 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.5383 - acc: 0.8220 - val_loss: 0.4515 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5347 - acc: 0.8220 - val_loss: 0.4484 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5313 - acc: 0.8220 - val_loss: 0.4451 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5282 - acc: 0.8220 - val_loss: 0.4419 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5247 - acc: 0.8220 - val_loss: 0.4385 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.5217 - acc: 0.8220 - val_loss: 0.4354 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5184 - acc: 0.8220 - val_loss: 0.4324 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.5154 - acc: 0.8220 - val_loss: 0.4291 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.5124 - acc: 0.8220 - val_loss: 0.4261 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5096 - acc: 0.8220 - val_loss: 0.4234 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5067 - acc: 0.8220 - val_loss: 0.4199 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5038 - acc: 0.8220 - val_loss: 0.4174 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.5011 - acc: 0.8220 - val_loss: 0.4146 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4985 - acc: 0.8220 - val_loss: 0.4121 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4960 - acc: 0.8220 - val_loss: 0.4097 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4934 - acc: 0.8220 - val_loss: 0.4070 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4910 - acc: 0.8305 - val_loss: 0.4044 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.4885 - acc: 0.8305 - val_loss: 0.4024 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.4862 - acc: 0.8305 - val_loss: 0.4000 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4838 - acc: 0.8305 - val_loss: 0.3975 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4814 - acc: 0.8305 - val_loss: 0.3951 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4792 - acc: 0.8305 - val_loss: 0.3928 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.4770 - acc: 0.8305 - val_loss: 0.3906 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4750 - acc: 0.8305 - val_loss: 0.3887 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4730 - acc: 0.8305 - val_loss: 0.3866 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4709 - acc: 0.8305 - val_loss: 0.3846 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.4690 - acc: 0.8305 - val_loss: 0.3827 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4670 - acc: 0.8305 - val_loss: 0.3806 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.4653 - acc: 0.8305 - val_loss: 0.3786 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4635 - acc: 0.8305 - val_loss: 0.3767 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4620 - acc: 0.8305 - val_loss: 0.3749 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4602 - acc: 0.8305 - val_loss: 0.3732 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.4587 - acc: 0.8305 - val_loss: 0.3717 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4571 - acc: 0.8305 - val_loss: 0.3701 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4555 - acc: 0.8305 - val_loss: 0.3681 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4540 - acc: 0.8305 - val_loss: 0.3667 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.4524 - acc: 0.8305 - val_loss: 0.3649 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.4509 - acc: 0.8305 - val_loss: 0.3634 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 2.4883 - acc: 0.6271 - val_loss: 2.0223 - val_acc: 0.6923\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 2.3608 - acc: 0.6525 - val_loss: 1.9722 - val_acc: 0.7692\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.2216 - acc: 0.6695 - val_loss: 1.8506 - val_acc: 0.7692\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 2.0864 - acc: 0.6864 - val_loss: 1.7426 - val_acc: 0.7692\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.9557 - acc: 0.6949 - val_loss: 1.6323 - val_acc: 0.7692\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.8317 - acc: 0.7034 - val_loss: 1.5333 - val_acc: 0.7692\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.7209 - acc: 0.7119 - val_loss: 1.4381 - val_acc: 0.7692\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.6126 - acc: 0.7288 - val_loss: 1.3531 - val_acc: 0.7692\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.5109 - acc: 0.7373 - val_loss: 1.2689 - val_acc: 0.8462\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.4179 - acc: 0.7373 - val_loss: 1.1898 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.3326 - acc: 0.7458 - val_loss: 1.1161 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.2546 - acc: 0.7542 - val_loss: 1.0451 - val_acc: 0.8462\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1822 - acc: 0.7712 - val_loss: 0.9824 - val_acc: 0.8462\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.1172 - acc: 0.7712 - val_loss: 0.9279 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0586 - acc: 0.7797 - val_loss: 0.8766 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0004 - acc: 0.7966 - val_loss: 0.8331 - val_acc: 0.9231\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9479 - acc: 0.7966 - val_loss: 0.7887 - val_acc: 0.9231\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.8996 - acc: 0.8051 - val_loss: 0.7480 - val_acc: 0.9231\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8554 - acc: 0.8136 - val_loss: 0.7113 - val_acc: 0.9231\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8140 - acc: 0.8136 - val_loss: 0.6788 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7793 - acc: 0.8220 - val_loss: 0.6486 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7525 - acc: 0.8390 - val_loss: 0.6258 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7346 - acc: 0.8475 - val_loss: 0.6101 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7207 - acc: 0.8475 - val_loss: 0.6018 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7101 - acc: 0.8475 - val_loss: 0.5940 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7004 - acc: 0.8475 - val_loss: 0.5867 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.6914 - acc: 0.8475 - val_loss: 0.5795 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6829 - acc: 0.8559 - val_loss: 0.5729 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6749 - acc: 0.8559 - val_loss: 0.5665 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6673 - acc: 0.8559 - val_loss: 0.5602 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6603 - acc: 0.8559 - val_loss: 0.5540 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.6534 - acc: 0.8559 - val_loss: 0.5483 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6468 - acc: 0.8644 - val_loss: 0.5427 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6404 - acc: 0.8644 - val_loss: 0.5374 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6343 - acc: 0.8644 - val_loss: 0.5322 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.6284 - acc: 0.8644 - val_loss: 0.5271 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6228 - acc: 0.8644 - val_loss: 0.5223 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6174 - acc: 0.8644 - val_loss: 0.5176 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6122 - acc: 0.8644 - val_loss: 0.5130 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6071 - acc: 0.8644 - val_loss: 0.5084 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6020 - acc: 0.8644 - val_loss: 0.5040 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5972 - acc: 0.8644 - val_loss: 0.4997 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5924 - acc: 0.8644 - val_loss: 0.4954 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.5877 - acc: 0.8644 - val_loss: 0.4912 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5832 - acc: 0.8644 - val_loss: 0.4871 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5788 - acc: 0.8644 - val_loss: 0.4831 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.5745 - acc: 0.8644 - val_loss: 0.4793 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5703 - acc: 0.8644 - val_loss: 0.4754 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.5662 - acc: 0.8644 - val_loss: 0.4715 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5621 - acc: 0.8644 - val_loss: 0.4677 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5581 - acc: 0.8644 - val_loss: 0.4641 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5541 - acc: 0.8729 - val_loss: 0.4603 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5502 - acc: 0.8729 - val_loss: 0.4568 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.5464 - acc: 0.8729 - val_loss: 0.4531 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5425 - acc: 0.8814 - val_loss: 0.4496 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5388 - acc: 0.8898 - val_loss: 0.4461 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5349 - acc: 0.9068 - val_loss: 0.4426 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5313 - acc: 0.9322 - val_loss: 0.4392 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5276 - acc: 0.9746 - val_loss: 0.4359 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5239 - acc: 0.9831 - val_loss: 0.4325 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5203 - acc: 0.9831 - val_loss: 0.4292 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5167 - acc: 0.9831 - val_loss: 0.4258 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.5131 - acc: 0.9831 - val_loss: 0.4226 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5096 - acc: 0.9915 - val_loss: 0.4193 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5061 - acc: 0.9915 - val_loss: 0.4161 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5027 - acc: 0.9915 - val_loss: 0.4129 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.4992 - acc: 0.9915 - val_loss: 0.4098 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4957 - acc: 0.9915 - val_loss: 0.4066 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4923 - acc: 0.9915 - val_loss: 0.4035 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4888 - acc: 0.9915 - val_loss: 0.4003 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4854 - acc: 0.9915 - val_loss: 0.3973 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4820 - acc: 0.9915 - val_loss: 0.3941 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4786 - acc: 0.9915 - val_loss: 0.3911 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4753 - acc: 0.9915 - val_loss: 0.3880 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4720 - acc: 0.9915 - val_loss: 0.3850 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.4686 - acc: 0.9915 - val_loss: 0.3821 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4653 - acc: 0.9915 - val_loss: 0.3790 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.4621 - acc: 0.9915 - val_loss: 0.3761 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4587 - acc: 0.9915 - val_loss: 0.3731 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4555 - acc: 0.9915 - val_loss: 0.3703 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4522 - acc: 0.9915 - val_loss: 0.3673 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.4490 - acc: 0.9915 - val_loss: 0.3644 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4458 - acc: 0.9915 - val_loss: 0.3616 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.4426 - acc: 0.9915 - val_loss: 0.3588 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4395 - acc: 0.9915 - val_loss: 0.3559 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4363 - acc: 0.9915 - val_loss: 0.3532 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4333 - acc: 0.9915 - val_loss: 0.3504 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.4301 - acc: 0.9915 - val_loss: 0.3476 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4270 - acc: 0.9915 - val_loss: 0.3449 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 289us/step - loss: 0.4240 - acc: 0.9915 - val_loss: 0.3422 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4209 - acc: 0.9915 - val_loss: 0.3396 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4179 - acc: 0.9915 - val_loss: 0.3369 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4149 - acc: 0.9915 - val_loss: 0.3342 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4119 - acc: 0.9915 - val_loss: 0.3316 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4090 - acc: 0.9915 - val_loss: 0.3290 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4060 - acc: 0.9915 - val_loss: 0.3264 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4030 - acc: 0.9915 - val_loss: 0.3239 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.4001 - acc: 0.9915 - val_loss: 0.3214 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3972 - acc: 0.9915 - val_loss: 0.3188 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.3944 - acc: 0.9915 - val_loss: 0.3164 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 0.8078 - acc: 0.5169 - val_loss: 0.7488 - val_acc: 0.5385\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7695 - acc: 0.5169 - val_loss: 0.7109 - val_acc: 0.5385\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7340 - acc: 0.5932 - val_loss: 0.6746 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7012 - acc: 0.6271 - val_loss: 0.6412 - val_acc: 0.6923\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6717 - acc: 0.7203 - val_loss: 0.6107 - val_acc: 0.9231\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.6436 - acc: 0.8559 - val_loss: 0.5825 - val_acc: 0.9231\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.6177 - acc: 0.8644 - val_loss: 0.5559 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.5936 - acc: 0.8814 - val_loss: 0.5311 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.5709 - acc: 0.8814 - val_loss: 0.5081 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.5497 - acc: 0.8814 - val_loss: 0.4862 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.5297 - acc: 0.9068 - val_loss: 0.4651 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5108 - acc: 0.9237 - val_loss: 0.4460 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4932 - acc: 0.9237 - val_loss: 0.4279 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.4765 - acc: 0.9322 - val_loss: 0.4108 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4607 - acc: 0.9322 - val_loss: 0.3950 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.4460 - acc: 0.9492 - val_loss: 0.3796 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4319 - acc: 0.9576 - val_loss: 0.3658 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.4185 - acc: 0.9576 - val_loss: 0.3523 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4058 - acc: 0.9576 - val_loss: 0.3397 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.3938 - acc: 0.9576 - val_loss: 0.3279 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.3825 - acc: 0.9576 - val_loss: 0.3165 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3717 - acc: 0.9576 - val_loss: 0.3058 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3615 - acc: 0.9576 - val_loss: 0.2956 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.3517 - acc: 0.9576 - val_loss: 0.2859 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.3425 - acc: 0.9661 - val_loss: 0.2769 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 299us/step - loss: 0.3337 - acc: 0.9661 - val_loss: 0.2681 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3253 - acc: 0.9661 - val_loss: 0.2598 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3172 - acc: 0.9746 - val_loss: 0.2520 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.3095 - acc: 0.9746 - val_loss: 0.2444 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3020 - acc: 0.9746 - val_loss: 0.2372 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.2949 - acc: 0.9746 - val_loss: 0.2304 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.2881 - acc: 0.9746 - val_loss: 0.2239 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.2816 - acc: 0.9746 - val_loss: 0.2176 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2754 - acc: 0.9746 - val_loss: 0.2117 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.2695 - acc: 0.9746 - val_loss: 0.2060 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2638 - acc: 0.9746 - val_loss: 0.2006 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.2583 - acc: 0.9746 - val_loss: 0.1955 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2530 - acc: 0.9831 - val_loss: 0.1905 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2479 - acc: 0.9831 - val_loss: 0.1856 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2431 - acc: 0.9831 - val_loss: 0.1811 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2384 - acc: 0.9831 - val_loss: 0.1767 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2339 - acc: 0.9831 - val_loss: 0.1726 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2296 - acc: 0.9831 - val_loss: 0.1685 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2254 - acc: 0.9831 - val_loss: 0.1647 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2214 - acc: 0.9831 - val_loss: 0.1610 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.2174 - acc: 0.9831 - val_loss: 0.1574 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2137 - acc: 0.9831 - val_loss: 0.1540 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2100 - acc: 0.9831 - val_loss: 0.1506 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2065 - acc: 0.9831 - val_loss: 0.1475 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2031 - acc: 0.9831 - val_loss: 0.1443 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1997 - acc: 0.9831 - val_loss: 0.1413 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1965 - acc: 0.9831 - val_loss: 0.1384 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1934 - acc: 0.9831 - val_loss: 0.1357 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1904 - acc: 0.9831 - val_loss: 0.1330 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.1874 - acc: 0.9831 - val_loss: 0.1303 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1846 - acc: 0.9831 - val_loss: 0.1279 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1818 - acc: 0.9831 - val_loss: 0.1254 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1791 - acc: 0.9831 - val_loss: 0.1231 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1766 - acc: 0.9831 - val_loss: 0.1208 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1740 - acc: 0.9915 - val_loss: 0.1186 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1716 - acc: 0.9915 - val_loss: 0.1165 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1692 - acc: 1.0000 - val_loss: 0.1144 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1669 - acc: 1.0000 - val_loss: 0.1124 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1646 - acc: 1.0000 - val_loss: 0.1105 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1624 - acc: 1.0000 - val_loss: 0.1086 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1602 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1582 - acc: 1.0000 - val_loss: 0.1050 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1561 - acc: 1.0000 - val_loss: 0.1033 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1542 - acc: 1.0000 - val_loss: 0.1016 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1522 - acc: 1.0000 - val_loss: 0.1000 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1503 - acc: 1.0000 - val_loss: 0.0984 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.1485 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.1467 - acc: 1.0000 - val_loss: 0.0954 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1450 - acc: 1.0000 - val_loss: 0.0939 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1433 - acc: 1.0000 - val_loss: 0.0925 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1416 - acc: 1.0000 - val_loss: 0.0911 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1400 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1384 - acc: 1.0000 - val_loss: 0.0885 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1369 - acc: 1.0000 - val_loss: 0.0872 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1354 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.1339 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1324 - acc: 1.0000 - val_loss: 0.0835 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1310 - acc: 1.0000 - val_loss: 0.0824 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1296 - acc: 1.0000 - val_loss: 0.0813 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1283 - acc: 1.0000 - val_loss: 0.0802 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1270 - acc: 1.0000 - val_loss: 0.0791 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.1257 - acc: 1.0000 - val_loss: 0.0781 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1244 - acc: 1.0000 - val_loss: 0.0770 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.1231 - acc: 1.0000 - val_loss: 0.0761 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.1219 - acc: 1.0000 - val_loss: 0.0751 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.1207 - acc: 1.0000 - val_loss: 0.0741 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1195 - acc: 1.0000 - val_loss: 0.0732 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1184 - acc: 1.0000 - val_loss: 0.0723 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1173 - acc: 1.0000 - val_loss: 0.0714 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1162 - acc: 1.0000 - val_loss: 0.0705 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1151 - acc: 1.0000 - val_loss: 0.0697 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1141 - acc: 1.0000 - val_loss: 0.0688 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1130 - acc: 1.0000 - val_loss: 0.0680 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1120 - acc: 1.0000 - val_loss: 0.0672 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1110 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "e75ff150-e671-4405-f099-ba3132c3002b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.46815712,  2.41613978],\n",
              "       [-2.04213808, -1.20055142],\n",
              "       [-2.26598031, -2.51797746],\n",
              "       [-1.62561816, -1.97201226],\n",
              "       [-2.12949607, -1.76064808],\n",
              "       [-1.32853889, -2.05415965],\n",
              "       [-1.86366758, -1.96105974],\n",
              "       [ 3.1426414 ,  0.23838493],\n",
              "       [ 2.76856724,  0.87507987],\n",
              "       [ 3.33006485,  0.80105572],\n",
              "       [ 3.99255757,  0.1366972 ],\n",
              "       [ 2.79798337,  1.20174003],\n",
              "       [ 1.67824072,  0.59967329]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ce5dc1cc-ace4-4840-9f46-605bf3e4208e",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c6e5735f-917b-4ea7-98e5-cb3b977e9602",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e7536899-36a1-456f-a968-20a1ea6957e9",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "096e21b8-aa8c-4560-83df-2db055bfe900",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4e6c92f8d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/T+7m9lxuAofrAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "159bd071-e12c-4553-c251-8a895652790e",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4e6c8a2ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXgUVdbA4d8hYd+XsO+KQliCEBYF\n2WFAERRQQBgERUZH1OFzY9z3XUcdEUVAQRBEVAQVJSwOCqIEJEFQAQNIwmLYIWwJnO+PqoROyNJA\nOk13n/d5+klX1e3qU12dOl333rolqooxxpjQVcjfARhjjPEvSwTGGBPiLBEYY0yIs0RgjDEhzhKB\nMcaEOEsExhgT4iwRhAARCRORwyJSOz/L+pOIXCwi+d73WUS6icgWj+nfReRKb8qew3tNFJEHz/X1\noU5ERorIt7ks/15EhhdcRIEr3N8BmDOJyGGPyRLAceCkO/0PVZ1+NutT1ZNAqfwuGwpU9dL8WI+I\njASGqmonj3WPzI91G3O+LBFcgFQ140Ds/uIcqaoLcyovIuGqmlYQsRmTF/s+Bh6rGgpAIvK0iHwk\nIjNE5BAwVEQuF5EVIrJfRHaIyBsiUtgtHy4iKiJ13elp7vL5InJIRH4QkXpnW9Zd3ktENojIARH5\nr4gsy+l03MsY/yEim0Rkn4i84fHaMBH5j4jsEZEEoGcun89DIjIzy7xxIvKq+3ykiPzqbs8f7q/1\nnNaVKCKd3OclROQDN7Z1QMssZR8WkQR3vetEpI87vynwJnClW+222+Ozfdzj9be5275HROaISDVv\nPpuz+ZzT4xGRhSKyV0R2isj9Hu/ziPuZHBSRWBGpnl01nGe1i/t5LnXfZy/wsIg0EJEl7nvsdj+3\nsh6vr+NuY7K7/HURKebG3MijXDUROSIiFXPaXo+yPcWpyjsgIq8D4rEs13hCnqra4wJ+AFuAblnm\nPQ2cAK7BSebFgVZAG5yzvPrABmC0Wz4cUKCuOz0N2A1EA4WBj4Bp51C2MnAI6Osu+z8gFRiew7Z4\nE+PnQFmgLrA3fduB0cA6oCZQEVjqfH2zfZ/6wGGgpMe6/wKi3elr3DICdAGOAs3cZd2ALR7rSgQ6\nuc9fBr4FygN1gPVZyt4AVHP3yY1uDFXcZSOBb7PEOQ143H3ew42xOVAMeAtY7M1nc5afc1lgF3A3\nUBQoA7R2l/0biAMauNvQHKgAXJz1swa+T9/P7ralAbcDYTjfx0uArkAR93uyDHjZY3t+cT/Pkm75\ndu6yCcAzHu9zD/BZDtuZ8Zm673EYuA7nu3ifG1N6jDnGYw+1RHChP8g5ESzO43X3Ah+7z7M7uL/t\nUbYP8Ms5lL0Z+M5jmQA7yCEReBljW4/lnwL3us+X4lSRpS+7KuvBKcu6VwA3us97Ab/nUvYL4A73\neW6J4E/PfQH807NsNuv9BbjafZ5XIpgCPOuxrAxOu1DNvD6bs/yc/w6szKHcH+nxZpnvTSJIyCOG\nAenvC1wJ7ATCsinXDtgMiDu9BuiXwzo9E8HNwPceywrl9l30jMcealVDAWyb54SINBSRL91T/YPA\nk0ClXF6/0+P5EXJvIM6pbHXPONT5D0vMaSVexujVewFbc4kX4ENgsPv8Rnc6PY7eIvKjW02wH+fX\neG6fVbpqucUgIsNFJM6t3tgPNPRyveBsX8b6VPUgsA+o4VHGq32Wx+dcC+eAn53cluUl6/exqojM\nEpEkN4b3s8SwRZ2OCZmo6jKcX/LtRaQJUBv40ov3z/pdPIXHdzGPeEKeJYLAlbXr5Ds4v0AvVtUy\nwKN41JH6yA6cX6wAiIiQ+cCV1fnEuAPnAJIur+6ts4BuIlIDp+rqQzfG4sBs4DmcaptywAIv49iZ\nUwwiUh8Yj1M9UtFd728e682rq+t2nOqm9PWVxqmCSvIirqxy+5y3ARfl8LqclqW4MZXwmFc1S5ms\n2/cCTm+3pm4Mw7PEUEdEwnKIYyowFOfsZZaqHs+hnKdM3w8RKYTHdzOPeEKeJYLgURo4AKS4jW3/\nKID3/AJoISLXiEg4Tr1zhI9inAX8S0RquA2HD+RWWFV34lRfvI9TLbTRXVQUp544GTgpIr1x6o69\njeFBESknznUWoz2WlcI5GCbj5MRbcc4I0u0Cano22mYxA7hFRJqJSFGcRPWdquZ4hpWL3D7nuUBt\nERktIkVFpIyItHaXTQSeFpGLxNFcRCrgJMCdOJ0SwkRkFB5JK5cYUoADIlILp3oq3Q/AHuBZcRrg\ni4tIO4/lH+BU3dyIkxS88QXQXET6up/xGDJ/F3OLJ+RZIgge9wA34TTevoPTqOtTqroLGAi8ivOP\nfRHwM84vr/yOcTywCFgLrMT5VZ+XD3Hq/DOqhVR1P85B4jOcBtcBOAcRbzyG88tzCzAfj4OUqsYD\n/wV+cstcCvzo8doYYCOwS0Q8q3jSX/81ThXOZ+7rawNDvIwrqxw/Z1U9AHQH+uMkpw1AR3fxS8Ac\nnM/5IE7DbTG3yu9W4EGcjgMXZ9m27DwGtMZJSHOBTzxiSAN6A41wzg7+xNkP6cu34Ozn46q63JsN\n9vguvuTGWDtLjDnGY043yBhz3txT/e3AAFX9zt/xmMAlIlNxGqAf93csocAuKDPnRUR64vTQOYrT\n/TAV51exMefEbW/pCzT1dyyhwqqGzPlqDyTg1I3/DbjOy8Y9Y84gIs/hXMvwrKr+6e94QoVVDRlj\nTIjz2RmBiEwWkb9E5Jcclot7SfomEYkXkRa+isUYY0zOfNlG8D7O+Co5df/qhXMpewOcy+HHu39z\nValSJa1bt27+RGiMMSFi1apVu1U12+7dPksEqrpU3IHLctAXmOp2TVvh9s2upqo7cltv3bp1iY2N\nzcdIjTEm+IlIjlfj+7OxuAaZL0tPJIerUkVklDgjIcYmJycXSHDGGBMqAqLXkKpOUNVoVY2OiMjt\nwlVjjDFny5+JIInM47bU5NzGVTHGGHMe/JkI5gLD3N5DbYEDebUPGGOMyX8+aywWkRlAJ6CSiCTi\njPVRGEBV3wa+whlTfhPOkLojfBWLMcaYnPmy19DgPJYrcIev3t8YY4x3AqKx2BhjjO/YoHPGBJLj\nx+Htt2HPHn9HYvzhmmugVat8X60lAmMCxeHD0K8fxMSA2M21QlL16pYIjAlZe/fCVVdBbCy8/z7c\ndFOuxU+cgB07YPv204+kJOdvSkrBhHy+VOHQodNxHzzo74jOVKqUc2yuUQPKls07Px87dnp/7N7t\nbOPZeAu47ZyjzZklAmMuFOvWwcsvO9U/WcXGwtatMHs2XHstqanwyy+wejX8+efpg2X6AX/37jNX\nUbiwc9AqXdr3m5JfSpaERo2gWzfvDrQFyTNRJSXBX3/l/ZoiRZyk0bo1RERAobNspW3Z8txizYsl\nAmMuBCtWOL/4T56EKlXOXF6yJMyfz9LwLjzSEX76yfl1Cc7BsUoV5yBfuza0bXv6V2r16lCtmvO8\nYsWzP/CY0GCJwBh/W7gQrr3WOWLHxEA2o+smJ8P99zu1QrVrwz//6VQVR0c7xcPtP9mcB/v6GONp\n61a44w7nyFtQ1qyBhg3hm2+gatWM2UeOODni88/h00+dtuKxY+GRR6BEiYILzwQ/SwTGpPvtN+je\n3an4vfzyAnvbtMFDSRrzCol/lCMhxmkOiI116v+PHYMyZeDqq+Ghh6Bx4wILy4QQSwTGAKxaBT17\nQlgY/O9/EBXl9UtPnDhdX6/q9G7J2lNn+3bYv//0a44ePd2rZ88eYMrpZcWLQ4sWcPvtTrNBhw5O\nI6MxvmKJwISO6dOdivb0o7angwehRg22T4lhfmwDjiw9s4iqc9D2PLhv3553LVJ4uFP9X7786V4v\nRYpA/frQvn3mht1ateCSS6zO3xQs+7qZ0PDmm3DnndCmDRrdiuPHnf70KSlwOAX2pBTnqYN3E9Mp\n23sjZciuh061apm7ZJYs6RzY0w/u59JN0JiCZInABJ2jR2HtWvdiHVUqjn+ai6c+yo/VrmXEXzPY\nPKnYGScFIk6zwAsvQO/e2ffgBKcvu/1aN8HGvtImaGzc6AzD8957sG8fhJHG69zNHbzF+9zE8yUm\n0qxFOFfXcX6pZ+1rX7y4v7fAGP+wRGAChirExTmP7Bpjt21zfq336wdD+hyi7euDqLzyK/4ceB99\nxz3P8IpWP2NMdiwRGL86duz0Jfrbt2c/uoKq06ln7lwo9ec6OuC05JYoDs3LQqdyULY6VGsJbdpC\n2dIKL7/r1A+9/Ta1//GPAt4qYwKLJQJToA4ehI8/huXLnb7y69Y5oyrkpVgxuD/qGx7ZeR3hJ446\nM4+6j50eBee4f8uUgS++cLqEGmNyZYnA+Fx6lc7bb8O0aU5PnYoVnSESrrkGGjQ4XWef0xWz1b+f\nRdFbhjpXVM2a5Rzoc1OmjFX6G+MlSwTm/Gzd6hzhT5zINDs1FXbuhD8S4I9NsP8ANAqDTxpCsyio\nWsXtU38UiHcfOTl0CCZOhHbtYN48KFfOhxtkTOixRGDO2cEV6wm/qjtF9+8iNdz59a0Kp045j3JA\nS6B1GIQXdYZBlq3A1nN4s/79YcoUG2THGB+wRGDO2oIF8PnDK3lyZU9SKMJ1xVfzR8lmgHPFbNOm\nTrVP69bQqROUCKDx740JRZYIjHd+/JGjT7zAL3EnObodXpDFHC8bwY53Y1je/yK7ctaYAGaJwOTp\n5NETHL76Rk7t2UdhqUObqlD8so6UmjiBitWr+zs8Y8x5skRgcrVyJSy9bhz37EngoRbzGfFRT6pe\n7O+ojDH5yRKBydb+/fDwwzBj3F42yVPsbNaDp2N7XlD3jDXG5A9LBCYTVZg5E8aMcYZXXtj8acrF\n7Uc+eAksCRgTlCwRGOcKr4QEYmOd0ZpXrYZOkfDEE7u59M434eaboVkzf0dpjPERSwQhTn/fwPGO\n3Sm260+igffTF6wHbsMZXP+pp/wVnjGmAPg0EYhIT+B1IAyYqKrPZ1leB5gMRAB7gaGqmujLmIzj\n4EGY/+zPdHv5b5w8CfeUmkLP/iXp8Tco6nlbxKgoZ4xmY0zQ8lkiEJEwYBzQHUgEVorIXFVd71Hs\nZWCqqk4RkS7Ac8DffRWTcWzdCve1/Y53d/bmaOGyfP9EDC/+61JKlvR3ZMYYf/DlGUFrYJOqJgCI\nyEygL06lQ7pI4P/c50s4PXak8ZHERHiy7VdM2dkfrVWHsstiGFCrlr/DMsb4kS+vB60BbPOYTnTn\neYoD+rnPrwNKi0jFrCsSkVEiEisiscl53Snc5GjHDnil1Qze3tmXU5dGUmLVd87d0o0xIc3fAwPc\nC3QUkZ+BjkAScMbo9Ko6QVWjVTU6IiKioGMMCsePw6RWb/PKziEcaX4FJX9a4txV3RgT8nxZNZQE\neP7crOnOy6Cq23HPCESkFNBfVff7MKbQpMp3vZ7j4aSH2NmqN1X/N8vG6jfGZPDlGcFKoIGI1BOR\nIsAgYK5nARGpJCLpMfwbpweRyU+qJA+/j25LHmJ53RupuuxTSwLGmEx8dkagqmkiMhr4Bqf76GRV\nXSciTwKxqjoX6AQ8JyIKLAXu8FU8IUMVXn0VNmwA4NSWrUQs+IZJxe7g2p/egML+rg00xlxofHod\ngap+BXyVZd6jHs9nA7N9GUPImT0b7r0XKlWC8HBSjhTiRZ6k6fsPUzHCxogwxpzJriwOJqmp8OCD\n0KQJrFnD+t/DaNkSel4LT97g7+CMMRcqSwTBZOJE2LQJ5s3jxMkwhg6FUqVg/Hhs1FBjTI4sEQSL\nw4fhiSfgyivh6qt54mH4+Wf47DOoWtXfwRljLmSWCILFa6/Brl3w2WcsWy48/7wzaOi11/o7MGPM\nhc4SQTDYvRtefBGuvZYDkZfz98ugbl0nNxhjTF4sEQSDZ56BlBT0mWcZORL+/BOWLoXSpf0dmDEm\nEFgiCHRbtsBbb8GIEby1pBGzZzsnB1dc4e/AjDGBwhJBoHv0UShUiPh+j/N/18HVV8M99/g7KGNM\nILHLTANZXBxMm8aJ2+/iujtrUrkyTJkChWyvGmPOgp0RBLJ//xvKluWlQmNJSID//Q8qnjGItzHG\n5M5+Owaq2FiYP599t/2bp8eV58YboUMHfwdljAlElggC1QcfQNGi/N9voxCB55/P+yXGGJMdSwSB\nKDUVZsxgd9vevD+nHA88YDcaM8acO0sEgWjhQkhO5rltQ6lZE+67z98BGWMCmTUWB6Jp0zhRqjxv\nJvRi4lQoUcLfARljApmdEQSaw4dhzhy+r3YDJcoV5QYbXtoYc54sEQSaOXPgyBGeTxxK//5QtKi/\nAzLGBDqrGgo006aRElGHhclXEDPY38EYY4KBnREEkp07ISaGrysMoUrVQnTq5O+AjDHBwBJBIPny\nSzh1iucSBnLDDRAW5u+AjDHBwBJBIFmwgJRy1VmV2pTBVi1kjMkn1kYQKE6ehJgYlpXoS73yQps2\n/g7IGBMs7IwgUKxaBfv2MWVHDwYNspvRG2Pyj50RBIoFC5w/2o1Fg/wcizEmqFgiCBQLFrChdAvK\nV42gaVN/B2OMCSZWNRQIDh5Ef/iBzw73YMAAqxYyxuQvSwSB4NtvkbQ0vtYe9O/v72CMMcHGqoYC\nwYIFHAsrwfaaV9Cihb+DMcYEG5+eEYhITxH5XUQ2icjYbJbXFpElIvKziMSLyFW+jCdQnfx6AYtP\ndeKaAUWtWsgYk+98lghEJAwYB/QCIoHBIhKZpdjDwCxVvQwYBLzlq3gC1ubNhP2xka/1b1YtZIzx\nCV+eEbQGNqlqgqqeAGYCfbOUUaCM+7wssN2H8QSmZcsA+CWii11EZozxCV8mghrANo/pRHeep8eB\noSKSCHwF3JndikRklIjEikhscnKyL2K9YJ1Yv4lTCM2vb0Aha9o3xviAvw8tg4H3VbUmcBXwgYic\nEZOqTlDVaFWNjoiIKPAg/WnH8gQSqUmf6+3GA8YY3/BlIkgCPG+pXtOd5+kWYBaAqv4AFAMq+TCm\ngHNqYwKbC13EFVf4OxJjTLDyZSJYCTQQkXoiUgSnMXhuljJ/Al0BRKQRTiIIrbqfPJRO/oNDlepT\npIi/IzHGBCufJQJVTQNGA98Av+L0DlonIk+KSB+32D3ArSISB8wAhquq+iqmQHN83xEqpe4krEF9\nf4dijAliPr2gTFW/wmkE9pz3qMfz9UA7X8YQyH77KoEooEKri/wdijEmiPm7sdjkYuuSBADqd7Mz\nAmOM71giuIDtW+Ukgoi2dkZgjPEdSwQXsFOb/uBI4TJQoYK/QzHGBDFLBBeopCSofDiBw5Xr27jT\nxhifskRwgVqxAuqTQPglVi1kjPEtSwQXqBXLT1GPzZS9zBqKjTG+ZfcjuEBt/N92inEc7BoCY4yP\n5XlGICJ3ikj5ggjGOE6cgMPxTo8hLrKqIWOMb3lTNVQFWCkis9wbzVjLpY+tWQO1Uv9wJurbGYEx\nxrfyTASq+jDQAJgEDAc2isizImI/VX1k0SKnoVjDwqB2bX+HY4wJcl41Frvj/+x0H2lAeWC2iLzo\nw9hCkipMngxtI/5AateGwoX9HZIxJsh500Zwt4isAl4ElgFNVfV2oCVgN0/MZ0uXwqZN0Lx0glUL\nGWMKhDe9hioA/VR1q+dMVT0lIr19E1bomjQJypSBSgcT4KLr/B2OMSYEeFM1NB/Ymz4hImVEpA2A\nqv7qq8BC0f798PHHMGLAIWR3sp0RGGMKhDeJYDxw2GP6sDvP5LMPP4Rjx2BUN+s6aowpON4kAvG8\nWYyqnsIuRPOJiROheXNoVMS6jhpjCo43iSBBRO4SkcLu424gwdeBhZrVq+Hnn+H/+v6B3HcvlC0L\nl1zi77CMMSHAm0RwG3AFzo3nE4E2wChfBhWKxo+H6CLxDHm7PRw8CAsXQqlS/g7LGBMC8qziUdW/\ncG48b3wkORnip/zMt9KFQuElYcliaNTI32EZY0JEnolARIoBtwCNgWLp81X1Zh/GFVLeHq+8kXob\nRSKKw/ffQ926/g7JGBNCvKka+gCoCvwN+B9QEzjky6BCyfHjsOXVT2nDTxR+4RlLAsaYAudNIrhY\nVR8BUlR1CnA1TjuByQczp6Vx/4EHOVwnEoYN83c4xpgQ5E0iSHX/7heRJkBZoLLvQgodqrD1sclc\nygZKvv4chIX5OyRjTAjy5nqACe79CB4G5gKlgEd8GlWI+PbLFEYmPc6ui9tRpc81/g7HGBOick0E\nIlIIOKiq+4ClgF3hlI92PPIWndnB8Xc+thvUG2P8JteqIfcq4vsLKJaQkpaqRMdP5rcqHSjapZ2/\nwzHGhDBv2ggWisi9IlJLRCqkP3weWZBbO2U1l5z6jcN9h/o7FGNMiPOmjWCg+/cOj3mKVROdl5R3\npnGcIjR4cIC/QzHGhDhvriyud64rF5GewOtAGDBRVZ/Psvw/QGd3sgRQWVXLnev7BYy0NBqumcGP\nEb3pUKe8v6MxxoQ4b64szrZzu6pOzeN1YcA4oDvOGEUrRWSuqq73WMcYj/J3Apd5GXdA2zl9EVXT\ndrH/qiH+DsUYY7yqGmrl8bwY0BVYDeSaCIDWwCZVTQAQkZlAX2B9DuUHA495EU/A2//WdIpSjkb3\nXOXvUIwxxquqoTs9p0WkHDDTi3XXALZ5TKePXHoGEakD1AMW57B8FO6Ip7Vr1/birS9gKSnUWfUp\nc8vcyMCmxfIub4wxPuZNr6GsUnAO2vlpEDBbVU9mt1BVJ6hqtKpGR0RE5PNbF6xjsz6n+MkUkntY\ntZAx5sLgTRvBPJxeQuAkjkhglhfrTgJqeUzXdOdlZxCZeyUFrX3vfEwaNWk06kp/h2KMMYB3bQQv\nezxPA7aqaqIXr1sJNBCRejgJYBBwY9ZCItIQKA/84MU6A1tqKuVWL2JG4cEM7XguJ2PGGJP/vEkE\nfwI7VPUYgIgUF5G6qroltxepapqIjAa+wek+OllV14nIk0Csqs51iw4CZnreFzlY6YofKZ56iD1t\ne1CkiL+jMcYYhzeJ4GOcW1WmO+nOa5V98dNU9SvgqyzzHs0y/bgXMQSFnR8soDKFqDG0i79DMcaY\nDN7UT4Sr6on0Cfe5/Z49ByfnL+AnWtN1gF1EZoy5cHiTCJJFpE/6hIj0BXb7LqQgtXcv1RJXsq56\nD6pU8XcwxhhzmjdVQ7cB00XkTXc6EbBbaZ2l/Z8uphynCOvVw9+hGGNMJt5cUPYH0FZESrnTh30e\nVRDaNW0BQhmaj2rt71CMMSaTPKuGRORZESmnqodV9bCIlBeRpwsiuKChSvmVC1herAvNWxX2dzTG\nGJOJN20EvVR1f/qEe7cyGyTnLJxYt5HKR7ayt2UPuxGZMeaC400iCBORoukTIlIcKJpLeZPF5ncW\nAFDl79Y+YIy58HjTWDwdWCQi7wECDAem+DKoYFNozickUJ/Lh17k71CMMeYMeZ4RqOoLwNNAI+BS\nnCuF6/g4rqBx8NOFNEj8lqVN/0nJkv6OxhhjzuTNGQHALpyB564HNgOf+CyiYHLqFAf+OZb91KL1\n+yExpp4xJgDlmAhE5BKcm8UMxrmA7CNAVLVzTq8xmSWPn02tXauYdOX73NLC7j1gjLkw5XZG8Bvw\nHdBbVTcBiMiYXMobT6mpnPr3g/wiTfjbB0P9HY0xxuQotzaCfsAOYImIvCsiXXEai40XEh97lyqH\n/iC233PUrBPm73CMMSZHOSYCVZ2jqoOAhsAS4F9AZREZLyLWDzIX+sWXRLxwD8vCO9B3wtX+DscY\nY3LlTa+hFFX9UFWvwbnL2M/AAz6PLFB9+CGn+l7L2lON2fjcJ5SvYCdRxpgL21ndJktV97n3D+7q\nq4AC2rvvokOHskzb8Vy3xdx0TyV/R2SMMXnytvuoyUtKCjpmDD+V6sqNheeycmpxG07CGBMQLBHk\nlzlzkJQU7uEx3vikONWq+TsgY4zxjiWCfJL2/jSSpA61b7iCfv38HY0xxnjvrNoITA527aLQogVM\n0yHcPcY+UmNMYLGjVn746CMK6Sl+qDeE1nbfGWNMgLGqoXxwbNI01nMZV/4j0hqIjTEBx84Iztfv\nv1MsfiXTZShDbSQJY0wAsjOC86TTpnOKQuzsOIgaNfwdjTHGnD1LBOcjKYnjb09mKV3pc1t1f0dj\njDHnxKqGztWmTdC+Pbr/AC+VfIK+ff0dkDHGnBtLBOciPh7atydt/yG6sIQGwy6nmN1uwBgToKxq\n6GylpEDXrmjRogwou4SEIo34/HF/B2WMMefOp2cEItJTRH4XkU0iMjaHMjeIyHoRWSciH/oynnwx\nZw7s3s077afx+YZGTJoElSv7OyhjjDl3PjsjEJEwYBzQHUgEVorIXFVd71GmAfBvoJ2q7hORC/+Q\nOnUqR6vW5Y6POjBqFPTu7e+AjDHm/PjyjKA1sElVE1T1BDATyNqkeiswTlX3AajqXz6M5/wlJaEL\nFzLhyN+pf3EhXnnF3wEZY8z582UiqAFs85hOdOd5ugS4RESWicgKEemZ3YpEZJSIxIpIbHJyso/C\nzdvJDz5ETp1iwtG/8+GHUKqU30Ixxph84+9eQ+FAA6ATMBh4V0TKZS3k3gwnWlWjIyIiCjjEjCDY\n9dIUlnM5D0xsQKtW/gnDGGPymy8TQRJQy2O6pjvPUyIwV1VTVXUzsAEnMVxw5jy+hup715HUZRjD\nhvk7GmOMyT++TAQrgQYiUk9EigCDgLlZyszBORtARCrhVBUl+DCmc/LXX/Dn01M5IUW4bsYN/g7H\nGGPylc96DalqmoiMBr4BwoDJqrpORJ4EYlV1rrush4isB04C96nqHl/FdFaOHoXRo2HfPo5shWGn\nFnOk2zWUq1zB35EZY0y+ElX1dwxnJTo6WmNjY33/RnPnQt++cMklbNlZlMNHw2m85E2k3RW+f29j\njMlnIrJKVaOzW2ZXFuckJgZKlODoj/FEVi/KLf+A/7bzd1DGGJP//N1r6MIVEwMdO7Lo+6IcPQp9\n+vg7IGOM8Q1LBNnZtg1+/x26dWPuXChdGjp29HdQxhjjG5YIsrNwIQCnunZn3jzo1QuKFPFzTMYY\n4yOWCLITEwNVqxJ7rAk7d8I11/g7IGOM8R1LBFmdOuWcEXTrxtx5QlgYXHWVv4MyxhjfsUSQVXw8\nJCdntA+0bw8V7NIBY0wQs5awq6sAABYWSURBVESQlds+sD2yG2vXWrWQMSb4WSLIKiYGIiNZt98Z\nKDU628svjDEmeFgi8HTsGCxdCt26keCOeFS/vn9DMsYYX7NE4GnZMicZdO9OQoLTZbR6dX8HZYwx\nvmWJwFNMDISHQ8eObN4MdepAWJi/gzLGGN+yROBp4UK4/HIoXZqEBKsWMsaEBksE6fbsgdWroXt3\nAEsExpiQYYkg3aJFoArdurF/P+zbB/Xq+TsoY4zxPUsE6WJioGxZaNWKzZudWXZGYIwJBZYIwDkT\niImBzp0hPNy6jhpjQoolAoA//oCtWzO1D4AlAmNMaLBEAM7ZAGRKBOXLOzVFxhgT7CwRgJMIateG\niy8GYPNmOxswxoQOSwRpabB4sXM2IAJY11FjTGixRLBqFRw4kFEtdPIkbNliicAYEzosESxb5vx1\nb0qclASpqXYNgTEmdFgiWLEC6taFqlUB7BoCY0zICfd3AH63YgW0a5cxaV1HTaBITU0lMTGRY8eO\n+TsUcwEpVqwYNWvWpHDhwl6/JrQTQVISbNsGbdtmzEpIgEKFnE5ExlzIEhMTKV26NHXr1kXcjg4m\ntKkqe/bsITExkXpnUb8d2lVDP/7o/M2SCGrVgrNIpsb4xbFjx6hYsaIlAZNBRKhYseJZnyWGdiJY\nscK5+0zz5hmz7BoCE0gsCZiszuU74dNEICI9ReR3EdkkImOzWT5cRJJFZI37GOnLeM7w449w2WVQ\ntGjGLLuGwBgTanyWCEQkDBgH9AIigcEiEplN0Y9Utbn7mOireM6QlgYrV2aqFkpJgV27rOuoMd7Y\ns2cPzZs3p3nz5lStWpUaNWpkTJ84ccKrdYwYMYLff/891zLjxo1j+vTp+RGyyYEvG4tbA5tUNQFA\nRGYCfYH1PnxP761dC0ePZkoE1nXUGO9VrFiRNWvWAPD4449TqlQp7r333kxlVBVVpVCh7H9zvvfe\ne3m+zx133HH+wRawtLQ0wsMDpy+OL6uGagDbPKYT3XlZ9ReReBGZLSK1fBhPZitWOH89EsHs2c7f\nVq0KLApj8sW//gWdOuXv41//OrdYNm3aRGRkJEOGDKFx48bs2LGDUaNGER0dTePGjXnyySczyrZv\n3541a9aQlpZGuXLlGDt2LFFRUVx++eX89ddfADz88MO89tprGeXHjh1L69atufTSS1m+fDkAKSkp\n9O/fn8jISAYMGEB0dHRGkvL02GOP0apVK5o0acJtt92GqgKwYcMGunTpQlRUFC1atGDLli0APPvs\nszRt2pSoqCgeeuihTDED7Ny5k4vdMcomTpzItddeS+fOnfnb3/7GwYMH6dKlCy1atKBZs2Z88cUX\nGXG89957NGvWjKioKEaMGMGBAweoX78+aWlpAOzbty/TtK/5u7F4HlBXVZsBMcCU7AqJyCgRiRWR\n2OTk5Px55xUroEoV5w71wLFj8NZb0Lt3xthzxphz9NtvvzFmzBjWr19PjRo1eP7554mNjSUuLo6Y\nmBjWrz+zYuDAgQN07NiRuLg4Lr/8ciZPnpztulWVn376iZdeeikjqfz3v/+latWqrF+/nkceeYSf\nf/4529fefffdrFy5krVr13LgwAG+/vprAAYPHsyYMWOIi4tj+fLlVK5cmXnz5jF//nx++ukn4uLi\nuOeee/Lc7p9//plPP/2URYsWUbx4cebMmcPq1atZuHAhY8aMASAuLo4XXniBb7/9lri4OF555RXK\nli1Lu3btMuKZMWMG119/fYGdVfjyXZIAz1/4Nd15GVR1j8fkRODF7FakqhOACQDR0dGaL9GtWAFt\n2mQMNDd9OiQng7uvjAko7g/mC8ZFF11EdHR0xvSMGTOYNGkSaWlpbN++nfXr1xMZmbnJsHjx4vTq\n1QuAli1b8t1332W77n79+mWUSf/l/v333/PAAw8AEBUVRePGjbN97aJFi3jppZc4duwYu3fvpmXL\nlrRt25bdu3dzzTXXAM4FWQALFy7k5ptvpnjx4gBUqFAhz+3u0aMH5cuXB5yENXbsWL7//nsKFSrE\ntm3b2L17N4sXL2bgwIEZ60v/O3LkSN544w169+7Ne++9xwcffJDn++UXX54RrAQaiEg9ESkCDALm\nehYQkWoek32AX30Yz2l798KGDRnVQqrwn/9As2bOTcqMMeenZMmSGc83btzI66+/zuLFi4mPj6dn\nz57Z9nMvUqRIxvOwsLAcq0WKur38ciuTnSNHjjB69Gg+++wz4uPjufnmm8/pquzw8HBOnToFcMbr\nPbd76tSpHDhwgNWrV7NmzRoqVaqU6/t17NiRDRs2sGTJEgoXLkzDhg3POrZz5bNEoKppwGjgG5wD\n/CxVXSciT4pIH7fYXSKyTkTigLuA4b6KJ5MffnD+uolg4UJYtw7+7/8yThCMMfnk4MGDlC5dmjJl\nyrBjxw6++eabfH+Pdu3aMWvWLADWrl2bbdXT0aNHKVSoEJUqVeLQoUN88sknAJQvX56IiAjmzZsH\nOAf3I0eO0L17dyZPnszRo0cB2Lt3LwB169Zl1apVAMxOb1jMxoEDB6hcuTLh4eHExMSQlORUiHTp\n0oWPPvooY33pfwGGDh3KkCFDGDFixHl9HmfLp20EqvqVql6iqhep6jPuvEdVda77/N+q2lhVo1S1\ns6r+5st43KDgpZegUiWnagh49VWnuWDQIJ+/uzEhp0WLFkRGRtKwYUOGDRtGO4+xvfLLnXfeSVJS\nEpGRkTzxxBNERkZSNsstBitWrMhNN91EZGQkvXr1oo37/w8wffp0XnnlFZo1a0b79u1JTk6md+/e\n9OzZk+joaJo3b85//vMfAO677z5ef/11WrRowb59+3KM6e9//zvLly+nadOmzJw5kwYNGgBO1dX9\n999Phw4daN68Offdd1/Ga4YMGcKBAwcYOHBgfn48eUvv3hUoj5YtW+p5+fxzVVAdN05VVdevdyaf\nfPL8VmtMQVu/fr2/Q7hgpKam6tGjR1VVdcOGDVq3bl1NTU31c1Rnb8aMGTp8+PDzXk923w0gVnM4\nrgZOR9f8kJoK990HDRvCqFEAvPwyFC8Ot93m59iMMefs8OHDdO3albS0NFSVd955J6D68QPcfvvt\nLFy4MKPnUEEKrE/qfL3zjtNIPG8ehIeTlAQffODkhIgIfwdnjDlX5cqVy6i3D1Tjx4/323v7+zqC\ngrN/Pzz+uNMt6OqrAafL3alT4EX3YGOMCVqhkwhefdXpNvrKKyDC/v3OCcINN9jYQsaY0BY6VUP3\n3ANNmjijjQJvvw2HDjlNBsYYE8pC54ygbFnn5z/OcBKvvQY9emTkBWOMCVmhkwg8TJ/uDDd9//3+\njsSYwNW5c+czLg577bXXuP3223N9XalSpQDYvn07AwYMyLZMp06diI2NzXU9r732GkeOHMmYvuqq\nq9i/f783oZssQi4RqMJ//+sMJ9Gli7+jMSZwDR48mJkzZ2aaN3PmTAYPHuzV66tXr57rlbl5yZoI\nvvrqK8qVK3fO6ytoqpoxVIW/hVwiWLYM4uJg9GgbTsIEET+MQz1gwAC+/PLLjJvQbNmyhe3bt3Pl\nlVdm9Otv0aIFTZs25fPPPz/j9Vu2bKFJkyaAM/zDoEGDaNSoEdddd13GsA7g9K9PH8L6scceA+CN\nN95g+/btdO7cmc7uAGF169Zl9+7dALz66qs0adKEJk2aZAxhvWXLFho1asStt95K48aN6dGjR6b3\nSTdv3jzatGnDZZddRrdu3di1axfgXKswYsQImjZtSrNmzTKGqPj6669p0aIFUVFRdO3aFXDuz/Dy\nyy9nrLNJkyZs2bKFLVu2cOmllzJs2DCaNGnCtm3bst0+gJUrV3LFFVcQFRVF69atOXToEB06dMg0\nvHb79u2Ji4vLdT95I3Qai11vvgnlysGNN/o7EmMCW4UKFWjdujXz58+nb9++zJw5kxtuuAERoVix\nYnz22WeUKVOG3bt307ZtW/r06ZPj/XTHjx9PiRIl+PXXX4mPj6dFixYZy5555hkqVKjAyZMn6dq1\nK/Hx8dx11128+uqrLFmyhEqVKmVa16pVq3jvvff48ccfUVXatGlDx44dKV++PBs3bmTGjBm8++67\n3HDDDXzyyScMHTo00+vbt2/PihUrEBEmTpzIiy++yCuvvMJTTz1F2bJlWbt2LeDcMyA5OZlbb72V\npUuXUq9evUzjBuVk48aNTJkyhbbuWGfZbV/Dhg0ZOHAgH330Ea1ateLgwYMUL16cW265hffff5/X\nXnuNDRs2cOzYMaKios5qv2UnpBLB9u3wySdw113gMUigMYHPT+NQp1cPpSeCSZMmAU61x4MPPsjS\npUspVKgQSUlJ7Nq1i6pVq2a7nqVLl3LXXXcB0KxZM5o1a5axbNasWUyYMIG0tDR27NjB+vXrMy3P\n6vvvv+e6667LGAm0X79+fPfdd/Tp04d69erRvHlzIPMw1p4SExMZOHAgO3bs4MSJE9Rz+5cvXLgw\nU1VY+fLlmTdvHh06dMgo481Q1XXq1MlIAjltn4hQrVo1Wrl3ySpTpgwA119/PU899RQvvfQSkydP\nZvjw4Xm+nzdCqmronXfg5En45z/9HYkxwaFv374sWrSI1atXc+TIEVq2bAk4g7glJyezatUq1qxZ\nQ5UqVc5pyOfNmzfz8ssvs2jRIuLj47n66qvPaT3p0oewhpyHsb7zzjsZPXo0a9eu5Z133jnvoaoh\n83DVnkNVn+32lShRgu7du/P5558za9YshgwZctaxZSdkEsGJE04iuOoquOgif0djTHAoVaoUnTt3\n5uabb87USJw+BHPhwoVZsmQJW7duzXU9HTp04MMPPwTgl19+IT4+HnCGsC5ZsiRly5Zl165dzJ8/\nP+M1pUuX5tChQ2es68orr2TOnDkcOXKElJQUPvvsM6688kqvt+nAgQPUqOHcVXfKlNM3TezevTvj\nxo3LmN63bx9t27Zl6dKlbHZveO45VPXq1asBWL16dcbyrHLavksvvZQdO3awcuVKAA4dOpSRtEaO\nHMldd91Fq1atMm6Cc75CJhF88onTZXT0aH9HYkxwGTx4MHFxcZkSwZAhQ4iNjaVp06ZMnTo1z5us\n3H777Rw+fJhGjRrx6KOPZpxZREVFcdlll9GwYUNuvPHGTENYjxo1ip49e2Y0Fqdr0aIFw4cPp3Xr\n1rRp04aRI0dy2VlcMPT4449z/fXX07Jly0ztDw8//DD79u2jSZMmREVFsWTJEiIiIpgwYQL9+vUj\nKioqY/jo/v37s3fvXho3bsybb77JJZdcku175bR9RYoU4aOPPuLOO+8kKiqK7t27Z5wptGzZkjJl\nyuTrPQtENX/u/FhQoqOjNa/+xdmZNw8mTYJPP4VCIZP+TDD79ddfadSokb/DMAVs+/btdOrUid9+\n+41CORzMsvtuiMgqVY3OrnzIHBKvuQbmzLEkYIwJXFOnTqVNmzY888wzOSaBcxFSvYaMMSaQDRs2\njGHDhuX7eu33sTEBLNCqdo3vnct3whKBMQGqWLFi7Nmzx5KByaCq7Nmzh2LFip3V66xqyJgAVbNm\nTRITE0lOTvZ3KOYCUqxYMWrWrHlWr7FEYEyAKly4cMYVrcacD6saMsaYEGeJwBhjQpwlAmOMCXEB\nd2WxiCQDuQ9cklklYLePwrmQheJ2h+I2Q2hudyhuM5zfdtdR1YjsFgRcIjhbIhKb02XVwSwUtzsU\ntxlCc7tDcZvBd9ttVUPGGBPiLBEYY0yIC4VEMMHfAfhJKG53KG4zhOZ2h+I2g4+2O+jbCIwxxuQu\nFM4IjDHG5MISgTHGhLigTgQi0lNEfheRTSIy1t/x+IKI1BKRJSKyXkTWicjd7vwKIhIjIhvdv/lz\nc9MLiIiEicjPIvKFO11PRH509/dHIlLE3zHmNxEpJyKzReQ3EflVRC4PkX09xv1+/yIiM0SkWLDt\nbxGZLCJ/icgvHvOy3bfieMPd9ngRaXE+7x20iUBEwoBxQC8gEhgsIpH+jcon0oB7VDUSaAvc4W7n\nWGCRqjYAFrnTweZu4FeP6ReA/6jqxcA+4Ba/ROVbrwNfq2pDIApn+4N6X4tIDeAuIFpVmwBhwCCC\nb3+/D/TMMi+nfdsLaOA+RgHjz+eNgzYRAK2BTaqaoKongJlAXz/HlO9UdYeqrnafH8I5MNTA2dYp\nbrEpwLX+idA3RKQmcDUw0Z0WoAsw2y0SjNtcFugATAJQ1ROqup8g39eucKC4iIQDJYAdBNn+VtWl\nwN4ss3Pat32BqepYAZQTkWrn+t7BnAhqANs8phPdeUFLROoClwE/AlVUdYe7aCdQxU9h+cprwP3A\nKXe6IrBfVdPc6WDc3/WAZOA9t0psooiUJMj3taomAS8Df+IkgAPAKoJ/f0PO+zZfj2/BnAhCioiU\nAj4B/qWqBz2XqdNHOGj6CYtIb+AvVV3l71gKWDjQAhivqpcBKWSpBgq2fQ3g1ov3xUmE1YGSnFmF\nEvR8uW+DOREkAbU8pmu684KOiBTGSQLTVfVTd/au9FNF9+9f/orPB9oBfURkC06VXxecuvNybtUB\nBOf+TgQSVfVHd3o2TmII5n0N0A3YrKrJqpoKfIrzHQj2/Q0579t8Pb4FcyJYCTRwexYUwWlcmuvn\nmPKdWzc+CfhVVV/1WDQXuMl9fhPweUHH5iuq+m9VramqdXH262JVHQIsAQa4xYJqmwFUdSewTUQu\ndWd1BdYTxPva9SfQVkRKuN/39O0O6v3tymnfzgWGub2H2gIHPKqQzp6qBu0DuArYAPwBPOTveHy0\nje1xThfjgTXu4yqcOvNFwEZgIVDB37H6aPs7AV+4z+sDPwGbgI+Bov6Ozwfb2xyIdff3HKB8KOxr\n4AngN+AX4AOgaLDtb2AGThtIKs7Z3y057VtAcHpF/gGsxelRdc7vbUNMGGNMiAvmqiFjjDFesERg\njDEhzhKBMcaEOEsExhgT4iwRGGNMiLNEYIxLRE6KyBqPR74N3iYidT1HlTTmQhKedxFjQsZRVW3u\n7yCMKWh2RmBMHkRki4i8KCJrReQnEbnYnV9XRBa748EvEpHa7vwqIvKZiMS5jyvcVYWJyLvuuPoL\nRKS4W/4u934S8SIy00+baUKYJQJjTiuepWpooMeyA6raFHgTZ+RTgP8CU1S1GTAdeMOd/wbwP1WN\nwhkLaJ07vwEwTlUbA/uB/u78scBl7npu89XGGZMTu7LYGJeIHFbVUtnM3wJ0UdUEd4C/napaUUR2\nA9VUNdWdv0NVK4lIMlBTVY97rKMuEKPODUYQkQeAwqr6tIh8DRzGGTJijqoe9vGmGpOJnREY4x3N\n4fnZOO7x/CSn2+iuxhk3pgWw0mNETWMKhCUCY7wz0OPvD+7z5TijnwIMAb5zny8CboeM+yqXzWml\nIlIIqKWqS4AHgLLAGWclxviS/fIw5rTiIrLGY/prVU3vQlpeROJxftUPdufdiXO3sPtw7hw2wp1/\nNzBBRG7B+eV/O86oktkJA6a5yUKAN9S5/aQxBcbaCIzJg9tGEK2qu/0dizG+YFVDxhgT4uyMwBhj\nQpydERhjTIizRGCMMSHOEoExxoQ4SwTGGBPiLBEYY0yI+39w/BkjcAnrVQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8447c94d-a05b-47c4-b61f-6729b070988e",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 7ms/step - loss: 2.1446 - acc: 0.0382 \n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.8731 - acc: 0.0382\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.6309 - acc: 0.0458\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.4270 - acc: 0.0916\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.2641 - acc: 0.2290\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.1317 - acc: 0.3969\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0276 - acc: 0.4809\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9409 - acc: 0.5191\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8701 - acc: 0.6107\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8119 - acc: 0.6718\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7617 - acc: 0.7099\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.7177 - acc: 0.7252\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.6778 - acc: 0.7557\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.6406 - acc: 0.7710\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.6081 - acc: 0.7939\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.5805 - acc: 0.7939\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.5565 - acc: 0.8015\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5338 - acc: 0.8015\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.5122 - acc: 0.8168\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.4926 - acc: 0.8321\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.4742 - acc: 0.8397\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.4574 - acc: 0.8397\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4424 - acc: 0.8473\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.4287 - acc: 0.8550\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4160 - acc: 0.8550\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.4045 - acc: 0.8626\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.3940 - acc: 0.8702\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.3840 - acc: 0.8702\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.3748 - acc: 0.8702\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.3662 - acc: 0.8702\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3580 - acc: 0.8702\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.3501 - acc: 0.8702\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3429 - acc: 0.8702\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.3362 - acc: 0.8702\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.3296 - acc: 0.8702\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.3234 - acc: 0.8702\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3173 - acc: 0.8702\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.3118 - acc: 0.8702\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3065 - acc: 0.8702\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3015 - acc: 0.8702\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2967 - acc: 0.8702\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2923 - acc: 0.8702\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2878 - acc: 0.8702\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.2835 - acc: 0.8702\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2795 - acc: 0.8702\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.2756 - acc: 0.8702\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.2718 - acc: 0.8702\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.2682 - acc: 0.8702\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2647 - acc: 0.8702\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2614 - acc: 0.8702\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2583 - acc: 0.8702\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.2551 - acc: 0.8702\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2521 - acc: 0.8779\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2491 - acc: 0.8779\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.2462 - acc: 0.8779\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.2434 - acc: 0.9008\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.2407 - acc: 0.9313\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 229us/step - loss: 0.2380 - acc: 0.9618\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.2354 - acc: 0.9695\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 259us/step - loss: 0.2329 - acc: 0.9695\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2303 - acc: 0.9695\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.2277 - acc: 0.9695\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.2253 - acc: 0.9695\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 247us/step - loss: 0.2228 - acc: 0.9695\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.2204 - acc: 0.9695\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.2178 - acc: 0.9695\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.2155 - acc: 0.9695\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.2131 - acc: 0.9695\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 323us/step - loss: 0.2108 - acc: 0.9695\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.2086 - acc: 0.9771\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.2063 - acc: 0.9771\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.2042 - acc: 0.9771\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.2020 - acc: 0.9771\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.1998 - acc: 0.9771\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.1977 - acc: 0.9771\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1956 - acc: 0.9771\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.1935 - acc: 0.9771\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.1914 - acc: 0.9771\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.1894 - acc: 0.9771\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.1874 - acc: 0.9771\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.1854 - acc: 0.9771\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.1833 - acc: 0.9771\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.1813 - acc: 0.9771\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.1793 - acc: 0.9847\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.1771 - acc: 0.9847\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.1752 - acc: 0.9847\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.1731 - acc: 0.9847\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.1711 - acc: 0.9847\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.1693 - acc: 0.9847\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1675 - acc: 0.9847\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1656 - acc: 0.9847\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1638 - acc: 0.9847\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.1619 - acc: 0.9847\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.1601 - acc: 0.9924\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.1583 - acc: 0.9924\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.1565 - acc: 0.9924\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.1548 - acc: 0.9924\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.1530 - acc: 0.9924\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 228us/step - loss: 0.1512 - acc: 0.9924\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.1494 - acc: 0.9924\n",
            "34/34 [==============================] - 0s 10ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dde194d3-70f2-4f77-f79e-dd2d3a70895d",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b1dec5b-33d5-4166-c742-086ea0814d01",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14705882352941177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZN9I7JNNUyS",
        "colab_type": "text"
      },
      "source": [
        "# Prova remove correlated features with treshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wACff-R4Ib",
        "colab_type": "text"
      },
      "source": [
        "https://campus.datacamp.com/courses/dimensionality-reduction-in-python/feature-selection-i-selecting-for-feature-information?ex=14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNufM8B3NYs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a positive correlation matrix\n",
        "corr_df = train_data_stand.corr().abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8ShRUvN63m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create and apply mask\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DgfSECpOwPu",
        "colab_type": "code",
        "outputId": "ba1f5512-b8d0-4eb7-9a8a-a67da45757f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [False,  True,  True, ...,  True,  True,  True],\n",
              "       [False, False,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [False, False, False, ...,  True,  True,  True],\n",
              "       [False, False, False, ..., False,  True,  True],\n",
              "       [False, False, False, ..., False, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xQLptVOw6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri_df = corr_df.mask(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDvePEPO8mn",
        "colab_type": "code",
        "outputId": "cf73534d-fdcc-4287-a83d-1d12647f7434",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tri_df"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>VoxelVolume</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <td>0.821982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MeshVolume</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <td>0.785606</td>\n",
              "      <td>0.964760</td>\n",
              "      <td>0.785457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sphericity</th>\n",
              "      <td>0.329751</td>\n",
              "      <td>0.678628</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.694906</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coarseness</th>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.586992</td>\n",
              "      <td>0.401370</td>\n",
              "      <td>0.546184</td>\n",
              "      <td>0.356708</td>\n",
              "      <td>0.569919</td>\n",
              "      <td>0.048602</td>\n",
              "      <td>0.813595</td>\n",
              "      <td>0.614989</td>\n",
              "      <td>0.094676</td>\n",
              "      <td>0.471734</td>\n",
              "      <td>0.598815</td>\n",
              "      <td>0.567029</td>\n",
              "      <td>0.554227</td>\n",
              "      <td>0.192052</td>\n",
              "      <td>0.605729</td>\n",
              "      <td>0.246122</td>\n",
              "      <td>0.406714</td>\n",
              "      <td>0.351606</td>\n",
              "      <td>0.669357</td>\n",
              "      <td>0.183179</td>\n",
              "      <td>0.731964</td>\n",
              "      <td>0.417524</td>\n",
              "      <td>0.077465</td>\n",
              "      <td>0.379163</td>\n",
              "      <td>0.437111</td>\n",
              "      <td>0.787425</td>\n",
              "      <td>0.182358</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>0.119708</td>\n",
              "      <td>0.324753</td>\n",
              "      <td>0.661086</td>\n",
              "      <td>0.326758</td>\n",
              "      <td>0.468498</td>\n",
              "      <td>0.481677</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.319663</td>\n",
              "      <td>0.667850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350082</td>\n",
              "      <td>0.292163</td>\n",
              "      <td>0.539925</td>\n",
              "      <td>0.345026</td>\n",
              "      <td>0.148147</td>\n",
              "      <td>0.251303</td>\n",
              "      <td>0.399211</td>\n",
              "      <td>0.336092</td>\n",
              "      <td>0.360524</td>\n",
              "      <td>0.381037</td>\n",
              "      <td>0.556410</td>\n",
              "      <td>0.412333</td>\n",
              "      <td>0.521415</td>\n",
              "      <td>0.480184</td>\n",
              "      <td>0.512788</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.194296</td>\n",
              "      <td>0.595126</td>\n",
              "      <td>0.552550</td>\n",
              "      <td>0.201612</td>\n",
              "      <td>0.272599</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.374723</td>\n",
              "      <td>0.426885</td>\n",
              "      <td>0.417651</td>\n",
              "      <td>0.272696</td>\n",
              "      <td>0.501061</td>\n",
              "      <td>0.667822</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.511486</td>\n",
              "      <td>0.366279</td>\n",
              "      <td>0.704958</td>\n",
              "      <td>0.523589</td>\n",
              "      <td>0.754391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Complexity</th>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.257297</td>\n",
              "      <td>0.277025</td>\n",
              "      <td>0.229141</td>\n",
              "      <td>0.182615</td>\n",
              "      <td>0.268450</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.214902</td>\n",
              "      <td>0.256226</td>\n",
              "      <td>0.019975</td>\n",
              "      <td>0.283738</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>0.441669</td>\n",
              "      <td>0.213055</td>\n",
              "      <td>0.423394</td>\n",
              "      <td>0.392267</td>\n",
              "      <td>0.103854</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.422629</td>\n",
              "      <td>0.058114</td>\n",
              "      <td>0.198407</td>\n",
              "      <td>0.214767</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.125140</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.275899</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.322120</td>\n",
              "      <td>0.410309</td>\n",
              "      <td>0.276311</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>0.275068</td>\n",
              "      <td>0.221594</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.125930</td>\n",
              "      <td>0.298911</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267658</td>\n",
              "      <td>0.198372</td>\n",
              "      <td>0.116352</td>\n",
              "      <td>0.255976</td>\n",
              "      <td>0.464523</td>\n",
              "      <td>0.267090</td>\n",
              "      <td>0.228132</td>\n",
              "      <td>0.195523</td>\n",
              "      <td>0.139835</td>\n",
              "      <td>0.198482</td>\n",
              "      <td>0.296969</td>\n",
              "      <td>0.359280</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>0.168824</td>\n",
              "      <td>0.228273</td>\n",
              "      <td>0.355694</td>\n",
              "      <td>0.220089</td>\n",
              "      <td>0.151630</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.011797</td>\n",
              "      <td>0.469854</td>\n",
              "      <td>0.036549</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.274863</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.351102</td>\n",
              "      <td>0.063147</td>\n",
              "      <td>0.152285</td>\n",
              "      <td>0.074162</td>\n",
              "      <td>0.313890</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.083326</td>\n",
              "      <td>0.166795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strength</th>\n",
              "      <td>0.510155</td>\n",
              "      <td>0.642559</td>\n",
              "      <td>0.509976</td>\n",
              "      <td>0.607597</td>\n",
              "      <td>0.339595</td>\n",
              "      <td>0.645544</td>\n",
              "      <td>0.021965</td>\n",
              "      <td>0.758610</td>\n",
              "      <td>0.670268</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.562243</td>\n",
              "      <td>0.659201</td>\n",
              "      <td>0.624228</td>\n",
              "      <td>0.620794</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.388883</td>\n",
              "      <td>0.341710</td>\n",
              "      <td>0.538344</td>\n",
              "      <td>0.425689</td>\n",
              "      <td>0.458791</td>\n",
              "      <td>0.226763</td>\n",
              "      <td>0.486770</td>\n",
              "      <td>0.290047</td>\n",
              "      <td>0.085559</td>\n",
              "      <td>0.272032</td>\n",
              "      <td>0.325476</td>\n",
              "      <td>0.564383</td>\n",
              "      <td>0.104481</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.099738</td>\n",
              "      <td>0.184149</td>\n",
              "      <td>0.141523</td>\n",
              "      <td>0.432332</td>\n",
              "      <td>0.139576</td>\n",
              "      <td>0.296993</td>\n",
              "      <td>0.271914</td>\n",
              "      <td>0.332061</td>\n",
              "      <td>0.195143</td>\n",
              "      <td>0.422246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126145</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.283099</td>\n",
              "      <td>0.224183</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>0.155122</td>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.240018</td>\n",
              "      <td>0.438144</td>\n",
              "      <td>0.264077</td>\n",
              "      <td>0.333647</td>\n",
              "      <td>0.530074</td>\n",
              "      <td>0.343866</td>\n",
              "      <td>0.337177</td>\n",
              "      <td>0.347487</td>\n",
              "      <td>0.027798</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.372899</td>\n",
              "      <td>0.365016</td>\n",
              "      <td>0.083608</td>\n",
              "      <td>0.329544</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.390861</td>\n",
              "      <td>0.567592</td>\n",
              "      <td>0.552634</td>\n",
              "      <td>0.329531</td>\n",
              "      <td>0.225342</td>\n",
              "      <td>0.451164</td>\n",
              "      <td>0.206294</td>\n",
              "      <td>0.221538</td>\n",
              "      <td>0.253885</td>\n",
              "      <td>0.383804</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.544355</td>\n",
              "      <td>0.611316</td>\n",
              "      <td>0.746738</td>\n",
              "      <td>0.074615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contrast.1</th>\n",
              "      <td>0.458069</td>\n",
              "      <td>0.592542</td>\n",
              "      <td>0.457835</td>\n",
              "      <td>0.555373</td>\n",
              "      <td>0.380133</td>\n",
              "      <td>0.588236</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>0.639843</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.065998</td>\n",
              "      <td>0.515229</td>\n",
              "      <td>0.623406</td>\n",
              "      <td>0.576338</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.699380</td>\n",
              "      <td>0.654218</td>\n",
              "      <td>0.165224</td>\n",
              "      <td>0.399133</td>\n",
              "      <td>0.462206</td>\n",
              "      <td>0.847990</td>\n",
              "      <td>0.404501</td>\n",
              "      <td>0.807251</td>\n",
              "      <td>0.503620</td>\n",
              "      <td>0.040094</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>0.507856</td>\n",
              "      <td>0.811825</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.674231</td>\n",
              "      <td>0.184830</td>\n",
              "      <td>0.427215</td>\n",
              "      <td>0.958897</td>\n",
              "      <td>0.507007</td>\n",
              "      <td>0.941239</td>\n",
              "      <td>0.822280</td>\n",
              "      <td>0.836388</td>\n",
              "      <td>0.858476</td>\n",
              "      <td>0.916872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777488</td>\n",
              "      <td>0.592929</td>\n",
              "      <td>0.882098</td>\n",
              "      <td>0.561351</td>\n",
              "      <td>0.652744</td>\n",
              "      <td>0.457706</td>\n",
              "      <td>0.693751</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.473433</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.601669</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.679507</td>\n",
              "      <td>0.566054</td>\n",
              "      <td>0.635554</td>\n",
              "      <td>0.154862</td>\n",
              "      <td>0.617903</td>\n",
              "      <td>0.665488</td>\n",
              "      <td>0.705727</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.390378</td>\n",
              "      <td>0.398000</td>\n",
              "      <td>0.502327</td>\n",
              "      <td>0.354174</td>\n",
              "      <td>0.437122</td>\n",
              "      <td>0.390572</td>\n",
              "      <td>0.610009</td>\n",
              "      <td>0.852276</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.242269</td>\n",
              "      <td>0.640887</td>\n",
              "      <td>0.494642</td>\n",
              "      <td>0.794082</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.759051</td>\n",
              "      <td>0.551508</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.374697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Busyness</th>\n",
              "      <td>0.937894</td>\n",
              "      <td>0.810919</td>\n",
              "      <td>0.937881</td>\n",
              "      <td>0.773599</td>\n",
              "      <td>0.339451</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.682675</td>\n",
              "      <td>0.843581</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>0.913619</td>\n",
              "      <td>0.867644</td>\n",
              "      <td>0.805965</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>0.176933</td>\n",
              "      <td>0.469093</td>\n",
              "      <td>0.023465</td>\n",
              "      <td>0.890974</td>\n",
              "      <td>0.884836</td>\n",
              "      <td>0.556691</td>\n",
              "      <td>0.356571</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.253053</td>\n",
              "      <td>0.425932</td>\n",
              "      <td>0.366938</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.364459</td>\n",
              "      <td>0.187027</td>\n",
              "      <td>0.262478</td>\n",
              "      <td>0.474823</td>\n",
              "      <td>0.313668</td>\n",
              "      <td>0.411803</td>\n",
              "      <td>0.469366</td>\n",
              "      <td>0.490211</td>\n",
              "      <td>0.354087</td>\n",
              "      <td>0.516006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294360</td>\n",
              "      <td>0.307158</td>\n",
              "      <td>0.382443</td>\n",
              "      <td>0.108961</td>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>0.428333</td>\n",
              "      <td>0.320272</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.365808</td>\n",
              "      <td>0.400702</td>\n",
              "      <td>0.915079</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.448260</td>\n",
              "      <td>0.462777</td>\n",
              "      <td>0.126498</td>\n",
              "      <td>0.276150</td>\n",
              "      <td>0.466663</td>\n",
              "      <td>0.497241</td>\n",
              "      <td>0.130804</td>\n",
              "      <td>0.770513</td>\n",
              "      <td>0.135920</td>\n",
              "      <td>0.372307</td>\n",
              "      <td>0.858147</td>\n",
              "      <td>0.912460</td>\n",
              "      <td>0.770458</td>\n",
              "      <td>0.381146</td>\n",
              "      <td>0.557089</td>\n",
              "      <td>0.516059</td>\n",
              "      <td>0.592938</td>\n",
              "      <td>0.403923</td>\n",
              "      <td>0.370737</td>\n",
              "      <td>0.363137</td>\n",
              "      <td>0.403091</td>\n",
              "      <td>0.382465</td>\n",
              "      <td>0.434561</td>\n",
              "      <td>0.093528</td>\n",
              "      <td>0.602417</td>\n",
              "      <td>0.418697</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "VoxelVolume                NaN                NaN  ...         NaN       NaN\n",
              "Maximum3DDiameter     0.821982                NaN  ...         NaN       NaN\n",
              "MeshVolume            0.999999           0.821800  ...         NaN       NaN\n",
              "MajorAxisLength       0.785606           0.964760  ...         NaN       NaN\n",
              "Sphericity            0.329751           0.678628  ...         NaN       NaN\n",
              "...                        ...                ...  ...         ...       ...\n",
              "Coarseness            0.401634           0.586992  ...         NaN       NaN\n",
              "Complexity            0.277302           0.257297  ...         NaN       NaN\n",
              "Strength              0.510155           0.642559  ...         NaN       NaN\n",
              "Contrast.1            0.458069           0.592542  ...         NaN       NaN\n",
              "Busyness              0.937894           0.810919  ...    0.418697       NaN\n",
              "\n",
              "[107 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1MkWP2O9hU",
        "colab_type": "code",
        "outputId": "c1ea588e-2e25-4ecc-9940-8fdecfbc0636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Find columns that meet threshold\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.70)]\n",
        "to_drop"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VoxelVolume',\n",
              " 'Maximum3DDiameter',\n",
              " 'MeshVolume',\n",
              " 'MajorAxisLength',\n",
              " 'LeastAxisLength',\n",
              " 'Elongation',\n",
              " 'SurfaceVolumeRatio',\n",
              " 'Maximum2DDiameterSlice',\n",
              " 'SurfaceArea',\n",
              " 'MinorAxisLength',\n",
              " 'Maximum2DDiameterColumn',\n",
              " 'Maximum2DDiameterRow',\n",
              " 'GrayLevelVariance',\n",
              " 'HighGrayLevelEmphasis',\n",
              " 'DependenceEntropy',\n",
              " 'DependenceNonUniformity',\n",
              " 'GrayLevelNonUniformity',\n",
              " 'SmallDependenceEmphasis',\n",
              " 'SmallDependenceHighGrayLevelEmphasis',\n",
              " 'DependenceNonUniformityNormalized',\n",
              " 'LargeDependenceEmphasis',\n",
              " 'LargeDependenceLowGrayLevelEmphasis',\n",
              " 'DependenceVariance',\n",
              " 'LargeDependenceHighGrayLevelEmphasis',\n",
              " 'SmallDependenceLowGrayLevelEmphasis',\n",
              " 'LowGrayLevelEmphasis',\n",
              " 'JointAverage',\n",
              " 'SumAverage',\n",
              " 'JointEntropy',\n",
              " 'ClusterShade',\n",
              " 'MaximumProbability',\n",
              " 'Idmn',\n",
              " 'JointEnergy',\n",
              " 'Contrast',\n",
              " 'DifferenceEntropy',\n",
              " 'InverseVariance',\n",
              " 'DifferenceVariance',\n",
              " 'Idn',\n",
              " 'Idm',\n",
              " 'Correlation',\n",
              " 'Autocorrelation',\n",
              " 'SumEntropy',\n",
              " 'SumSquares',\n",
              " 'ClusterProminence',\n",
              " 'Imc2',\n",
              " 'DifferenceAverage',\n",
              " 'Id',\n",
              " 'ClusterTendency',\n",
              " 'InterquartileRange',\n",
              " 'Skewness',\n",
              " 'Uniformity',\n",
              " 'Median',\n",
              " 'Energy',\n",
              " 'RobustMeanAbsoluteDeviation',\n",
              " 'MeanAbsoluteDeviation',\n",
              " 'Maximum',\n",
              " 'RootMeanSquared',\n",
              " 'Minimum',\n",
              " 'Entropy',\n",
              " 'Range',\n",
              " 'Variance',\n",
              " '10Percentile',\n",
              " 'Kurtosis',\n",
              " 'Mean',\n",
              " 'ShortRunLowGrayLevelEmphasis',\n",
              " 'GrayLevelVariance.1',\n",
              " 'LowGrayLevelRunEmphasis',\n",
              " 'GrayLevelNonUniformityNormalized',\n",
              " 'RunVariance',\n",
              " 'GrayLevelNonUniformity.1',\n",
              " 'LongRunEmphasis',\n",
              " 'ShortRunHighGrayLevelEmphasis',\n",
              " 'RunLengthNonUniformity',\n",
              " 'ShortRunEmphasis',\n",
              " 'LongRunHighGrayLevelEmphasis',\n",
              " 'RunPercentage',\n",
              " 'LongRunLowGrayLevelEmphasis',\n",
              " 'RunEntropy',\n",
              " 'HighGrayLevelRunEmphasis',\n",
              " 'RunLengthNonUniformityNormalized',\n",
              " 'ZoneVariance',\n",
              " 'SizeZoneNonUniformityNormalized',\n",
              " 'SizeZoneNonUniformity',\n",
              " 'GrayLevelNonUniformity.2',\n",
              " 'LargeAreaEmphasis',\n",
              " 'SmallAreaHighGrayLevelEmphasis',\n",
              " 'ZonePercentage',\n",
              " 'LowGrayLevelZoneEmphasis',\n",
              " 'SmallAreaLowGrayLevelEmphasis',\n",
              " 'Coarseness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKeereQoci",
        "colab_type": "text"
      },
      "source": [
        "The reason we used the mask to set half of the matrix to NA value is that we ewnt to avoid removing both features when thay have a strong correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R-tG-WPNXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "train_data_stand_reduced = train_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNUiUoZ0Vdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "test_data_stand_reduced = test_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-uuBifRO8G",
        "colab_type": "code",
        "outputId": "fe565cfb-fc5d-4303-c63c-a09ea8a31ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wuGcLV0acQ",
        "colab_type": "code",
        "outputId": "fc045ee2-3eab-454c-9e5a-9c782da44557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data_stand_reduced.shape"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1qR3re7QwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_reduced = train_data_stand_reduced.to_numpy()\n",
        "test_data_stand_reduced = test_data_stand_reduced.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUG3a0o5W1O",
        "colab_type": "code",
        "outputId": "91845cbf-7651-4c3b-80d0-b063db44a02c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8wkFES71GJ",
        "colab_type": "code",
        "outputId": "8074f8a9-7b6e-49c3-ae96-c96fbf7ac4d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljMaQeuSHe1",
        "colab_type": "text"
      },
      "source": [
        "funziona bene, però bisogna stare attenti a basarsi unicamente sul coefficiente di correlazione. Se y = x^2, x e y risulteranno scorrelate secondo il coeffiente di correlazione di Pearson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxuH4u741mLR",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(17,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmeGHBV1xs_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5hBUph6149E"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7NwNpEO149J"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MD1VB0S149O",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ceef6573-3f58-43c4-bab1-28d12b989cad",
        "id": "VkpRnrey149Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_reduced, train_labels_dec)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d281003b-b2e8-4529-c5fb-a790a1276317",
        "id": "F3zK4E6o149g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "96bf9ddc-b7b1-4f51-8c82-375efebfcd4c",
        "id": "BDX_0MCd149o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9QlfChU149v",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvCKZfjj1490",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bjrjhWo1497"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Du7DFfm1498",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2e17c191-4bf1-436b-a4ca-edd68d475125",
        "id": "kHNpJxBL14-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_reduced = []\n",
        "all_loss_histories_reduced = []\n",
        "all_val_acc_histories_reduced = []\n",
        "all_val_loss_histories_reduced = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_reduced[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_reduced[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history_reduced = history.history['acc']\n",
        "  all_acc_histories_reduced.append(acc_history)\n",
        "\n",
        "  loss_history_reduced = history.history['loss']\n",
        "  all_loss_histories_reduced.append(loss_history)\n",
        "\n",
        "  acc_val_history_reduced = history.history['val_acc']\n",
        "  all_val_acc_histories_reduced.append(acc_val_history)\n",
        "\n",
        "  loss_val_history_reduced = history.history['val_loss']\n",
        "  all_val_loss_histories_reduced.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 2s 13ms/step - loss: 1.2474 - acc: 0.4017 - val_loss: 1.3207 - val_acc: 0.1429\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 1.1199 - acc: 0.4530 - val_loss: 1.2686 - val_acc: 0.1429\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 1.0583 - acc: 0.4444 - val_loss: 1.2220 - val_acc: 0.1429\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 1.0186 - acc: 0.4615 - val_loss: 1.1963 - val_acc: 0.2143\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.9874 - acc: 0.4615 - val_loss: 1.1706 - val_acc: 0.2857\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.9625 - acc: 0.4872 - val_loss: 1.1484 - val_acc: 0.2143\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 259us/step - loss: 0.9368 - acc: 0.4957 - val_loss: 1.1326 - val_acc: 0.2143\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.9198 - acc: 0.5214 - val_loss: 1.1180 - val_acc: 0.2143\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 302us/step - loss: 0.9009 - acc: 0.5385 - val_loss: 1.1057 - val_acc: 0.2143\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 232us/step - loss: 0.8880 - acc: 0.5385 - val_loss: 1.0983 - val_acc: 0.2143\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 241us/step - loss: 0.8788 - acc: 0.5726 - val_loss: 1.0984 - val_acc: 0.2143\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.8672 - acc: 0.5812 - val_loss: 1.0932 - val_acc: 0.2857\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.8584 - acc: 0.6068 - val_loss: 1.0884 - val_acc: 0.2857\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 0.8525 - acc: 0.5897 - val_loss: 1.0881 - val_acc: 0.3571\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 249us/step - loss: 0.8455 - acc: 0.5897 - val_loss: 1.0896 - val_acc: 0.3571\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.8389 - acc: 0.5983 - val_loss: 1.0919 - val_acc: 0.3571\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 303us/step - loss: 0.8324 - acc: 0.6068 - val_loss: 1.0924 - val_acc: 0.3571\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 263us/step - loss: 0.8253 - acc: 0.6068 - val_loss: 1.0941 - val_acc: 0.3571\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.8197 - acc: 0.6154 - val_loss: 1.0929 - val_acc: 0.3571\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.8162 - acc: 0.6154 - val_loss: 1.0938 - val_acc: 0.3571\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 241us/step - loss: 0.8107 - acc: 0.6154 - val_loss: 1.0921 - val_acc: 0.3571\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 243us/step - loss: 0.8044 - acc: 0.6325 - val_loss: 1.0966 - val_acc: 0.3571\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.7998 - acc: 0.6325 - val_loss: 1.0956 - val_acc: 0.3571\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.7931 - acc: 0.6239 - val_loss: 1.1014 - val_acc: 0.3571\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 245us/step - loss: 0.7899 - acc: 0.6325 - val_loss: 1.1057 - val_acc: 0.3571\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.7857 - acc: 0.6496 - val_loss: 1.1078 - val_acc: 0.3571\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.7810 - acc: 0.6410 - val_loss: 1.1119 - val_acc: 0.3571\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.7743 - acc: 0.6496 - val_loss: 1.1198 - val_acc: 0.3571\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.7717 - acc: 0.6496 - val_loss: 1.1264 - val_acc: 0.3571\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.7664 - acc: 0.6581 - val_loss: 1.1296 - val_acc: 0.3571\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.7639 - acc: 0.6581 - val_loss: 1.1379 - val_acc: 0.3571\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.7596 - acc: 0.6581 - val_loss: 1.1435 - val_acc: 0.3571\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.7556 - acc: 0.6581 - val_loss: 1.1478 - val_acc: 0.3571\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.7515 - acc: 0.6752 - val_loss: 1.1521 - val_acc: 0.3571\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.7447 - acc: 0.6752 - val_loss: 1.1598 - val_acc: 0.3571\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.7420 - acc: 0.6838 - val_loss: 1.1655 - val_acc: 0.3571\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.7366 - acc: 0.6752 - val_loss: 1.1678 - val_acc: 0.3571\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.7352 - acc: 0.6667 - val_loss: 1.1767 - val_acc: 0.3571\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.7309 - acc: 0.6752 - val_loss: 1.1803 - val_acc: 0.3571\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 0.7255 - acc: 0.6838 - val_loss: 1.1866 - val_acc: 0.3571\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.7223 - acc: 0.6838 - val_loss: 1.1930 - val_acc: 0.3571\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.7178 - acc: 0.6923 - val_loss: 1.1935 - val_acc: 0.3571\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 239us/step - loss: 0.7126 - acc: 0.7094 - val_loss: 1.1957 - val_acc: 0.3571\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.7107 - acc: 0.7009 - val_loss: 1.2060 - val_acc: 0.3571\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.7092 - acc: 0.6923 - val_loss: 1.2095 - val_acc: 0.3571\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.7032 - acc: 0.7009 - val_loss: 1.2135 - val_acc: 0.3571\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.6990 - acc: 0.7179 - val_loss: 1.2151 - val_acc: 0.3571\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.6948 - acc: 0.7179 - val_loss: 1.2193 - val_acc: 0.3571\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.6926 - acc: 0.7179 - val_loss: 1.2238 - val_acc: 0.3571\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.6890 - acc: 0.7094 - val_loss: 1.2306 - val_acc: 0.3571\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.6837 - acc: 0.7009 - val_loss: 1.2344 - val_acc: 0.3571\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 240us/step - loss: 0.6795 - acc: 0.7179 - val_loss: 1.2367 - val_acc: 0.3571\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 0.6769 - acc: 0.7179 - val_loss: 1.2338 - val_acc: 0.3571\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.6721 - acc: 0.7009 - val_loss: 1.2369 - val_acc: 0.3571\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.6711 - acc: 0.7009 - val_loss: 1.2404 - val_acc: 0.3571\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.6646 - acc: 0.7094 - val_loss: 1.2473 - val_acc: 0.3571\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.6619 - acc: 0.7179 - val_loss: 1.2485 - val_acc: 0.3571\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.6570 - acc: 0.7094 - val_loss: 1.2545 - val_acc: 0.3571\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.6567 - acc: 0.7094 - val_loss: 1.2615 - val_acc: 0.3571\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.6519 - acc: 0.7265 - val_loss: 1.2583 - val_acc: 0.3571\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 244us/step - loss: 0.6497 - acc: 0.7179 - val_loss: 1.2671 - val_acc: 0.3571\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.6450 - acc: 0.7265 - val_loss: 1.2677 - val_acc: 0.3571\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 0.6408 - acc: 0.7179 - val_loss: 1.2748 - val_acc: 0.3571\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.6386 - acc: 0.7265 - val_loss: 1.2740 - val_acc: 0.4286\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 0.6344 - acc: 0.7521 - val_loss: 1.2757 - val_acc: 0.4286\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.6300 - acc: 0.7607 - val_loss: 1.2736 - val_acc: 0.4286\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.6301 - acc: 0.7436 - val_loss: 1.2864 - val_acc: 0.4286\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.6250 - acc: 0.7521 - val_loss: 1.2814 - val_acc: 0.4286\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.6282 - acc: 0.7607 - val_loss: 1.2939 - val_acc: 0.4286\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 0.6208 - acc: 0.7436 - val_loss: 1.3013 - val_acc: 0.4286\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.6176 - acc: 0.7607 - val_loss: 1.3065 - val_acc: 0.4286\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.6150 - acc: 0.7607 - val_loss: 1.3154 - val_acc: 0.4286\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 0.6127 - acc: 0.7436 - val_loss: 1.3193 - val_acc: 0.4286\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 239us/step - loss: 0.6082 - acc: 0.7521 - val_loss: 1.3275 - val_acc: 0.4286\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 241us/step - loss: 0.6078 - acc: 0.7436 - val_loss: 1.3371 - val_acc: 0.4286\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.6041 - acc: 0.7521 - val_loss: 1.3411 - val_acc: 0.4286\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.6010 - acc: 0.7265 - val_loss: 1.3441 - val_acc: 0.4286\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 0.5990 - acc: 0.7607 - val_loss: 1.3472 - val_acc: 0.4286\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.5949 - acc: 0.7607 - val_loss: 1.3526 - val_acc: 0.4286\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 276us/step - loss: 0.5917 - acc: 0.7436 - val_loss: 1.3569 - val_acc: 0.4286\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 243us/step - loss: 0.5903 - acc: 0.7692 - val_loss: 1.3620 - val_acc: 0.4286\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 0.5875 - acc: 0.7863 - val_loss: 1.3635 - val_acc: 0.4286\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.5848 - acc: 0.7692 - val_loss: 1.3702 - val_acc: 0.4286\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.5856 - acc: 0.7778 - val_loss: 1.3815 - val_acc: 0.4286\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 0.5783 - acc: 0.7778 - val_loss: 1.3752 - val_acc: 0.4286\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.5783 - acc: 0.7863 - val_loss: 1.3810 - val_acc: 0.4286\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.5743 - acc: 0.7863 - val_loss: 1.3788 - val_acc: 0.4286\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 0.5728 - acc: 0.7778 - val_loss: 1.3805 - val_acc: 0.4286\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 231us/step - loss: 0.5686 - acc: 0.7778 - val_loss: 1.3816 - val_acc: 0.4286\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.5702 - acc: 0.7863 - val_loss: 1.3934 - val_acc: 0.4286\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.5653 - acc: 0.7863 - val_loss: 1.4073 - val_acc: 0.4286\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.5660 - acc: 0.7692 - val_loss: 1.4055 - val_acc: 0.4286\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.5621 - acc: 0.7863 - val_loss: 1.4053 - val_acc: 0.4286\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.5592 - acc: 0.7778 - val_loss: 1.4076 - val_acc: 0.4286\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.5592 - acc: 0.7692 - val_loss: 1.4128 - val_acc: 0.4286\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.5533 - acc: 0.7863 - val_loss: 1.4279 - val_acc: 0.4286\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.5532 - acc: 0.7863 - val_loss: 1.4228 - val_acc: 0.4286\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.5503 - acc: 0.7863 - val_loss: 1.4245 - val_acc: 0.4286\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.5499 - acc: 0.8034 - val_loss: 1.4334 - val_acc: 0.4286\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.5479 - acc: 0.7949 - val_loss: 1.4419 - val_acc: 0.5000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 1.1160 - acc: 0.4322 - val_loss: 1.3098 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0476 - acc: 0.4661 - val_loss: 1.2632 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0076 - acc: 0.5000 - val_loss: 1.2299 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9848 - acc: 0.5085 - val_loss: 1.2087 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9659 - acc: 0.5339 - val_loss: 1.2067 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9513 - acc: 0.5339 - val_loss: 1.1953 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9425 - acc: 0.5424 - val_loss: 1.1839 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9309 - acc: 0.5593 - val_loss: 1.1817 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9223 - acc: 0.5508 - val_loss: 1.1737 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9156 - acc: 0.5847 - val_loss: 1.1722 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9076 - acc: 0.5763 - val_loss: 1.1715 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.9018 - acc: 0.5932 - val_loss: 1.1727 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8950 - acc: 0.6017 - val_loss: 1.1766 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8910 - acc: 0.6102 - val_loss: 1.1784 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8863 - acc: 0.6186 - val_loss: 1.1809 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8792 - acc: 0.6102 - val_loss: 1.1832 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8768 - acc: 0.6186 - val_loss: 1.1817 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8699 - acc: 0.6186 - val_loss: 1.1829 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8665 - acc: 0.6356 - val_loss: 1.1868 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.8616 - acc: 0.6186 - val_loss: 1.1867 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8573 - acc: 0.6271 - val_loss: 1.1859 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8532 - acc: 0.6186 - val_loss: 1.1882 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8485 - acc: 0.6102 - val_loss: 1.1918 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8439 - acc: 0.6186 - val_loss: 1.1961 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8409 - acc: 0.6186 - val_loss: 1.1971 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8371 - acc: 0.6186 - val_loss: 1.1936 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8335 - acc: 0.6271 - val_loss: 1.1951 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8315 - acc: 0.6186 - val_loss: 1.1975 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.8257 - acc: 0.6271 - val_loss: 1.1969 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8243 - acc: 0.6271 - val_loss: 1.2020 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8192 - acc: 0.6271 - val_loss: 1.1962 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8170 - acc: 0.6186 - val_loss: 1.2024 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8128 - acc: 0.6186 - val_loss: 1.2053 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8097 - acc: 0.6356 - val_loss: 1.2127 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8051 - acc: 0.6271 - val_loss: 1.2151 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.8038 - acc: 0.6441 - val_loss: 1.2151 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.7993 - acc: 0.6356 - val_loss: 1.2175 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7978 - acc: 0.6186 - val_loss: 1.2202 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.7921 - acc: 0.6356 - val_loss: 1.2233 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7888 - acc: 0.6441 - val_loss: 1.2256 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7863 - acc: 0.6271 - val_loss: 1.2288 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7830 - acc: 0.6271 - val_loss: 1.2268 - val_acc: 0.3077\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7774 - acc: 0.6356 - val_loss: 1.2267 - val_acc: 0.3077\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7750 - acc: 0.6356 - val_loss: 1.2299 - val_acc: 0.3077\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7712 - acc: 0.6356 - val_loss: 1.2282 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.7669 - acc: 0.6610 - val_loss: 1.2261 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.7637 - acc: 0.6441 - val_loss: 1.2278 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.7608 - acc: 0.6610 - val_loss: 1.2273 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.7573 - acc: 0.6525 - val_loss: 1.2272 - val_acc: 0.3077\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7545 - acc: 0.6695 - val_loss: 1.2312 - val_acc: 0.3077\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7491 - acc: 0.6695 - val_loss: 1.2277 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7462 - acc: 0.6610 - val_loss: 1.2283 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.7433 - acc: 0.6695 - val_loss: 1.2271 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7381 - acc: 0.6780 - val_loss: 1.2275 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7359 - acc: 0.6695 - val_loss: 1.2273 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7332 - acc: 0.6610 - val_loss: 1.2272 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7281 - acc: 0.6780 - val_loss: 1.2274 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7253 - acc: 0.6695 - val_loss: 1.2266 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7241 - acc: 0.6864 - val_loss: 1.2265 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7189 - acc: 0.6864 - val_loss: 1.2272 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7175 - acc: 0.6780 - val_loss: 1.2282 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7130 - acc: 0.6780 - val_loss: 1.2306 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7113 - acc: 0.6695 - val_loss: 1.2330 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7062 - acc: 0.6780 - val_loss: 1.2343 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7025 - acc: 0.6864 - val_loss: 1.2356 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6997 - acc: 0.7034 - val_loss: 1.2364 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.6982 - acc: 0.6949 - val_loss: 1.2364 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6937 - acc: 0.6864 - val_loss: 1.2382 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6913 - acc: 0.7034 - val_loss: 1.2420 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6878 - acc: 0.6949 - val_loss: 1.2398 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6851 - acc: 0.7034 - val_loss: 1.2391 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6826 - acc: 0.6949 - val_loss: 1.2430 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6786 - acc: 0.6949 - val_loss: 1.2428 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6770 - acc: 0.6864 - val_loss: 1.2421 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6735 - acc: 0.6864 - val_loss: 1.2436 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6719 - acc: 0.6864 - val_loss: 1.2436 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6670 - acc: 0.7034 - val_loss: 1.2493 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6652 - acc: 0.7034 - val_loss: 1.2498 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6623 - acc: 0.7034 - val_loss: 1.2531 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.6622 - acc: 0.7034 - val_loss: 1.2552 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.6568 - acc: 0.6949 - val_loss: 1.2561 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6563 - acc: 0.6949 - val_loss: 1.2568 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6548 - acc: 0.6949 - val_loss: 1.2607 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6490 - acc: 0.7203 - val_loss: 1.2624 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6473 - acc: 0.7034 - val_loss: 1.2599 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6440 - acc: 0.7203 - val_loss: 1.2683 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6426 - acc: 0.7119 - val_loss: 1.2735 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6404 - acc: 0.7034 - val_loss: 1.2755 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.6384 - acc: 0.7288 - val_loss: 1.2788 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6347 - acc: 0.7203 - val_loss: 1.2799 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6314 - acc: 0.7119 - val_loss: 1.2830 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6297 - acc: 0.7288 - val_loss: 1.2844 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6258 - acc: 0.7288 - val_loss: 1.2865 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6266 - acc: 0.7203 - val_loss: 1.2892 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.6241 - acc: 0.7288 - val_loss: 1.2914 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.6209 - acc: 0.7288 - val_loss: 1.2935 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6194 - acc: 0.7373 - val_loss: 1.2886 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6161 - acc: 0.7288 - val_loss: 1.2933 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6146 - acc: 0.7288 - val_loss: 1.2978 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6143 - acc: 0.7203 - val_loss: 1.2997 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 13ms/step - loss: 1.5011 - acc: 0.2966 - val_loss: 1.6354 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.3005 - acc: 0.2881 - val_loss: 1.4718 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.1681 - acc: 0.3305 - val_loss: 1.3675 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0893 - acc: 0.3814 - val_loss: 1.2923 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0350 - acc: 0.4576 - val_loss: 1.2501 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9978 - acc: 0.4576 - val_loss: 1.2190 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9706 - acc: 0.4831 - val_loss: 1.1979 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9518 - acc: 0.4915 - val_loss: 1.1851 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.9378 - acc: 0.5254 - val_loss: 1.1806 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9263 - acc: 0.5254 - val_loss: 1.1776 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.9197 - acc: 0.5508 - val_loss: 1.1769 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9112 - acc: 0.5424 - val_loss: 1.1786 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9022 - acc: 0.5593 - val_loss: 1.1813 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8954 - acc: 0.5678 - val_loss: 1.1834 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8893 - acc: 0.5847 - val_loss: 1.1889 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8837 - acc: 0.5847 - val_loss: 1.1908 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8783 - acc: 0.6017 - val_loss: 1.1960 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8738 - acc: 0.5932 - val_loss: 1.2014 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.8698 - acc: 0.6102 - val_loss: 1.2059 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8662 - acc: 0.6271 - val_loss: 1.2130 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.8613 - acc: 0.6271 - val_loss: 1.2191 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.8581 - acc: 0.6271 - val_loss: 1.2234 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.8519 - acc: 0.6271 - val_loss: 1.2309 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8467 - acc: 0.6186 - val_loss: 1.2366 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.8449 - acc: 0.6271 - val_loss: 1.2440 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.8388 - acc: 0.6186 - val_loss: 1.2520 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.8355 - acc: 0.6186 - val_loss: 1.2619 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 270us/step - loss: 0.8310 - acc: 0.6271 - val_loss: 1.2728 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.8285 - acc: 0.6271 - val_loss: 1.2797 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.8239 - acc: 0.6356 - val_loss: 1.2878 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.8201 - acc: 0.6525 - val_loss: 1.2985 - val_acc: 0.3077\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8152 - acc: 0.6441 - val_loss: 1.3065 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.8109 - acc: 0.6441 - val_loss: 1.3185 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.8075 - acc: 0.6356 - val_loss: 1.3244 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.8052 - acc: 0.6610 - val_loss: 1.3338 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 272us/step - loss: 0.7996 - acc: 0.6525 - val_loss: 1.3430 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.7938 - acc: 0.6525 - val_loss: 1.3501 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7885 - acc: 0.6525 - val_loss: 1.3614 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7845 - acc: 0.6525 - val_loss: 1.3714 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.7836 - acc: 0.6441 - val_loss: 1.3789 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.7761 - acc: 0.6525 - val_loss: 1.3902 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7725 - acc: 0.6610 - val_loss: 1.3983 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7686 - acc: 0.6525 - val_loss: 1.4066 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7644 - acc: 0.6695 - val_loss: 1.4180 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.7614 - acc: 0.6610 - val_loss: 1.4230 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7578 - acc: 0.6610 - val_loss: 1.4396 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7537 - acc: 0.6695 - val_loss: 1.4494 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7509 - acc: 0.6525 - val_loss: 1.4581 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7475 - acc: 0.6525 - val_loss: 1.4710 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7440 - acc: 0.6864 - val_loss: 1.4842 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7406 - acc: 0.6441 - val_loss: 1.4910 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7370 - acc: 0.6610 - val_loss: 1.5016 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.7336 - acc: 0.6780 - val_loss: 1.5128 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.7304 - acc: 0.6610 - val_loss: 1.5191 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7269 - acc: 0.6695 - val_loss: 1.5333 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7243 - acc: 0.6695 - val_loss: 1.5417 - val_acc: 0.3077\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7212 - acc: 0.6525 - val_loss: 1.5582 - val_acc: 0.3077\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7161 - acc: 0.6695 - val_loss: 1.5657 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7131 - acc: 0.6780 - val_loss: 1.5785 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7110 - acc: 0.6610 - val_loss: 1.5854 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7068 - acc: 0.6864 - val_loss: 1.5991 - val_acc: 0.3077\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7040 - acc: 0.6780 - val_loss: 1.6132 - val_acc: 0.3077\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7012 - acc: 0.6780 - val_loss: 1.6186 - val_acc: 0.3077\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6980 - acc: 0.6610 - val_loss: 1.6265 - val_acc: 0.3077\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6934 - acc: 0.6864 - val_loss: 1.6422 - val_acc: 0.3077\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6915 - acc: 0.6949 - val_loss: 1.6453 - val_acc: 0.3077\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6870 - acc: 0.6780 - val_loss: 1.6559 - val_acc: 0.3077\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6853 - acc: 0.6864 - val_loss: 1.6675 - val_acc: 0.3077\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6827 - acc: 0.6780 - val_loss: 1.6750 - val_acc: 0.3077\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6790 - acc: 0.6780 - val_loss: 1.6830 - val_acc: 0.3077\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6765 - acc: 0.6864 - val_loss: 1.6932 - val_acc: 0.3077\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 261us/step - loss: 0.6724 - acc: 0.6864 - val_loss: 1.7026 - val_acc: 0.3077\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6703 - acc: 0.6949 - val_loss: 1.7146 - val_acc: 0.3077\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.6691 - acc: 0.7034 - val_loss: 1.7238 - val_acc: 0.2308\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6614 - acc: 0.6949 - val_loss: 1.7337 - val_acc: 0.2308\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 259us/step - loss: 0.6626 - acc: 0.7203 - val_loss: 1.7410 - val_acc: 0.2308\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6590 - acc: 0.7034 - val_loss: 1.7471 - val_acc: 0.2308\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6556 - acc: 0.7034 - val_loss: 1.7531 - val_acc: 0.3077\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.6529 - acc: 0.7034 - val_loss: 1.7708 - val_acc: 0.3077\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6502 - acc: 0.7034 - val_loss: 1.7773 - val_acc: 0.2308\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.6474 - acc: 0.7119 - val_loss: 1.7885 - val_acc: 0.3077\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6439 - acc: 0.7203 - val_loss: 1.7940 - val_acc: 0.3077\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.6411 - acc: 0.7034 - val_loss: 1.7951 - val_acc: 0.3077\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.6368 - acc: 0.7288 - val_loss: 1.8073 - val_acc: 0.3077\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 279us/step - loss: 0.6358 - acc: 0.7373 - val_loss: 1.8167 - val_acc: 0.3077\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6332 - acc: 0.7542 - val_loss: 1.8296 - val_acc: 0.3077\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.6304 - acc: 0.7458 - val_loss: 1.8379 - val_acc: 0.3077\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.6264 - acc: 0.7373 - val_loss: 1.8405 - val_acc: 0.3077\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.6245 - acc: 0.7373 - val_loss: 1.8481 - val_acc: 0.3077\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6188 - acc: 0.7288 - val_loss: 1.8622 - val_acc: 0.3077\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6153 - acc: 0.7288 - val_loss: 1.8625 - val_acc: 0.3077\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6147 - acc: 0.7373 - val_loss: 1.8740 - val_acc: 0.3077\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.6113 - acc: 0.7458 - val_loss: 1.8790 - val_acc: 0.3077\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.6075 - acc: 0.7288 - val_loss: 1.8883 - val_acc: 0.3077\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.6081 - acc: 0.7288 - val_loss: 1.8967 - val_acc: 0.3077\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.6035 - acc: 0.7373 - val_loss: 1.8910 - val_acc: 0.3077\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6004 - acc: 0.7458 - val_loss: 1.9077 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6006 - acc: 0.7542 - val_loss: 1.9225 - val_acc: 0.3077\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.5976 - acc: 0.7797 - val_loss: 1.9328 - val_acc: 0.3077\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.5943 - acc: 0.7542 - val_loss: 1.9324 - val_acc: 0.3077\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 1.1991 - acc: 0.3305 - val_loss: 1.2416 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.1113 - acc: 0.4068 - val_loss: 1.1631 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0573 - acc: 0.4746 - val_loss: 1.1118 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.0211 - acc: 0.4831 - val_loss: 1.0795 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.9955 - acc: 0.5169 - val_loss: 1.0579 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9765 - acc: 0.5254 - val_loss: 1.0359 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9631 - acc: 0.5169 - val_loss: 1.0228 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.9501 - acc: 0.5593 - val_loss: 1.0080 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.9396 - acc: 0.5847 - val_loss: 0.9970 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 279us/step - loss: 0.9313 - acc: 0.5847 - val_loss: 0.9869 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.9236 - acc: 0.5847 - val_loss: 0.9755 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9148 - acc: 0.5847 - val_loss: 0.9674 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9089 - acc: 0.5932 - val_loss: 0.9560 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9026 - acc: 0.6017 - val_loss: 0.9506 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8970 - acc: 0.6017 - val_loss: 0.9442 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8918 - acc: 0.6102 - val_loss: 0.9401 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8851 - acc: 0.6186 - val_loss: 0.9327 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.8838 - acc: 0.6186 - val_loss: 0.9280 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.8760 - acc: 0.6271 - val_loss: 0.9241 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8717 - acc: 0.6271 - val_loss: 0.9206 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8668 - acc: 0.6186 - val_loss: 0.9146 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8637 - acc: 0.6356 - val_loss: 0.9118 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8596 - acc: 0.6441 - val_loss: 0.9050 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.8554 - acc: 0.6271 - val_loss: 0.9049 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8516 - acc: 0.6441 - val_loss: 0.8982 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8498 - acc: 0.6271 - val_loss: 0.8958 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8453 - acc: 0.6695 - val_loss: 0.8982 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8411 - acc: 0.6525 - val_loss: 0.8969 - val_acc: 0.6154\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8371 - acc: 0.6441 - val_loss: 0.8937 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8327 - acc: 0.6441 - val_loss: 0.8973 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8289 - acc: 0.6356 - val_loss: 0.8989 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8254 - acc: 0.6525 - val_loss: 0.8960 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.8236 - acc: 0.6610 - val_loss: 0.8988 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8194 - acc: 0.6441 - val_loss: 0.8989 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.8173 - acc: 0.6441 - val_loss: 0.8997 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8127 - acc: 0.6525 - val_loss: 0.8989 - val_acc: 0.6154\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8092 - acc: 0.6525 - val_loss: 0.8976 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8059 - acc: 0.6864 - val_loss: 0.8957 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8013 - acc: 0.6695 - val_loss: 0.9020 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7976 - acc: 0.6864 - val_loss: 0.8968 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.7945 - acc: 0.6864 - val_loss: 0.8992 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.7922 - acc: 0.6695 - val_loss: 0.9005 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7884 - acc: 0.7034 - val_loss: 0.8997 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7851 - acc: 0.6949 - val_loss: 0.9023 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7818 - acc: 0.6780 - val_loss: 0.8990 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7797 - acc: 0.6949 - val_loss: 0.9029 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7764 - acc: 0.6949 - val_loss: 0.9071 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7734 - acc: 0.6864 - val_loss: 0.9028 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7696 - acc: 0.6864 - val_loss: 0.9042 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7661 - acc: 0.6949 - val_loss: 0.9045 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7648 - acc: 0.6949 - val_loss: 0.9047 - val_acc: 0.6923\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.7578 - acc: 0.7034 - val_loss: 0.9098 - val_acc: 0.6923\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7578 - acc: 0.7034 - val_loss: 0.9114 - val_acc: 0.6923\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.7528 - acc: 0.7119 - val_loss: 0.9155 - val_acc: 0.6923\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7483 - acc: 0.7034 - val_loss: 0.9143 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7478 - acc: 0.6864 - val_loss: 0.9167 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7445 - acc: 0.7203 - val_loss: 0.9194 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7434 - acc: 0.7034 - val_loss: 0.9195 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7399 - acc: 0.7034 - val_loss: 0.9299 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7373 - acc: 0.7203 - val_loss: 0.9289 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.7337 - acc: 0.7288 - val_loss: 0.9325 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.7308 - acc: 0.7288 - val_loss: 0.9314 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.7303 - acc: 0.7373 - val_loss: 0.9309 - val_acc: 0.6154\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7250 - acc: 0.7373 - val_loss: 0.9340 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7233 - acc: 0.7458 - val_loss: 0.9331 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7207 - acc: 0.7373 - val_loss: 0.9395 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7171 - acc: 0.7458 - val_loss: 0.9462 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7133 - acc: 0.7458 - val_loss: 0.9468 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7131 - acc: 0.7288 - val_loss: 0.9542 - val_acc: 0.6154\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7101 - acc: 0.7458 - val_loss: 0.9594 - val_acc: 0.6154\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7077 - acc: 0.7542 - val_loss: 0.9571 - val_acc: 0.6154\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7058 - acc: 0.7542 - val_loss: 0.9605 - val_acc: 0.6154\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7006 - acc: 0.7542 - val_loss: 0.9613 - val_acc: 0.6154\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.6974 - acc: 0.7458 - val_loss: 0.9570 - val_acc: 0.6154\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.6957 - acc: 0.7542 - val_loss: 0.9640 - val_acc: 0.6154\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.6930 - acc: 0.7458 - val_loss: 0.9629 - val_acc: 0.6154\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 290us/step - loss: 0.6890 - acc: 0.7458 - val_loss: 0.9609 - val_acc: 0.6154\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6875 - acc: 0.7627 - val_loss: 0.9630 - val_acc: 0.6154\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6847 - acc: 0.7712 - val_loss: 0.9703 - val_acc: 0.6154\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6834 - acc: 0.7797 - val_loss: 0.9778 - val_acc: 0.6154\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6810 - acc: 0.7712 - val_loss: 0.9856 - val_acc: 0.6154\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6773 - acc: 0.7542 - val_loss: 0.9865 - val_acc: 0.6154\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.6736 - acc: 0.7542 - val_loss: 0.9853 - val_acc: 0.6154\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.6724 - acc: 0.7627 - val_loss: 0.9901 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.6686 - acc: 0.7627 - val_loss: 0.9933 - val_acc: 0.6154\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6685 - acc: 0.7627 - val_loss: 0.9975 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6667 - acc: 0.7712 - val_loss: 1.0067 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6611 - acc: 0.7712 - val_loss: 1.0119 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6602 - acc: 0.7542 - val_loss: 1.0123 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.6587 - acc: 0.7712 - val_loss: 1.0120 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6567 - acc: 0.7712 - val_loss: 1.0081 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6544 - acc: 0.7712 - val_loss: 1.0152 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6510 - acc: 0.7797 - val_loss: 1.0203 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6472 - acc: 0.7881 - val_loss: 1.0256 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6451 - acc: 0.7712 - val_loss: 1.0210 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6389 - acc: 0.7797 - val_loss: 1.0176 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.6384 - acc: 0.7797 - val_loss: 1.0257 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6372 - acc: 0.7627 - val_loss: 1.0388 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.6335 - acc: 0.7797 - val_loss: 1.0380 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6306 - acc: 0.7627 - val_loss: 1.0357 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 1.1923 - acc: 0.4322 - val_loss: 1.2215 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.1003 - acc: 0.4492 - val_loss: 1.1516 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0445 - acc: 0.4661 - val_loss: 1.1085 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0095 - acc: 0.5000 - val_loss: 1.0872 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9862 - acc: 0.5000 - val_loss: 1.0646 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9625 - acc: 0.4831 - val_loss: 1.0558 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9491 - acc: 0.5339 - val_loss: 1.0498 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9319 - acc: 0.5254 - val_loss: 1.0460 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9192 - acc: 0.5424 - val_loss: 1.0461 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9074 - acc: 0.5508 - val_loss: 1.0435 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8992 - acc: 0.5593 - val_loss: 1.0452 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8919 - acc: 0.5593 - val_loss: 1.0496 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8833 - acc: 0.5932 - val_loss: 1.0515 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8712 - acc: 0.5678 - val_loss: 1.0577 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8636 - acc: 0.6102 - val_loss: 1.0606 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8565 - acc: 0.6271 - val_loss: 1.0638 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8500 - acc: 0.6102 - val_loss: 1.0684 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8402 - acc: 0.6271 - val_loss: 1.0699 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8364 - acc: 0.6525 - val_loss: 1.0792 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8284 - acc: 0.6525 - val_loss: 1.0852 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8236 - acc: 0.6525 - val_loss: 1.0922 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8161 - acc: 0.6610 - val_loss: 1.0941 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8115 - acc: 0.6610 - val_loss: 1.0984 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8052 - acc: 0.6695 - val_loss: 1.1035 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8030 - acc: 0.6610 - val_loss: 1.1057 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.7952 - acc: 0.6780 - val_loss: 1.1091 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7893 - acc: 0.6695 - val_loss: 1.1089 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7857 - acc: 0.6695 - val_loss: 1.1243 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7822 - acc: 0.6610 - val_loss: 1.1232 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7757 - acc: 0.6695 - val_loss: 1.1321 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7700 - acc: 0.6780 - val_loss: 1.1389 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.7663 - acc: 0.6610 - val_loss: 1.1480 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.7617 - acc: 0.6525 - val_loss: 1.1535 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7543 - acc: 0.6610 - val_loss: 1.1628 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7525 - acc: 0.6864 - val_loss: 1.1614 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.7491 - acc: 0.6695 - val_loss: 1.1597 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7439 - acc: 0.6610 - val_loss: 1.1656 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7407 - acc: 0.6695 - val_loss: 1.1725 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.7357 - acc: 0.6695 - val_loss: 1.1787 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7362 - acc: 0.6695 - val_loss: 1.1873 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 276us/step - loss: 0.7310 - acc: 0.6695 - val_loss: 1.1854 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7247 - acc: 0.6695 - val_loss: 1.1894 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7237 - acc: 0.6864 - val_loss: 1.1925 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7200 - acc: 0.6780 - val_loss: 1.1939 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.7152 - acc: 0.7034 - val_loss: 1.1988 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.7168 - acc: 0.6949 - val_loss: 1.2060 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7107 - acc: 0.7034 - val_loss: 1.2126 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7045 - acc: 0.6864 - val_loss: 1.2109 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7039 - acc: 0.7119 - val_loss: 1.2066 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.7014 - acc: 0.6864 - val_loss: 1.2086 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6977 - acc: 0.7034 - val_loss: 1.2149 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 279us/step - loss: 0.6919 - acc: 0.7119 - val_loss: 1.2274 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6945 - acc: 0.7119 - val_loss: 1.2185 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 292us/step - loss: 0.6889 - acc: 0.7034 - val_loss: 1.2276 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.6841 - acc: 0.7119 - val_loss: 1.2228 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6828 - acc: 0.7119 - val_loss: 1.2330 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.6813 - acc: 0.7119 - val_loss: 1.2460 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6799 - acc: 0.7458 - val_loss: 1.2477 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6735 - acc: 0.7288 - val_loss: 1.2457 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6718 - acc: 0.7288 - val_loss: 1.2456 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6694 - acc: 0.7034 - val_loss: 1.2484 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6681 - acc: 0.7119 - val_loss: 1.2593 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6665 - acc: 0.7203 - val_loss: 1.2670 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6620 - acc: 0.7119 - val_loss: 1.2680 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6590 - acc: 0.7373 - val_loss: 1.2750 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6561 - acc: 0.7119 - val_loss: 1.2732 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6541 - acc: 0.7203 - val_loss: 1.2759 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6527 - acc: 0.7119 - val_loss: 1.2822 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6542 - acc: 0.6949 - val_loss: 1.2901 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.6514 - acc: 0.7203 - val_loss: 1.2930 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6450 - acc: 0.7203 - val_loss: 1.2984 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.6444 - acc: 0.7119 - val_loss: 1.3034 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6415 - acc: 0.7119 - val_loss: 1.3155 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6372 - acc: 0.7288 - val_loss: 1.3162 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.6385 - acc: 0.7288 - val_loss: 1.3206 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6374 - acc: 0.7119 - val_loss: 1.3237 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6295 - acc: 0.7203 - val_loss: 1.3279 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6272 - acc: 0.7288 - val_loss: 1.3238 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.6271 - acc: 0.7119 - val_loss: 1.3289 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6196 - acc: 0.7288 - val_loss: 1.3447 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6192 - acc: 0.7288 - val_loss: 1.3491 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.6166 - acc: 0.7373 - val_loss: 1.3475 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6167 - acc: 0.7288 - val_loss: 1.3459 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6155 - acc: 0.7288 - val_loss: 1.3579 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6102 - acc: 0.7203 - val_loss: 1.3695 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6044 - acc: 0.7373 - val_loss: 1.3721 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6066 - acc: 0.7458 - val_loss: 1.3783 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.6028 - acc: 0.7203 - val_loss: 1.3793 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5975 - acc: 0.7458 - val_loss: 1.3789 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5967 - acc: 0.7288 - val_loss: 1.3936 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.5917 - acc: 0.7373 - val_loss: 1.3892 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5899 - acc: 0.7288 - val_loss: 1.3936 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.5880 - acc: 0.7458 - val_loss: 1.3948 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5868 - acc: 0.7458 - val_loss: 1.4145 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.5858 - acc: 0.7288 - val_loss: 1.4252 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.5786 - acc: 0.7288 - val_loss: 1.4208 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.5758 - acc: 0.7373 - val_loss: 1.4203 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 287us/step - loss: 0.5789 - acc: 0.7373 - val_loss: 1.4334 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.5704 - acc: 0.7203 - val_loss: 1.4366 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5715 - acc: 0.7458 - val_loss: 1.4472 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 14ms/step - loss: 1.3048 - acc: 0.3644 - val_loss: 1.3264 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.1507 - acc: 0.4068 - val_loss: 1.2102 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 1.0763 - acc: 0.4068 - val_loss: 1.1488 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 1.0367 - acc: 0.4915 - val_loss: 1.1138 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0151 - acc: 0.4915 - val_loss: 1.0888 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9997 - acc: 0.4915 - val_loss: 1.0705 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 280us/step - loss: 0.9872 - acc: 0.4915 - val_loss: 1.0601 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9796 - acc: 0.5169 - val_loss: 1.0491 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.9730 - acc: 0.4915 - val_loss: 1.0382 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9655 - acc: 0.4915 - val_loss: 1.0336 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9580 - acc: 0.5000 - val_loss: 1.0252 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9531 - acc: 0.4915 - val_loss: 1.0227 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9470 - acc: 0.5085 - val_loss: 1.0179 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9430 - acc: 0.4915 - val_loss: 1.0142 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9389 - acc: 0.5085 - val_loss: 1.0107 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9326 - acc: 0.5085 - val_loss: 1.0080 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9277 - acc: 0.5169 - val_loss: 1.0029 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9261 - acc: 0.5169 - val_loss: 1.0008 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9207 - acc: 0.5085 - val_loss: 0.9966 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.9159 - acc: 0.5254 - val_loss: 0.9926 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9140 - acc: 0.5254 - val_loss: 0.9907 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9104 - acc: 0.5424 - val_loss: 0.9857 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9050 - acc: 0.5508 - val_loss: 0.9823 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8995 - acc: 0.5424 - val_loss: 0.9794 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.8957 - acc: 0.5593 - val_loss: 0.9737 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.8898 - acc: 0.5593 - val_loss: 0.9672 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.8868 - acc: 0.5593 - val_loss: 0.9636 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.8801 - acc: 0.5763 - val_loss: 0.9569 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8761 - acc: 0.5678 - val_loss: 0.9580 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.8723 - acc: 0.5763 - val_loss: 0.9525 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 264us/step - loss: 0.8674 - acc: 0.5932 - val_loss: 0.9495 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8635 - acc: 0.5847 - val_loss: 0.9426 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8596 - acc: 0.5847 - val_loss: 0.9417 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8580 - acc: 0.5763 - val_loss: 0.9362 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.8536 - acc: 0.5847 - val_loss: 0.9337 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.8523 - acc: 0.6017 - val_loss: 0.9299 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.8502 - acc: 0.5847 - val_loss: 0.9285 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 267us/step - loss: 0.8434 - acc: 0.6017 - val_loss: 0.9185 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8405 - acc: 0.6017 - val_loss: 0.9160 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8383 - acc: 0.6017 - val_loss: 0.9122 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8364 - acc: 0.6017 - val_loss: 0.9094 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8312 - acc: 0.6102 - val_loss: 0.9077 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 340us/step - loss: 0.8284 - acc: 0.6102 - val_loss: 0.9071 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 323us/step - loss: 0.8269 - acc: 0.6102 - val_loss: 0.9053 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8242 - acc: 0.6102 - val_loss: 0.9061 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8221 - acc: 0.6186 - val_loss: 0.9010 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.8184 - acc: 0.6186 - val_loss: 0.8988 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8168 - acc: 0.6271 - val_loss: 0.8990 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.8140 - acc: 0.6186 - val_loss: 0.8947 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.8112 - acc: 0.6102 - val_loss: 0.8925 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.8094 - acc: 0.6356 - val_loss: 0.8929 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8070 - acc: 0.6271 - val_loss: 0.8924 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.8040 - acc: 0.6271 - val_loss: 0.8950 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8026 - acc: 0.6271 - val_loss: 0.8961 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.8038 - acc: 0.6102 - val_loss: 0.8931 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7979 - acc: 0.6441 - val_loss: 0.8924 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.7962 - acc: 0.6356 - val_loss: 0.8933 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.7939 - acc: 0.6356 - val_loss: 0.8952 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7921 - acc: 0.6271 - val_loss: 0.8941 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 280us/step - loss: 0.7876 - acc: 0.6271 - val_loss: 0.8919 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.7886 - acc: 0.6441 - val_loss: 0.8915 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7879 - acc: 0.6356 - val_loss: 0.8877 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.7841 - acc: 0.6271 - val_loss: 0.8892 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7817 - acc: 0.6610 - val_loss: 0.8890 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7794 - acc: 0.6695 - val_loss: 0.8926 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7761 - acc: 0.6695 - val_loss: 0.8929 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7744 - acc: 0.6525 - val_loss: 0.8957 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7712 - acc: 0.6695 - val_loss: 0.8927 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7744 - acc: 0.6695 - val_loss: 0.8976 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7676 - acc: 0.6695 - val_loss: 0.8941 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7649 - acc: 0.6695 - val_loss: 0.8943 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7664 - acc: 0.6695 - val_loss: 0.8957 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.7584 - acc: 0.6610 - val_loss: 0.8960 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7569 - acc: 0.6695 - val_loss: 0.9008 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7542 - acc: 0.6695 - val_loss: 0.9026 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7536 - acc: 0.6780 - val_loss: 0.9026 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.7489 - acc: 0.6780 - val_loss: 0.9042 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 259us/step - loss: 0.7472 - acc: 0.6780 - val_loss: 0.9054 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7453 - acc: 0.6695 - val_loss: 0.9048 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7432 - acc: 0.6780 - val_loss: 0.9078 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.7452 - acc: 0.6695 - val_loss: 0.9052 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7388 - acc: 0.6780 - val_loss: 0.9053 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7398 - acc: 0.6864 - val_loss: 0.9091 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7350 - acc: 0.6864 - val_loss: 0.9105 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 273us/step - loss: 0.7341 - acc: 0.6780 - val_loss: 0.9118 - val_acc: 0.6154\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.7337 - acc: 0.6695 - val_loss: 0.9093 - val_acc: 0.6154\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.7276 - acc: 0.6864 - val_loss: 0.9088 - val_acc: 0.6154\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7288 - acc: 0.6864 - val_loss: 0.9116 - val_acc: 0.6154\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7266 - acc: 0.6780 - val_loss: 0.9122 - val_acc: 0.6154\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7274 - acc: 0.6780 - val_loss: 0.9097 - val_acc: 0.6154\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7198 - acc: 0.6949 - val_loss: 0.9099 - val_acc: 0.6154\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7215 - acc: 0.6864 - val_loss: 0.9095 - val_acc: 0.6154\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7171 - acc: 0.6949 - val_loss: 0.9169 - val_acc: 0.6154\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7157 - acc: 0.6780 - val_loss: 0.9100 - val_acc: 0.6154\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7152 - acc: 0.7034 - val_loss: 0.9125 - val_acc: 0.6154\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7110 - acc: 0.6780 - val_loss: 0.9142 - val_acc: 0.6154\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7116 - acc: 0.6780 - val_loss: 0.9150 - val_acc: 0.6154\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 264us/step - loss: 0.7111 - acc: 0.6949 - val_loss: 0.9119 - val_acc: 0.6154\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7063 - acc: 0.6864 - val_loss: 0.9150 - val_acc: 0.6154\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7050 - acc: 0.6864 - val_loss: 0.9141 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.2375 - acc: 0.3644 - val_loss: 1.2706 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.1298 - acc: 0.4322 - val_loss: 1.1738 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0638 - acc: 0.4746 - val_loss: 1.1224 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.0150 - acc: 0.4831 - val_loss: 1.0804 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9884 - acc: 0.5000 - val_loss: 1.0498 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9687 - acc: 0.5085 - val_loss: 1.0336 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9424 - acc: 0.5085 - val_loss: 1.0173 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9282 - acc: 0.5339 - val_loss: 1.0054 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9146 - acc: 0.5424 - val_loss: 1.0005 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9045 - acc: 0.5763 - val_loss: 0.9965 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8957 - acc: 0.5932 - val_loss: 0.9947 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8823 - acc: 0.6102 - val_loss: 0.9942 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8743 - acc: 0.6186 - val_loss: 0.9934 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8687 - acc: 0.6271 - val_loss: 0.9926 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8603 - acc: 0.6271 - val_loss: 0.9976 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8544 - acc: 0.6356 - val_loss: 0.9984 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8493 - acc: 0.6441 - val_loss: 1.0021 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8435 - acc: 0.6441 - val_loss: 1.0035 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8353 - acc: 0.6356 - val_loss: 1.0093 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.8308 - acc: 0.6525 - val_loss: 1.0117 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8267 - acc: 0.6356 - val_loss: 1.0136 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.8195 - acc: 0.6525 - val_loss: 1.0173 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8156 - acc: 0.6695 - val_loss: 1.0206 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8120 - acc: 0.6525 - val_loss: 1.0225 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8071 - acc: 0.6441 - val_loss: 1.0243 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8015 - acc: 0.6610 - val_loss: 1.0254 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7966 - acc: 0.6441 - val_loss: 1.0294 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7935 - acc: 0.6695 - val_loss: 1.0326 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7886 - acc: 0.6695 - val_loss: 1.0350 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7839 - acc: 0.6695 - val_loss: 1.0363 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.7844 - acc: 0.6610 - val_loss: 1.0389 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7789 - acc: 0.6610 - val_loss: 1.0443 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.7736 - acc: 0.6610 - val_loss: 1.0454 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7700 - acc: 0.6695 - val_loss: 1.0479 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7652 - acc: 0.6610 - val_loss: 1.0504 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.7611 - acc: 0.6780 - val_loss: 1.0551 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7583 - acc: 0.6441 - val_loss: 1.0561 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7559 - acc: 0.6610 - val_loss: 1.0591 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7504 - acc: 0.6695 - val_loss: 1.0647 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.7496 - acc: 0.6610 - val_loss: 1.0675 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7472 - acc: 0.6695 - val_loss: 1.0700 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.7412 - acc: 0.6610 - val_loss: 1.0761 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7390 - acc: 0.6695 - val_loss: 1.0791 - val_acc: 0.3077\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7350 - acc: 0.6695 - val_loss: 1.0847 - val_acc: 0.3077\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7327 - acc: 0.6695 - val_loss: 1.0881 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7302 - acc: 0.6610 - val_loss: 1.0922 - val_acc: 0.3077\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7255 - acc: 0.6610 - val_loss: 1.0973 - val_acc: 0.2308\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.7212 - acc: 0.6695 - val_loss: 1.1001 - val_acc: 0.2308\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.7198 - acc: 0.6695 - val_loss: 1.1034 - val_acc: 0.2308\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.7145 - acc: 0.6780 - val_loss: 1.1060 - val_acc: 0.2308\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.7104 - acc: 0.6610 - val_loss: 1.1104 - val_acc: 0.2308\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.7054 - acc: 0.6864 - val_loss: 1.1117 - val_acc: 0.2308\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.7026 - acc: 0.6695 - val_loss: 1.1161 - val_acc: 0.2308\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.6988 - acc: 0.6610 - val_loss: 1.1219 - val_acc: 0.2308\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.6972 - acc: 0.6780 - val_loss: 1.1256 - val_acc: 0.2308\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6940 - acc: 0.6949 - val_loss: 1.1301 - val_acc: 0.2308\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.6893 - acc: 0.6864 - val_loss: 1.1293 - val_acc: 0.2308\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.6865 - acc: 0.6949 - val_loss: 1.1328 - val_acc: 0.2308\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6850 - acc: 0.7034 - val_loss: 1.1370 - val_acc: 0.2308\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6826 - acc: 0.7034 - val_loss: 1.1448 - val_acc: 0.2308\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.6798 - acc: 0.7119 - val_loss: 1.1485 - val_acc: 0.2308\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6775 - acc: 0.7203 - val_loss: 1.1524 - val_acc: 0.2308\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6734 - acc: 0.7119 - val_loss: 1.1538 - val_acc: 0.2308\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.6712 - acc: 0.7203 - val_loss: 1.1587 - val_acc: 0.2308\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6652 - acc: 0.7119 - val_loss: 1.1649 - val_acc: 0.2308\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6644 - acc: 0.7288 - val_loss: 1.1713 - val_acc: 0.2308\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6621 - acc: 0.7288 - val_loss: 1.1706 - val_acc: 0.2308\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6594 - acc: 0.7119 - val_loss: 1.1845 - val_acc: 0.3077\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6564 - acc: 0.7288 - val_loss: 1.1860 - val_acc: 0.3077\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6557 - acc: 0.7288 - val_loss: 1.1899 - val_acc: 0.3077\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.6531 - acc: 0.7373 - val_loss: 1.1907 - val_acc: 0.3077\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6509 - acc: 0.7458 - val_loss: 1.1933 - val_acc: 0.3077\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6500 - acc: 0.7458 - val_loss: 1.1947 - val_acc: 0.3077\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6420 - acc: 0.7627 - val_loss: 1.1983 - val_acc: 0.3077\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.6425 - acc: 0.7373 - val_loss: 1.2038 - val_acc: 0.3077\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6393 - acc: 0.7458 - val_loss: 1.2083 - val_acc: 0.3077\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6358 - acc: 0.7458 - val_loss: 1.2112 - val_acc: 0.3077\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6350 - acc: 0.7458 - val_loss: 1.2164 - val_acc: 0.3077\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6335 - acc: 0.7627 - val_loss: 1.2190 - val_acc: 0.3077\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6288 - acc: 0.7542 - val_loss: 1.2222 - val_acc: 0.3077\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6268 - acc: 0.7627 - val_loss: 1.2259 - val_acc: 0.3077\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6252 - acc: 0.7627 - val_loss: 1.2320 - val_acc: 0.3077\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6230 - acc: 0.7458 - val_loss: 1.2365 - val_acc: 0.3077\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.6198 - acc: 0.7627 - val_loss: 1.2437 - val_acc: 0.3077\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.6185 - acc: 0.7542 - val_loss: 1.2466 - val_acc: 0.3077\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.6162 - acc: 0.7627 - val_loss: 1.2499 - val_acc: 0.3077\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 274us/step - loss: 0.6137 - acc: 0.7627 - val_loss: 1.2514 - val_acc: 0.3077\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6113 - acc: 0.7542 - val_loss: 1.2565 - val_acc: 0.3077\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6078 - acc: 0.7627 - val_loss: 1.2594 - val_acc: 0.3077\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6049 - acc: 0.7627 - val_loss: 1.2653 - val_acc: 0.3077\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.6034 - acc: 0.7458 - val_loss: 1.2648 - val_acc: 0.3077\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.5997 - acc: 0.7627 - val_loss: 1.2675 - val_acc: 0.3077\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.5972 - acc: 0.7458 - val_loss: 1.2746 - val_acc: 0.3077\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.5946 - acc: 0.7712 - val_loss: 1.2801 - val_acc: 0.3077\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.5942 - acc: 0.7712 - val_loss: 1.2846 - val_acc: 0.3077\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.5903 - acc: 0.7627 - val_loss: 1.2850 - val_acc: 0.3077\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.5885 - acc: 0.7542 - val_loss: 1.2875 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5886 - acc: 0.7627 - val_loss: 1.2949 - val_acc: 0.3077\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.5853 - acc: 0.7542 - val_loss: 1.3011 - val_acc: 0.3077\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.5836 - acc: 0.7542 - val_loss: 1.3046 - val_acc: 0.3077\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.2909 - acc: 0.2373 - val_loss: 1.2473 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 1.1512 - acc: 0.3220 - val_loss: 1.1306 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0760 - acc: 0.3729 - val_loss: 1.0719 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0309 - acc: 0.4068 - val_loss: 1.0357 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0046 - acc: 0.4576 - val_loss: 1.0111 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9862 - acc: 0.4661 - val_loss: 0.9965 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9747 - acc: 0.5000 - val_loss: 0.9834 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9601 - acc: 0.4915 - val_loss: 0.9782 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9525 - acc: 0.5085 - val_loss: 0.9738 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9455 - acc: 0.5424 - val_loss: 0.9690 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9392 - acc: 0.5339 - val_loss: 0.9641 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9345 - acc: 0.5339 - val_loss: 0.9625 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9291 - acc: 0.5508 - val_loss: 0.9598 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9217 - acc: 0.5678 - val_loss: 0.9539 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9183 - acc: 0.5593 - val_loss: 0.9526 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9129 - acc: 0.5678 - val_loss: 0.9504 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9101 - acc: 0.5763 - val_loss: 0.9481 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9066 - acc: 0.5678 - val_loss: 0.9465 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9028 - acc: 0.5678 - val_loss: 0.9437 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.9010 - acc: 0.5678 - val_loss: 0.9434 - val_acc: 0.6923\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.8976 - acc: 0.5678 - val_loss: 0.9412 - val_acc: 0.6923\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8948 - acc: 0.5847 - val_loss: 0.9402 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.8906 - acc: 0.5847 - val_loss: 0.9415 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8869 - acc: 0.5847 - val_loss: 0.9380 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.8835 - acc: 0.5763 - val_loss: 0.9377 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.8806 - acc: 0.5932 - val_loss: 0.9351 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8785 - acc: 0.5678 - val_loss: 0.9355 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8743 - acc: 0.5847 - val_loss: 0.9345 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8707 - acc: 0.5763 - val_loss: 0.9340 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8685 - acc: 0.5763 - val_loss: 0.9324 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.8657 - acc: 0.5763 - val_loss: 0.9317 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8643 - acc: 0.5763 - val_loss: 0.9323 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8606 - acc: 0.5847 - val_loss: 0.9311 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.8553 - acc: 0.5932 - val_loss: 0.9314 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8538 - acc: 0.5847 - val_loss: 0.9319 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8516 - acc: 0.5932 - val_loss: 0.9297 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.8475 - acc: 0.5932 - val_loss: 0.9341 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8451 - acc: 0.5847 - val_loss: 0.9315 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8416 - acc: 0.5763 - val_loss: 0.9309 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8404 - acc: 0.5678 - val_loss: 0.9310 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8374 - acc: 0.5763 - val_loss: 0.9293 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8356 - acc: 0.5763 - val_loss: 0.9318 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.8333 - acc: 0.5932 - val_loss: 0.9304 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8298 - acc: 0.5847 - val_loss: 0.9344 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8279 - acc: 0.5847 - val_loss: 0.9349 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8276 - acc: 0.5847 - val_loss: 0.9360 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8225 - acc: 0.5932 - val_loss: 0.9307 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8195 - acc: 0.5932 - val_loss: 0.9349 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8192 - acc: 0.5847 - val_loss: 0.9362 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8173 - acc: 0.5932 - val_loss: 0.9395 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8137 - acc: 0.5932 - val_loss: 0.9402 - val_acc: 0.6923\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8103 - acc: 0.5932 - val_loss: 0.9412 - val_acc: 0.6923\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8091 - acc: 0.5932 - val_loss: 0.9452 - val_acc: 0.6923\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8073 - acc: 0.6017 - val_loss: 0.9412 - val_acc: 0.6923\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8030 - acc: 0.6186 - val_loss: 0.9446 - val_acc: 0.6923\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8010 - acc: 0.6017 - val_loss: 0.9466 - val_acc: 0.6923\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8013 - acc: 0.5932 - val_loss: 0.9508 - val_acc: 0.6923\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7967 - acc: 0.6186 - val_loss: 0.9515 - val_acc: 0.6923\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7939 - acc: 0.6186 - val_loss: 0.9497 - val_acc: 0.6923\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7916 - acc: 0.6017 - val_loss: 0.9541 - val_acc: 0.6923\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7887 - acc: 0.6271 - val_loss: 0.9547 - val_acc: 0.6923\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7858 - acc: 0.6186 - val_loss: 0.9583 - val_acc: 0.6923\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7847 - acc: 0.6271 - val_loss: 0.9612 - val_acc: 0.6923\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.7824 - acc: 0.6186 - val_loss: 0.9637 - val_acc: 0.6923\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7814 - acc: 0.6441 - val_loss: 0.9628 - val_acc: 0.6923\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.7791 - acc: 0.5932 - val_loss: 0.9672 - val_acc: 0.6923\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.7762 - acc: 0.6017 - val_loss: 0.9675 - val_acc: 0.6923\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.7732 - acc: 0.6525 - val_loss: 0.9674 - val_acc: 0.6923\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7712 - acc: 0.6441 - val_loss: 0.9692 - val_acc: 0.6923\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.7696 - acc: 0.6356 - val_loss: 0.9687 - val_acc: 0.6923\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7674 - acc: 0.6525 - val_loss: 0.9688 - val_acc: 0.6923\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7666 - acc: 0.6525 - val_loss: 0.9691 - val_acc: 0.6923\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7644 - acc: 0.6610 - val_loss: 0.9680 - val_acc: 0.6923\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.7616 - acc: 0.6610 - val_loss: 0.9643 - val_acc: 0.6923\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.7579 - acc: 0.6695 - val_loss: 0.9656 - val_acc: 0.6923\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.7562 - acc: 0.6610 - val_loss: 0.9698 - val_acc: 0.6923\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7554 - acc: 0.6441 - val_loss: 0.9726 - val_acc: 0.6923\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7521 - acc: 0.6610 - val_loss: 0.9701 - val_acc: 0.6923\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7516 - acc: 0.6695 - val_loss: 0.9729 - val_acc: 0.6923\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7489 - acc: 0.6610 - val_loss: 0.9772 - val_acc: 0.6923\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.7470 - acc: 0.6610 - val_loss: 0.9745 - val_acc: 0.6923\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7441 - acc: 0.6695 - val_loss: 0.9737 - val_acc: 0.6923\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7431 - acc: 0.6610 - val_loss: 0.9769 - val_acc: 0.6923\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7409 - acc: 0.6780 - val_loss: 0.9750 - val_acc: 0.6923\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7387 - acc: 0.6695 - val_loss: 0.9747 - val_acc: 0.6923\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7374 - acc: 0.6780 - val_loss: 0.9754 - val_acc: 0.6923\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7341 - acc: 0.6780 - val_loss: 0.9802 - val_acc: 0.6923\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7333 - acc: 0.6695 - val_loss: 0.9803 - val_acc: 0.6923\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7311 - acc: 0.6695 - val_loss: 0.9788 - val_acc: 0.6923\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7278 - acc: 0.6864 - val_loss: 0.9842 - val_acc: 0.6923\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7263 - acc: 0.6864 - val_loss: 0.9841 - val_acc: 0.6923\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7262 - acc: 0.7034 - val_loss: 0.9799 - val_acc: 0.6923\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7229 - acc: 0.6949 - val_loss: 0.9829 - val_acc: 0.6923\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7208 - acc: 0.6949 - val_loss: 0.9853 - val_acc: 0.6923\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7205 - acc: 0.7034 - val_loss: 0.9921 - val_acc: 0.6923\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7178 - acc: 0.7119 - val_loss: 0.9946 - val_acc: 0.6923\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7169 - acc: 0.7119 - val_loss: 0.9930 - val_acc: 0.6923\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7137 - acc: 0.7119 - val_loss: 0.9979 - val_acc: 0.6923\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7122 - acc: 0.7119 - val_loss: 0.9976 - val_acc: 0.6923\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7082 - acc: 0.7288 - val_loss: 0.9973 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.4183 - acc: 0.3051 - val_loss: 1.2568 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.2628 - acc: 0.3390 - val_loss: 1.2036 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.1613 - acc: 0.3983 - val_loss: 1.1624 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0873 - acc: 0.4153 - val_loss: 1.1340 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0353 - acc: 0.4407 - val_loss: 1.1075 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9977 - acc: 0.4576 - val_loss: 1.0935 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9764 - acc: 0.4746 - val_loss: 1.0792 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9555 - acc: 0.5254 - val_loss: 1.0645 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9432 - acc: 0.5678 - val_loss: 1.0580 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.9296 - acc: 0.5763 - val_loss: 1.0537 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9197 - acc: 0.6017 - val_loss: 1.0500 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9108 - acc: 0.5847 - val_loss: 1.0431 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.9032 - acc: 0.6017 - val_loss: 1.0400 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 293us/step - loss: 0.8963 - acc: 0.5932 - val_loss: 1.0342 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8919 - acc: 0.6102 - val_loss: 1.0304 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8835 - acc: 0.5847 - val_loss: 1.0300 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8805 - acc: 0.6102 - val_loss: 1.0327 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.8724 - acc: 0.6017 - val_loss: 1.0323 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.8687 - acc: 0.6186 - val_loss: 1.0321 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.8631 - acc: 0.6186 - val_loss: 1.0288 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8574 - acc: 0.6271 - val_loss: 1.0267 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8523 - acc: 0.6271 - val_loss: 1.0274 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8501 - acc: 0.6356 - val_loss: 1.0252 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8445 - acc: 0.6441 - val_loss: 1.0212 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.8437 - acc: 0.6441 - val_loss: 1.0219 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.8378 - acc: 0.6271 - val_loss: 1.0232 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 275us/step - loss: 0.8321 - acc: 0.6441 - val_loss: 1.0279 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8294 - acc: 0.6441 - val_loss: 1.0288 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8244 - acc: 0.6525 - val_loss: 1.0325 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8217 - acc: 0.6525 - val_loss: 1.0363 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8177 - acc: 0.6525 - val_loss: 1.0411 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8123 - acc: 0.6695 - val_loss: 1.0348 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.8095 - acc: 0.6441 - val_loss: 1.0372 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8060 - acc: 0.6610 - val_loss: 1.0429 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8039 - acc: 0.6610 - val_loss: 1.0467 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8008 - acc: 0.6610 - val_loss: 1.0479 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7979 - acc: 0.6525 - val_loss: 1.0503 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7907 - acc: 0.6525 - val_loss: 1.0591 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.7896 - acc: 0.6610 - val_loss: 1.0567 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7848 - acc: 0.6525 - val_loss: 1.0529 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7810 - acc: 0.6610 - val_loss: 1.0578 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7793 - acc: 0.6695 - val_loss: 1.0683 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7753 - acc: 0.6780 - val_loss: 1.0674 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7712 - acc: 0.6780 - val_loss: 1.0702 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7677 - acc: 0.6695 - val_loss: 1.0714 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.7676 - acc: 0.6695 - val_loss: 1.0721 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7618 - acc: 0.6949 - val_loss: 1.0810 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.7604 - acc: 0.6864 - val_loss: 1.0822 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7553 - acc: 0.6864 - val_loss: 1.0874 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7530 - acc: 0.6780 - val_loss: 1.0940 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7484 - acc: 0.6780 - val_loss: 1.0911 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7457 - acc: 0.7119 - val_loss: 1.0906 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7431 - acc: 0.7034 - val_loss: 1.0976 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7397 - acc: 0.6864 - val_loss: 1.0933 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7371 - acc: 0.7203 - val_loss: 1.0958 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7332 - acc: 0.7203 - val_loss: 1.0951 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7305 - acc: 0.7034 - val_loss: 1.0924 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.7269 - acc: 0.7203 - val_loss: 1.0954 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7260 - acc: 0.7034 - val_loss: 1.1073 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7221 - acc: 0.7119 - val_loss: 1.1116 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7191 - acc: 0.7119 - val_loss: 1.1066 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7163 - acc: 0.6864 - val_loss: 1.1133 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7133 - acc: 0.7203 - val_loss: 1.1095 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 273us/step - loss: 0.7106 - acc: 0.7119 - val_loss: 1.1054 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.7070 - acc: 0.7288 - val_loss: 1.1112 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.7040 - acc: 0.7034 - val_loss: 1.1259 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7021 - acc: 0.7034 - val_loss: 1.1283 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6979 - acc: 0.7034 - val_loss: 1.1282 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6940 - acc: 0.7119 - val_loss: 1.1335 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6922 - acc: 0.7288 - val_loss: 1.1409 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6901 - acc: 0.7119 - val_loss: 1.1424 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6878 - acc: 0.7034 - val_loss: 1.1375 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6850 - acc: 0.7119 - val_loss: 1.1307 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6841 - acc: 0.6949 - val_loss: 1.1453 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6785 - acc: 0.7203 - val_loss: 1.1355 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.6769 - acc: 0.7373 - val_loss: 1.1494 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6754 - acc: 0.7288 - val_loss: 1.1495 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6728 - acc: 0.7119 - val_loss: 1.1434 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6686 - acc: 0.7373 - val_loss: 1.1513 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6666 - acc: 0.7288 - val_loss: 1.1575 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6641 - acc: 0.7203 - val_loss: 1.1595 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6617 - acc: 0.7373 - val_loss: 1.1720 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6581 - acc: 0.7203 - val_loss: 1.1717 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6542 - acc: 0.7288 - val_loss: 1.1642 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6554 - acc: 0.7373 - val_loss: 1.1740 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6524 - acc: 0.7458 - val_loss: 1.1899 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.6483 - acc: 0.7458 - val_loss: 1.1712 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6474 - acc: 0.7458 - val_loss: 1.1896 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6441 - acc: 0.7458 - val_loss: 1.1868 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6432 - acc: 0.7373 - val_loss: 1.1767 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6411 - acc: 0.7373 - val_loss: 1.1938 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6381 - acc: 0.7288 - val_loss: 1.1914 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.6366 - acc: 0.7288 - val_loss: 1.1882 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6334 - acc: 0.7458 - val_loss: 1.2114 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.6308 - acc: 0.7373 - val_loss: 1.2155 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6291 - acc: 0.7373 - val_loss: 1.2161 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6298 - acc: 0.7542 - val_loss: 1.2278 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6235 - acc: 0.7542 - val_loss: 1.2291 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6242 - acc: 0.7542 - val_loss: 1.2297 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6199 - acc: 0.7542 - val_loss: 1.2444 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 2s 15ms/step - loss: 1.2887 - acc: 0.3983 - val_loss: 1.3897 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.1151 - acc: 0.4237 - val_loss: 1.2958 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 1.0133 - acc: 0.4661 - val_loss: 1.2379 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9532 - acc: 0.5254 - val_loss: 1.1988 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9182 - acc: 0.5678 - val_loss: 1.1809 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8942 - acc: 0.5508 - val_loss: 1.1747 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8782 - acc: 0.5678 - val_loss: 1.1766 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8655 - acc: 0.5763 - val_loss: 1.1754 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8548 - acc: 0.5763 - val_loss: 1.1747 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8448 - acc: 0.5847 - val_loss: 1.1803 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8352 - acc: 0.5932 - val_loss: 1.1858 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8252 - acc: 0.5847 - val_loss: 1.2011 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8197 - acc: 0.6017 - val_loss: 1.2091 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8100 - acc: 0.5847 - val_loss: 1.2186 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8054 - acc: 0.6017 - val_loss: 1.2226 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7972 - acc: 0.6186 - val_loss: 1.2194 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7913 - acc: 0.6356 - val_loss: 1.2316 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7830 - acc: 0.6441 - val_loss: 1.2395 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7774 - acc: 0.6186 - val_loss: 1.2425 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7711 - acc: 0.6441 - val_loss: 1.2497 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.7655 - acc: 0.6695 - val_loss: 1.2600 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7594 - acc: 0.6695 - val_loss: 1.2590 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7536 - acc: 0.6695 - val_loss: 1.2704 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7495 - acc: 0.6525 - val_loss: 1.2858 - val_acc: 0.2308\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.7451 - acc: 0.6610 - val_loss: 1.3033 - val_acc: 0.2308\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.7386 - acc: 0.6695 - val_loss: 1.2931 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7355 - acc: 0.6695 - val_loss: 1.3073 - val_acc: 0.2308\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7319 - acc: 0.6695 - val_loss: 1.3022 - val_acc: 0.2308\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.7273 - acc: 0.6695 - val_loss: 1.3089 - val_acc: 0.2308\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7221 - acc: 0.6525 - val_loss: 1.3215 - val_acc: 0.2308\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7190 - acc: 0.6780 - val_loss: 1.3353 - val_acc: 0.2308\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7138 - acc: 0.6695 - val_loss: 1.3540 - val_acc: 0.2308\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7097 - acc: 0.6780 - val_loss: 1.3633 - val_acc: 0.2308\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7091 - acc: 0.6695 - val_loss: 1.3724 - val_acc: 0.2308\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7038 - acc: 0.6695 - val_loss: 1.3743 - val_acc: 0.2308\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7005 - acc: 0.6864 - val_loss: 1.3865 - val_acc: 0.2308\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6984 - acc: 0.7034 - val_loss: 1.3993 - val_acc: 0.2308\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6923 - acc: 0.6949 - val_loss: 1.4093 - val_acc: 0.2308\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6889 - acc: 0.6949 - val_loss: 1.4285 - val_acc: 0.2308\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.6886 - acc: 0.6864 - val_loss: 1.4429 - val_acc: 0.2308\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6841 - acc: 0.6780 - val_loss: 1.4676 - val_acc: 0.2308\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6802 - acc: 0.6949 - val_loss: 1.4732 - val_acc: 0.2308\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6777 - acc: 0.7034 - val_loss: 1.4803 - val_acc: 0.2308\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.6737 - acc: 0.6949 - val_loss: 1.4882 - val_acc: 0.2308\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6709 - acc: 0.7034 - val_loss: 1.4988 - val_acc: 0.2308\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6669 - acc: 0.7034 - val_loss: 1.5253 - val_acc: 0.2308\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6633 - acc: 0.7119 - val_loss: 1.5301 - val_acc: 0.2308\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6611 - acc: 0.7119 - val_loss: 1.5292 - val_acc: 0.2308\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6573 - acc: 0.7034 - val_loss: 1.5337 - val_acc: 0.2308\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6539 - acc: 0.7119 - val_loss: 1.5569 - val_acc: 0.2308\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6509 - acc: 0.7034 - val_loss: 1.5715 - val_acc: 0.2308\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6480 - acc: 0.7034 - val_loss: 1.5788 - val_acc: 0.2308\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6452 - acc: 0.7034 - val_loss: 1.5944 - val_acc: 0.2308\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6414 - acc: 0.7288 - val_loss: 1.6092 - val_acc: 0.2308\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6387 - acc: 0.7542 - val_loss: 1.6228 - val_acc: 0.2308\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 273us/step - loss: 0.6356 - acc: 0.7458 - val_loss: 1.6314 - val_acc: 0.2308\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6337 - acc: 0.7458 - val_loss: 1.6574 - val_acc: 0.2308\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.6328 - acc: 0.7288 - val_loss: 1.6675 - val_acc: 0.2308\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6299 - acc: 0.7458 - val_loss: 1.6760 - val_acc: 0.2308\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.6258 - acc: 0.7373 - val_loss: 1.6970 - val_acc: 0.2308\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6242 - acc: 0.7627 - val_loss: 1.6945 - val_acc: 0.2308\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6202 - acc: 0.7373 - val_loss: 1.7094 - val_acc: 0.2308\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6171 - acc: 0.7542 - val_loss: 1.7250 - val_acc: 0.2308\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.6151 - acc: 0.7542 - val_loss: 1.7451 - val_acc: 0.2308\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6130 - acc: 0.7542 - val_loss: 1.7581 - val_acc: 0.2308\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6094 - acc: 0.7542 - val_loss: 1.7499 - val_acc: 0.2308\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.6068 - acc: 0.7542 - val_loss: 1.7737 - val_acc: 0.2308\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6076 - acc: 0.7797 - val_loss: 1.7829 - val_acc: 0.2308\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6021 - acc: 0.7797 - val_loss: 1.7988 - val_acc: 0.2308\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.6002 - acc: 0.7627 - val_loss: 1.8170 - val_acc: 0.2308\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.5976 - acc: 0.7712 - val_loss: 1.8230 - val_acc: 0.2308\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.5963 - acc: 0.7881 - val_loss: 1.8442 - val_acc: 0.2308\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.5927 - acc: 0.7881 - val_loss: 1.8486 - val_acc: 0.2308\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.5905 - acc: 0.7712 - val_loss: 1.8508 - val_acc: 0.2308\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.5855 - acc: 0.7797 - val_loss: 1.8618 - val_acc: 0.2308\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.5836 - acc: 0.7881 - val_loss: 1.8862 - val_acc: 0.2308\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.5805 - acc: 0.7797 - val_loss: 1.8951 - val_acc: 0.2308\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5775 - acc: 0.7881 - val_loss: 1.9090 - val_acc: 0.2308\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.5743 - acc: 0.7881 - val_loss: 1.9120 - val_acc: 0.2308\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.5706 - acc: 0.7881 - val_loss: 1.9342 - val_acc: 0.2308\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.5685 - acc: 0.7881 - val_loss: 1.9294 - val_acc: 0.2308\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.5660 - acc: 0.7966 - val_loss: 1.9366 - val_acc: 0.2308\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5630 - acc: 0.7797 - val_loss: 1.9600 - val_acc: 0.2308\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5613 - acc: 0.7881 - val_loss: 1.9622 - val_acc: 0.2308\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5596 - acc: 0.7881 - val_loss: 1.9736 - val_acc: 0.2308\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.5566 - acc: 0.7881 - val_loss: 1.9632 - val_acc: 0.2308\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.5540 - acc: 0.7881 - val_loss: 1.9914 - val_acc: 0.3077\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5527 - acc: 0.7966 - val_loss: 2.0083 - val_acc: 0.3077\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5482 - acc: 0.8051 - val_loss: 2.0121 - val_acc: 0.3077\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5473 - acc: 0.7966 - val_loss: 2.0095 - val_acc: 0.3077\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5449 - acc: 0.8051 - val_loss: 2.0228 - val_acc: 0.3077\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5437 - acc: 0.7881 - val_loss: 2.0554 - val_acc: 0.3077\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.5423 - acc: 0.7881 - val_loss: 2.0569 - val_acc: 0.3077\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5398 - acc: 0.7966 - val_loss: 2.0675 - val_acc: 0.3077\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.5372 - acc: 0.7881 - val_loss: 2.0753 - val_acc: 0.3077\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5351 - acc: 0.7966 - val_loss: 2.0878 - val_acc: 0.3077\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5330 - acc: 0.8051 - val_loss: 2.1008 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.5312 - acc: 0.7966 - val_loss: 2.0922 - val_acc: 0.3077\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5303 - acc: 0.7966 - val_loss: 2.1115 - val_acc: 0.3077\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5279 - acc: 0.7966 - val_loss: 2.1215 - val_acc: 0.3077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bc44266b-9908-4928-f051-a0f39cf11761",
        "id": "6OH7qa6a14-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.34257050e-01,  9.68988268e-02,  3.85769839e-01,\n",
              "         1.61200050e-01, -5.40961150e-01, -3.83658358e-01,\n",
              "        -1.13750969e-01, -2.28591476e-01, -1.15055286e-01,\n",
              "        -3.30299955e-01,  1.35982249e-01,  3.71533408e-01,\n",
              "        -1.21254356e-01, -4.84378805e-01, -5.30458542e-01,\n",
              "        -4.87994766e-01,  1.88481002e-01],\n",
              "       [ 1.54957687e+00,  1.90539890e+00, -2.22052320e-01,\n",
              "         3.87307146e-01, -5.45340485e-01, -1.01385115e+00,\n",
              "        -2.73626628e-01, -6.67618363e-01, -3.33713506e-01,\n",
              "        -3.30970545e-01, -1.05552128e+00, -1.16554466e+00,\n",
              "         7.92773556e-01, -6.04756035e-03, -1.36103435e-01,\n",
              "         2.84618093e-01, -7.68000271e-01],\n",
              "       [ 1.00150735e+00, -8.58201787e-01,  3.11646964e-01,\n",
              "        -1.43442573e+00, -5.49929348e-01, -9.04252402e-01,\n",
              "        -5.28211743e-01, -1.20002808e-01, -3.35769175e-01,\n",
              "        -3.30973760e-01, -4.42456637e-01,  2.35041057e+00,\n",
              "        -1.82929991e+00,  2.68994980e-01,  2.63286133e+00,\n",
              "         1.29973450e+00, -1.07303619e+00],\n",
              "       [ 5.22267532e-01, -1.06875098e-01, -6.47157410e-01,\n",
              "        -2.81002626e-02, -5.47216346e-01,  5.47366256e-02,\n",
              "        -9.22810177e-01,  2.97018333e-01, -1.85535435e-01,\n",
              "        -3.30751288e-01, -1.01051841e+00,  5.33572761e-01,\n",
              "        -8.41591880e-01, -1.03863818e+00, -5.91370564e-01,\n",
              "        -4.78529164e-01,  7.65523623e-02],\n",
              "       [ 2.31051081e-01, -3.20246036e-01,  3.33606515e-01,\n",
              "        -1.92448965e+00, -5.47077623e-01, -1.39470679e+00,\n",
              "         1.71658188e-01, -8.49813810e-01, -3.35016786e-01,\n",
              "        -3.30973339e-01, -1.13057908e+00, -3.96647241e-02,\n",
              "        -7.76402925e-02, -1.46431122e-01,  1.57365298e+00,\n",
              "         2.06583169e+00, -9.92270817e-01],\n",
              "       [-6.44453936e-01, -8.78723589e-03, -1.04418142e-01,\n",
              "        -5.22952367e-01,  9.09313099e-01,  1.69871781e+00,\n",
              "        -1.21474566e+00,  1.75610763e+00, -9.44599440e-02,\n",
              "        -3.30471019e-01,  8.05783791e-01, -1.11896137e+00,\n",
              "        -5.71419421e-01, -1.13904175e+00,  2.36554102e-01,\n",
              "        -7.98738149e-01, -5.48945706e-01],\n",
              "       [-7.06431308e-01, -9.89748699e-03,  1.32801751e+00,\n",
              "        -1.37618450e+00, -5.39625628e-01,  6.60326170e+00,\n",
              "         3.83648904e+00, -1.69002919e+00, -2.97148140e-01,\n",
              "        -3.30858604e-01,  2.98396859e+00, -8.38653288e-01,\n",
              "         2.67323855e+00,  1.97331940e+00,  4.59022522e-01,\n",
              "        -5.16599725e-01, -7.19356677e-01],\n",
              "       [ 2.06934501e+00,  1.21207305e+00, -8.02529761e-01,\n",
              "        -3.23665850e+00,  3.52435672e-01, -1.01385115e+00,\n",
              "         5.82901521e-02, -7.18648977e-01, -3.35432838e-01,\n",
              "        -3.30974284e-01, -1.66414662e+00,  2.66798240e+00,\n",
              "        -1.79401751e+00,  3.67356718e-01,  2.40487403e+00,\n",
              "         4.02913212e+00, -1.04661333e+00],\n",
              "       [ 1.78305420e+00,  1.37711411e+00,  1.49916132e-01,\n",
              "        -5.15432158e-01, -5.46664013e-01, -1.17824927e+00,\n",
              "        -7.14148531e-01, -3.26597763e-01, -3.31802117e-01,\n",
              "        -3.30964935e-01, -1.10153207e+00, -3.25076660e-01,\n",
              "        -2.76899548e-01, -4.81278689e-01,  1.63951096e-01,\n",
              "         7.64810335e-01, -8.13461099e-01],\n",
              "       [ 1.33916856e+00,  1.74729366e+00,  9.32576683e-01,\n",
              "        -3.92248540e-01, -5.42436939e-01, -5.48056477e-01,\n",
              "         5.42853861e-01, -3.37319011e-01,  1.32217300e-01,\n",
              "        -3.30650739e-01,  9.93278185e-02,  2.18568209e+00,\n",
              "        -7.52734919e-01, -5.58159325e-01, -2.94368551e-01,\n",
              "         3.00978986e-04, -1.51329794e-01],\n",
              "       [ 4.98694935e-01,  7.35612793e-01,  6.35586964e-01,\n",
              "         6.54250049e-01,  3.16485533e-02, -1.64460866e-01,\n",
              "        -3.77661612e-01,  2.09671116e+00, -2.80318175e-01,\n",
              "        -3.30830305e-01,  6.43998628e-01,  5.19144703e-01,\n",
              "        -1.77373780e+00, -1.05512656e+00,  3.58224772e+00,\n",
              "        -7.01535374e-01, -9.40195936e-01],\n",
              "       [-1.70779933e+00, -1.82177506e+00,  6.30653686e-01,\n",
              "         7.72815870e-02, -5.41667522e-01, -7.94653656e-01,\n",
              "        -2.36091860e-01, -5.49058455e-01, -3.32324336e-01,\n",
              "        -3.30964704e-01, -2.78998649e-01,  7.31967045e-01,\n",
              "        -5.20673201e-02,  1.47627164e+00, -2.60634812e-01,\n",
              "         1.91408748e-01, -6.63483974e-01],\n",
              "       [-2.61293931e-01, -8.16561598e-01,  1.04338706e+00,\n",
              "        -1.15650705e+00, -5.41614180e-01,  2.60290747e+00,\n",
              "         1.30285401e+00, -7.54923071e-01, -2.82469107e-01,\n",
              "        -3.30793928e-01,  5.28076978e-01, -1.69820568e-01,\n",
              "         8.39421745e-01, -4.61928261e-01, -2.21762703e-01,\n",
              "        -2.37855717e-01, -5.14552169e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WoqA4rT14-Q",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eeddafce-9b99-4f7e-b25c-16406acfe4a3",
        "id": "YEyjhsg414-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f19122f9-f82a-42c9-c23a-4dceae62f7b9",
        "id": "9WylvzXG14-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw9GTwCC14-f",
        "colab": {}
      },
      "source": [
        "average_acc_history_reduced = [np.mean([x[i] for x in all_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_reduced = [np.mean([x[i] for x in all_loss_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_acc_history_reduced = [np.mean([x[i] for x in all_val_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_loss_history_reduced = [np.mean([x[i] for x in all_val_loss_histories_reduced]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f33038f4-f142-4fd4-e126-a84237f9de98",
        "id": "MdMC3doS14-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history_reduced)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "outputId": "bea16242-a9a4-4fc6-c236-0de4da47ace1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_42\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_83 (Dense)             (None, 10)                180       \n",
            "_________________________________________________________________\n",
            "dense_84 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrcg4Bx625fC"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9_gKwYk25fK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCcaAgTi25fZ",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0124ed20-edea-4f64-c167-12c7dc8d0db5",
        "id": "6H8nOl_X25fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_reduced, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_reduced, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4e6b955978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncM65BOeBwDnnEpwHAuecS3D/D/MFvWkI+wwYAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMh2BG152967"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "215783b7-58c2-4843-8479-13fbcb2149ff",
        "id": "WNEq_n08297J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_reduced, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_reduced, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4e6b8ce198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niPv/Zsi28x1SU90AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5BQnjMi93Dgn"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b26a53f1-7d0e-4538-a64e-998d2dec3052",
        "id": "yANP0XPF3Dg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_reduced, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_reduced, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 2s 13ms/step - loss: 1.3335 - acc: 0.2824\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.1608 - acc: 0.3893\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 1.0732 - acc: 0.4122\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.0227 - acc: 0.4580\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9892 - acc: 0.4962\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.9692 - acc: 0.5496\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9525 - acc: 0.5496\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9371 - acc: 0.5344\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.9280 - acc: 0.5496\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9188 - acc: 0.5725\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.9109 - acc: 0.5954\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 234us/step - loss: 0.9050 - acc: 0.5954\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.8981 - acc: 0.5954\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.8921 - acc: 0.5954\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.8859 - acc: 0.6107\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8828 - acc: 0.6336\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.8782 - acc: 0.6336\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 230us/step - loss: 0.8723 - acc: 0.6183\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 234us/step - loss: 0.8689 - acc: 0.6031\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.8652 - acc: 0.6260\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 0.8612 - acc: 0.6183\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8583 - acc: 0.6031\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.8544 - acc: 0.6107\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.8521 - acc: 0.6183\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8477 - acc: 0.6260\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 261us/step - loss: 0.8443 - acc: 0.6260\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 240us/step - loss: 0.8415 - acc: 0.6107\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.8373 - acc: 0.6183\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8349 - acc: 0.6107\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.8318 - acc: 0.6183\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8281 - acc: 0.6107\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.8283 - acc: 0.6107\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8221 - acc: 0.6031\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8202 - acc: 0.6183\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.8175 - acc: 0.6107\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 261us/step - loss: 0.8125 - acc: 0.6412\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.8108 - acc: 0.6336\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.8050 - acc: 0.6260\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.8019 - acc: 0.6412\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8014 - acc: 0.6412\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.7971 - acc: 0.6336\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.7942 - acc: 0.6412\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 224us/step - loss: 0.7922 - acc: 0.6489\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.7886 - acc: 0.6336\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.7863 - acc: 0.6489\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.7845 - acc: 0.6412\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.7822 - acc: 0.6489\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.7797 - acc: 0.6336\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 243us/step - loss: 0.7742 - acc: 0.6489\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.7700 - acc: 0.6489\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 230us/step - loss: 0.7683 - acc: 0.6718\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.7656 - acc: 0.6794\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.7629 - acc: 0.6718\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.7605 - acc: 0.6641\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.7572 - acc: 0.6794\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 233us/step - loss: 0.7543 - acc: 0.6565\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.7518 - acc: 0.6794\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 0.7506 - acc: 0.6718\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.7482 - acc: 0.6641\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 236us/step - loss: 0.7454 - acc: 0.6794\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.7433 - acc: 0.6718\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.7396 - acc: 0.6947\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 233us/step - loss: 0.7364 - acc: 0.6870\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 237us/step - loss: 0.7362 - acc: 0.6718\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.7329 - acc: 0.6870\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.7291 - acc: 0.6794\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.7262 - acc: 0.6947\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 229us/step - loss: 0.7236 - acc: 0.6947\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.7250 - acc: 0.6794\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.7209 - acc: 0.7099\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 243us/step - loss: 0.7182 - acc: 0.6870\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.7164 - acc: 0.6718\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.7135 - acc: 0.7176\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.7086 - acc: 0.7023\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.7065 - acc: 0.7176\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.7029 - acc: 0.6870\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.7031 - acc: 0.7176\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.7002 - acc: 0.6947\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6972 - acc: 0.7023\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.6969 - acc: 0.7023\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.6912 - acc: 0.7252\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 248us/step - loss: 0.6929 - acc: 0.7328\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.6894 - acc: 0.7176\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.6866 - acc: 0.7328\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.6872 - acc: 0.7252\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.6817 - acc: 0.6947\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6788 - acc: 0.7328\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 242us/step - loss: 0.6776 - acc: 0.7252\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.6741 - acc: 0.7176\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.6701 - acc: 0.7176\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.6698 - acc: 0.7252\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.6688 - acc: 0.7328\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 266us/step - loss: 0.6661 - acc: 0.7328\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.6616 - acc: 0.7328\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.6609 - acc: 0.7176\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.6575 - acc: 0.7252\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.6572 - acc: 0.7252\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.6560 - acc: 0.7099\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.6507 - acc: 0.7557\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.6527 - acc: 0.7176\n",
            "34/34 [==============================] - 1s 19ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4bdd7a71-6691-4d4f-ea02-3aba19c09403",
        "id": "kQLgWQtS3DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f8cbcc91-d287-48f9-85a2-c75082c22ea3",
        "id": "_c5TGT4H3DhR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11764705882352941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMbKL5Yw3DhZ"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "8933d854-f838-424f-d05f-e4cf7eff1966",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "afe1370f-1ae7-4978-a43f-9c3945c6fa85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "366020f8-499f-41d6-d3de-2d41e92e8ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "f1ffd928-ccbe-4df8-bd57-59f6c0671df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "cb98c250-fb3a-4922-f91a-8fe0a5b75344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "236e20c5-b4d6-4caa-b775-9f7630e11af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "b1315d62-8214-4a3d-e59b-905bd9365878",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "ca601ebf-5418-4f46-bada-9d0e9428a1c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 3.3206 - acc: 0.4017 - val_loss: 3.7047 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 2.9899 - acc: 0.4017 - val_loss: 3.3222 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 2.7019 - acc: 0.4103 - val_loss: 2.9999 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 2.4530 - acc: 0.4188 - val_loss: 2.7043 - val_acc: 0.5000\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 213us/step - loss: 2.2509 - acc: 0.4188 - val_loss: 2.4662 - val_acc: 0.5000\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 2.0845 - acc: 0.4017 - val_loss: 2.2641 - val_acc: 0.5000\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 1.9383 - acc: 0.3932 - val_loss: 2.0559 - val_acc: 0.4286\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 1.8071 - acc: 0.3932 - val_loss: 1.8882 - val_acc: 0.4286\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 1.6994 - acc: 0.3932 - val_loss: 1.7397 - val_acc: 0.4286\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.5972 - acc: 0.4017 - val_loss: 1.6037 - val_acc: 0.4286\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 1.5076 - acc: 0.4103 - val_loss: 1.4880 - val_acc: 0.4286\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 301us/step - loss: 1.4328 - acc: 0.4103 - val_loss: 1.4008 - val_acc: 0.4286\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 1.3725 - acc: 0.4103 - val_loss: 1.3285 - val_acc: 0.4286\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.3224 - acc: 0.4103 - val_loss: 1.2722 - val_acc: 0.4286\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 1.2854 - acc: 0.4274 - val_loss: 1.2297 - val_acc: 0.4286\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.2560 - acc: 0.4274 - val_loss: 1.1958 - val_acc: 0.4286\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.2301 - acc: 0.4444 - val_loss: 1.1689 - val_acc: 0.5000\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 1.2087 - acc: 0.4530 - val_loss: 1.1457 - val_acc: 0.5714\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 1.1919 - acc: 0.4615 - val_loss: 1.1288 - val_acc: 0.5714\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.1748 - acc: 0.4701 - val_loss: 1.1159 - val_acc: 0.5714\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.1602 - acc: 0.4530 - val_loss: 1.1063 - val_acc: 0.5714\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 1.1473 - acc: 0.4444 - val_loss: 1.0963 - val_acc: 0.5714\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 1.1357 - acc: 0.4530 - val_loss: 1.0888 - val_acc: 0.5714\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 244us/step - loss: 1.1256 - acc: 0.4530 - val_loss: 1.0804 - val_acc: 0.5714\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 1.1145 - acc: 0.4530 - val_loss: 1.0737 - val_acc: 0.5714\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 257us/step - loss: 1.1049 - acc: 0.4359 - val_loss: 1.0695 - val_acc: 0.5714\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 1.0962 - acc: 0.4530 - val_loss: 1.0645 - val_acc: 0.5714\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 1.0867 - acc: 0.4444 - val_loss: 1.0596 - val_acc: 0.5714\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 226us/step - loss: 1.0791 - acc: 0.4530 - val_loss: 1.0574 - val_acc: 0.5714\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 1.0729 - acc: 0.4188 - val_loss: 1.0595 - val_acc: 0.5714\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 1.0648 - acc: 0.4444 - val_loss: 1.0562 - val_acc: 0.5714\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 1.0596 - acc: 0.4359 - val_loss: 1.0567 - val_acc: 0.5714\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 1.0517 - acc: 0.4444 - val_loss: 1.0565 - val_acc: 0.5714\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 246us/step - loss: 1.0466 - acc: 0.4444 - val_loss: 1.0552 - val_acc: 0.5000\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 1.0403 - acc: 0.4444 - val_loss: 1.0539 - val_acc: 0.5000\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 1.0347 - acc: 0.4444 - val_loss: 1.0526 - val_acc: 0.5000\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 1.0306 - acc: 0.4615 - val_loss: 1.0506 - val_acc: 0.5000\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 1.0254 - acc: 0.4615 - val_loss: 1.0521 - val_acc: 0.5000\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 1.0204 - acc: 0.4359 - val_loss: 1.0517 - val_acc: 0.5000\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 255us/step - loss: 1.0151 - acc: 0.4359 - val_loss: 1.0510 - val_acc: 0.5000\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 1.0107 - acc: 0.4701 - val_loss: 1.0540 - val_acc: 0.5000\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 1.0069 - acc: 0.4530 - val_loss: 1.0539 - val_acc: 0.5000\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 267us/step - loss: 1.0023 - acc: 0.4615 - val_loss: 1.0527 - val_acc: 0.5000\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 0.9981 - acc: 0.4530 - val_loss: 1.0535 - val_acc: 0.5000\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9953 - acc: 0.4615 - val_loss: 1.0532 - val_acc: 0.5000\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9912 - acc: 0.4615 - val_loss: 1.0542 - val_acc: 0.5000\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9874 - acc: 0.4701 - val_loss: 1.0526 - val_acc: 0.5000\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9841 - acc: 0.4701 - val_loss: 1.0535 - val_acc: 0.5000\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.9808 - acc: 0.4786 - val_loss: 1.0521 - val_acc: 0.5000\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9771 - acc: 0.4872 - val_loss: 1.0535 - val_acc: 0.5000\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.9738 - acc: 0.4872 - val_loss: 1.0533 - val_acc: 0.5000\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9708 - acc: 0.4786 - val_loss: 1.0521 - val_acc: 0.5000\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9686 - acc: 0.4957 - val_loss: 1.0522 - val_acc: 0.5000\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.9651 - acc: 0.4957 - val_loss: 1.0510 - val_acc: 0.5000\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.9629 - acc: 0.4872 - val_loss: 1.0537 - val_acc: 0.5000\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.9592 - acc: 0.5128 - val_loss: 1.0524 - val_acc: 0.5000\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9573 - acc: 0.4957 - val_loss: 1.0542 - val_acc: 0.5000\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9548 - acc: 0.5128 - val_loss: 1.0514 - val_acc: 0.5000\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9517 - acc: 0.5043 - val_loss: 1.0525 - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.9504 - acc: 0.5128 - val_loss: 1.0526 - val_acc: 0.5000\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.9480 - acc: 0.5128 - val_loss: 1.0547 - val_acc: 0.4286\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.9452 - acc: 0.5214 - val_loss: 1.0544 - val_acc: 0.4286\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.9429 - acc: 0.5214 - val_loss: 1.0545 - val_acc: 0.4286\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.9425 - acc: 0.5128 - val_loss: 1.0522 - val_acc: 0.4286\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.9399 - acc: 0.5214 - val_loss: 1.0544 - val_acc: 0.4286\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9370 - acc: 0.5128 - val_loss: 1.0538 - val_acc: 0.4286\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9352 - acc: 0.5214 - val_loss: 1.0550 - val_acc: 0.4286\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.9340 - acc: 0.5214 - val_loss: 1.0557 - val_acc: 0.4286\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.9322 - acc: 0.5214 - val_loss: 1.0547 - val_acc: 0.4286\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.9305 - acc: 0.5128 - val_loss: 1.0541 - val_acc: 0.4286\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 228us/step - loss: 0.9291 - acc: 0.5128 - val_loss: 1.0562 - val_acc: 0.4286\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.9277 - acc: 0.5128 - val_loss: 1.0573 - val_acc: 0.4286\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 0.9255 - acc: 0.5043 - val_loss: 1.0552 - val_acc: 0.4286\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.9244 - acc: 0.5043 - val_loss: 1.0542 - val_acc: 0.4286\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9227 - acc: 0.5043 - val_loss: 1.0561 - val_acc: 0.3571\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9221 - acc: 0.5128 - val_loss: 1.0571 - val_acc: 0.3571\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.9204 - acc: 0.5043 - val_loss: 1.0591 - val_acc: 0.3571\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9183 - acc: 0.5043 - val_loss: 1.0605 - val_acc: 0.3571\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9176 - acc: 0.5128 - val_loss: 1.0587 - val_acc: 0.3571\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.9161 - acc: 0.5128 - val_loss: 1.0588 - val_acc: 0.3571\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9158 - acc: 0.5128 - val_loss: 1.0592 - val_acc: 0.3571\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9139 - acc: 0.5128 - val_loss: 1.0587 - val_acc: 0.3571\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9126 - acc: 0.5128 - val_loss: 1.0568 - val_acc: 0.3571\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9123 - acc: 0.5128 - val_loss: 1.0561 - val_acc: 0.3571\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.9112 - acc: 0.5128 - val_loss: 1.0581 - val_acc: 0.3571\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9101 - acc: 0.5128 - val_loss: 1.0552 - val_acc: 0.3571\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.9101 - acc: 0.5214 - val_loss: 1.0565 - val_acc: 0.3571\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.9081 - acc: 0.5214 - val_loss: 1.0568 - val_acc: 0.3571\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.9070 - acc: 0.5128 - val_loss: 1.0571 - val_acc: 0.3571\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.9066 - acc: 0.5214 - val_loss: 1.0567 - val_acc: 0.3571\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 269us/step - loss: 0.9057 - acc: 0.5128 - val_loss: 1.0558 - val_acc: 0.3571\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.9047 - acc: 0.5043 - val_loss: 1.0594 - val_acc: 0.3571\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.9047 - acc: 0.5043 - val_loss: 1.0586 - val_acc: 0.3571\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 236us/step - loss: 0.9031 - acc: 0.5128 - val_loss: 1.0614 - val_acc: 0.3571\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 260us/step - loss: 0.9025 - acc: 0.5043 - val_loss: 1.0622 - val_acc: 0.3571\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.9011 - acc: 0.5043 - val_loss: 1.0650 - val_acc: 0.3571\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.9010 - acc: 0.5214 - val_loss: 1.0644 - val_acc: 0.3571\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9004 - acc: 0.5214 - val_loss: 1.0670 - val_acc: 0.3571\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.9006 - acc: 0.5214 - val_loss: 1.0665 - val_acc: 0.3571\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.8998 - acc: 0.5128 - val_loss: 1.0662 - val_acc: 0.3571\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 1ms/step - loss: 1.2313 - acc: 0.4576 - val_loss: 1.8762 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1991 - acc: 0.4661 - val_loss: 1.8005 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.1722 - acc: 0.4746 - val_loss: 1.7171 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.1484 - acc: 0.4746 - val_loss: 1.6528 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1288 - acc: 0.4661 - val_loss: 1.6010 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.1125 - acc: 0.4831 - val_loss: 1.5458 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0978 - acc: 0.4831 - val_loss: 1.5094 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0856 - acc: 0.4746 - val_loss: 1.4746 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0756 - acc: 0.4746 - val_loss: 1.4443 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0689 - acc: 0.4746 - val_loss: 1.4152 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0575 - acc: 0.4746 - val_loss: 1.3982 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0514 - acc: 0.4746 - val_loss: 1.3764 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0451 - acc: 0.4746 - val_loss: 1.3606 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0389 - acc: 0.4746 - val_loss: 1.3444 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 268us/step - loss: 1.0341 - acc: 0.4746 - val_loss: 1.3310 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0295 - acc: 0.4746 - val_loss: 1.3223 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0252 - acc: 0.4661 - val_loss: 1.3139 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 269us/step - loss: 1.0227 - acc: 0.4746 - val_loss: 1.3020 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 1.0182 - acc: 0.4661 - val_loss: 1.2975 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 1.0153 - acc: 0.4661 - val_loss: 1.2960 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0121 - acc: 0.4661 - val_loss: 1.2902 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.0113 - acc: 0.4661 - val_loss: 1.2848 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0065 - acc: 0.4831 - val_loss: 1.2810 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0037 - acc: 0.4831 - val_loss: 1.2749 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.0012 - acc: 0.4831 - val_loss: 1.2759 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 288us/step - loss: 0.9996 - acc: 0.4831 - val_loss: 1.2672 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.9977 - acc: 0.4831 - val_loss: 1.2673 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9963 - acc: 0.4746 - val_loss: 1.2632 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9930 - acc: 0.4831 - val_loss: 1.2572 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9923 - acc: 0.4831 - val_loss: 1.2551 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9900 - acc: 0.4746 - val_loss: 1.2539 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9883 - acc: 0.4746 - val_loss: 1.2449 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9864 - acc: 0.4746 - val_loss: 1.2430 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9842 - acc: 0.4831 - val_loss: 1.2380 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9830 - acc: 0.4831 - val_loss: 1.2362 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9815 - acc: 0.4831 - val_loss: 1.2308 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9799 - acc: 0.5000 - val_loss: 1.2286 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9791 - acc: 0.5000 - val_loss: 1.2253 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.9775 - acc: 0.4915 - val_loss: 1.2219 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9758 - acc: 0.4915 - val_loss: 1.2183 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9743 - acc: 0.4915 - val_loss: 1.2172 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9734 - acc: 0.5000 - val_loss: 1.2161 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9719 - acc: 0.4915 - val_loss: 1.2127 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9701 - acc: 0.4915 - val_loss: 1.2095 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9695 - acc: 0.5000 - val_loss: 1.2111 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9687 - acc: 0.4915 - val_loss: 1.2032 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9666 - acc: 0.5000 - val_loss: 1.2010 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9657 - acc: 0.5000 - val_loss: 1.2010 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9640 - acc: 0.4915 - val_loss: 1.2018 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9631 - acc: 0.4915 - val_loss: 1.1983 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9620 - acc: 0.5000 - val_loss: 1.1971 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9614 - acc: 0.5085 - val_loss: 1.1959 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9611 - acc: 0.5085 - val_loss: 1.1940 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9599 - acc: 0.5085 - val_loss: 1.1940 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9572 - acc: 0.5085 - val_loss: 1.1930 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9570 - acc: 0.5085 - val_loss: 1.1906 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9558 - acc: 0.5085 - val_loss: 1.1903 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.9542 - acc: 0.5085 - val_loss: 1.1857 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9538 - acc: 0.5085 - val_loss: 1.1848 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9530 - acc: 0.5085 - val_loss: 1.1834 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9523 - acc: 0.5169 - val_loss: 1.1825 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9516 - acc: 0.5169 - val_loss: 1.1837 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9506 - acc: 0.5169 - val_loss: 1.1808 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9495 - acc: 0.5169 - val_loss: 1.1830 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9484 - acc: 0.5169 - val_loss: 1.1818 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.9473 - acc: 0.5169 - val_loss: 1.1816 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9465 - acc: 0.5169 - val_loss: 1.1794 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9468 - acc: 0.5169 - val_loss: 1.1807 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9458 - acc: 0.5254 - val_loss: 1.1805 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9451 - acc: 0.5254 - val_loss: 1.1770 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9449 - acc: 0.5254 - val_loss: 1.1754 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9449 - acc: 0.5254 - val_loss: 1.1742 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9434 - acc: 0.5254 - val_loss: 1.1742 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9426 - acc: 0.5254 - val_loss: 1.1740 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9420 - acc: 0.5254 - val_loss: 1.1732 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.9418 - acc: 0.5254 - val_loss: 1.1734 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.9423 - acc: 0.5254 - val_loss: 1.1708 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9407 - acc: 0.5254 - val_loss: 1.1708 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.9399 - acc: 0.5254 - val_loss: 1.1698 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9397 - acc: 0.5254 - val_loss: 1.1657 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9400 - acc: 0.5254 - val_loss: 1.1638 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9382 - acc: 0.5254 - val_loss: 1.1613 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.9393 - acc: 0.5254 - val_loss: 1.1621 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9384 - acc: 0.5254 - val_loss: 1.1634 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9379 - acc: 0.5254 - val_loss: 1.1644 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 265us/step - loss: 0.9369 - acc: 0.5254 - val_loss: 1.1670 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 270us/step - loss: 0.9366 - acc: 0.5254 - val_loss: 1.1616 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9371 - acc: 0.5254 - val_loss: 1.1640 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9359 - acc: 0.5254 - val_loss: 1.1613 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9358 - acc: 0.5254 - val_loss: 1.1606 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9358 - acc: 0.5254 - val_loss: 1.1613 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9344 - acc: 0.5254 - val_loss: 1.1602 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9347 - acc: 0.5254 - val_loss: 1.1623 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9333 - acc: 0.5254 - val_loss: 1.1602 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9332 - acc: 0.5254 - val_loss: 1.1587 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9324 - acc: 0.5254 - val_loss: 1.1588 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9332 - acc: 0.5254 - val_loss: 1.1601 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9314 - acc: 0.5339 - val_loss: 1.1573 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9310 - acc: 0.5254 - val_loss: 1.1566 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9307 - acc: 0.5254 - val_loss: 1.1573 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.6383 - acc: 0.3390 - val_loss: 1.8061 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.6000 - acc: 0.3390 - val_loss: 1.7621 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.5616 - acc: 0.3559 - val_loss: 1.7229 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.5288 - acc: 0.3644 - val_loss: 1.6855 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.4999 - acc: 0.3729 - val_loss: 1.6469 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 1.4728 - acc: 0.3814 - val_loss: 1.6128 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 1.4441 - acc: 0.3729 - val_loss: 1.5752 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.4087 - acc: 0.3729 - val_loss: 1.5397 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.3752 - acc: 0.3644 - val_loss: 1.5044 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.3462 - acc: 0.3559 - val_loss: 1.4803 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.3195 - acc: 0.3729 - val_loss: 1.4570 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.2991 - acc: 0.3729 - val_loss: 1.4375 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 1.2761 - acc: 0.3644 - val_loss: 1.4151 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2544 - acc: 0.3983 - val_loss: 1.4002 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.2358 - acc: 0.3983 - val_loss: 1.3901 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2175 - acc: 0.4153 - val_loss: 1.3748 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1989 - acc: 0.4237 - val_loss: 1.3641 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1862 - acc: 0.4237 - val_loss: 1.3543 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.1701 - acc: 0.4322 - val_loss: 1.3468 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1560 - acc: 0.4237 - val_loss: 1.3387 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.1436 - acc: 0.4237 - val_loss: 1.3311 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1304 - acc: 0.4237 - val_loss: 1.3238 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 1.1189 - acc: 0.4237 - val_loss: 1.3206 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1068 - acc: 0.4237 - val_loss: 1.3155 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0954 - acc: 0.4153 - val_loss: 1.3123 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0898 - acc: 0.4407 - val_loss: 1.3087 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0766 - acc: 0.4237 - val_loss: 1.3049 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0681 - acc: 0.4492 - val_loss: 1.3031 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0604 - acc: 0.4661 - val_loss: 1.3014 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 1.0512 - acc: 0.4661 - val_loss: 1.3018 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0458 - acc: 0.4831 - val_loss: 1.3010 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0365 - acc: 0.4831 - val_loss: 1.2994 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0316 - acc: 0.4915 - val_loss: 1.2994 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0273 - acc: 0.5085 - val_loss: 1.2996 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0203 - acc: 0.5000 - val_loss: 1.2990 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0170 - acc: 0.5254 - val_loss: 1.3000 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0097 - acc: 0.5254 - val_loss: 1.3017 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0094 - acc: 0.5169 - val_loss: 1.3021 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9996 - acc: 0.5508 - val_loss: 1.3035 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9981 - acc: 0.5424 - val_loss: 1.3034 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 268us/step - loss: 0.9938 - acc: 0.5593 - val_loss: 1.3052 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9897 - acc: 0.5424 - val_loss: 1.3070 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9893 - acc: 0.5424 - val_loss: 1.3075 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9844 - acc: 0.5424 - val_loss: 1.3115 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9815 - acc: 0.5424 - val_loss: 1.3105 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 288us/step - loss: 0.9806 - acc: 0.5424 - val_loss: 1.3141 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9765 - acc: 0.5339 - val_loss: 1.3150 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9742 - acc: 0.5339 - val_loss: 1.3172 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9724 - acc: 0.5254 - val_loss: 1.3187 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9704 - acc: 0.5593 - val_loss: 1.3184 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9685 - acc: 0.5424 - val_loss: 1.3200 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9644 - acc: 0.5508 - val_loss: 1.3186 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9641 - acc: 0.5339 - val_loss: 1.3187 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9630 - acc: 0.5339 - val_loss: 1.3206 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.9596 - acc: 0.5339 - val_loss: 1.3223 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9588 - acc: 0.5339 - val_loss: 1.3217 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9571 - acc: 0.5424 - val_loss: 1.3224 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9586 - acc: 0.5424 - val_loss: 1.3257 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9550 - acc: 0.5254 - val_loss: 1.3241 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9555 - acc: 0.5339 - val_loss: 1.3256 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9526 - acc: 0.5593 - val_loss: 1.3256 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9508 - acc: 0.5593 - val_loss: 1.3259 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9498 - acc: 0.5593 - val_loss: 1.3247 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9487 - acc: 0.5593 - val_loss: 1.3242 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9480 - acc: 0.5508 - val_loss: 1.3238 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9476 - acc: 0.5424 - val_loss: 1.3239 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9464 - acc: 0.5508 - val_loss: 1.3225 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9461 - acc: 0.5508 - val_loss: 1.3222 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9448 - acc: 0.5593 - val_loss: 1.3212 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9456 - acc: 0.5424 - val_loss: 1.3208 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 256us/step - loss: 0.9438 - acc: 0.5593 - val_loss: 1.3183 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9413 - acc: 0.5508 - val_loss: 1.3215 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9425 - acc: 0.5424 - val_loss: 1.3182 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9399 - acc: 0.5593 - val_loss: 1.3196 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9412 - acc: 0.5508 - val_loss: 1.3176 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9408 - acc: 0.5593 - val_loss: 1.3172 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9367 - acc: 0.5508 - val_loss: 1.3180 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9358 - acc: 0.5508 - val_loss: 1.3161 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9372 - acc: 0.5424 - val_loss: 1.3166 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9361 - acc: 0.5424 - val_loss: 1.3152 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9350 - acc: 0.5508 - val_loss: 1.3135 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9330 - acc: 0.5508 - val_loss: 1.3138 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9361 - acc: 0.5424 - val_loss: 1.3134 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9368 - acc: 0.5508 - val_loss: 1.3144 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.9329 - acc: 0.5424 - val_loss: 1.3113 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9316 - acc: 0.5508 - val_loss: 1.3103 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9312 - acc: 0.5508 - val_loss: 1.3085 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9289 - acc: 0.5593 - val_loss: 1.3098 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9297 - acc: 0.5508 - val_loss: 1.3073 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.9282 - acc: 0.5424 - val_loss: 1.3070 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9280 - acc: 0.5508 - val_loss: 1.3089 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9259 - acc: 0.5508 - val_loss: 1.3060 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9254 - acc: 0.5508 - val_loss: 1.3063 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9273 - acc: 0.5424 - val_loss: 1.3057 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 323us/step - loss: 0.9258 - acc: 0.5508 - val_loss: 1.3050 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9247 - acc: 0.5424 - val_loss: 1.3025 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9234 - acc: 0.5424 - val_loss: 1.3014 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9226 - acc: 0.5508 - val_loss: 1.3020 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9218 - acc: 0.5508 - val_loss: 1.3017 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9221 - acc: 0.5593 - val_loss: 1.3007 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.1527 - acc: 0.3051 - val_loss: 2.6330 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 2.0713 - acc: 0.3220 - val_loss: 2.6106 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.9908 - acc: 0.3305 - val_loss: 2.5892 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.9216 - acc: 0.3475 - val_loss: 2.5715 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.8596 - acc: 0.3559 - val_loss: 2.5544 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.7965 - acc: 0.3644 - val_loss: 2.5394 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.7458 - acc: 0.3644 - val_loss: 2.5076 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.6997 - acc: 0.3644 - val_loss: 2.4362 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 1.6516 - acc: 0.3644 - val_loss: 2.3703 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.6111 - acc: 0.3644 - val_loss: 2.3033 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.5702 - acc: 0.3814 - val_loss: 2.2318 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.5330 - acc: 0.3898 - val_loss: 2.1605 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.4957 - acc: 0.3898 - val_loss: 2.0909 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.4632 - acc: 0.3898 - val_loss: 2.0231 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.4338 - acc: 0.3814 - val_loss: 1.9605 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.4030 - acc: 0.3729 - val_loss: 1.8985 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.3761 - acc: 0.3983 - val_loss: 1.8422 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.3524 - acc: 0.3983 - val_loss: 1.7871 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.3321 - acc: 0.3898 - val_loss: 1.7375 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.3093 - acc: 0.3898 - val_loss: 1.6931 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.2877 - acc: 0.3983 - val_loss: 1.6513 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.2698 - acc: 0.3983 - val_loss: 1.6107 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2527 - acc: 0.3898 - val_loss: 1.5719 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.2374 - acc: 0.4068 - val_loss: 1.5384 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.2238 - acc: 0.3983 - val_loss: 1.5073 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.2075 - acc: 0.3898 - val_loss: 1.4800 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.1935 - acc: 0.3898 - val_loss: 1.4519 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1803 - acc: 0.3898 - val_loss: 1.4260 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1689 - acc: 0.3898 - val_loss: 1.4027 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.1596 - acc: 0.3814 - val_loss: 1.3732 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 1.1479 - acc: 0.3898 - val_loss: 1.3539 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1362 - acc: 0.3898 - val_loss: 1.3323 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.1272 - acc: 0.3898 - val_loss: 1.3170 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1181 - acc: 0.3898 - val_loss: 1.2997 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1102 - acc: 0.4068 - val_loss: 1.2845 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1023 - acc: 0.3983 - val_loss: 1.2704 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0947 - acc: 0.3983 - val_loss: 1.2578 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0885 - acc: 0.4153 - val_loss: 1.2428 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 1.0810 - acc: 0.4322 - val_loss: 1.2308 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.0739 - acc: 0.4237 - val_loss: 1.2224 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0690 - acc: 0.4237 - val_loss: 1.2131 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0643 - acc: 0.4237 - val_loss: 1.2028 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0580 - acc: 0.4237 - val_loss: 1.1943 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0526 - acc: 0.4237 - val_loss: 1.1869 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0479 - acc: 0.4407 - val_loss: 1.1775 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.0449 - acc: 0.4407 - val_loss: 1.1694 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.0398 - acc: 0.4322 - val_loss: 1.1627 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0348 - acc: 0.4407 - val_loss: 1.1539 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0310 - acc: 0.4492 - val_loss: 1.1466 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 1.0278 - acc: 0.4492 - val_loss: 1.1381 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0239 - acc: 0.4576 - val_loss: 1.1345 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0195 - acc: 0.4576 - val_loss: 1.1285 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 264us/step - loss: 1.0157 - acc: 0.4492 - val_loss: 1.1232 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.0121 - acc: 0.4322 - val_loss: 1.1184 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0084 - acc: 0.4407 - val_loss: 1.1136 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0045 - acc: 0.4407 - val_loss: 1.1100 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0012 - acc: 0.4407 - val_loss: 1.1068 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9986 - acc: 0.4322 - val_loss: 1.1021 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9966 - acc: 0.4407 - val_loss: 1.0999 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9928 - acc: 0.4322 - val_loss: 1.0959 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9915 - acc: 0.4576 - val_loss: 1.0918 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9905 - acc: 0.4237 - val_loss: 1.0898 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9882 - acc: 0.4576 - val_loss: 1.0858 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9868 - acc: 0.4492 - val_loss: 1.0840 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9858 - acc: 0.4576 - val_loss: 1.0814 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9850 - acc: 0.4492 - val_loss: 1.0794 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9829 - acc: 0.4492 - val_loss: 1.0775 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9813 - acc: 0.4576 - val_loss: 1.0748 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9802 - acc: 0.4492 - val_loss: 1.0738 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9808 - acc: 0.4576 - val_loss: 1.0727 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9786 - acc: 0.4492 - val_loss: 1.0688 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9773 - acc: 0.4576 - val_loss: 1.0685 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9762 - acc: 0.4576 - val_loss: 1.0677 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9758 - acc: 0.4576 - val_loss: 1.0655 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9745 - acc: 0.4661 - val_loss: 1.0636 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9732 - acc: 0.4661 - val_loss: 1.0631 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9734 - acc: 0.4576 - val_loss: 1.0612 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9718 - acc: 0.4576 - val_loss: 1.0592 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9719 - acc: 0.4661 - val_loss: 1.0581 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9706 - acc: 0.4661 - val_loss: 1.0567 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9699 - acc: 0.4661 - val_loss: 1.0559 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9691 - acc: 0.4661 - val_loss: 1.0554 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9687 - acc: 0.4661 - val_loss: 1.0543 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9681 - acc: 0.4661 - val_loss: 1.0525 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9670 - acc: 0.4831 - val_loss: 1.0532 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9671 - acc: 0.4831 - val_loss: 1.0528 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9664 - acc: 0.4831 - val_loss: 1.0529 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9660 - acc: 0.4746 - val_loss: 1.0526 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9648 - acc: 0.4661 - val_loss: 1.0519 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9638 - acc: 0.4661 - val_loss: 1.0501 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9655 - acc: 0.4492 - val_loss: 1.0501 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9626 - acc: 0.4661 - val_loss: 1.0488 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9642 - acc: 0.4661 - val_loss: 1.0484 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9623 - acc: 0.4831 - val_loss: 1.0478 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9625 - acc: 0.4746 - val_loss: 1.0475 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9617 - acc: 0.4831 - val_loss: 1.0469 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9607 - acc: 0.4746 - val_loss: 1.0475 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9605 - acc: 0.4915 - val_loss: 1.0471 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9599 - acc: 0.4831 - val_loss: 1.0464 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9594 - acc: 0.4831 - val_loss: 1.0460 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.3016 - acc: 0.4237 - val_loss: 1.8075 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.2787 - acc: 0.4153 - val_loss: 1.7648 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.2545 - acc: 0.4153 - val_loss: 1.7343 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.2332 - acc: 0.4068 - val_loss: 1.7071 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2144 - acc: 0.4068 - val_loss: 1.6742 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1986 - acc: 0.4068 - val_loss: 1.6502 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.1813 - acc: 0.4153 - val_loss: 1.6263 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1681 - acc: 0.4153 - val_loss: 1.6088 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1549 - acc: 0.4153 - val_loss: 1.5850 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1427 - acc: 0.4237 - val_loss: 1.5667 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1327 - acc: 0.3983 - val_loss: 1.5485 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.1226 - acc: 0.3983 - val_loss: 1.5230 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.1104 - acc: 0.4153 - val_loss: 1.4966 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1014 - acc: 0.4153 - val_loss: 1.4808 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.0918 - acc: 0.4237 - val_loss: 1.4511 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0835 - acc: 0.4407 - val_loss: 1.4297 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0766 - acc: 0.4492 - val_loss: 1.4138 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0699 - acc: 0.4322 - val_loss: 1.3990 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0651 - acc: 0.4576 - val_loss: 1.3818 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0598 - acc: 0.4322 - val_loss: 1.3741 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0548 - acc: 0.4322 - val_loss: 1.3504 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0507 - acc: 0.4576 - val_loss: 1.3317 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.0462 - acc: 0.4492 - val_loss: 1.3103 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0418 - acc: 0.4576 - val_loss: 1.2993 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0378 - acc: 0.4746 - val_loss: 1.2836 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0337 - acc: 0.4746 - val_loss: 1.2759 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0321 - acc: 0.4746 - val_loss: 1.2638 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0281 - acc: 0.4746 - val_loss: 1.2506 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.0251 - acc: 0.4661 - val_loss: 1.2395 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0225 - acc: 0.5169 - val_loss: 1.2279 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0211 - acc: 0.5085 - val_loss: 1.2242 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0173 - acc: 0.5339 - val_loss: 1.2102 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0158 - acc: 0.5169 - val_loss: 1.2017 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0128 - acc: 0.5254 - val_loss: 1.1918 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0101 - acc: 0.5339 - val_loss: 1.1810 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0077 - acc: 0.5254 - val_loss: 1.1751 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0063 - acc: 0.5254 - val_loss: 1.1711 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0040 - acc: 0.5339 - val_loss: 1.1603 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 1.0027 - acc: 0.5339 - val_loss: 1.1555 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9999 - acc: 0.5339 - val_loss: 1.1449 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9991 - acc: 0.5339 - val_loss: 1.1400 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9962 - acc: 0.5424 - val_loss: 1.1326 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9942 - acc: 0.5508 - val_loss: 1.1255 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9939 - acc: 0.5508 - val_loss: 1.1194 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9916 - acc: 0.5508 - val_loss: 1.1140 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9894 - acc: 0.5424 - val_loss: 1.1082 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.9887 - acc: 0.5424 - val_loss: 1.1072 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9867 - acc: 0.5508 - val_loss: 1.0993 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9852 - acc: 0.5424 - val_loss: 1.0947 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9840 - acc: 0.5424 - val_loss: 1.0915 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9825 - acc: 0.5424 - val_loss: 1.0846 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9808 - acc: 0.5339 - val_loss: 1.0791 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9807 - acc: 0.5339 - val_loss: 1.0751 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9787 - acc: 0.5254 - val_loss: 1.0732 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9771 - acc: 0.5339 - val_loss: 1.0650 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9769 - acc: 0.5339 - val_loss: 1.0642 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9753 - acc: 0.5254 - val_loss: 1.0584 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9741 - acc: 0.5254 - val_loss: 1.0556 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9724 - acc: 0.5254 - val_loss: 1.0499 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9729 - acc: 0.5424 - val_loss: 1.0482 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9713 - acc: 0.5254 - val_loss: 1.0471 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9695 - acc: 0.5424 - val_loss: 1.0385 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9688 - acc: 0.5508 - val_loss: 1.0347 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9681 - acc: 0.5424 - val_loss: 1.0338 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9668 - acc: 0.5424 - val_loss: 1.0269 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9659 - acc: 0.5424 - val_loss: 1.0245 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9653 - acc: 0.5508 - val_loss: 1.0242 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9645 - acc: 0.5508 - val_loss: 1.0183 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9643 - acc: 0.5593 - val_loss: 1.0190 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9634 - acc: 0.5593 - val_loss: 1.0152 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9614 - acc: 0.5508 - val_loss: 1.0141 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9609 - acc: 0.5424 - val_loss: 1.0124 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9608 - acc: 0.5678 - val_loss: 1.0113 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9598 - acc: 0.5508 - val_loss: 1.0083 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9598 - acc: 0.5593 - val_loss: 1.0029 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9583 - acc: 0.5424 - val_loss: 1.0015 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.9572 - acc: 0.5593 - val_loss: 0.9973 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9560 - acc: 0.5593 - val_loss: 0.9998 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9549 - acc: 0.5593 - val_loss: 0.9949 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.9551 - acc: 0.5678 - val_loss: 0.9940 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9544 - acc: 0.5678 - val_loss: 0.9920 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 293us/step - loss: 0.9538 - acc: 0.5593 - val_loss: 0.9894 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9531 - acc: 0.5593 - val_loss: 0.9806 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9524 - acc: 0.5678 - val_loss: 0.9843 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9512 - acc: 0.5763 - val_loss: 0.9797 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.9512 - acc: 0.5763 - val_loss: 0.9798 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9519 - acc: 0.5678 - val_loss: 0.9757 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9513 - acc: 0.5763 - val_loss: 0.9753 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9493 - acc: 0.5763 - val_loss: 0.9737 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9495 - acc: 0.5678 - val_loss: 0.9724 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9484 - acc: 0.5678 - val_loss: 0.9720 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9483 - acc: 0.5678 - val_loss: 0.9713 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9480 - acc: 0.5763 - val_loss: 0.9704 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9468 - acc: 0.5678 - val_loss: 0.9664 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9463 - acc: 0.5593 - val_loss: 0.9634 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9457 - acc: 0.5593 - val_loss: 0.9613 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9455 - acc: 0.5678 - val_loss: 0.9575 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9441 - acc: 0.5763 - val_loss: 0.9565 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9437 - acc: 0.5678 - val_loss: 0.9532 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9432 - acc: 0.5847 - val_loss: 0.9524 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 3.2214 - acc: 0.2119 - val_loss: 2.8097 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 2.7932 - acc: 0.2458 - val_loss: 2.4521 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 2.4143 - acc: 0.2458 - val_loss: 2.1573 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 2.1044 - acc: 0.2627 - val_loss: 1.9034 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.8667 - acc: 0.2627 - val_loss: 1.7069 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.6720 - acc: 0.2627 - val_loss: 1.5446 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.5222 - acc: 0.3305 - val_loss: 1.4219 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.4092 - acc: 0.3983 - val_loss: 1.3207 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.3177 - acc: 0.4237 - val_loss: 1.2450 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 1.2520 - acc: 0.4322 - val_loss: 1.1853 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.2055 - acc: 0.4068 - val_loss: 1.1385 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1707 - acc: 0.4237 - val_loss: 1.1049 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.1404 - acc: 0.4237 - val_loss: 1.0760 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.1166 - acc: 0.4322 - val_loss: 1.0530 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0977 - acc: 0.4576 - val_loss: 1.0357 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0815 - acc: 0.4576 - val_loss: 1.0213 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 1.0677 - acc: 0.4831 - val_loss: 1.0094 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0566 - acc: 0.4746 - val_loss: 1.0006 - val_acc: 0.6923\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0469 - acc: 0.4746 - val_loss: 0.9909 - val_acc: 0.6923\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 1.0390 - acc: 0.4661 - val_loss: 0.9848 - val_acc: 0.6923\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0306 - acc: 0.4746 - val_loss: 0.9797 - val_acc: 0.6923\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0232 - acc: 0.4915 - val_loss: 0.9713 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.0168 - acc: 0.4746 - val_loss: 0.9687 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0097 - acc: 0.5000 - val_loss: 0.9636 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0042 - acc: 0.5085 - val_loss: 0.9604 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9989 - acc: 0.5085 - val_loss: 0.9590 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9944 - acc: 0.5085 - val_loss: 0.9570 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9885 - acc: 0.5169 - val_loss: 0.9560 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9851 - acc: 0.5254 - val_loss: 0.9546 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9810 - acc: 0.5339 - val_loss: 0.9528 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9782 - acc: 0.5339 - val_loss: 0.9517 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9759 - acc: 0.5339 - val_loss: 0.9513 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9725 - acc: 0.5339 - val_loss: 0.9493 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9706 - acc: 0.5508 - val_loss: 0.9487 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.9688 - acc: 0.5508 - val_loss: 0.9474 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9676 - acc: 0.5593 - val_loss: 0.9455 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9650 - acc: 0.5508 - val_loss: 0.9454 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9635 - acc: 0.5678 - val_loss: 0.9456 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9611 - acc: 0.5678 - val_loss: 0.9441 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.9604 - acc: 0.5678 - val_loss: 0.9450 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9586 - acc: 0.5678 - val_loss: 0.9432 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9565 - acc: 0.5678 - val_loss: 0.9431 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9555 - acc: 0.5678 - val_loss: 0.9430 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9547 - acc: 0.5678 - val_loss: 0.9441 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9531 - acc: 0.5678 - val_loss: 0.9438 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9526 - acc: 0.5678 - val_loss: 0.9432 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9513 - acc: 0.5678 - val_loss: 0.9445 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9499 - acc: 0.5678 - val_loss: 0.9451 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9488 - acc: 0.5678 - val_loss: 0.9448 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9485 - acc: 0.5678 - val_loss: 0.9448 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9475 - acc: 0.5678 - val_loss: 0.9447 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9465 - acc: 0.5678 - val_loss: 0.9459 - val_acc: 0.6154\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9463 - acc: 0.5678 - val_loss: 0.9472 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9448 - acc: 0.5763 - val_loss: 0.9455 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9444 - acc: 0.5763 - val_loss: 0.9457 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9434 - acc: 0.5763 - val_loss: 0.9474 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9432 - acc: 0.5847 - val_loss: 0.9477 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9422 - acc: 0.5847 - val_loss: 0.9484 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9417 - acc: 0.5932 - val_loss: 0.9480 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9413 - acc: 0.5932 - val_loss: 0.9502 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9405 - acc: 0.5932 - val_loss: 0.9503 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9400 - acc: 0.5847 - val_loss: 0.9504 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9397 - acc: 0.5847 - val_loss: 0.9514 - val_acc: 0.6154\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9387 - acc: 0.5932 - val_loss: 0.9529 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9379 - acc: 0.5932 - val_loss: 0.9538 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.9375 - acc: 0.6017 - val_loss: 0.9541 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9366 - acc: 0.5932 - val_loss: 0.9544 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9360 - acc: 0.5932 - val_loss: 0.9544 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.9366 - acc: 0.6017 - val_loss: 0.9554 - val_acc: 0.6154\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9351 - acc: 0.5847 - val_loss: 0.9556 - val_acc: 0.6154\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9357 - acc: 0.5847 - val_loss: 0.9556 - val_acc: 0.6154\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9348 - acc: 0.5847 - val_loss: 0.9571 - val_acc: 0.6154\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9340 - acc: 0.5932 - val_loss: 0.9562 - val_acc: 0.6154\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9341 - acc: 0.5847 - val_loss: 0.9578 - val_acc: 0.6154\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9338 - acc: 0.5847 - val_loss: 0.9565 - val_acc: 0.6154\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9325 - acc: 0.5847 - val_loss: 0.9562 - val_acc: 0.6154\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9324 - acc: 0.5932 - val_loss: 0.9573 - val_acc: 0.6154\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9316 - acc: 0.5847 - val_loss: 0.9571 - val_acc: 0.6154\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.9319 - acc: 0.5847 - val_loss: 0.9574 - val_acc: 0.6154\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9308 - acc: 0.5932 - val_loss: 0.9575 - val_acc: 0.6154\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.9307 - acc: 0.5847 - val_loss: 0.9581 - val_acc: 0.6154\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9300 - acc: 0.5847 - val_loss: 0.9577 - val_acc: 0.6154\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.9294 - acc: 0.5847 - val_loss: 0.9586 - val_acc: 0.6154\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9294 - acc: 0.6017 - val_loss: 0.9572 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9285 - acc: 0.5932 - val_loss: 0.9584 - val_acc: 0.6154\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9283 - acc: 0.5847 - val_loss: 0.9582 - val_acc: 0.6154\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9277 - acc: 0.5847 - val_loss: 0.9585 - val_acc: 0.6154\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9273 - acc: 0.5932 - val_loss: 0.9577 - val_acc: 0.6154\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 275us/step - loss: 0.9274 - acc: 0.5932 - val_loss: 0.9578 - val_acc: 0.6154\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9265 - acc: 0.5847 - val_loss: 0.9588 - val_acc: 0.6154\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9268 - acc: 0.5847 - val_loss: 0.9577 - val_acc: 0.6154\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9257 - acc: 0.5847 - val_loss: 0.9582 - val_acc: 0.6154\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.9255 - acc: 0.5847 - val_loss: 0.9583 - val_acc: 0.6154\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9249 - acc: 0.5932 - val_loss: 0.9583 - val_acc: 0.6154\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9247 - acc: 0.5847 - val_loss: 0.9591 - val_acc: 0.6154\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9244 - acc: 0.5932 - val_loss: 0.9592 - val_acc: 0.6154\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9238 - acc: 0.5932 - val_loss: 0.9587 - val_acc: 0.6154\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9240 - acc: 0.5847 - val_loss: 0.9593 - val_acc: 0.6154\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9232 - acc: 0.5847 - val_loss: 0.9605 - val_acc: 0.6154\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9226 - acc: 0.5932 - val_loss: 0.9607 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.4186 - acc: 0.4237 - val_loss: 1.5678 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.3598 - acc: 0.4237 - val_loss: 1.5064 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.3141 - acc: 0.4153 - val_loss: 1.4543 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.2754 - acc: 0.4237 - val_loss: 1.4147 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2450 - acc: 0.4322 - val_loss: 1.3796 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.2209 - acc: 0.4237 - val_loss: 1.3490 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.2022 - acc: 0.4322 - val_loss: 1.3233 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1861 - acc: 0.4322 - val_loss: 1.3035 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1728 - acc: 0.4831 - val_loss: 1.2853 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1603 - acc: 0.4915 - val_loss: 1.2677 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1505 - acc: 0.4831 - val_loss: 1.2529 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1415 - acc: 0.4915 - val_loss: 1.2393 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1336 - acc: 0.5085 - val_loss: 1.2259 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1243 - acc: 0.5169 - val_loss: 1.2148 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1179 - acc: 0.5169 - val_loss: 1.2032 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1112 - acc: 0.5169 - val_loss: 1.1933 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.1052 - acc: 0.5085 - val_loss: 1.1847 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 1.0992 - acc: 0.5169 - val_loss: 1.1757 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0947 - acc: 0.5169 - val_loss: 1.1674 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 300us/step - loss: 1.0897 - acc: 0.5085 - val_loss: 1.1598 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.0849 - acc: 0.5254 - val_loss: 1.1536 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0793 - acc: 0.5339 - val_loss: 1.1450 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 269us/step - loss: 1.0750 - acc: 0.5254 - val_loss: 1.1372 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0707 - acc: 0.5424 - val_loss: 1.1307 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0668 - acc: 0.5254 - val_loss: 1.1243 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 1.0634 - acc: 0.5339 - val_loss: 1.1180 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0600 - acc: 0.5339 - val_loss: 1.1134 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0560 - acc: 0.5339 - val_loss: 1.1063 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0532 - acc: 0.5339 - val_loss: 1.1018 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0499 - acc: 0.5339 - val_loss: 1.0940 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0473 - acc: 0.5339 - val_loss: 1.0897 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0441 - acc: 0.5254 - val_loss: 1.0857 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0418 - acc: 0.5339 - val_loss: 1.0815 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.0392 - acc: 0.5254 - val_loss: 1.0765 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0373 - acc: 0.5254 - val_loss: 1.0735 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0354 - acc: 0.5169 - val_loss: 1.0694 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0324 - acc: 0.5254 - val_loss: 1.0658 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0298 - acc: 0.5339 - val_loss: 1.0616 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.0280 - acc: 0.5508 - val_loss: 1.0576 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0254 - acc: 0.5508 - val_loss: 1.0547 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.0241 - acc: 0.5508 - val_loss: 1.0522 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.0231 - acc: 0.5508 - val_loss: 1.0485 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.0214 - acc: 0.5508 - val_loss: 1.0458 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0197 - acc: 0.5508 - val_loss: 1.0435 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.0181 - acc: 0.5508 - val_loss: 1.0399 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0168 - acc: 0.5508 - val_loss: 1.0387 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.0163 - acc: 0.5508 - val_loss: 1.0345 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 276us/step - loss: 1.0147 - acc: 0.5508 - val_loss: 1.0325 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0133 - acc: 0.5508 - val_loss: 1.0300 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0116 - acc: 0.5508 - val_loss: 1.0279 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 284us/step - loss: 1.0103 - acc: 0.5593 - val_loss: 1.0260 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0097 - acc: 0.5678 - val_loss: 1.0241 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0088 - acc: 0.5678 - val_loss: 1.0225 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0084 - acc: 0.5593 - val_loss: 1.0205 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0070 - acc: 0.5593 - val_loss: 1.0192 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0056 - acc: 0.5678 - val_loss: 1.0187 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0051 - acc: 0.5593 - val_loss: 1.0167 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0037 - acc: 0.5678 - val_loss: 1.0140 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0038 - acc: 0.5678 - val_loss: 1.0128 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0024 - acc: 0.5678 - val_loss: 1.0110 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0017 - acc: 0.5678 - val_loss: 1.0101 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0012 - acc: 0.5593 - val_loss: 1.0085 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0006 - acc: 0.5678 - val_loss: 1.0073 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0001 - acc: 0.5593 - val_loss: 1.0064 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9997 - acc: 0.5593 - val_loss: 1.0045 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9989 - acc: 0.5508 - val_loss: 1.0027 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9985 - acc: 0.5424 - val_loss: 1.0013 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9974 - acc: 0.5593 - val_loss: 1.0010 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9963 - acc: 0.5508 - val_loss: 0.9998 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9961 - acc: 0.5508 - val_loss: 0.9989 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9966 - acc: 0.5593 - val_loss: 0.9979 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.9960 - acc: 0.5593 - val_loss: 0.9971 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9943 - acc: 0.5678 - val_loss: 0.9960 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9939 - acc: 0.5678 - val_loss: 0.9960 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9935 - acc: 0.5678 - val_loss: 0.9943 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9935 - acc: 0.5508 - val_loss: 0.9931 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9934 - acc: 0.5678 - val_loss: 0.9921 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9916 - acc: 0.5508 - val_loss: 0.9909 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9917 - acc: 0.5593 - val_loss: 0.9896 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9909 - acc: 0.5763 - val_loss: 0.9879 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9908 - acc: 0.5763 - val_loss: 0.9869 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9899 - acc: 0.5678 - val_loss: 0.9859 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9892 - acc: 0.5678 - val_loss: 0.9858 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9889 - acc: 0.5593 - val_loss: 0.9852 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9886 - acc: 0.5593 - val_loss: 0.9832 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9882 - acc: 0.5593 - val_loss: 0.9822 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9877 - acc: 0.5593 - val_loss: 0.9808 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9867 - acc: 0.5593 - val_loss: 0.9802 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9868 - acc: 0.5593 - val_loss: 0.9790 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9861 - acc: 0.5593 - val_loss: 0.9781 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9859 - acc: 0.5593 - val_loss: 0.9779 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9853 - acc: 0.5593 - val_loss: 0.9765 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9848 - acc: 0.5593 - val_loss: 0.9762 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9846 - acc: 0.5678 - val_loss: 0.9743 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.9844 - acc: 0.5593 - val_loss: 0.9738 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9836 - acc: 0.5593 - val_loss: 0.9726 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9832 - acc: 0.5593 - val_loss: 0.9723 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 0.9830 - acc: 0.5593 - val_loss: 0.9717 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9825 - acc: 0.5593 - val_loss: 0.9714 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9825 - acc: 0.5593 - val_loss: 0.9693 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.1184 - acc: 0.3983 - val_loss: 2.0020 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 2.0132 - acc: 0.3983 - val_loss: 1.9076 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.9195 - acc: 0.3898 - val_loss: 1.8187 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.8399 - acc: 0.3898 - val_loss: 1.7353 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.7611 - acc: 0.3983 - val_loss: 1.6606 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.6932 - acc: 0.4068 - val_loss: 1.5913 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.6337 - acc: 0.3983 - val_loss: 1.5341 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.5799 - acc: 0.4153 - val_loss: 1.4820 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.5362 - acc: 0.4153 - val_loss: 1.4372 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.4961 - acc: 0.4237 - val_loss: 1.3997 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.4593 - acc: 0.4492 - val_loss: 1.3717 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.4272 - acc: 0.4661 - val_loss: 1.3487 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3963 - acc: 0.4661 - val_loss: 1.3239 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.3671 - acc: 0.4576 - val_loss: 1.3034 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.3370 - acc: 0.4831 - val_loss: 1.2842 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.3066 - acc: 0.5085 - val_loss: 1.2653 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.2814 - acc: 0.5169 - val_loss: 1.2481 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.2621 - acc: 0.5254 - val_loss: 1.2316 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.2422 - acc: 0.5424 - val_loss: 1.2196 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2285 - acc: 0.5339 - val_loss: 1.2087 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2149 - acc: 0.5339 - val_loss: 1.1997 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.2016 - acc: 0.5339 - val_loss: 1.1905 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.1890 - acc: 0.5339 - val_loss: 1.1825 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.1768 - acc: 0.5508 - val_loss: 1.1743 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 273us/step - loss: 1.1643 - acc: 0.5339 - val_loss: 1.1675 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.1528 - acc: 0.5339 - val_loss: 1.1614 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.1418 - acc: 0.5424 - val_loss: 1.1563 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1325 - acc: 0.5424 - val_loss: 1.1505 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 261us/step - loss: 1.1226 - acc: 0.5339 - val_loss: 1.1456 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1131 - acc: 0.5169 - val_loss: 1.1401 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.1037 - acc: 0.5424 - val_loss: 1.1323 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0948 - acc: 0.5339 - val_loss: 1.1278 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0855 - acc: 0.5424 - val_loss: 1.1243 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.0768 - acc: 0.5424 - val_loss: 1.1215 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0696 - acc: 0.5508 - val_loss: 1.1185 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.0629 - acc: 0.5508 - val_loss: 1.1170 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0561 - acc: 0.5678 - val_loss: 1.1160 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0499 - acc: 0.5678 - val_loss: 1.1155 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0445 - acc: 0.5593 - val_loss: 1.1139 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0394 - acc: 0.5678 - val_loss: 1.1141 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0345 - acc: 0.5763 - val_loss: 1.1139 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.0296 - acc: 0.5847 - val_loss: 1.1123 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0252 - acc: 0.5763 - val_loss: 1.1126 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0210 - acc: 0.5678 - val_loss: 1.1129 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0173 - acc: 0.5932 - val_loss: 1.1123 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0117 - acc: 0.6017 - val_loss: 1.1118 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0094 - acc: 0.5847 - val_loss: 1.1112 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0043 - acc: 0.6017 - val_loss: 1.1116 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0005 - acc: 0.6017 - val_loss: 1.1110 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9966 - acc: 0.6017 - val_loss: 1.1102 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9926 - acc: 0.6017 - val_loss: 1.1107 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9893 - acc: 0.6102 - val_loss: 1.1106 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9850 - acc: 0.6017 - val_loss: 1.1109 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9817 - acc: 0.6017 - val_loss: 1.1109 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9784 - acc: 0.6102 - val_loss: 1.1099 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9766 - acc: 0.6017 - val_loss: 1.1102 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.9723 - acc: 0.6017 - val_loss: 1.1115 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9687 - acc: 0.6102 - val_loss: 1.1110 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9654 - acc: 0.6186 - val_loss: 1.1116 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 0.9625 - acc: 0.6271 - val_loss: 1.1120 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.9591 - acc: 0.6186 - val_loss: 1.1134 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.9567 - acc: 0.6186 - val_loss: 1.1147 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9537 - acc: 0.6102 - val_loss: 1.1163 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9512 - acc: 0.6186 - val_loss: 1.1151 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9489 - acc: 0.6271 - val_loss: 1.1154 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9470 - acc: 0.6186 - val_loss: 1.1154 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9443 - acc: 0.6186 - val_loss: 1.1184 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9423 - acc: 0.6186 - val_loss: 1.1176 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9402 - acc: 0.6186 - val_loss: 1.1184 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9386 - acc: 0.6186 - val_loss: 1.1174 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9360 - acc: 0.6186 - val_loss: 1.1177 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9346 - acc: 0.6102 - val_loss: 1.1184 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9327 - acc: 0.6102 - val_loss: 1.1183 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9310 - acc: 0.6102 - val_loss: 1.1170 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9286 - acc: 0.6102 - val_loss: 1.1180 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9272 - acc: 0.6102 - val_loss: 1.1186 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9260 - acc: 0.6186 - val_loss: 1.1191 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9246 - acc: 0.6186 - val_loss: 1.1186 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9232 - acc: 0.6102 - val_loss: 1.1197 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9218 - acc: 0.6102 - val_loss: 1.1191 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 267us/step - loss: 0.9197 - acc: 0.6186 - val_loss: 1.1206 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9185 - acc: 0.6102 - val_loss: 1.1200 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9174 - acc: 0.6186 - val_loss: 1.1204 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9161 - acc: 0.6271 - val_loss: 1.1205 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9143 - acc: 0.6186 - val_loss: 1.1198 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9138 - acc: 0.6186 - val_loss: 1.1199 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9122 - acc: 0.6186 - val_loss: 1.1195 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9109 - acc: 0.6186 - val_loss: 1.1185 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9087 - acc: 0.6186 - val_loss: 1.1187 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9076 - acc: 0.6186 - val_loss: 1.1195 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9063 - acc: 0.6271 - val_loss: 1.1194 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9050 - acc: 0.6271 - val_loss: 1.1194 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9035 - acc: 0.6271 - val_loss: 1.1186 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9026 - acc: 0.6271 - val_loss: 1.1182 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9011 - acc: 0.6186 - val_loss: 1.1180 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8997 - acc: 0.6102 - val_loss: 1.1164 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8989 - acc: 0.6102 - val_loss: 1.1163 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8975 - acc: 0.6186 - val_loss: 1.1149 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8970 - acc: 0.6102 - val_loss: 1.1144 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8953 - acc: 0.6102 - val_loss: 1.1140 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.8325 - acc: 0.3644 - val_loss: 6.6478 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 2.6876 - acc: 0.3644 - val_loss: 6.2986 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 2.5579 - acc: 0.3814 - val_loss: 5.9289 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 2.4402 - acc: 0.3729 - val_loss: 5.5642 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.3179 - acc: 0.3729 - val_loss: 5.0854 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 2.1753 - acc: 0.3729 - val_loss: 4.6592 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 2.0368 - acc: 0.3729 - val_loss: 4.2086 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.8975 - acc: 0.3559 - val_loss: 3.8012 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.7674 - acc: 0.3644 - val_loss: 3.3992 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.6533 - acc: 0.3729 - val_loss: 3.0413 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.5540 - acc: 0.3814 - val_loss: 2.7222 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.4660 - acc: 0.4068 - val_loss: 2.4671 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.3897 - acc: 0.4237 - val_loss: 2.2213 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.3235 - acc: 0.4492 - val_loss: 2.0209 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2669 - acc: 0.4492 - val_loss: 1.8574 - val_acc: 0.2308\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.2251 - acc: 0.4915 - val_loss: 1.7092 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1908 - acc: 0.4915 - val_loss: 1.6209 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.1611 - acc: 0.5085 - val_loss: 1.5310 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1386 - acc: 0.5169 - val_loss: 1.4629 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.1121 - acc: 0.5085 - val_loss: 1.4048 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0922 - acc: 0.5254 - val_loss: 1.3628 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.0769 - acc: 0.5254 - val_loss: 1.3295 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0642 - acc: 0.5339 - val_loss: 1.2916 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0535 - acc: 0.5424 - val_loss: 1.2713 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0437 - acc: 0.5508 - val_loss: 1.2550 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0363 - acc: 0.5593 - val_loss: 1.2472 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0308 - acc: 0.5508 - val_loss: 1.2391 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0263 - acc: 0.5424 - val_loss: 1.2365 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0210 - acc: 0.5508 - val_loss: 1.2313 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0159 - acc: 0.5678 - val_loss: 1.2195 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0132 - acc: 0.5593 - val_loss: 1.2166 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0095 - acc: 0.5508 - val_loss: 1.2109 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0057 - acc: 0.5763 - val_loss: 1.2135 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0028 - acc: 0.5678 - val_loss: 1.2157 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9999 - acc: 0.5763 - val_loss: 1.2193 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9971 - acc: 0.5678 - val_loss: 1.2189 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9963 - acc: 0.5763 - val_loss: 1.2188 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9940 - acc: 0.5593 - val_loss: 1.2233 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9919 - acc: 0.5763 - val_loss: 1.2203 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9890 - acc: 0.5763 - val_loss: 1.2142 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9876 - acc: 0.5763 - val_loss: 1.2124 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9864 - acc: 0.5678 - val_loss: 1.2150 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9842 - acc: 0.5678 - val_loss: 1.2182 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9822 - acc: 0.5763 - val_loss: 1.2130 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9820 - acc: 0.5678 - val_loss: 1.2106 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9782 - acc: 0.5678 - val_loss: 1.2058 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9791 - acc: 0.5678 - val_loss: 1.2097 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9755 - acc: 0.5763 - val_loss: 1.2120 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9731 - acc: 0.5678 - val_loss: 1.2084 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9723 - acc: 0.5678 - val_loss: 1.2064 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9695 - acc: 0.5763 - val_loss: 1.2090 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9680 - acc: 0.5763 - val_loss: 1.2079 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.9658 - acc: 0.5678 - val_loss: 1.2020 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9636 - acc: 0.5678 - val_loss: 1.1995 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9631 - acc: 0.5593 - val_loss: 1.1990 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9612 - acc: 0.5678 - val_loss: 1.1992 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9582 - acc: 0.5678 - val_loss: 1.1969 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9580 - acc: 0.5508 - val_loss: 1.1976 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9567 - acc: 0.5508 - val_loss: 1.1983 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9553 - acc: 0.5678 - val_loss: 1.1972 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9546 - acc: 0.5508 - val_loss: 1.1983 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9531 - acc: 0.5678 - val_loss: 1.1989 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9535 - acc: 0.5678 - val_loss: 1.1988 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9520 - acc: 0.5678 - val_loss: 1.1955 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9491 - acc: 0.5763 - val_loss: 1.1949 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9490 - acc: 0.5593 - val_loss: 1.1959 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9468 - acc: 0.5763 - val_loss: 1.1987 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9463 - acc: 0.5763 - val_loss: 1.2003 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9453 - acc: 0.5678 - val_loss: 1.2007 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.9441 - acc: 0.5678 - val_loss: 1.1996 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9428 - acc: 0.5678 - val_loss: 1.1979 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9420 - acc: 0.5678 - val_loss: 1.1962 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9418 - acc: 0.5678 - val_loss: 1.1972 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9412 - acc: 0.5678 - val_loss: 1.1980 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9403 - acc: 0.5678 - val_loss: 1.1986 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9396 - acc: 0.5678 - val_loss: 1.1984 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9386 - acc: 0.5593 - val_loss: 1.1959 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9380 - acc: 0.5678 - val_loss: 1.1970 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9373 - acc: 0.5763 - val_loss: 1.1987 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9366 - acc: 0.5763 - val_loss: 1.1986 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9360 - acc: 0.5678 - val_loss: 1.2007 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9356 - acc: 0.5593 - val_loss: 1.1966 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 259us/step - loss: 0.9353 - acc: 0.5763 - val_loss: 1.1974 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9337 - acc: 0.5763 - val_loss: 1.1984 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9348 - acc: 0.5678 - val_loss: 1.1963 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 277us/step - loss: 0.9327 - acc: 0.5763 - val_loss: 1.1974 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 310us/step - loss: 0.9321 - acc: 0.5593 - val_loss: 1.1956 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9316 - acc: 0.5678 - val_loss: 1.1972 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9312 - acc: 0.5678 - val_loss: 1.1974 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9301 - acc: 0.5593 - val_loss: 1.1964 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9306 - acc: 0.5763 - val_loss: 1.1942 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9299 - acc: 0.5763 - val_loss: 1.1940 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.9287 - acc: 0.5678 - val_loss: 1.1915 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9283 - acc: 0.5678 - val_loss: 1.1920 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9274 - acc: 0.5763 - val_loss: 1.1945 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9278 - acc: 0.5678 - val_loss: 1.1941 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9274 - acc: 0.5678 - val_loss: 1.1902 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.9255 - acc: 0.5678 - val_loss: 1.1920 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9268 - acc: 0.5763 - val_loss: 1.1933 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9259 - acc: 0.5763 - val_loss: 1.1947 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 2.6426 - acc: 0.1949 - val_loss: 2.1554 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 2.3757 - acc: 0.1949 - val_loss: 1.9731 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 2.1449 - acc: 0.1949 - val_loss: 1.8365 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.9664 - acc: 0.2119 - val_loss: 1.7208 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.8160 - acc: 0.2203 - val_loss: 1.6206 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.6956 - acc: 0.2458 - val_loss: 1.5404 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.5933 - acc: 0.2542 - val_loss: 1.4753 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.5072 - acc: 0.2712 - val_loss: 1.4172 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.4337 - acc: 0.2712 - val_loss: 1.3679 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.3745 - acc: 0.2797 - val_loss: 1.3299 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.3242 - acc: 0.3051 - val_loss: 1.2966 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.2839 - acc: 0.3220 - val_loss: 1.2698 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.2533 - acc: 0.3475 - val_loss: 1.2428 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.2252 - acc: 0.3305 - val_loss: 1.2225 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.2003 - acc: 0.3559 - val_loss: 1.2023 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.1815 - acc: 0.3644 - val_loss: 1.1860 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.1637 - acc: 0.3644 - val_loss: 1.1680 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1476 - acc: 0.3729 - val_loss: 1.1547 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.1337 - acc: 0.3729 - val_loss: 1.1390 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.1215 - acc: 0.3729 - val_loss: 1.1268 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1104 - acc: 0.3729 - val_loss: 1.1156 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.1014 - acc: 0.3898 - val_loss: 1.1055 - val_acc: 0.2308\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 1.0923 - acc: 0.3729 - val_loss: 1.0965 - val_acc: 0.2308\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 1.0838 - acc: 0.3729 - val_loss: 1.0903 - val_acc: 0.2308\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0778 - acc: 0.3814 - val_loss: 1.0877 - val_acc: 0.2308\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0704 - acc: 0.3814 - val_loss: 1.0776 - val_acc: 0.2308\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0650 - acc: 0.3983 - val_loss: 1.0731 - val_acc: 0.2308\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0606 - acc: 0.3983 - val_loss: 1.0666 - val_acc: 0.3077\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0539 - acc: 0.4068 - val_loss: 1.0629 - val_acc: 0.3077\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0492 - acc: 0.3983 - val_loss: 1.0613 - val_acc: 0.3077\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0447 - acc: 0.4237 - val_loss: 1.0580 - val_acc: 0.3077\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0409 - acc: 0.4153 - val_loss: 1.0529 - val_acc: 0.3077\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0369 - acc: 0.4237 - val_loss: 1.0526 - val_acc: 0.3077\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0339 - acc: 0.3983 - val_loss: 1.0475 - val_acc: 0.3077\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0306 - acc: 0.4068 - val_loss: 1.0478 - val_acc: 0.3077\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0274 - acc: 0.4153 - val_loss: 1.0459 - val_acc: 0.3077\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0244 - acc: 0.4153 - val_loss: 1.0417 - val_acc: 0.3077\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0219 - acc: 0.4237 - val_loss: 1.0426 - val_acc: 0.3077\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0188 - acc: 0.4153 - val_loss: 1.0417 - val_acc: 0.3077\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0167 - acc: 0.4237 - val_loss: 1.0422 - val_acc: 0.3077\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0143 - acc: 0.4237 - val_loss: 1.0429 - val_acc: 0.3077\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0130 - acc: 0.4237 - val_loss: 1.0429 - val_acc: 0.3077\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0102 - acc: 0.4237 - val_loss: 1.0398 - val_acc: 0.3077\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0094 - acc: 0.4322 - val_loss: 1.0401 - val_acc: 0.3077\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0067 - acc: 0.4407 - val_loss: 1.0405 - val_acc: 0.3077\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0055 - acc: 0.4237 - val_loss: 1.0375 - val_acc: 0.3077\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0035 - acc: 0.4407 - val_loss: 1.0360 - val_acc: 0.3077\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 1.0026 - acc: 0.4576 - val_loss: 1.0382 - val_acc: 0.3077\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0007 - acc: 0.4492 - val_loss: 1.0348 - val_acc: 0.3077\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 283us/step - loss: 0.9992 - acc: 0.4492 - val_loss: 1.0369 - val_acc: 0.3077\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9981 - acc: 0.4576 - val_loss: 1.0358 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9977 - acc: 0.4576 - val_loss: 1.0350 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.9957 - acc: 0.4576 - val_loss: 1.0347 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 261us/step - loss: 0.9942 - acc: 0.4746 - val_loss: 1.0358 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9934 - acc: 0.4915 - val_loss: 1.0339 - val_acc: 0.3077\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9917 - acc: 0.4746 - val_loss: 1.0353 - val_acc: 0.3077\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9906 - acc: 0.4915 - val_loss: 1.0376 - val_acc: 0.3077\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9898 - acc: 0.5000 - val_loss: 1.0370 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9885 - acc: 0.4915 - val_loss: 1.0387 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9881 - acc: 0.5000 - val_loss: 1.0381 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9870 - acc: 0.4915 - val_loss: 1.0386 - val_acc: 0.3077\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9859 - acc: 0.4915 - val_loss: 1.0373 - val_acc: 0.3077\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9847 - acc: 0.4831 - val_loss: 1.0408 - val_acc: 0.2308\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9840 - acc: 0.4831 - val_loss: 1.0385 - val_acc: 0.3077\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9833 - acc: 0.4915 - val_loss: 1.0391 - val_acc: 0.2308\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9825 - acc: 0.4915 - val_loss: 1.0366 - val_acc: 0.3077\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9812 - acc: 0.4831 - val_loss: 1.0384 - val_acc: 0.2308\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9806 - acc: 0.4831 - val_loss: 1.0380 - val_acc: 0.2308\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9801 - acc: 0.4915 - val_loss: 1.0404 - val_acc: 0.2308\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9797 - acc: 0.5000 - val_loss: 1.0386 - val_acc: 0.2308\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9777 - acc: 0.5000 - val_loss: 1.0401 - val_acc: 0.2308\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9775 - acc: 0.4915 - val_loss: 1.0399 - val_acc: 0.2308\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9767 - acc: 0.4831 - val_loss: 1.0397 - val_acc: 0.2308\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9762 - acc: 0.4915 - val_loss: 1.0442 - val_acc: 0.2308\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9755 - acc: 0.4915 - val_loss: 1.0403 - val_acc: 0.2308\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9746 - acc: 0.4915 - val_loss: 1.0433 - val_acc: 0.2308\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9739 - acc: 0.4831 - val_loss: 1.0409 - val_acc: 0.2308\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9733 - acc: 0.5000 - val_loss: 1.0404 - val_acc: 0.2308\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9727 - acc: 0.4915 - val_loss: 1.0372 - val_acc: 0.2308\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9724 - acc: 0.5000 - val_loss: 1.0388 - val_acc: 0.2308\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9714 - acc: 0.4831 - val_loss: 1.0381 - val_acc: 0.2308\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9707 - acc: 0.4746 - val_loss: 1.0375 - val_acc: 0.2308\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9703 - acc: 0.4915 - val_loss: 1.0392 - val_acc: 0.2308\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9699 - acc: 0.5000 - val_loss: 1.0421 - val_acc: 0.2308\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9688 - acc: 0.4915 - val_loss: 1.0408 - val_acc: 0.2308\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9685 - acc: 0.5000 - val_loss: 1.0422 - val_acc: 0.2308\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9682 - acc: 0.5000 - val_loss: 1.0444 - val_acc: 0.2308\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9680 - acc: 0.4915 - val_loss: 1.0427 - val_acc: 0.2308\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9669 - acc: 0.4915 - val_loss: 1.0441 - val_acc: 0.2308\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9667 - acc: 0.4915 - val_loss: 1.0432 - val_acc: 0.2308\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9664 - acc: 0.4915 - val_loss: 1.0451 - val_acc: 0.2308\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9655 - acc: 0.5000 - val_loss: 1.0430 - val_acc: 0.2308\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9649 - acc: 0.4915 - val_loss: 1.0436 - val_acc: 0.2308\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9643 - acc: 0.5000 - val_loss: 1.0457 - val_acc: 0.2308\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9636 - acc: 0.4915 - val_loss: 1.0437 - val_acc: 0.2308\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9634 - acc: 0.4915 - val_loss: 1.0453 - val_acc: 0.2308\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9626 - acc: 0.4915 - val_loss: 1.0461 - val_acc: 0.2308\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9621 - acc: 0.4915 - val_loss: 1.0485 - val_acc: 0.2308\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9619 - acc: 0.4915 - val_loss: 1.0494 - val_acc: 0.2308\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9615 - acc: 0.4915 - val_loss: 1.0493 - val_acc: 0.2308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "defea07f-b20a-4b39-81b3-dd2dd64b8637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "01115524-0ae1-4585-e533-63962a61e429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "95732096-ef3f-4f13-e256-be7eb1c506b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "aa4fa66d-ab45-4422-991e-78980a0f19e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f952ebfb940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsgRhjDEmLEsQxhhjwvp/vZVcI4i5FUUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "1fa8f9ab-f845-46ee-a8b4-06c86033bcce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f952ebd80b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "e600ff55-bfd6-4080-8929-4dac29d08976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 3.1072 - acc: 0.3817\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 2.8188 - acc: 0.4122\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 2.5810 - acc: 0.4046\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 2.3886 - acc: 0.4198\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 2.2397 - acc: 0.4427\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 2.1045 - acc: 0.4427\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.9878 - acc: 0.4427\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.8806 - acc: 0.4580\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 1.7812 - acc: 0.4656\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.6993 - acc: 0.4733\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.6247 - acc: 0.4733\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 1.5484 - acc: 0.5038\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.4866 - acc: 0.5038\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.4299 - acc: 0.4962\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.3820 - acc: 0.5038\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 1.3347 - acc: 0.5115\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 225us/step - loss: 1.2926 - acc: 0.5038\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.2559 - acc: 0.5115\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.2232 - acc: 0.5191\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 1.1917 - acc: 0.5344\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 1.1672 - acc: 0.5267\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.1441 - acc: 0.5267\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.1225 - acc: 0.5420\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 1.1042 - acc: 0.5420\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.0879 - acc: 0.5573\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0725 - acc: 0.5344\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 1.0613 - acc: 0.5420\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0474 - acc: 0.5267\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0366 - acc: 0.5267\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.0273 - acc: 0.5344\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.0188 - acc: 0.5420\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.0129 - acc: 0.5344\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0037 - acc: 0.5344\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9961 - acc: 0.5267\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9902 - acc: 0.5191\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9843 - acc: 0.5267\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.9793 - acc: 0.5420\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9748 - acc: 0.5420\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9714 - acc: 0.5573\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9676 - acc: 0.5496\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9638 - acc: 0.5649\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9592 - acc: 0.5725\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9563 - acc: 0.5725\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9528 - acc: 0.5725\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9502 - acc: 0.5725\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9468 - acc: 0.5725\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9440 - acc: 0.5878\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9413 - acc: 0.5725\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9395 - acc: 0.5802\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9369 - acc: 0.5725\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9335 - acc: 0.5649\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9316 - acc: 0.5649\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9302 - acc: 0.5802\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9282 - acc: 0.5725\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.9264 - acc: 0.5878\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9253 - acc: 0.5802\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9240 - acc: 0.5878\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9223 - acc: 0.5878\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9211 - acc: 0.5878\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9196 - acc: 0.5802\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9179 - acc: 0.5802\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9183 - acc: 0.5802\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9159 - acc: 0.5802\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9150 - acc: 0.5802\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9142 - acc: 0.5725\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9132 - acc: 0.5725\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9133 - acc: 0.5725\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.9118 - acc: 0.5649\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.9112 - acc: 0.5573\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9104 - acc: 0.5573\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9106 - acc: 0.5573\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9097 - acc: 0.5649\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9095 - acc: 0.5725\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 221us/step - loss: 0.9089 - acc: 0.5649\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9080 - acc: 0.5649\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9088 - acc: 0.5725\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.9073 - acc: 0.5725\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9068 - acc: 0.5649\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9056 - acc: 0.5649\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9053 - acc: 0.5802\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9048 - acc: 0.5802\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9045 - acc: 0.5802\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9043 - acc: 0.5802\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9039 - acc: 0.5725\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.9033 - acc: 0.5878\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.9024 - acc: 0.5802\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9014 - acc: 0.5878\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9013 - acc: 0.5802\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9008 - acc: 0.5802\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9002 - acc: 0.5725\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8995 - acc: 0.5878\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8998 - acc: 0.5954\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.8987 - acc: 0.5802\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8979 - acc: 0.5802\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8977 - acc: 0.5878\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.8972 - acc: 0.5878\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8974 - acc: 0.5878\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.8970 - acc: 0.5802\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.8965 - acc: 0.5878\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.8957 - acc: 0.5954\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "97cba7c0-de1c-4673-c9f8-725c437c65f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "83e41cd3-7861-499a-c50e-1f68f813e497",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23529411764705882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "94cfd544-6316-4d6a-c0ed-418eadeeac22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "266dd7f1-d12f-4ec1-e219-901d435c2165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "09297477-83e6-4a5e-f0b7-1dc7bc7ee43a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "d461ebae-2eee-4ea9-edbc-d9e05d912ff7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "9d2695c7-d9fc-400f-f71d-ce002e85a44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "522bec05-10bc-4c25-97ed-7ef1baca989c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "da98bf6c-7ad4-459f-d00f-39548a83a563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "c4a749ed-9f2f-43b6-85e2-4be42029eeee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "117/117 [==============================] - 1s 4ms/step - loss: 2.7214 - acc: 0.2906 - val_loss: 1.4189 - val_acc: 0.3571\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 286us/step - loss: 2.5103 - acc: 0.2906 - val_loss: 1.3321 - val_acc: 0.3571\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 2.3206 - acc: 0.2821 - val_loss: 1.2623 - val_acc: 0.3571\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 2.1581 - acc: 0.2821 - val_loss: 1.2071 - val_acc: 0.3571\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 2.0238 - acc: 0.2821 - val_loss: 1.1658 - val_acc: 0.3571\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 1.9066 - acc: 0.2735 - val_loss: 1.1355 - val_acc: 0.4286\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 1.8068 - acc: 0.2735 - val_loss: 1.1093 - val_acc: 0.4286\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 1.7207 - acc: 0.2906 - val_loss: 1.0894 - val_acc: 0.4286\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 1.6424 - acc: 0.2821 - val_loss: 1.0715 - val_acc: 0.4286\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 1.5779 - acc: 0.2821 - val_loss: 1.0570 - val_acc: 0.4286\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 1.5188 - acc: 0.2906 - val_loss: 1.0466 - val_acc: 0.4286\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 1.4695 - acc: 0.2821 - val_loss: 1.0368 - val_acc: 0.4286\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 1.4251 - acc: 0.2821 - val_loss: 1.0297 - val_acc: 0.3571\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 1.3860 - acc: 0.2991 - val_loss: 1.0232 - val_acc: 0.5000\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.3524 - acc: 0.3333 - val_loss: 1.0184 - val_acc: 0.4286\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 1.3231 - acc: 0.3419 - val_loss: 1.0152 - val_acc: 0.4286\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 1.2967 - acc: 0.3504 - val_loss: 1.0123 - val_acc: 0.4286\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 279us/step - loss: 1.2750 - acc: 0.3590 - val_loss: 1.0100 - val_acc: 0.4286\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 1.2553 - acc: 0.3590 - val_loss: 1.0082 - val_acc: 0.4286\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.2368 - acc: 0.3846 - val_loss: 1.0060 - val_acc: 0.4286\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 1.2204 - acc: 0.4103 - val_loss: 1.0054 - val_acc: 0.3571\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 213us/step - loss: 1.2048 - acc: 0.4274 - val_loss: 1.0039 - val_acc: 0.4286\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 1.1915 - acc: 0.4359 - val_loss: 1.0034 - val_acc: 0.3571\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 1.1797 - acc: 0.4615 - val_loss: 1.0035 - val_acc: 0.3571\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 1.1663 - acc: 0.4701 - val_loss: 1.0026 - val_acc: 0.3571\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.1553 - acc: 0.4957 - val_loss: 1.0035 - val_acc: 0.4286\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 1.1460 - acc: 0.4786 - val_loss: 1.0037 - val_acc: 0.4286\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 1.1363 - acc: 0.4786 - val_loss: 1.0042 - val_acc: 0.5000\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 1.1271 - acc: 0.5043 - val_loss: 1.0042 - val_acc: 0.5000\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 1.1203 - acc: 0.5043 - val_loss: 1.0049 - val_acc: 0.5000\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 1.1124 - acc: 0.5299 - val_loss: 1.0058 - val_acc: 0.4286\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.1056 - acc: 0.5299 - val_loss: 1.0061 - val_acc: 0.4286\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 1.0991 - acc: 0.5214 - val_loss: 1.0061 - val_acc: 0.4286\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 1.0934 - acc: 0.5214 - val_loss: 1.0074 - val_acc: 0.4286\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.0867 - acc: 0.5214 - val_loss: 1.0084 - val_acc: 0.4286\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 249us/step - loss: 1.0814 - acc: 0.5214 - val_loss: 1.0085 - val_acc: 0.4286\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 225us/step - loss: 1.0762 - acc: 0.5214 - val_loss: 1.0090 - val_acc: 0.3571\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 1.0723 - acc: 0.5299 - val_loss: 1.0098 - val_acc: 0.3571\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 1.0673 - acc: 0.5299 - val_loss: 1.0108 - val_acc: 0.3571\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.0631 - acc: 0.5299 - val_loss: 1.0122 - val_acc: 0.3571\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.0587 - acc: 0.5385 - val_loss: 1.0126 - val_acc: 0.3571\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 1.0548 - acc: 0.5470 - val_loss: 1.0137 - val_acc: 0.3571\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 154us/step - loss: 1.0506 - acc: 0.5470 - val_loss: 1.0149 - val_acc: 0.3571\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 149us/step - loss: 1.0475 - acc: 0.5470 - val_loss: 1.0158 - val_acc: 0.3571\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 1.0428 - acc: 0.5470 - val_loss: 1.0169 - val_acc: 0.3571\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 218us/step - loss: 1.0398 - acc: 0.5470 - val_loss: 1.0182 - val_acc: 0.3571\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 1.0370 - acc: 0.5470 - val_loss: 1.0189 - val_acc: 0.3571\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 1.0340 - acc: 0.5470 - val_loss: 1.0199 - val_acc: 0.3571\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 1.0317 - acc: 0.5470 - val_loss: 1.0204 - val_acc: 0.3571\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.0292 - acc: 0.5470 - val_loss: 1.0215 - val_acc: 0.3571\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 1.0262 - acc: 0.5470 - val_loss: 1.0213 - val_acc: 0.3571\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 1.0234 - acc: 0.5470 - val_loss: 1.0221 - val_acc: 0.3571\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.0208 - acc: 0.5470 - val_loss: 1.0226 - val_acc: 0.3571\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 1.0181 - acc: 0.5470 - val_loss: 1.0225 - val_acc: 0.3571\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 1.0170 - acc: 0.5470 - val_loss: 1.0233 - val_acc: 0.3571\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 1.0142 - acc: 0.5385 - val_loss: 1.0243 - val_acc: 0.3571\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 1.0116 - acc: 0.5470 - val_loss: 1.0246 - val_acc: 0.3571\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.0101 - acc: 0.5556 - val_loss: 1.0252 - val_acc: 0.3571\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.0081 - acc: 0.5470 - val_loss: 1.0256 - val_acc: 0.3571\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 1.0059 - acc: 0.5556 - val_loss: 1.0269 - val_acc: 0.3571\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 154us/step - loss: 1.0047 - acc: 0.5726 - val_loss: 1.0273 - val_acc: 0.3571\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 1.0022 - acc: 0.5641 - val_loss: 1.0281 - val_acc: 0.3571\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 1.0010 - acc: 0.5556 - val_loss: 1.0288 - val_acc: 0.3571\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.9986 - acc: 0.5556 - val_loss: 1.0290 - val_acc: 0.3571\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9976 - acc: 0.5470 - val_loss: 1.0295 - val_acc: 0.3571\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9959 - acc: 0.5726 - val_loss: 1.0299 - val_acc: 0.3571\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9943 - acc: 0.5470 - val_loss: 1.0305 - val_acc: 0.3571\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9929 - acc: 0.5641 - val_loss: 1.0307 - val_acc: 0.3571\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9913 - acc: 0.5556 - val_loss: 1.0312 - val_acc: 0.3571\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9900 - acc: 0.5556 - val_loss: 1.0313 - val_acc: 0.3571\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.9883 - acc: 0.5556 - val_loss: 1.0320 - val_acc: 0.3571\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.9870 - acc: 0.5470 - val_loss: 1.0325 - val_acc: 0.3571\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.9856 - acc: 0.5556 - val_loss: 1.0328 - val_acc: 0.3571\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.9839 - acc: 0.5556 - val_loss: 1.0332 - val_acc: 0.3571\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9830 - acc: 0.5556 - val_loss: 1.0335 - val_acc: 0.3571\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9814 - acc: 0.5556 - val_loss: 1.0336 - val_acc: 0.3571\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9803 - acc: 0.5556 - val_loss: 1.0333 - val_acc: 0.2857\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 151us/step - loss: 0.9793 - acc: 0.5556 - val_loss: 1.0336 - val_acc: 0.2857\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9782 - acc: 0.5556 - val_loss: 1.0333 - val_acc: 0.2857\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9772 - acc: 0.5641 - val_loss: 1.0327 - val_acc: 0.2857\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9768 - acc: 0.5641 - val_loss: 1.0333 - val_acc: 0.2857\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.9744 - acc: 0.5641 - val_loss: 1.0337 - val_acc: 0.2857\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.9740 - acc: 0.5556 - val_loss: 1.0342 - val_acc: 0.2857\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9730 - acc: 0.5641 - val_loss: 1.0347 - val_acc: 0.2857\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9723 - acc: 0.5556 - val_loss: 1.0346 - val_acc: 0.2857\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9711 - acc: 0.5641 - val_loss: 1.0353 - val_acc: 0.2857\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9701 - acc: 0.5641 - val_loss: 1.0362 - val_acc: 0.2857\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.9689 - acc: 0.5641 - val_loss: 1.0356 - val_acc: 0.2857\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9682 - acc: 0.5641 - val_loss: 1.0360 - val_acc: 0.2857\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.9674 - acc: 0.5726 - val_loss: 1.0359 - val_acc: 0.2857\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9666 - acc: 0.5641 - val_loss: 1.0361 - val_acc: 0.2857\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9656 - acc: 0.5556 - val_loss: 1.0360 - val_acc: 0.2857\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.9653 - acc: 0.5556 - val_loss: 1.0361 - val_acc: 0.2857\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9638 - acc: 0.5641 - val_loss: 1.0358 - val_acc: 0.2857\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9629 - acc: 0.5641 - val_loss: 1.0354 - val_acc: 0.2857\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9630 - acc: 0.5641 - val_loss: 1.0357 - val_acc: 0.2857\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.9617 - acc: 0.5726 - val_loss: 1.0353 - val_acc: 0.2857\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.9610 - acc: 0.5726 - val_loss: 1.0354 - val_acc: 0.2857\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9600 - acc: 0.5726 - val_loss: 1.0352 - val_acc: 0.2857\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9594 - acc: 0.5726 - val_loss: 1.0352 - val_acc: 0.2857\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 1ms/step - loss: 2.1578 - acc: 0.3983 - val_loss: 2.8124 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 2.0301 - acc: 0.4237 - val_loss: 2.6593 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.9248 - acc: 0.4153 - val_loss: 2.5310 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.8332 - acc: 0.4153 - val_loss: 2.4320 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.7549 - acc: 0.4407 - val_loss: 2.3442 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.6890 - acc: 0.4492 - val_loss: 2.2728 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.6306 - acc: 0.4576 - val_loss: 2.1991 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.5777 - acc: 0.4492 - val_loss: 2.1304 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.5304 - acc: 0.4407 - val_loss: 2.0772 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.4874 - acc: 0.4407 - val_loss: 2.0266 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.4488 - acc: 0.4492 - val_loss: 1.9767 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.4126 - acc: 0.4576 - val_loss: 1.9318 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.3828 - acc: 0.4576 - val_loss: 1.8835 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.3548 - acc: 0.4492 - val_loss: 1.8449 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.3307 - acc: 0.4576 - val_loss: 1.7997 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.3051 - acc: 0.4576 - val_loss: 1.7607 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2858 - acc: 0.4661 - val_loss: 1.7259 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2639 - acc: 0.4661 - val_loss: 1.6924 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.2466 - acc: 0.4576 - val_loss: 1.6605 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.2290 - acc: 0.4661 - val_loss: 1.6257 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2113 - acc: 0.4746 - val_loss: 1.6021 - val_acc: 0.2308\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1981 - acc: 0.4746 - val_loss: 1.5725 - val_acc: 0.2308\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.1825 - acc: 0.4831 - val_loss: 1.5450 - val_acc: 0.2308\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.1690 - acc: 0.4915 - val_loss: 1.5192 - val_acc: 0.2308\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.1576 - acc: 0.4915 - val_loss: 1.4977 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1457 - acc: 0.4915 - val_loss: 1.4734 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.1349 - acc: 0.5000 - val_loss: 1.4487 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.1256 - acc: 0.5000 - val_loss: 1.4338 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.1168 - acc: 0.5000 - val_loss: 1.4137 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1082 - acc: 0.5085 - val_loss: 1.3990 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0999 - acc: 0.5085 - val_loss: 1.3842 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0919 - acc: 0.5085 - val_loss: 1.3678 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0857 - acc: 0.5000 - val_loss: 1.3567 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.0789 - acc: 0.5085 - val_loss: 1.3320 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0730 - acc: 0.5085 - val_loss: 1.3208 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.0667 - acc: 0.5085 - val_loss: 1.3086 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0614 - acc: 0.5085 - val_loss: 1.2985 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0563 - acc: 0.5085 - val_loss: 1.2893 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0517 - acc: 0.5169 - val_loss: 1.2805 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.0471 - acc: 0.5169 - val_loss: 1.2729 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0437 - acc: 0.5085 - val_loss: 1.2631 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0402 - acc: 0.5085 - val_loss: 1.2570 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0361 - acc: 0.5085 - val_loss: 1.2478 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.0337 - acc: 0.5254 - val_loss: 1.2449 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0293 - acc: 0.5085 - val_loss: 1.2381 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0266 - acc: 0.5085 - val_loss: 1.2303 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.0225 - acc: 0.5169 - val_loss: 1.2307 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 1.0202 - acc: 0.5254 - val_loss: 1.2259 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.0169 - acc: 0.5339 - val_loss: 1.2214 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0145 - acc: 0.5254 - val_loss: 1.2189 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0118 - acc: 0.5254 - val_loss: 1.2081 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0089 - acc: 0.5339 - val_loss: 1.2042 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0058 - acc: 0.5424 - val_loss: 1.2012 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0046 - acc: 0.5424 - val_loss: 1.1987 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0018 - acc: 0.5424 - val_loss: 1.1973 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9999 - acc: 0.5339 - val_loss: 1.1964 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9974 - acc: 0.5339 - val_loss: 1.1956 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9944 - acc: 0.5339 - val_loss: 1.1918 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9921 - acc: 0.5339 - val_loss: 1.1912 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9904 - acc: 0.5254 - val_loss: 1.1886 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9887 - acc: 0.5339 - val_loss: 1.1876 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9865 - acc: 0.5339 - val_loss: 1.1819 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9837 - acc: 0.5339 - val_loss: 1.1829 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9823 - acc: 0.5254 - val_loss: 1.1806 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9798 - acc: 0.5254 - val_loss: 1.1759 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9782 - acc: 0.5254 - val_loss: 1.1733 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.9768 - acc: 0.5254 - val_loss: 1.1746 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9749 - acc: 0.5339 - val_loss: 1.1729 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9729 - acc: 0.5339 - val_loss: 1.1706 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9716 - acc: 0.5339 - val_loss: 1.1714 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9691 - acc: 0.5339 - val_loss: 1.1668 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.9678 - acc: 0.5508 - val_loss: 1.1662 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9661 - acc: 0.5508 - val_loss: 1.1652 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9642 - acc: 0.5508 - val_loss: 1.1667 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9632 - acc: 0.5593 - val_loss: 1.1615 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9616 - acc: 0.5593 - val_loss: 1.1623 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9605 - acc: 0.5593 - val_loss: 1.1633 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9591 - acc: 0.5678 - val_loss: 1.1622 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9580 - acc: 0.5678 - val_loss: 1.1613 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9566 - acc: 0.5678 - val_loss: 1.1622 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9552 - acc: 0.5593 - val_loss: 1.1584 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9548 - acc: 0.5678 - val_loss: 1.1584 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.9532 - acc: 0.5593 - val_loss: 1.1555 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9518 - acc: 0.5593 - val_loss: 1.1551 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9512 - acc: 0.5593 - val_loss: 1.1534 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9501 - acc: 0.5593 - val_loss: 1.1507 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9486 - acc: 0.5593 - val_loss: 1.1518 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9480 - acc: 0.5678 - val_loss: 1.1526 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9467 - acc: 0.5678 - val_loss: 1.1517 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9464 - acc: 0.5678 - val_loss: 1.1490 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9458 - acc: 0.5678 - val_loss: 1.1512 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9442 - acc: 0.5763 - val_loss: 1.1484 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9437 - acc: 0.5763 - val_loss: 1.1475 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9429 - acc: 0.5763 - val_loss: 1.1467 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.9417 - acc: 0.5763 - val_loss: 1.1458 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9407 - acc: 0.5763 - val_loss: 1.1450 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.9397 - acc: 0.5763 - val_loss: 1.1474 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9391 - acc: 0.5763 - val_loss: 1.1487 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9381 - acc: 0.5763 - val_loss: 1.1475 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9387 - acc: 0.5847 - val_loss: 1.1456 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.2479 - acc: 0.4237 - val_loss: 2.1924 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 2.0547 - acc: 0.4237 - val_loss: 2.0773 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.8891 - acc: 0.4237 - val_loss: 1.9717 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.7515 - acc: 0.3814 - val_loss: 1.8874 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.6361 - acc: 0.3814 - val_loss: 1.8149 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.5419 - acc: 0.3898 - val_loss: 1.7522 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.4637 - acc: 0.3814 - val_loss: 1.7011 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.3966 - acc: 0.3729 - val_loss: 1.6588 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.3438 - acc: 0.4153 - val_loss: 1.6242 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.3019 - acc: 0.4153 - val_loss: 1.5931 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.2611 - acc: 0.4237 - val_loss: 1.5681 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.2321 - acc: 0.4237 - val_loss: 1.5465 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.2048 - acc: 0.4322 - val_loss: 1.5303 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1806 - acc: 0.4322 - val_loss: 1.5140 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 147us/step - loss: 1.1627 - acc: 0.4322 - val_loss: 1.5009 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.1458 - acc: 0.4322 - val_loss: 1.4889 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.1314 - acc: 0.4407 - val_loss: 1.4782 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1178 - acc: 0.4407 - val_loss: 1.4700 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1078 - acc: 0.4322 - val_loss: 1.4605 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0980 - acc: 0.4322 - val_loss: 1.4535 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0888 - acc: 0.4322 - val_loss: 1.4470 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0800 - acc: 0.4322 - val_loss: 1.4399 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0723 - acc: 0.4407 - val_loss: 1.4354 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0654 - acc: 0.4407 - val_loss: 1.4294 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0600 - acc: 0.4492 - val_loss: 1.4239 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.0526 - acc: 0.4492 - val_loss: 1.4186 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.0469 - acc: 0.4492 - val_loss: 1.4124 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0414 - acc: 0.4492 - val_loss: 1.4074 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0365 - acc: 0.4576 - val_loss: 1.4024 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0319 - acc: 0.4661 - val_loss: 1.3966 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0266 - acc: 0.4492 - val_loss: 1.3919 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0237 - acc: 0.4576 - val_loss: 1.3873 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0191 - acc: 0.4576 - val_loss: 1.3815 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0143 - acc: 0.4576 - val_loss: 1.3767 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 270us/step - loss: 1.0106 - acc: 0.4746 - val_loss: 1.3706 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0062 - acc: 0.4661 - val_loss: 1.3656 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0026 - acc: 0.4746 - val_loss: 1.3619 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9996 - acc: 0.4746 - val_loss: 1.3575 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9963 - acc: 0.4746 - val_loss: 1.3537 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9931 - acc: 0.4746 - val_loss: 1.3502 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.9904 - acc: 0.4831 - val_loss: 1.3442 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9881 - acc: 0.4831 - val_loss: 1.3402 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9861 - acc: 0.4831 - val_loss: 1.3378 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9831 - acc: 0.4831 - val_loss: 1.3329 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9809 - acc: 0.4831 - val_loss: 1.3297 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9791 - acc: 0.4831 - val_loss: 1.3266 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9767 - acc: 0.4831 - val_loss: 1.3234 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9757 - acc: 0.4831 - val_loss: 1.3206 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9727 - acc: 0.4915 - val_loss: 1.3182 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9713 - acc: 0.4915 - val_loss: 1.3159 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9684 - acc: 0.5000 - val_loss: 1.3130 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9669 - acc: 0.5000 - val_loss: 1.3100 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9650 - acc: 0.4915 - val_loss: 1.3083 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9631 - acc: 0.4915 - val_loss: 1.3070 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9619 - acc: 0.4915 - val_loss: 1.3041 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9609 - acc: 0.4915 - val_loss: 1.3014 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9586 - acc: 0.5000 - val_loss: 1.3002 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9569 - acc: 0.4915 - val_loss: 1.2960 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9559 - acc: 0.5000 - val_loss: 1.2937 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9543 - acc: 0.5000 - val_loss: 1.2918 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9532 - acc: 0.5085 - val_loss: 1.2896 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9509 - acc: 0.5000 - val_loss: 1.2884 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 146us/step - loss: 0.9505 - acc: 0.5000 - val_loss: 1.2853 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9491 - acc: 0.5085 - val_loss: 1.2844 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9474 - acc: 0.5254 - val_loss: 1.2827 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9463 - acc: 0.5254 - val_loss: 1.2815 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9459 - acc: 0.5169 - val_loss: 1.2803 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9451 - acc: 0.5254 - val_loss: 1.2790 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9437 - acc: 0.5085 - val_loss: 1.2779 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9426 - acc: 0.5169 - val_loss: 1.2764 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9424 - acc: 0.5169 - val_loss: 1.2743 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9408 - acc: 0.5339 - val_loss: 1.2729 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9400 - acc: 0.5339 - val_loss: 1.2721 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.9389 - acc: 0.5339 - val_loss: 1.2708 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9383 - acc: 0.5424 - val_loss: 1.2693 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9374 - acc: 0.5169 - val_loss: 1.2676 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9366 - acc: 0.5339 - val_loss: 1.2668 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9355 - acc: 0.5254 - val_loss: 1.2646 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9340 - acc: 0.5339 - val_loss: 1.2631 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9339 - acc: 0.5339 - val_loss: 1.2620 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9334 - acc: 0.5254 - val_loss: 1.2613 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9322 - acc: 0.5424 - val_loss: 1.2594 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9312 - acc: 0.5424 - val_loss: 1.2577 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9310 - acc: 0.5424 - val_loss: 1.2570 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9302 - acc: 0.5339 - val_loss: 1.2550 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9289 - acc: 0.5339 - val_loss: 1.2543 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9285 - acc: 0.5424 - val_loss: 1.2533 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9280 - acc: 0.5424 - val_loss: 1.2513 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9274 - acc: 0.5339 - val_loss: 1.2502 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9265 - acc: 0.5424 - val_loss: 1.2492 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9251 - acc: 0.5424 - val_loss: 1.2483 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9250 - acc: 0.5424 - val_loss: 1.2474 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9243 - acc: 0.5339 - val_loss: 1.2454 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9236 - acc: 0.5339 - val_loss: 1.2450 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9232 - acc: 0.5339 - val_loss: 1.2433 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9218 - acc: 0.5508 - val_loss: 1.2424 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9211 - acc: 0.5424 - val_loss: 1.2417 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9204 - acc: 0.5508 - val_loss: 1.2408 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9195 - acc: 0.5508 - val_loss: 1.2397 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9194 - acc: 0.5508 - val_loss: 1.2378 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.8770 - acc: 0.3136 - val_loss: 2.4022 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 2.6443 - acc: 0.3220 - val_loss: 2.2160 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 2.4338 - acc: 0.3305 - val_loss: 2.0626 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 2.2495 - acc: 0.3390 - val_loss: 1.9351 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 2.0845 - acc: 0.3475 - val_loss: 1.8312 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.9426 - acc: 0.3559 - val_loss: 1.7445 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.8179 - acc: 0.3475 - val_loss: 1.6724 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.7019 - acc: 0.3390 - val_loss: 1.5937 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.5929 - acc: 0.3220 - val_loss: 1.5323 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.4955 - acc: 0.3220 - val_loss: 1.4774 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.4064 - acc: 0.3305 - val_loss: 1.4278 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3281 - acc: 0.3559 - val_loss: 1.3880 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.2648 - acc: 0.3729 - val_loss: 1.3552 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.2097 - acc: 0.3983 - val_loss: 1.3254 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1645 - acc: 0.3983 - val_loss: 1.3000 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1285 - acc: 0.4237 - val_loss: 1.2817 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0958 - acc: 0.4322 - val_loss: 1.2644 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0703 - acc: 0.4237 - val_loss: 1.2511 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0522 - acc: 0.4322 - val_loss: 1.2318 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0316 - acc: 0.4322 - val_loss: 1.2185 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0148 - acc: 0.4492 - val_loss: 1.2023 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0021 - acc: 0.4576 - val_loss: 1.1900 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9932 - acc: 0.4831 - val_loss: 1.1813 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9842 - acc: 0.4831 - val_loss: 1.1737 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9777 - acc: 0.5000 - val_loss: 1.1686 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9731 - acc: 0.4831 - val_loss: 1.1617 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9680 - acc: 0.5000 - val_loss: 1.1560 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9641 - acc: 0.5000 - val_loss: 1.1555 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9623 - acc: 0.5085 - val_loss: 1.1516 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9582 - acc: 0.5169 - val_loss: 1.1496 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9558 - acc: 0.5169 - val_loss: 1.1485 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9537 - acc: 0.5169 - val_loss: 1.1474 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9496 - acc: 0.5254 - val_loss: 1.1459 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9476 - acc: 0.5254 - val_loss: 1.1463 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9445 - acc: 0.5339 - val_loss: 1.1449 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9428 - acc: 0.5339 - val_loss: 1.1452 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9398 - acc: 0.5424 - val_loss: 1.1463 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9381 - acc: 0.5339 - val_loss: 1.1442 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9363 - acc: 0.5508 - val_loss: 1.1436 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9356 - acc: 0.5508 - val_loss: 1.1442 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9337 - acc: 0.5678 - val_loss: 1.1434 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9310 - acc: 0.5763 - val_loss: 1.1438 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9296 - acc: 0.5763 - val_loss: 1.1488 - val_acc: 0.3077\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9279 - acc: 0.5847 - val_loss: 1.1462 - val_acc: 0.3077\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9262 - acc: 0.5763 - val_loss: 1.1467 - val_acc: 0.3077\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9255 - acc: 0.5847 - val_loss: 1.1472 - val_acc: 0.3077\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9244 - acc: 0.5763 - val_loss: 1.1501 - val_acc: 0.3077\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9225 - acc: 0.5847 - val_loss: 1.1489 - val_acc: 0.3077\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9213 - acc: 0.5847 - val_loss: 1.1497 - val_acc: 0.3077\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9201 - acc: 0.5847 - val_loss: 1.1459 - val_acc: 0.3077\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9181 - acc: 0.5847 - val_loss: 1.1471 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9178 - acc: 0.5763 - val_loss: 1.1495 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.9164 - acc: 0.5847 - val_loss: 1.1488 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9157 - acc: 0.5763 - val_loss: 1.1503 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9137 - acc: 0.5763 - val_loss: 1.1518 - val_acc: 0.3077\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9125 - acc: 0.5847 - val_loss: 1.1482 - val_acc: 0.3077\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9120 - acc: 0.5847 - val_loss: 1.1481 - val_acc: 0.3077\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9110 - acc: 0.5763 - val_loss: 1.1504 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9097 - acc: 0.5847 - val_loss: 1.1520 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9088 - acc: 0.5932 - val_loss: 1.1496 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9079 - acc: 0.5847 - val_loss: 1.1465 - val_acc: 0.3077\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9066 - acc: 0.5847 - val_loss: 1.1492 - val_acc: 0.3077\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9056 - acc: 0.5847 - val_loss: 1.1510 - val_acc: 0.3077\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9050 - acc: 0.5847 - val_loss: 1.1495 - val_acc: 0.3077\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9060 - acc: 0.5847 - val_loss: 1.1473 - val_acc: 0.3077\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9029 - acc: 0.5847 - val_loss: 1.1529 - val_acc: 0.3077\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9022 - acc: 0.5847 - val_loss: 1.1492 - val_acc: 0.3077\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9007 - acc: 0.5847 - val_loss: 1.1481 - val_acc: 0.3077\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9010 - acc: 0.5847 - val_loss: 1.1487 - val_acc: 0.3077\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9003 - acc: 0.5847 - val_loss: 1.1518 - val_acc: 0.3077\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8991 - acc: 0.5847 - val_loss: 1.1518 - val_acc: 0.3077\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8982 - acc: 0.5932 - val_loss: 1.1507 - val_acc: 0.3077\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8976 - acc: 0.5932 - val_loss: 1.1503 - val_acc: 0.3077\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8969 - acc: 0.5847 - val_loss: 1.1499 - val_acc: 0.3077\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8968 - acc: 0.5932 - val_loss: 1.1516 - val_acc: 0.3077\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8961 - acc: 0.6017 - val_loss: 1.1515 - val_acc: 0.3077\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8952 - acc: 0.5932 - val_loss: 1.1521 - val_acc: 0.3077\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8951 - acc: 0.6017 - val_loss: 1.1546 - val_acc: 0.3077\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8936 - acc: 0.6102 - val_loss: 1.1574 - val_acc: 0.3077\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8925 - acc: 0.6017 - val_loss: 1.1567 - val_acc: 0.3077\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8924 - acc: 0.6102 - val_loss: 1.1548 - val_acc: 0.3077\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8915 - acc: 0.6186 - val_loss: 1.1541 - val_acc: 0.3077\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8910 - acc: 0.6102 - val_loss: 1.1547 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8904 - acc: 0.6017 - val_loss: 1.1536 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.8907 - acc: 0.6102 - val_loss: 1.1567 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8891 - acc: 0.6102 - val_loss: 1.1563 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8888 - acc: 0.6017 - val_loss: 1.1544 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8882 - acc: 0.6186 - val_loss: 1.1526 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.8869 - acc: 0.6102 - val_loss: 1.1564 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8865 - acc: 0.6186 - val_loss: 1.1547 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.8864 - acc: 0.6017 - val_loss: 1.1574 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8856 - acc: 0.6017 - val_loss: 1.1584 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8848 - acc: 0.6186 - val_loss: 1.1584 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8844 - acc: 0.6017 - val_loss: 1.1590 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8843 - acc: 0.6186 - val_loss: 1.1605 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8833 - acc: 0.6186 - val_loss: 1.1572 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8834 - acc: 0.6017 - val_loss: 1.1598 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8831 - acc: 0.6017 - val_loss: 1.1597 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8827 - acc: 0.6102 - val_loss: 1.1579 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8811 - acc: 0.6102 - val_loss: 1.1572 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 2.6732 - acc: 0.3814 - val_loss: 2.7612 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 2.4897 - acc: 0.3729 - val_loss: 2.5456 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 2.3290 - acc: 0.3814 - val_loss: 2.3618 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 2.1843 - acc: 0.3814 - val_loss: 2.1994 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 2.0493 - acc: 0.3898 - val_loss: 2.0422 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.9291 - acc: 0.4237 - val_loss: 1.9035 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.8234 - acc: 0.4322 - val_loss: 1.7794 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.7283 - acc: 0.4407 - val_loss: 1.6645 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.6424 - acc: 0.4661 - val_loss: 1.5674 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.5683 - acc: 0.4746 - val_loss: 1.4770 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.5005 - acc: 0.4746 - val_loss: 1.3948 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.4363 - acc: 0.4746 - val_loss: 1.3238 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.3832 - acc: 0.4915 - val_loss: 1.2557 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.3320 - acc: 0.5000 - val_loss: 1.1987 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.2866 - acc: 0.5085 - val_loss: 1.1467 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.2460 - acc: 0.5169 - val_loss: 1.0963 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.2091 - acc: 0.5339 - val_loss: 1.0476 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.1772 - acc: 0.5339 - val_loss: 1.0113 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 1.1497 - acc: 0.5339 - val_loss: 0.9790 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1255 - acc: 0.5339 - val_loss: 0.9521 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1037 - acc: 0.5339 - val_loss: 0.9281 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0841 - acc: 0.5339 - val_loss: 0.9056 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0685 - acc: 0.5508 - val_loss: 0.8869 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0530 - acc: 0.5508 - val_loss: 0.8694 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0398 - acc: 0.5508 - val_loss: 0.8552 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0289 - acc: 0.5508 - val_loss: 0.8425 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0197 - acc: 0.5424 - val_loss: 0.8334 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.0109 - acc: 0.5424 - val_loss: 0.8265 - val_acc: 0.6154\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0041 - acc: 0.5254 - val_loss: 0.8211 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9969 - acc: 0.5339 - val_loss: 0.8142 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9908 - acc: 0.5339 - val_loss: 0.8113 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9853 - acc: 0.5339 - val_loss: 0.8084 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9809 - acc: 0.5339 - val_loss: 0.8054 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9766 - acc: 0.5339 - val_loss: 0.8055 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9726 - acc: 0.5339 - val_loss: 0.8044 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9687 - acc: 0.5339 - val_loss: 0.8041 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9655 - acc: 0.5339 - val_loss: 0.8034 - val_acc: 0.7692\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9622 - acc: 0.5424 - val_loss: 0.8036 - val_acc: 0.7692\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9596 - acc: 0.5424 - val_loss: 0.8031 - val_acc: 0.7692\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9575 - acc: 0.5424 - val_loss: 0.8039 - val_acc: 0.7692\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9560 - acc: 0.5424 - val_loss: 0.8058 - val_acc: 0.7692\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 149us/step - loss: 0.9538 - acc: 0.5424 - val_loss: 0.8058 - val_acc: 0.7692\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9517 - acc: 0.5508 - val_loss: 0.8075 - val_acc: 0.7692\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9503 - acc: 0.5508 - val_loss: 0.8089 - val_acc: 0.7692\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9481 - acc: 0.5508 - val_loss: 0.8088 - val_acc: 0.7692\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9476 - acc: 0.5508 - val_loss: 0.8097 - val_acc: 0.7692\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9448 - acc: 0.5508 - val_loss: 0.8108 - val_acc: 0.7692\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9443 - acc: 0.5508 - val_loss: 0.8115 - val_acc: 0.7692\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9428 - acc: 0.5508 - val_loss: 0.8126 - val_acc: 0.7692\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9417 - acc: 0.5593 - val_loss: 0.8134 - val_acc: 0.7692\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9409 - acc: 0.5593 - val_loss: 0.8150 - val_acc: 0.7692\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9410 - acc: 0.5593 - val_loss: 0.8150 - val_acc: 0.7692\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9388 - acc: 0.5593 - val_loss: 0.8186 - val_acc: 0.7692\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9381 - acc: 0.5593 - val_loss: 0.8194 - val_acc: 0.7692\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9377 - acc: 0.5593 - val_loss: 0.8201 - val_acc: 0.7692\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9367 - acc: 0.5593 - val_loss: 0.8216 - val_acc: 0.7692\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.9360 - acc: 0.5593 - val_loss: 0.8227 - val_acc: 0.7692\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9359 - acc: 0.5593 - val_loss: 0.8233 - val_acc: 0.7692\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9350 - acc: 0.5593 - val_loss: 0.8245 - val_acc: 0.7692\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9351 - acc: 0.5593 - val_loss: 0.8253 - val_acc: 0.7692\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9345 - acc: 0.5593 - val_loss: 0.8249 - val_acc: 0.7692\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9331 - acc: 0.5593 - val_loss: 0.8264 - val_acc: 0.7692\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9326 - acc: 0.5593 - val_loss: 0.8272 - val_acc: 0.7692\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9330 - acc: 0.5593 - val_loss: 0.8275 - val_acc: 0.7692\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9322 - acc: 0.5508 - val_loss: 0.8280 - val_acc: 0.7692\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9324 - acc: 0.5508 - val_loss: 0.8284 - val_acc: 0.7692\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9319 - acc: 0.5593 - val_loss: 0.8308 - val_acc: 0.7692\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9306 - acc: 0.5593 - val_loss: 0.8309 - val_acc: 0.7692\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9311 - acc: 0.5508 - val_loss: 0.8319 - val_acc: 0.7692\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9303 - acc: 0.5508 - val_loss: 0.8324 - val_acc: 0.7692\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9303 - acc: 0.5678 - val_loss: 0.8326 - val_acc: 0.7692\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9296 - acc: 0.5763 - val_loss: 0.8334 - val_acc: 0.7692\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9288 - acc: 0.5593 - val_loss: 0.8344 - val_acc: 0.7692\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.9290 - acc: 0.5678 - val_loss: 0.8352 - val_acc: 0.7692\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9283 - acc: 0.5678 - val_loss: 0.8352 - val_acc: 0.7692\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9281 - acc: 0.5678 - val_loss: 0.8350 - val_acc: 0.7692\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9280 - acc: 0.5593 - val_loss: 0.8352 - val_acc: 0.7692\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9274 - acc: 0.5678 - val_loss: 0.8370 - val_acc: 0.7692\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9272 - acc: 0.5678 - val_loss: 0.8363 - val_acc: 0.7692\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9268 - acc: 0.5593 - val_loss: 0.8375 - val_acc: 0.7692\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.9267 - acc: 0.5678 - val_loss: 0.8379 - val_acc: 0.7692\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9270 - acc: 0.5593 - val_loss: 0.8376 - val_acc: 0.7692\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9258 - acc: 0.5763 - val_loss: 0.8393 - val_acc: 0.7692\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9256 - acc: 0.5678 - val_loss: 0.8390 - val_acc: 0.7692\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9261 - acc: 0.5678 - val_loss: 0.8401 - val_acc: 0.7692\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9256 - acc: 0.5678 - val_loss: 0.8409 - val_acc: 0.7692\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9253 - acc: 0.5593 - val_loss: 0.8404 - val_acc: 0.7692\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9250 - acc: 0.5678 - val_loss: 0.8403 - val_acc: 0.7692\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9244 - acc: 0.5678 - val_loss: 0.8411 - val_acc: 0.7692\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9243 - acc: 0.5678 - val_loss: 0.8416 - val_acc: 0.7692\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9243 - acc: 0.5593 - val_loss: 0.8414 - val_acc: 0.7692\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.9238 - acc: 0.5678 - val_loss: 0.8418 - val_acc: 0.7692\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9237 - acc: 0.5593 - val_loss: 0.8420 - val_acc: 0.7692\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9232 - acc: 0.5678 - val_loss: 0.8418 - val_acc: 0.7692\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9224 - acc: 0.5678 - val_loss: 0.8434 - val_acc: 0.7692\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9229 - acc: 0.5593 - val_loss: 0.8436 - val_acc: 0.7692\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9224 - acc: 0.5678 - val_loss: 0.8446 - val_acc: 0.7692\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.9221 - acc: 0.5593 - val_loss: 0.8447 - val_acc: 0.7692\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9217 - acc: 0.5678 - val_loss: 0.8451 - val_acc: 0.7692\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9216 - acc: 0.5678 - val_loss: 0.8455 - val_acc: 0.7692\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.4321 - acc: 0.4237 - val_loss: 1.4851 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 2.2771 - acc: 0.4068 - val_loss: 1.4553 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 2.1282 - acc: 0.4068 - val_loss: 1.4232 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.9958 - acc: 0.4153 - val_loss: 1.3976 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.8810 - acc: 0.4322 - val_loss: 1.3729 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.7769 - acc: 0.4492 - val_loss: 1.3514 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.6826 - acc: 0.4576 - val_loss: 1.3195 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.6002 - acc: 0.4492 - val_loss: 1.2877 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.5252 - acc: 0.4492 - val_loss: 1.2537 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.4599 - acc: 0.4492 - val_loss: 1.2277 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.4039 - acc: 0.4407 - val_loss: 1.2062 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.3571 - acc: 0.4237 - val_loss: 1.1842 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.3155 - acc: 0.4153 - val_loss: 1.1641 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2763 - acc: 0.4153 - val_loss: 1.1470 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2464 - acc: 0.4237 - val_loss: 1.1256 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2189 - acc: 0.4407 - val_loss: 1.1022 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1952 - acc: 0.4746 - val_loss: 1.0811 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1754 - acc: 0.4746 - val_loss: 1.0662 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1579 - acc: 0.4746 - val_loss: 1.0491 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.1429 - acc: 0.4831 - val_loss: 1.0371 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1297 - acc: 0.4831 - val_loss: 1.0226 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1184 - acc: 0.4746 - val_loss: 1.0115 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1076 - acc: 0.4831 - val_loss: 1.0008 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0986 - acc: 0.4831 - val_loss: 0.9931 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0908 - acc: 0.4831 - val_loss: 0.9854 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0811 - acc: 0.4915 - val_loss: 0.9781 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0745 - acc: 0.5000 - val_loss: 0.9689 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0675 - acc: 0.5169 - val_loss: 0.9634 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0595 - acc: 0.5085 - val_loss: 0.9586 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0548 - acc: 0.5000 - val_loss: 0.9542 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0476 - acc: 0.5169 - val_loss: 0.9480 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0416 - acc: 0.5085 - val_loss: 0.9455 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.0367 - acc: 0.5169 - val_loss: 0.9406 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0324 - acc: 0.5169 - val_loss: 0.9383 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 1.0287 - acc: 0.5000 - val_loss: 0.9340 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0239 - acc: 0.5000 - val_loss: 0.9330 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0210 - acc: 0.4915 - val_loss: 0.9303 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0168 - acc: 0.5000 - val_loss: 0.9295 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0143 - acc: 0.5000 - val_loss: 0.9286 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0118 - acc: 0.5000 - val_loss: 0.9270 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.0082 - acc: 0.4915 - val_loss: 0.9294 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0059 - acc: 0.5000 - val_loss: 0.9297 - val_acc: 0.6154\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0034 - acc: 0.5085 - val_loss: 0.9297 - val_acc: 0.6154\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.0008 - acc: 0.5000 - val_loss: 0.9270 - val_acc: 0.6154\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9980 - acc: 0.5254 - val_loss: 0.9300 - val_acc: 0.6154\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9952 - acc: 0.5254 - val_loss: 0.9302 - val_acc: 0.6154\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9931 - acc: 0.5169 - val_loss: 0.9320 - val_acc: 0.6154\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9908 - acc: 0.5339 - val_loss: 0.9298 - val_acc: 0.6154\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9891 - acc: 0.5254 - val_loss: 0.9289 - val_acc: 0.6154\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9883 - acc: 0.5339 - val_loss: 0.9277 - val_acc: 0.6154\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9862 - acc: 0.5254 - val_loss: 0.9274 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9854 - acc: 0.5254 - val_loss: 0.9254 - val_acc: 0.6154\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9846 - acc: 0.5339 - val_loss: 0.9238 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9833 - acc: 0.5339 - val_loss: 0.9234 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9828 - acc: 0.5339 - val_loss: 0.9213 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9822 - acc: 0.5339 - val_loss: 0.9224 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9811 - acc: 0.5508 - val_loss: 0.9206 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9801 - acc: 0.5339 - val_loss: 0.9200 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9797 - acc: 0.5424 - val_loss: 0.9197 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9779 - acc: 0.5508 - val_loss: 0.9195 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9780 - acc: 0.5339 - val_loss: 0.9195 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9776 - acc: 0.5424 - val_loss: 0.9204 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9760 - acc: 0.5508 - val_loss: 0.9185 - val_acc: 0.6154\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9756 - acc: 0.5424 - val_loss: 0.9183 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9745 - acc: 0.5508 - val_loss: 0.9177 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9736 - acc: 0.5508 - val_loss: 0.9171 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9738 - acc: 0.5593 - val_loss: 0.9165 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9726 - acc: 0.5508 - val_loss: 0.9174 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9723 - acc: 0.5508 - val_loss: 0.9165 - val_acc: 0.6154\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9712 - acc: 0.5508 - val_loss: 0.9165 - val_acc: 0.6154\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9714 - acc: 0.5593 - val_loss: 0.9169 - val_acc: 0.6154\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9693 - acc: 0.5508 - val_loss: 0.9169 - val_acc: 0.6154\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9701 - acc: 0.5593 - val_loss: 0.9156 - val_acc: 0.6154\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9684 - acc: 0.5508 - val_loss: 0.9149 - val_acc: 0.6154\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9690 - acc: 0.5678 - val_loss: 0.9140 - val_acc: 0.6154\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9683 - acc: 0.5593 - val_loss: 0.9143 - val_acc: 0.6154\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9671 - acc: 0.5678 - val_loss: 0.9137 - val_acc: 0.6154\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9664 - acc: 0.5424 - val_loss: 0.9127 - val_acc: 0.6154\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9655 - acc: 0.5678 - val_loss: 0.9126 - val_acc: 0.6154\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9658 - acc: 0.5593 - val_loss: 0.9127 - val_acc: 0.6154\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9658 - acc: 0.5508 - val_loss: 0.9108 - val_acc: 0.6154\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9643 - acc: 0.5424 - val_loss: 0.9116 - val_acc: 0.6154\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9642 - acc: 0.5508 - val_loss: 0.9127 - val_acc: 0.6154\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9638 - acc: 0.5678 - val_loss: 0.9120 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9636 - acc: 0.5339 - val_loss: 0.9107 - val_acc: 0.6154\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9619 - acc: 0.5424 - val_loss: 0.9118 - val_acc: 0.6154\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9628 - acc: 0.5593 - val_loss: 0.9102 - val_acc: 0.6154\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9612 - acc: 0.5508 - val_loss: 0.9083 - val_acc: 0.6154\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9602 - acc: 0.5678 - val_loss: 0.9084 - val_acc: 0.6154\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9610 - acc: 0.5424 - val_loss: 0.9076 - val_acc: 0.6154\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9603 - acc: 0.5424 - val_loss: 0.9073 - val_acc: 0.6154\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9595 - acc: 0.5424 - val_loss: 0.9069 - val_acc: 0.6154\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9590 - acc: 0.5508 - val_loss: 0.9066 - val_acc: 0.6923\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9587 - acc: 0.5593 - val_loss: 0.9078 - val_acc: 0.6923\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9574 - acc: 0.5678 - val_loss: 0.9074 - val_acc: 0.6923\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9572 - acc: 0.5678 - val_loss: 0.9076 - val_acc: 0.6923\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9572 - acc: 0.5678 - val_loss: 0.9073 - val_acc: 0.6923\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9563 - acc: 0.5678 - val_loss: 0.9083 - val_acc: 0.6923\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9562 - acc: 0.5678 - val_loss: 0.9073 - val_acc: 0.6923\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9564 - acc: 0.5678 - val_loss: 0.9076 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.3930 - acc: 0.2627 - val_loss: 1.3419 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 2.2128 - acc: 0.2712 - val_loss: 1.2586 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 2.0571 - acc: 0.2797 - val_loss: 1.1874 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.9304 - acc: 0.2712 - val_loss: 1.1370 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.8210 - acc: 0.2881 - val_loss: 1.0998 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.7197 - acc: 0.3051 - val_loss: 1.0728 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.6394 - acc: 0.3051 - val_loss: 1.0534 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.5683 - acc: 0.2966 - val_loss: 1.0398 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.5090 - acc: 0.3051 - val_loss: 1.0289 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.4569 - acc: 0.3051 - val_loss: 1.0240 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.4102 - acc: 0.3051 - val_loss: 1.0188 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.3709 - acc: 0.3051 - val_loss: 1.0161 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.3348 - acc: 0.3051 - val_loss: 1.0151 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.3041 - acc: 0.3136 - val_loss: 1.0139 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2748 - acc: 0.3051 - val_loss: 1.0128 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.2485 - acc: 0.3051 - val_loss: 1.0127 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.2279 - acc: 0.3051 - val_loss: 1.0143 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.2051 - acc: 0.2966 - val_loss: 1.0143 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1857 - acc: 0.3136 - val_loss: 1.0146 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.1693 - acc: 0.3051 - val_loss: 1.0161 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.1527 - acc: 0.3305 - val_loss: 1.0157 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1379 - acc: 0.3475 - val_loss: 1.0155 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1237 - acc: 0.3390 - val_loss: 1.0159 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1118 - acc: 0.3559 - val_loss: 1.0149 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.1002 - acc: 0.3729 - val_loss: 1.0148 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0899 - acc: 0.3729 - val_loss: 1.0148 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0806 - acc: 0.3898 - val_loss: 1.0144 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0713 - acc: 0.3898 - val_loss: 1.0143 - val_acc: 0.6154\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0622 - acc: 0.3983 - val_loss: 1.0146 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0550 - acc: 0.4068 - val_loss: 1.0147 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0480 - acc: 0.4237 - val_loss: 1.0141 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0411 - acc: 0.4322 - val_loss: 1.0139 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0342 - acc: 0.4492 - val_loss: 1.0139 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0291 - acc: 0.4576 - val_loss: 1.0136 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0235 - acc: 0.4746 - val_loss: 1.0140 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0182 - acc: 0.4915 - val_loss: 1.0139 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0133 - acc: 0.5000 - val_loss: 1.0143 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0092 - acc: 0.5000 - val_loss: 1.0141 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0049 - acc: 0.5085 - val_loss: 1.0139 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0008 - acc: 0.5085 - val_loss: 1.0143 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9974 - acc: 0.5169 - val_loss: 1.0143 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9940 - acc: 0.5339 - val_loss: 1.0147 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.9903 - acc: 0.5339 - val_loss: 1.0144 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9870 - acc: 0.5424 - val_loss: 1.0137 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9839 - acc: 0.5508 - val_loss: 1.0142 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9807 - acc: 0.5508 - val_loss: 1.0142 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9786 - acc: 0.5424 - val_loss: 1.0144 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9759 - acc: 0.5508 - val_loss: 1.0147 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9744 - acc: 0.5593 - val_loss: 1.0144 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9711 - acc: 0.5508 - val_loss: 1.0132 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9698 - acc: 0.5593 - val_loss: 1.0132 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9674 - acc: 0.5593 - val_loss: 1.0140 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9651 - acc: 0.5593 - val_loss: 1.0136 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9638 - acc: 0.5508 - val_loss: 1.0137 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9616 - acc: 0.5508 - val_loss: 1.0138 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9603 - acc: 0.5508 - val_loss: 1.0141 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9580 - acc: 0.5593 - val_loss: 1.0133 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9570 - acc: 0.5593 - val_loss: 1.0139 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9554 - acc: 0.5508 - val_loss: 1.0149 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9542 - acc: 0.5678 - val_loss: 1.0144 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9520 - acc: 0.5593 - val_loss: 1.0148 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9515 - acc: 0.5508 - val_loss: 1.0165 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9495 - acc: 0.5508 - val_loss: 1.0155 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9484 - acc: 0.5508 - val_loss: 1.0149 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 0.9469 - acc: 0.5508 - val_loss: 1.0156 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9471 - acc: 0.5508 - val_loss: 1.0153 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9452 - acc: 0.5593 - val_loss: 1.0158 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9438 - acc: 0.5593 - val_loss: 1.0160 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9432 - acc: 0.5593 - val_loss: 1.0162 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9414 - acc: 0.5508 - val_loss: 1.0167 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9409 - acc: 0.5678 - val_loss: 1.0170 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9397 - acc: 0.5678 - val_loss: 1.0178 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.9384 - acc: 0.5678 - val_loss: 1.0172 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.9382 - acc: 0.5678 - val_loss: 1.0188 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9367 - acc: 0.5678 - val_loss: 1.0192 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9365 - acc: 0.5678 - val_loss: 1.0190 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9351 - acc: 0.5593 - val_loss: 1.0202 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9354 - acc: 0.5593 - val_loss: 1.0200 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9338 - acc: 0.5593 - val_loss: 1.0207 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9339 - acc: 0.5508 - val_loss: 1.0203 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9321 - acc: 0.5678 - val_loss: 1.0214 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9316 - acc: 0.5678 - val_loss: 1.0209 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9304 - acc: 0.5508 - val_loss: 1.0222 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9308 - acc: 0.5678 - val_loss: 1.0216 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9293 - acc: 0.5593 - val_loss: 1.0221 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9289 - acc: 0.5593 - val_loss: 1.0216 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9289 - acc: 0.5593 - val_loss: 1.0212 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9274 - acc: 0.5678 - val_loss: 1.0221 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9265 - acc: 0.5678 - val_loss: 1.0216 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9261 - acc: 0.5678 - val_loss: 1.0225 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 149us/step - loss: 0.9255 - acc: 0.5678 - val_loss: 1.0217 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9250 - acc: 0.5678 - val_loss: 1.0225 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9240 - acc: 0.5593 - val_loss: 1.0225 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9237 - acc: 0.5678 - val_loss: 1.0233 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9230 - acc: 0.5678 - val_loss: 1.0237 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9227 - acc: 0.5678 - val_loss: 1.0228 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9224 - acc: 0.5678 - val_loss: 1.0249 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9212 - acc: 0.5678 - val_loss: 1.0236 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9210 - acc: 0.5678 - val_loss: 1.0236 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9204 - acc: 0.5678 - val_loss: 1.0233 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.8639 - acc: 0.5000 - val_loss: 2.1055 - val_acc: 0.5385\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.7398 - acc: 0.5000 - val_loss: 1.8744 - val_acc: 0.5385\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.6358 - acc: 0.5085 - val_loss: 1.6914 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.5441 - acc: 0.5085 - val_loss: 1.5235 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.4655 - acc: 0.5000 - val_loss: 1.3807 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4042 - acc: 0.5254 - val_loss: 1.2738 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.3600 - acc: 0.5169 - val_loss: 1.1858 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.3199 - acc: 0.5169 - val_loss: 1.1174 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.2867 - acc: 0.5169 - val_loss: 1.0596 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.2608 - acc: 0.5085 - val_loss: 1.0287 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 1.2398 - acc: 0.5085 - val_loss: 0.9990 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.2193 - acc: 0.5085 - val_loss: 0.9802 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.2005 - acc: 0.5085 - val_loss: 0.9560 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.1852 - acc: 0.5085 - val_loss: 0.9439 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1723 - acc: 0.5085 - val_loss: 0.9364 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1600 - acc: 0.5085 - val_loss: 0.9292 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1489 - acc: 0.5169 - val_loss: 0.9304 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1383 - acc: 0.5254 - val_loss: 0.9232 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1279 - acc: 0.5254 - val_loss: 0.9201 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1195 - acc: 0.5254 - val_loss: 0.9173 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1113 - acc: 0.5339 - val_loss: 0.9134 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1042 - acc: 0.5254 - val_loss: 0.9180 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0962 - acc: 0.5339 - val_loss: 0.9134 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.0896 - acc: 0.5424 - val_loss: 0.9118 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0832 - acc: 0.5424 - val_loss: 0.9108 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0789 - acc: 0.5424 - val_loss: 0.9090 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0723 - acc: 0.5424 - val_loss: 0.9079 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0675 - acc: 0.5508 - val_loss: 0.9091 - val_acc: 0.6154\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0629 - acc: 0.5593 - val_loss: 0.9069 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0589 - acc: 0.5508 - val_loss: 0.9086 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0548 - acc: 0.5593 - val_loss: 0.9083 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0508 - acc: 0.5508 - val_loss: 0.9080 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0473 - acc: 0.5508 - val_loss: 0.9094 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0434 - acc: 0.5508 - val_loss: 0.9104 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0400 - acc: 0.5593 - val_loss: 0.9109 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0366 - acc: 0.5508 - val_loss: 0.9116 - val_acc: 0.6154\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0336 - acc: 0.5508 - val_loss: 0.9107 - val_acc: 0.6154\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.0314 - acc: 0.5508 - val_loss: 0.9115 - val_acc: 0.6154\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0282 - acc: 0.5508 - val_loss: 0.9130 - val_acc: 0.6154\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 270us/step - loss: 1.0253 - acc: 0.5508 - val_loss: 0.9115 - val_acc: 0.6154\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0218 - acc: 0.5508 - val_loss: 0.9121 - val_acc: 0.6154\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.0193 - acc: 0.5508 - val_loss: 0.9120 - val_acc: 0.6154\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0158 - acc: 0.5508 - val_loss: 0.9104 - val_acc: 0.6154\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0109 - acc: 0.5593 - val_loss: 0.9095 - val_acc: 0.6154\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0067 - acc: 0.5593 - val_loss: 0.9081 - val_acc: 0.6154\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0028 - acc: 0.5593 - val_loss: 0.9064 - val_acc: 0.6154\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9991 - acc: 0.5593 - val_loss: 0.9078 - val_acc: 0.6154\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9944 - acc: 0.5593 - val_loss: 0.9071 - val_acc: 0.6154\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9904 - acc: 0.5424 - val_loss: 0.9065 - val_acc: 0.6154\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9853 - acc: 0.5678 - val_loss: 0.9068 - val_acc: 0.6154\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9818 - acc: 0.5763 - val_loss: 0.9067 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9782 - acc: 0.5763 - val_loss: 0.9063 - val_acc: 0.6154\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9749 - acc: 0.5847 - val_loss: 0.9063 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9726 - acc: 0.5763 - val_loss: 0.9087 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9696 - acc: 0.5932 - val_loss: 0.9114 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9670 - acc: 0.5932 - val_loss: 0.9120 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9650 - acc: 0.5932 - val_loss: 0.9132 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9628 - acc: 0.5847 - val_loss: 0.9131 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9605 - acc: 0.5847 - val_loss: 0.9141 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9584 - acc: 0.5847 - val_loss: 0.9162 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9571 - acc: 0.5932 - val_loss: 0.9186 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9544 - acc: 0.5932 - val_loss: 0.9197 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9523 - acc: 0.5847 - val_loss: 0.9204 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9509 - acc: 0.5763 - val_loss: 0.9221 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9493 - acc: 0.5763 - val_loss: 0.9240 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9478 - acc: 0.5847 - val_loss: 0.9255 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9458 - acc: 0.5763 - val_loss: 0.9274 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9437 - acc: 0.5763 - val_loss: 0.9274 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9425 - acc: 0.5763 - val_loss: 0.9300 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9415 - acc: 0.5763 - val_loss: 0.9305 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9389 - acc: 0.5763 - val_loss: 0.9319 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9374 - acc: 0.5763 - val_loss: 0.9320 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9353 - acc: 0.5763 - val_loss: 0.9335 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9344 - acc: 0.5763 - val_loss: 0.9340 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9321 - acc: 0.5763 - val_loss: 0.9345 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9304 - acc: 0.5763 - val_loss: 0.9371 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9294 - acc: 0.5763 - val_loss: 0.9393 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9281 - acc: 0.5763 - val_loss: 0.9402 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9267 - acc: 0.5763 - val_loss: 0.9406 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9258 - acc: 0.5678 - val_loss: 0.9440 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9238 - acc: 0.5678 - val_loss: 0.9464 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9228 - acc: 0.5678 - val_loss: 0.9468 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9214 - acc: 0.5678 - val_loss: 0.9479 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9205 - acc: 0.5847 - val_loss: 0.9476 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9191 - acc: 0.5763 - val_loss: 0.9506 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9181 - acc: 0.5763 - val_loss: 0.9507 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9166 - acc: 0.5763 - val_loss: 0.9515 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9153 - acc: 0.5847 - val_loss: 0.9531 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9143 - acc: 0.5678 - val_loss: 0.9551 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9137 - acc: 0.5763 - val_loss: 0.9544 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9114 - acc: 0.5763 - val_loss: 0.9558 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9108 - acc: 0.5763 - val_loss: 0.9566 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9104 - acc: 0.5763 - val_loss: 0.9592 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9087 - acc: 0.5763 - val_loss: 0.9606 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9077 - acc: 0.5763 - val_loss: 0.9601 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9064 - acc: 0.5678 - val_loss: 0.9622 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9061 - acc: 0.5763 - val_loss: 0.9629 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9046 - acc: 0.5763 - val_loss: 0.9647 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9037 - acc: 0.5763 - val_loss: 0.9644 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9031 - acc: 0.5932 - val_loss: 0.9662 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 2.8843 - acc: 0.2373 - val_loss: 2.8444 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 2.6946 - acc: 0.2627 - val_loss: 2.6768 - val_acc: 0.0769\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 2.5226 - acc: 0.2627 - val_loss: 2.5085 - val_acc: 0.0769\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 2.3367 - acc: 0.2712 - val_loss: 2.3296 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 2.1412 - acc: 0.2712 - val_loss: 2.1630 - val_acc: 0.0769\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.9641 - acc: 0.2712 - val_loss: 2.0149 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.8083 - acc: 0.2966 - val_loss: 1.8734 - val_acc: 0.0769\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.6654 - acc: 0.2966 - val_loss: 1.7564 - val_acc: 0.0769\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.5377 - acc: 0.3051 - val_loss: 1.6353 - val_acc: 0.0769\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.4281 - acc: 0.3051 - val_loss: 1.5394 - val_acc: 0.0769\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.3374 - acc: 0.3305 - val_loss: 1.4524 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 1.2591 - acc: 0.3559 - val_loss: 1.3872 - val_acc: 0.0000e+00\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.1966 - acc: 0.3983 - val_loss: 1.3257 - val_acc: 0.0769\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1453 - acc: 0.4237 - val_loss: 1.2791 - val_acc: 0.0769\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1061 - acc: 0.4322 - val_loss: 1.2372 - val_acc: 0.0769\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0738 - acc: 0.4407 - val_loss: 1.2065 - val_acc: 0.0769\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0487 - acc: 0.4746 - val_loss: 1.1870 - val_acc: 0.0769\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0308 - acc: 0.4915 - val_loss: 1.1660 - val_acc: 0.0769\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0149 - acc: 0.4915 - val_loss: 1.1553 - val_acc: 0.0769\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0015 - acc: 0.5085 - val_loss: 1.1469 - val_acc: 0.0769\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9917 - acc: 0.5000 - val_loss: 1.1367 - val_acc: 0.0769\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9824 - acc: 0.5000 - val_loss: 1.1309 - val_acc: 0.0769\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9768 - acc: 0.5000 - val_loss: 1.1255 - val_acc: 0.0769\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9711 - acc: 0.5169 - val_loss: 1.1204 - val_acc: 0.1538\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9667 - acc: 0.5169 - val_loss: 1.1156 - val_acc: 0.1538\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9619 - acc: 0.5254 - val_loss: 1.1148 - val_acc: 0.1538\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9577 - acc: 0.5169 - val_loss: 1.1117 - val_acc: 0.1538\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9544 - acc: 0.5424 - val_loss: 1.1115 - val_acc: 0.1538\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9525 - acc: 0.5339 - val_loss: 1.1089 - val_acc: 0.1538\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9493 - acc: 0.5254 - val_loss: 1.1069 - val_acc: 0.1538\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9472 - acc: 0.5339 - val_loss: 1.1062 - val_acc: 0.1538\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9458 - acc: 0.5339 - val_loss: 1.1041 - val_acc: 0.1538\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9441 - acc: 0.5339 - val_loss: 1.1052 - val_acc: 0.1538\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9414 - acc: 0.5254 - val_loss: 1.1014 - val_acc: 0.1538\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9405 - acc: 0.5339 - val_loss: 1.1001 - val_acc: 0.1538\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9386 - acc: 0.5254 - val_loss: 1.1000 - val_acc: 0.1538\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9370 - acc: 0.5254 - val_loss: 1.0963 - val_acc: 0.1538\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9365 - acc: 0.5254 - val_loss: 1.0941 - val_acc: 0.1538\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9346 - acc: 0.5339 - val_loss: 1.0936 - val_acc: 0.1538\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9334 - acc: 0.5169 - val_loss: 1.0906 - val_acc: 0.1538\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9317 - acc: 0.5339 - val_loss: 1.0907 - val_acc: 0.1538\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9309 - acc: 0.5169 - val_loss: 1.0925 - val_acc: 0.1538\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9294 - acc: 0.5169 - val_loss: 1.0924 - val_acc: 0.1538\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9290 - acc: 0.5254 - val_loss: 1.0894 - val_acc: 0.1538\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9282 - acc: 0.5169 - val_loss: 1.0891 - val_acc: 0.1538\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9274 - acc: 0.5169 - val_loss: 1.0871 - val_acc: 0.1538\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9250 - acc: 0.5254 - val_loss: 1.0872 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9248 - acc: 0.5593 - val_loss: 1.0864 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9245 - acc: 0.5508 - val_loss: 1.0887 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9224 - acc: 0.5508 - val_loss: 1.0858 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9213 - acc: 0.5508 - val_loss: 1.0860 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9210 - acc: 0.5678 - val_loss: 1.0849 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9204 - acc: 0.5508 - val_loss: 1.0856 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9202 - acc: 0.5424 - val_loss: 1.0827 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9184 - acc: 0.5339 - val_loss: 1.0821 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9174 - acc: 0.5339 - val_loss: 1.0822 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9170 - acc: 0.5508 - val_loss: 1.0781 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9165 - acc: 0.5424 - val_loss: 1.0780 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9151 - acc: 0.5424 - val_loss: 1.0775 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9146 - acc: 0.5593 - val_loss: 1.0775 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9155 - acc: 0.5508 - val_loss: 1.0745 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9140 - acc: 0.5508 - val_loss: 1.0726 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9140 - acc: 0.5508 - val_loss: 1.0701 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9126 - acc: 0.5424 - val_loss: 1.0727 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9124 - acc: 0.5424 - val_loss: 1.0720 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9129 - acc: 0.5424 - val_loss: 1.0715 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9119 - acc: 0.5508 - val_loss: 1.0703 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9108 - acc: 0.5424 - val_loss: 1.0699 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.9112 - acc: 0.5508 - val_loss: 1.0695 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9102 - acc: 0.5508 - val_loss: 1.0673 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9093 - acc: 0.5424 - val_loss: 1.0688 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9097 - acc: 0.5508 - val_loss: 1.0685 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9095 - acc: 0.5424 - val_loss: 1.0684 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9088 - acc: 0.5508 - val_loss: 1.0662 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9079 - acc: 0.5424 - val_loss: 1.0666 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9080 - acc: 0.5424 - val_loss: 1.0644 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9081 - acc: 0.5424 - val_loss: 1.0628 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9074 - acc: 0.5424 - val_loss: 1.0647 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9065 - acc: 0.5424 - val_loss: 1.0635 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9069 - acc: 0.5424 - val_loss: 1.0622 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9061 - acc: 0.5593 - val_loss: 1.0600 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 0.9065 - acc: 0.5508 - val_loss: 1.0563 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9051 - acc: 0.5508 - val_loss: 1.0596 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9050 - acc: 0.5508 - val_loss: 1.0586 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9052 - acc: 0.5508 - val_loss: 1.0563 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9051 - acc: 0.5508 - val_loss: 1.0569 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9041 - acc: 0.5508 - val_loss: 1.0553 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9037 - acc: 0.5508 - val_loss: 1.0545 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9040 - acc: 0.5508 - val_loss: 1.0543 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9035 - acc: 0.5508 - val_loss: 1.0576 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9028 - acc: 0.5508 - val_loss: 1.0547 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9026 - acc: 0.5508 - val_loss: 1.0552 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9026 - acc: 0.5593 - val_loss: 1.0547 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9020 - acc: 0.5593 - val_loss: 1.0555 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9025 - acc: 0.5593 - val_loss: 1.0527 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9015 - acc: 0.5593 - val_loss: 1.0509 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9013 - acc: 0.5678 - val_loss: 1.0522 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9016 - acc: 0.5508 - val_loss: 1.0515 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9006 - acc: 0.5593 - val_loss: 1.0513 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9005 - acc: 0.5508 - val_loss: 1.0511 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 2.6540 - acc: 0.4746 - val_loss: 1.5320 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 2.5613 - acc: 0.3898 - val_loss: 1.4539 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 2.4731 - acc: 0.3729 - val_loss: 1.3910 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 2.3767 - acc: 0.4576 - val_loss: 1.3193 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 2.2906 - acc: 0.4831 - val_loss: 1.2740 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 2.2088 - acc: 0.4831 - val_loss: 1.2187 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 2.1328 - acc: 0.4407 - val_loss: 1.1689 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 2.0582 - acc: 0.4831 - val_loss: 1.1133 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.9886 - acc: 0.4746 - val_loss: 1.0675 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.9263 - acc: 0.4746 - val_loss: 1.0297 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.8652 - acc: 0.4746 - val_loss: 0.9987 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.8094 - acc: 0.4746 - val_loss: 0.9703 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.7583 - acc: 0.4661 - val_loss: 0.9443 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.7107 - acc: 0.4746 - val_loss: 0.9166 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 1.6692 - acc: 0.5000 - val_loss: 0.9023 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.6296 - acc: 0.4915 - val_loss: 0.8802 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.5940 - acc: 0.4831 - val_loss: 0.8671 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.5602 - acc: 0.4831 - val_loss: 0.8504 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.5308 - acc: 0.4915 - val_loss: 0.8406 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.4993 - acc: 0.5000 - val_loss: 0.8326 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.4734 - acc: 0.5000 - val_loss: 0.8230 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.4489 - acc: 0.4831 - val_loss: 0.8154 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.4268 - acc: 0.4915 - val_loss: 0.8126 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.4030 - acc: 0.4661 - val_loss: 0.8058 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.3802 - acc: 0.4576 - val_loss: 0.8017 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.3590 - acc: 0.4576 - val_loss: 0.7980 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.3399 - acc: 0.4661 - val_loss: 0.7940 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.3219 - acc: 0.4661 - val_loss: 0.7932 - val_acc: 0.6154\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.3049 - acc: 0.4746 - val_loss: 0.7924 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2876 - acc: 0.4661 - val_loss: 0.7903 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.2704 - acc: 0.4746 - val_loss: 0.7892 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.2560 - acc: 0.4746 - val_loss: 0.7864 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.2419 - acc: 0.4831 - val_loss: 0.7870 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 1.2279 - acc: 0.4915 - val_loss: 0.7819 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.2167 - acc: 0.4915 - val_loss: 0.7818 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.2038 - acc: 0.4915 - val_loss: 0.7818 - val_acc: 0.6154\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1913 - acc: 0.4915 - val_loss: 0.7824 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1801 - acc: 0.5085 - val_loss: 0.7813 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.1694 - acc: 0.5169 - val_loss: 0.7802 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.1596 - acc: 0.5169 - val_loss: 0.7805 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1512 - acc: 0.5085 - val_loss: 0.7795 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.1414 - acc: 0.5169 - val_loss: 0.7802 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1329 - acc: 0.5169 - val_loss: 0.7824 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.1252 - acc: 0.5254 - val_loss: 0.7842 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1169 - acc: 0.5169 - val_loss: 0.7854 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1093 - acc: 0.5339 - val_loss: 0.7879 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1017 - acc: 0.5339 - val_loss: 0.7926 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0951 - acc: 0.5254 - val_loss: 0.7952 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0901 - acc: 0.5339 - val_loss: 0.7958 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0829 - acc: 0.5339 - val_loss: 0.7996 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0770 - acc: 0.5254 - val_loss: 0.8026 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0713 - acc: 0.5339 - val_loss: 0.8031 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0657 - acc: 0.5339 - val_loss: 0.8054 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0605 - acc: 0.5339 - val_loss: 0.8095 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0564 - acc: 0.5424 - val_loss: 0.8114 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0508 - acc: 0.5508 - val_loss: 0.8163 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0466 - acc: 0.5424 - val_loss: 0.8182 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0419 - acc: 0.5508 - val_loss: 0.8178 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0380 - acc: 0.5508 - val_loss: 0.8222 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0337 - acc: 0.5678 - val_loss: 0.8261 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0293 - acc: 0.5678 - val_loss: 0.8269 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0253 - acc: 0.5678 - val_loss: 0.8286 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0218 - acc: 0.5678 - val_loss: 0.8352 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0187 - acc: 0.5678 - val_loss: 0.8399 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0158 - acc: 0.5678 - val_loss: 0.8444 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0136 - acc: 0.5508 - val_loss: 0.8427 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0108 - acc: 0.5593 - val_loss: 0.8448 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0080 - acc: 0.5678 - val_loss: 0.8496 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0052 - acc: 0.5678 - val_loss: 0.8510 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0033 - acc: 0.5678 - val_loss: 0.8599 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0012 - acc: 0.5678 - val_loss: 0.8573 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9991 - acc: 0.5678 - val_loss: 0.8635 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9973 - acc: 0.5678 - val_loss: 0.8653 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9954 - acc: 0.5678 - val_loss: 0.8680 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9934 - acc: 0.5678 - val_loss: 0.8715 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9915 - acc: 0.5678 - val_loss: 0.8748 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9897 - acc: 0.5678 - val_loss: 0.8771 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9883 - acc: 0.5678 - val_loss: 0.8794 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9866 - acc: 0.5678 - val_loss: 0.8807 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9846 - acc: 0.5678 - val_loss: 0.8853 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9824 - acc: 0.5678 - val_loss: 0.8879 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.9809 - acc: 0.5678 - val_loss: 0.8883 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9797 - acc: 0.5678 - val_loss: 0.8894 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9779 - acc: 0.5678 - val_loss: 0.8919 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9771 - acc: 0.5593 - val_loss: 0.8946 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9746 - acc: 0.5678 - val_loss: 0.8985 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9742 - acc: 0.5593 - val_loss: 0.8982 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9720 - acc: 0.5593 - val_loss: 0.9040 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9709 - acc: 0.5678 - val_loss: 0.9054 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9701 - acc: 0.5678 - val_loss: 0.9058 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9690 - acc: 0.5678 - val_loss: 0.9071 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9679 - acc: 0.5678 - val_loss: 0.9090 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9663 - acc: 0.5678 - val_loss: 0.9102 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9658 - acc: 0.5678 - val_loss: 0.9135 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9650 - acc: 0.5678 - val_loss: 0.9141 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9637 - acc: 0.5678 - val_loss: 0.9142 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9630 - acc: 0.5678 - val_loss: 0.9178 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9622 - acc: 0.5678 - val_loss: 0.9174 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9612 - acc: 0.5763 - val_loss: 0.9192 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9600 - acc: 0.5763 - val_loss: 0.9222 - val_acc: 0.4615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "4dfb5a1c-ed5b-4f59-8a9d-63846da22813",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "88c3050f-593c-43da-aaba-759a0cc3def9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "db951629-185b-4276-c7c0-4c18e2c257e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "0c1cfda6-a097-44f8-eb1b-9ee0d21fc1c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc75330d240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrJkOezUrNgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "d23655d6-0a6d-4653-bcc4-8b85d30e3a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc752e46c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOefC+n+oddUCGwsY8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "a919bd57-b072-407d-fde3-d5df276ac4d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 2.4286 - acc: 0.3511\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 2.2671 - acc: 0.3740\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 2.1284 - acc: 0.3664\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 2.0034 - acc: 0.3817\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.8863 - acc: 0.3893\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.7888 - acc: 0.3893\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.7028 - acc: 0.3740\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 1.6336 - acc: 0.3817\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.5717 - acc: 0.3740\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.5184 - acc: 0.3588\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.4697 - acc: 0.3740\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.4262 - acc: 0.3359\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 140us/step - loss: 1.3901 - acc: 0.3588\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.3570 - acc: 0.3588\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.3295 - acc: 0.3664\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.3003 - acc: 0.3664\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 146us/step - loss: 1.2761 - acc: 0.3664\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.2528 - acc: 0.3893\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.2329 - acc: 0.3969\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.2160 - acc: 0.3969\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.1997 - acc: 0.4122\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1851 - acc: 0.4198\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 143us/step - loss: 1.1721 - acc: 0.4122\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.1602 - acc: 0.4122\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.1488 - acc: 0.4275\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 1.1385 - acc: 0.4504\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 1.1289 - acc: 0.4351\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.1201 - acc: 0.4656\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 233us/step - loss: 1.1130 - acc: 0.4733\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.1047 - acc: 0.4809\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 1.0977 - acc: 0.4885\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0915 - acc: 0.4962\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.0855 - acc: 0.4885\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0804 - acc: 0.4962\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0750 - acc: 0.4885\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0705 - acc: 0.4885\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0656 - acc: 0.4809\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.0613 - acc: 0.4809\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 143us/step - loss: 1.0564 - acc: 0.4885\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0529 - acc: 0.4885\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.0496 - acc: 0.4885\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0450 - acc: 0.4962\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.0416 - acc: 0.4962\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 1.0379 - acc: 0.4962\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0349 - acc: 0.4962\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.0316 - acc: 0.5038\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 146us/step - loss: 1.0276 - acc: 0.4962\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 1.0246 - acc: 0.4962\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.0211 - acc: 0.4962\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0180 - acc: 0.4962\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0151 - acc: 0.4962\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.0122 - acc: 0.4962\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0099 - acc: 0.5038\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0069 - acc: 0.5038\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 1.0046 - acc: 0.5115\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0017 - acc: 0.5115\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9986 - acc: 0.5115\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9955 - acc: 0.5191\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9927 - acc: 0.5191\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9898 - acc: 0.5191\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9875 - acc: 0.5191\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9850 - acc: 0.5191\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9824 - acc: 0.5191\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9808 - acc: 0.5115\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9788 - acc: 0.5191\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9772 - acc: 0.5191\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9762 - acc: 0.5115\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9740 - acc: 0.5115\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9723 - acc: 0.5115\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9712 - acc: 0.5115\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9699 - acc: 0.5115\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9687 - acc: 0.5038\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9678 - acc: 0.5038\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9671 - acc: 0.5115\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9660 - acc: 0.5038\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9649 - acc: 0.5115\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9641 - acc: 0.5115\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9627 - acc: 0.5115\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9621 - acc: 0.5115\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9612 - acc: 0.5115\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9601 - acc: 0.5191\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9592 - acc: 0.5191\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9583 - acc: 0.5191\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9575 - acc: 0.5191\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.9567 - acc: 0.5191\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9556 - acc: 0.5191\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9544 - acc: 0.5191\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.9531 - acc: 0.5191\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9523 - acc: 0.5191\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9513 - acc: 0.5191\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9505 - acc: 0.5191\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9496 - acc: 0.5191\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9485 - acc: 0.5191\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9479 - acc: 0.5191\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9475 - acc: 0.5267\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9464 - acc: 0.5267\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9456 - acc: 0.5267\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9448 - acc: 0.5267\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9436 - acc: 0.5267\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9430 - acc: 0.5267\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "22b2fcfb-279b-4ff3-c42e-0cfe5934315a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "a455e7c6-b31c-48ab-b27d-ce616f4e231d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29411764705882354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "a84d1d93-7b4a-415d-eadc-122e522e64cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "e02c8b3a-68fe-4c25-8108-1ff0cf83a36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b28de9af-3ef4-4559-beea-dc1888d49714",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "541f3dca-f76f-4fb5-896b-679ab17a4e14",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "607d1a2b-26ce-4151-d628-b9b3f766de43",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b7e5a3a1-6188-4d35-b569-cd74964618be",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 1.3651 - acc: 0.5812 - val_loss: 1.4449 - val_acc: 0.5000\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 1.2821 - acc: 0.5812 - val_loss: 1.3398 - val_acc: 0.5000\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 1.2068 - acc: 0.5983 - val_loss: 1.2507 - val_acc: 0.5000\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.1331 - acc: 0.6239 - val_loss: 1.1544 - val_acc: 0.5714\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 1.0714 - acc: 0.6325 - val_loss: 1.0803 - val_acc: 0.5714\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 1.0162 - acc: 0.6838 - val_loss: 1.0130 - val_acc: 0.6429\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 302us/step - loss: 0.9711 - acc: 0.6923 - val_loss: 0.9606 - val_acc: 0.6429\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9328 - acc: 0.7179 - val_loss: 0.9176 - val_acc: 0.7143\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 0.9007 - acc: 0.7607 - val_loss: 0.8819 - val_acc: 0.9286\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.8735 - acc: 0.8547 - val_loss: 0.8528 - val_acc: 0.9286\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 156us/step - loss: 0.8503 - acc: 0.8803 - val_loss: 0.8292 - val_acc: 0.9286\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.8301 - acc: 0.8974 - val_loss: 0.8086 - val_acc: 0.9286\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.8116 - acc: 0.8974 - val_loss: 0.7899 - val_acc: 0.9286\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.7954 - acc: 0.8974 - val_loss: 0.7739 - val_acc: 0.9286\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 0.7804 - acc: 0.8974 - val_loss: 0.7591 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.7662 - acc: 0.8974 - val_loss: 0.7451 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 155us/step - loss: 0.7529 - acc: 0.9060 - val_loss: 0.7317 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.7401 - acc: 0.9231 - val_loss: 0.7189 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.7276 - acc: 0.9231 - val_loss: 0.7067 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.7155 - acc: 0.9402 - val_loss: 0.6950 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.7035 - acc: 0.9402 - val_loss: 0.6835 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 0.6923 - acc: 0.9402 - val_loss: 0.6725 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 157us/step - loss: 0.6810 - acc: 0.9402 - val_loss: 0.6619 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.6701 - acc: 0.9487 - val_loss: 0.6515 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.6594 - acc: 0.9487 - val_loss: 0.6413 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.6486 - acc: 0.9658 - val_loss: 0.6313 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.6378 - acc: 0.9658 - val_loss: 0.6210 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.6275 - acc: 0.9658 - val_loss: 0.6107 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.6171 - acc: 0.9658 - val_loss: 0.6008 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.6069 - acc: 0.9658 - val_loss: 0.5910 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.5970 - acc: 0.9658 - val_loss: 0.5813 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.5871 - acc: 0.9658 - val_loss: 0.5719 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 232us/step - loss: 0.5773 - acc: 0.9658 - val_loss: 0.5627 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 231us/step - loss: 0.5677 - acc: 0.9658 - val_loss: 0.5527 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.5578 - acc: 0.9658 - val_loss: 0.5419 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.5481 - acc: 0.9658 - val_loss: 0.5313 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.5387 - acc: 0.9658 - val_loss: 0.5210 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.5294 - acc: 0.9658 - val_loss: 0.5108 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.5202 - acc: 0.9658 - val_loss: 0.5008 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.5111 - acc: 0.9658 - val_loss: 0.4909 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.5023 - acc: 0.9658 - val_loss: 0.4814 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.4936 - acc: 0.9658 - val_loss: 0.4719 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.4848 - acc: 0.9658 - val_loss: 0.4626 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.4763 - acc: 0.9658 - val_loss: 0.4534 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.4680 - acc: 0.9658 - val_loss: 0.4444 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.4598 - acc: 0.9744 - val_loss: 0.4358 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.4519 - acc: 0.9829 - val_loss: 0.4273 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.4442 - acc: 0.9829 - val_loss: 0.4190 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.4367 - acc: 0.9829 - val_loss: 0.4109 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.4294 - acc: 0.9829 - val_loss: 0.4030 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.4221 - acc: 0.9829 - val_loss: 0.3952 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.4151 - acc: 0.9829 - val_loss: 0.3874 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.4082 - acc: 0.9829 - val_loss: 0.3795 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.4012 - acc: 0.9829 - val_loss: 0.3718 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.3942 - acc: 0.9829 - val_loss: 0.3641 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.3875 - acc: 0.9829 - val_loss: 0.3568 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.3806 - acc: 0.9829 - val_loss: 0.3494 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.3739 - acc: 0.9829 - val_loss: 0.3421 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.3674 - acc: 0.9829 - val_loss: 0.3351 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.3610 - acc: 0.9829 - val_loss: 0.3283 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.3549 - acc: 0.9829 - val_loss: 0.3216 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.3487 - acc: 0.9829 - val_loss: 0.3150 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.3429 - acc: 0.9829 - val_loss: 0.3086 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.3370 - acc: 0.9829 - val_loss: 0.3024 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.3312 - acc: 0.9829 - val_loss: 0.2962 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.3254 - acc: 0.9829 - val_loss: 0.2900 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.3198 - acc: 0.9829 - val_loss: 0.2841 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.3142 - acc: 0.9829 - val_loss: 0.2781 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.3086 - acc: 0.9829 - val_loss: 0.2723 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.3031 - acc: 0.9829 - val_loss: 0.2667 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.2977 - acc: 0.9915 - val_loss: 0.2611 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.2925 - acc: 0.9915 - val_loss: 0.2557 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.2873 - acc: 0.9915 - val_loss: 0.2504 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.2823 - acc: 0.9915 - val_loss: 0.2452 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.2775 - acc: 0.9915 - val_loss: 0.2403 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.2727 - acc: 0.9915 - val_loss: 0.2354 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 0.2681 - acc: 0.9915 - val_loss: 0.2307 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.2637 - acc: 0.9915 - val_loss: 0.2261 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.2593 - acc: 0.9915 - val_loss: 0.2216 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.2550 - acc: 0.9915 - val_loss: 0.2172 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.2509 - acc: 0.9915 - val_loss: 0.2130 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.2468 - acc: 0.9915 - val_loss: 0.2088 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.2429 - acc: 0.9915 - val_loss: 0.2047 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.2390 - acc: 0.9915 - val_loss: 0.2008 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.2354 - acc: 0.9915 - val_loss: 0.1970 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.2317 - acc: 0.9915 - val_loss: 0.1933 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.2281 - acc: 0.9915 - val_loss: 0.1895 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.2246 - acc: 0.9915 - val_loss: 0.1860 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.2212 - acc: 0.9915 - val_loss: 0.1826 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.2180 - acc: 0.9915 - val_loss: 0.1793 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.2148 - acc: 0.9915 - val_loss: 0.1759 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.2115 - acc: 0.9915 - val_loss: 0.1726 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.2083 - acc: 0.9915 - val_loss: 0.1695 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.2052 - acc: 0.9915 - val_loss: 0.1663 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.2022 - acc: 1.0000 - val_loss: 0.1633 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.1991 - acc: 1.0000 - val_loss: 0.1603 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.1962 - acc: 1.0000 - val_loss: 0.1574 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.1933 - acc: 1.0000 - val_loss: 0.1546 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.1904 - acc: 1.0000 - val_loss: 0.1518 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.1877 - acc: 1.0000 - val_loss: 0.1492 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 2.8314 - acc: 0.0000e+00 - val_loss: 2.8345 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 2.5823 - acc: 0.0000e+00 - val_loss: 2.5986 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 2.3621 - acc: 0.0000e+00 - val_loss: 2.3863 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.1674 - acc: 0.0593 - val_loss: 2.1983 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.9981 - acc: 0.0847 - val_loss: 2.0335 - val_acc: 0.0769\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.8487 - acc: 0.0847 - val_loss: 1.8919 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.7172 - acc: 0.1610 - val_loss: 1.7617 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.6018 - acc: 0.2288 - val_loss: 1.6505 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.4987 - acc: 0.2712 - val_loss: 1.5513 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.4082 - acc: 0.3051 - val_loss: 1.4620 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.3279 - acc: 0.3220 - val_loss: 1.3833 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.2561 - acc: 0.3644 - val_loss: 1.3118 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.1926 - acc: 0.3644 - val_loss: 1.2491 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1363 - acc: 0.3814 - val_loss: 1.1918 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0854 - acc: 0.3983 - val_loss: 1.1403 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0403 - acc: 0.3983 - val_loss: 1.0936 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9988 - acc: 0.4492 - val_loss: 1.0506 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9614 - acc: 0.4915 - val_loss: 1.0119 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9272 - acc: 0.5085 - val_loss: 0.9764 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8960 - acc: 0.5508 - val_loss: 0.9439 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8673 - acc: 0.5508 - val_loss: 0.9142 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.8409 - acc: 0.5508 - val_loss: 0.8866 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.8169 - acc: 0.5508 - val_loss: 0.8614 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.7946 - acc: 0.5508 - val_loss: 0.8373 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.7739 - acc: 0.5678 - val_loss: 0.8160 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7545 - acc: 0.5763 - val_loss: 0.7959 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.7363 - acc: 0.5763 - val_loss: 0.7768 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7196 - acc: 0.5847 - val_loss: 0.7593 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7038 - acc: 0.6017 - val_loss: 0.7427 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.6892 - acc: 0.6102 - val_loss: 0.7271 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6754 - acc: 0.6441 - val_loss: 0.7124 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6625 - acc: 0.6610 - val_loss: 0.6992 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6504 - acc: 0.6780 - val_loss: 0.6871 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6389 - acc: 0.7034 - val_loss: 0.6754 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.6278 - acc: 0.7373 - val_loss: 0.6648 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6175 - acc: 0.7542 - val_loss: 0.6546 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6077 - acc: 0.7881 - val_loss: 0.6449 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.5988 - acc: 0.7881 - val_loss: 0.6359 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.5901 - acc: 0.7966 - val_loss: 0.6273 - val_acc: 0.7692\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.5820 - acc: 0.8305 - val_loss: 0.6193 - val_acc: 0.7692\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5741 - acc: 0.8475 - val_loss: 0.6116 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5667 - acc: 0.8644 - val_loss: 0.6043 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.5596 - acc: 0.8814 - val_loss: 0.5971 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5529 - acc: 0.9153 - val_loss: 0.5903 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.5463 - acc: 0.9153 - val_loss: 0.5838 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 307us/step - loss: 0.5400 - acc: 0.9237 - val_loss: 0.5776 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.5339 - acc: 0.9407 - val_loss: 0.5715 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5280 - acc: 0.9407 - val_loss: 0.5656 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.5224 - acc: 0.9407 - val_loss: 0.5601 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5170 - acc: 0.9407 - val_loss: 0.5547 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5117 - acc: 0.9492 - val_loss: 0.5494 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5066 - acc: 0.9492 - val_loss: 0.5444 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5017 - acc: 0.9576 - val_loss: 0.5395 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4968 - acc: 0.9576 - val_loss: 0.5347 - val_acc: 0.8462\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4921 - acc: 0.9576 - val_loss: 0.5300 - val_acc: 0.8462\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4875 - acc: 0.9576 - val_loss: 0.5255 - val_acc: 0.8462\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4830 - acc: 0.9661 - val_loss: 0.5211 - val_acc: 0.8462\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4788 - acc: 0.9661 - val_loss: 0.5168 - val_acc: 0.8462\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.4745 - acc: 0.9661 - val_loss: 0.5126 - val_acc: 0.8462\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4703 - acc: 0.9661 - val_loss: 0.5085 - val_acc: 0.8462\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4663 - acc: 0.9831 - val_loss: 0.5046 - val_acc: 0.8462\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4623 - acc: 0.9831 - val_loss: 0.5007 - val_acc: 0.8462\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4584 - acc: 0.9831 - val_loss: 0.4968 - val_acc: 0.8462\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4546 - acc: 0.9831 - val_loss: 0.4931 - val_acc: 0.8462\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4509 - acc: 0.9831 - val_loss: 0.4894 - val_acc: 0.8462\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.4473 - acc: 0.9831 - val_loss: 0.4859 - val_acc: 0.8462\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4437 - acc: 0.9831 - val_loss: 0.4824 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4402 - acc: 0.9831 - val_loss: 0.4789 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4368 - acc: 0.9831 - val_loss: 0.4755 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4335 - acc: 0.9831 - val_loss: 0.4723 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.4302 - acc: 0.9831 - val_loss: 0.4690 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4270 - acc: 0.9831 - val_loss: 0.4658 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.4238 - acc: 0.9831 - val_loss: 0.4626 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.4206 - acc: 0.9831 - val_loss: 0.4595 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4176 - acc: 0.9831 - val_loss: 0.4565 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4146 - acc: 0.9831 - val_loss: 0.4535 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4116 - acc: 0.9915 - val_loss: 0.4505 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4086 - acc: 0.9915 - val_loss: 0.4476 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4057 - acc: 0.9915 - val_loss: 0.4447 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.4029 - acc: 0.9915 - val_loss: 0.4419 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4001 - acc: 0.9915 - val_loss: 0.4391 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3973 - acc: 0.9915 - val_loss: 0.4364 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3945 - acc: 0.9915 - val_loss: 0.4337 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3918 - acc: 0.9915 - val_loss: 0.4310 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.3891 - acc: 0.9915 - val_loss: 0.4284 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3865 - acc: 0.9915 - val_loss: 0.4258 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.3839 - acc: 0.9915 - val_loss: 0.4232 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3813 - acc: 0.9915 - val_loss: 0.4206 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.3787 - acc: 0.9915 - val_loss: 0.4181 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3762 - acc: 0.9915 - val_loss: 0.4156 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.3737 - acc: 0.9915 - val_loss: 0.4131 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3712 - acc: 0.9915 - val_loss: 0.4107 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3688 - acc: 0.9915 - val_loss: 0.4083 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3663 - acc: 0.9915 - val_loss: 0.4059 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3639 - acc: 0.9915 - val_loss: 0.4035 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3616 - acc: 0.9915 - val_loss: 0.4012 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3593 - acc: 0.9915 - val_loss: 0.3989 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3569 - acc: 0.9915 - val_loss: 0.3965 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3546 - acc: 0.9915 - val_loss: 0.3942 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3523 - acc: 0.9915 - val_loss: 0.3919 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 2.9288 - acc: 0.1186 - val_loss: 2.3336 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 2.6941 - acc: 0.1186 - val_loss: 2.1507 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 2.4797 - acc: 0.1186 - val_loss: 1.9855 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 2.2892 - acc: 0.1271 - val_loss: 1.8394 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.1140 - acc: 0.1356 - val_loss: 1.7115 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.9589 - acc: 0.1356 - val_loss: 1.5947 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.8189 - acc: 0.1441 - val_loss: 1.4863 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.6931 - acc: 0.1525 - val_loss: 1.3920 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.5805 - acc: 0.1780 - val_loss: 1.3068 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.4794 - acc: 0.1864 - val_loss: 1.2310 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.3878 - acc: 0.1864 - val_loss: 1.1647 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.3061 - acc: 0.2034 - val_loss: 1.1031 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.2322 - acc: 0.2034 - val_loss: 1.0514 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.1666 - acc: 0.2627 - val_loss: 1.0031 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.1076 - acc: 0.2966 - val_loss: 0.9623 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0554 - acc: 0.3136 - val_loss: 0.9260 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0095 - acc: 0.3220 - val_loss: 0.8958 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9715 - acc: 0.3559 - val_loss: 0.8697 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9383 - acc: 0.3983 - val_loss: 0.8472 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9085 - acc: 0.4153 - val_loss: 0.8266 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8824 - acc: 0.5000 - val_loss: 0.8077 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8582 - acc: 0.5508 - val_loss: 0.7900 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.8361 - acc: 0.6017 - val_loss: 0.7750 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8160 - acc: 0.6271 - val_loss: 0.7609 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7979 - acc: 0.6525 - val_loss: 0.7476 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7813 - acc: 0.6949 - val_loss: 0.7351 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.7666 - acc: 0.7203 - val_loss: 0.7235 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7530 - acc: 0.7373 - val_loss: 0.7124 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7401 - acc: 0.7542 - val_loss: 0.7025 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7282 - acc: 0.7627 - val_loss: 0.6930 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7171 - acc: 0.7881 - val_loss: 0.6839 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7064 - acc: 0.8136 - val_loss: 0.6749 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6963 - acc: 0.8136 - val_loss: 0.6660 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6867 - acc: 0.8220 - val_loss: 0.6576 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6777 - acc: 0.8220 - val_loss: 0.6494 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6692 - acc: 0.8220 - val_loss: 0.6415 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6611 - acc: 0.8305 - val_loss: 0.6336 - val_acc: 0.7692\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.6532 - acc: 0.8390 - val_loss: 0.6260 - val_acc: 0.7692\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6456 - acc: 0.8475 - val_loss: 0.6187 - val_acc: 0.7692\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6381 - acc: 0.8559 - val_loss: 0.6114 - val_acc: 0.7692\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.6312 - acc: 0.8559 - val_loss: 0.6044 - val_acc: 0.7692\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6245 - acc: 0.8559 - val_loss: 0.5977 - val_acc: 0.7692\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6180 - acc: 0.8559 - val_loss: 0.5918 - val_acc: 0.7692\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.6118 - acc: 0.8559 - val_loss: 0.5860 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6057 - acc: 0.8559 - val_loss: 0.5803 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5997 - acc: 0.8729 - val_loss: 0.5747 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5939 - acc: 0.8729 - val_loss: 0.5691 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.5883 - acc: 0.8729 - val_loss: 0.5637 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.5828 - acc: 0.8729 - val_loss: 0.5582 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5773 - acc: 0.8729 - val_loss: 0.5529 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5720 - acc: 0.8814 - val_loss: 0.5476 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.5668 - acc: 0.8814 - val_loss: 0.5424 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5617 - acc: 0.8898 - val_loss: 0.5373 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5567 - acc: 0.8898 - val_loss: 0.5324 - val_acc: 0.8462\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.5518 - acc: 0.8983 - val_loss: 0.5274 - val_acc: 0.8462\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5470 - acc: 0.8983 - val_loss: 0.5226 - val_acc: 0.8462\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5423 - acc: 0.9068 - val_loss: 0.5178 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.5376 - acc: 0.9068 - val_loss: 0.5131 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5330 - acc: 0.9068 - val_loss: 0.5084 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.5286 - acc: 0.9068 - val_loss: 0.5038 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5242 - acc: 0.9068 - val_loss: 0.4992 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5198 - acc: 0.9068 - val_loss: 0.4949 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5156 - acc: 0.9068 - val_loss: 0.4904 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5114 - acc: 0.9153 - val_loss: 0.4861 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5072 - acc: 0.9153 - val_loss: 0.4818 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.5030 - acc: 0.9153 - val_loss: 0.4775 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4989 - acc: 0.9153 - val_loss: 0.4733 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4949 - acc: 0.9153 - val_loss: 0.4691 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4909 - acc: 0.9237 - val_loss: 0.4650 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4870 - acc: 0.9237 - val_loss: 0.4611 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4831 - acc: 0.9237 - val_loss: 0.4572 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4793 - acc: 0.9322 - val_loss: 0.4534 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4755 - acc: 0.9322 - val_loss: 0.4496 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4717 - acc: 0.9322 - val_loss: 0.4459 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4681 - acc: 0.9322 - val_loss: 0.4423 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.4645 - acc: 0.9322 - val_loss: 0.4387 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4609 - acc: 0.9322 - val_loss: 0.4350 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4573 - acc: 0.9407 - val_loss: 0.4315 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.4538 - acc: 0.9407 - val_loss: 0.4281 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4503 - acc: 0.9407 - val_loss: 0.4247 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4469 - acc: 0.9407 - val_loss: 0.4214 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4435 - acc: 0.9407 - val_loss: 0.4184 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4401 - acc: 0.9492 - val_loss: 0.4154 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4368 - acc: 0.9492 - val_loss: 0.4125 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4336 - acc: 0.9492 - val_loss: 0.4096 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4304 - acc: 0.9492 - val_loss: 0.4067 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4272 - acc: 0.9492 - val_loss: 0.4039 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4241 - acc: 0.9492 - val_loss: 0.4011 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4210 - acc: 0.9492 - val_loss: 0.3983 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4180 - acc: 0.9492 - val_loss: 0.3956 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.4150 - acc: 0.9492 - val_loss: 0.3929 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4121 - acc: 0.9492 - val_loss: 0.3903 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4091 - acc: 0.9492 - val_loss: 0.3876 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4063 - acc: 0.9492 - val_loss: 0.3850 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.4034 - acc: 0.9492 - val_loss: 0.3825 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4006 - acc: 0.9492 - val_loss: 0.3800 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3979 - acc: 0.9492 - val_loss: 0.3775 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3951 - acc: 0.9576 - val_loss: 0.3750 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3924 - acc: 0.9576 - val_loss: 0.3725 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3897 - acc: 0.9576 - val_loss: 0.3702 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 3.5580 - acc: 0.2203 - val_loss: 3.2106 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 3.2962 - acc: 0.2373 - val_loss: 2.9822 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 3.0525 - acc: 0.2373 - val_loss: 2.7673 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 2.8297 - acc: 0.2458 - val_loss: 2.5720 - val_acc: 0.1538\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 2.6277 - acc: 0.2458 - val_loss: 2.3933 - val_acc: 0.1538\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 2.4424 - acc: 0.2458 - val_loss: 2.2305 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 2.2738 - acc: 0.2627 - val_loss: 2.0793 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 2.1195 - acc: 0.2966 - val_loss: 1.9352 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.9769 - acc: 0.3390 - val_loss: 1.8052 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.8497 - acc: 0.4322 - val_loss: 1.6807 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.7298 - acc: 0.4661 - val_loss: 1.5756 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.6229 - acc: 0.4576 - val_loss: 1.4709 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.5244 - acc: 0.4576 - val_loss: 1.3775 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.4368 - acc: 0.4576 - val_loss: 1.2965 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 1.3562 - acc: 0.4576 - val_loss: 1.2209 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.2826 - acc: 0.4661 - val_loss: 1.1545 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.2147 - acc: 0.4661 - val_loss: 1.0879 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1515 - acc: 0.4746 - val_loss: 1.0294 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0929 - acc: 0.5000 - val_loss: 0.9789 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0407 - acc: 0.5254 - val_loss: 0.9321 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9948 - acc: 0.5424 - val_loss: 0.8896 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9534 - acc: 0.5593 - val_loss: 0.8521 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9167 - acc: 0.5847 - val_loss: 0.8194 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8846 - acc: 0.5932 - val_loss: 0.7910 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8551 - acc: 0.6017 - val_loss: 0.7660 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8289 - acc: 0.6102 - val_loss: 0.7430 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8049 - acc: 0.6356 - val_loss: 0.7223 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7818 - acc: 0.6441 - val_loss: 0.7043 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7616 - acc: 0.6610 - val_loss: 0.6882 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.7429 - acc: 0.6695 - val_loss: 0.6733 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7255 - acc: 0.6864 - val_loss: 0.6591 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7094 - acc: 0.6949 - val_loss: 0.6457 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6947 - acc: 0.7203 - val_loss: 0.6331 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.6809 - acc: 0.7373 - val_loss: 0.6212 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6678 - acc: 0.7458 - val_loss: 0.6100 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6555 - acc: 0.7458 - val_loss: 0.5994 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6439 - acc: 0.7627 - val_loss: 0.5893 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.6328 - acc: 0.7627 - val_loss: 0.5798 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6224 - acc: 0.7712 - val_loss: 0.5708 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6126 - acc: 0.7712 - val_loss: 0.5623 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6033 - acc: 0.7881 - val_loss: 0.5543 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5945 - acc: 0.7966 - val_loss: 0.5470 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.5859 - acc: 0.8051 - val_loss: 0.5398 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5778 - acc: 0.8051 - val_loss: 0.5331 - val_acc: 0.7692\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5700 - acc: 0.8220 - val_loss: 0.5265 - val_acc: 0.7692\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.5626 - acc: 0.8390 - val_loss: 0.5203 - val_acc: 0.7692\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.5555 - acc: 0.8475 - val_loss: 0.5142 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.5485 - acc: 0.8559 - val_loss: 0.5084 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.5418 - acc: 0.8644 - val_loss: 0.5028 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5353 - acc: 0.8729 - val_loss: 0.4972 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5291 - acc: 0.8729 - val_loss: 0.4919 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5232 - acc: 0.8814 - val_loss: 0.4868 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5175 - acc: 0.8898 - val_loss: 0.4818 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.5121 - acc: 0.8898 - val_loss: 0.4772 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.5068 - acc: 0.8898 - val_loss: 0.4726 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5018 - acc: 0.8898 - val_loss: 0.4681 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4968 - acc: 0.9068 - val_loss: 0.4639 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4921 - acc: 0.9153 - val_loss: 0.4597 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 280us/step - loss: 0.4874 - acc: 0.9153 - val_loss: 0.4555 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4829 - acc: 0.9153 - val_loss: 0.4516 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.4785 - acc: 0.9153 - val_loss: 0.4476 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4742 - acc: 0.9153 - val_loss: 0.4438 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4699 - acc: 0.9153 - val_loss: 0.4401 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4658 - acc: 0.9153 - val_loss: 0.4364 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4618 - acc: 0.9153 - val_loss: 0.4329 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4579 - acc: 0.9153 - val_loss: 0.4294 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4540 - acc: 0.9237 - val_loss: 0.4260 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4503 - acc: 0.9237 - val_loss: 0.4226 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4466 - acc: 0.9237 - val_loss: 0.4193 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.4430 - acc: 0.9237 - val_loss: 0.4161 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4395 - acc: 0.9322 - val_loss: 0.4129 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4360 - acc: 0.9322 - val_loss: 0.4098 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4326 - acc: 0.9322 - val_loss: 0.4067 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.4293 - acc: 0.9322 - val_loss: 0.4037 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.4261 - acc: 0.9322 - val_loss: 0.4007 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4228 - acc: 0.9322 - val_loss: 0.3977 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4196 - acc: 0.9322 - val_loss: 0.3949 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4165 - acc: 0.9322 - val_loss: 0.3921 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.4134 - acc: 0.9322 - val_loss: 0.3893 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.4104 - acc: 0.9322 - val_loss: 0.3866 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4074 - acc: 0.9322 - val_loss: 0.3839 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4045 - acc: 0.9322 - val_loss: 0.3812 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4016 - acc: 0.9322 - val_loss: 0.3786 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3987 - acc: 0.9322 - val_loss: 0.3760 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3959 - acc: 0.9407 - val_loss: 0.3735 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3931 - acc: 0.9407 - val_loss: 0.3710 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3904 - acc: 0.9407 - val_loss: 0.3685 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3877 - acc: 0.9407 - val_loss: 0.3660 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3850 - acc: 0.9407 - val_loss: 0.3636 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3823 - acc: 0.9407 - val_loss: 0.3613 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.3797 - acc: 0.9407 - val_loss: 0.3589 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3772 - acc: 0.9407 - val_loss: 0.3565 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.3747 - acc: 0.9407 - val_loss: 0.3543 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3722 - acc: 0.9407 - val_loss: 0.3520 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3698 - acc: 0.9407 - val_loss: 0.3497 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.3673 - acc: 0.9407 - val_loss: 0.3476 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3650 - acc: 0.9407 - val_loss: 0.3454 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3626 - acc: 0.9407 - val_loss: 0.3433 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3603 - acc: 0.9407 - val_loss: 0.3412 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3580 - acc: 0.9407 - val_loss: 0.3391 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.8011 - acc: 0.3729 - val_loss: 2.3624 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.6427 - acc: 0.3898 - val_loss: 2.1597 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.5045 - acc: 0.4153 - val_loss: 1.9865 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.3852 - acc: 0.4407 - val_loss: 1.8298 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.2828 - acc: 0.4831 - val_loss: 1.6918 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.1945 - acc: 0.5339 - val_loss: 1.5724 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1185 - acc: 0.5593 - val_loss: 1.4644 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0512 - acc: 0.5678 - val_loss: 1.3728 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9927 - acc: 0.5847 - val_loss: 1.2863 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9405 - acc: 0.6017 - val_loss: 1.2131 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8947 - acc: 0.6017 - val_loss: 1.1441 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.8539 - acc: 0.6017 - val_loss: 1.0843 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8178 - acc: 0.6186 - val_loss: 1.0297 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.7851 - acc: 0.6186 - val_loss: 0.9804 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.7558 - acc: 0.6271 - val_loss: 0.9363 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.7293 - acc: 0.6356 - val_loss: 0.8953 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7051 - acc: 0.6441 - val_loss: 0.8594 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.6831 - acc: 0.6525 - val_loss: 0.8258 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6630 - acc: 0.6780 - val_loss: 0.7955 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6445 - acc: 0.6949 - val_loss: 0.7671 - val_acc: 0.6923\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6272 - acc: 0.7119 - val_loss: 0.7409 - val_acc: 0.6923\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.6116 - acc: 0.7288 - val_loss: 0.7163 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5964 - acc: 0.7288 - val_loss: 0.6934 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5826 - acc: 0.7458 - val_loss: 0.6724 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5696 - acc: 0.7542 - val_loss: 0.6529 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.5572 - acc: 0.7627 - val_loss: 0.6341 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5457 - acc: 0.7627 - val_loss: 0.6165 - val_acc: 0.7692\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5347 - acc: 0.7797 - val_loss: 0.6001 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5243 - acc: 0.8051 - val_loss: 0.5850 - val_acc: 0.7692\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5143 - acc: 0.8220 - val_loss: 0.5701 - val_acc: 0.7692\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.5049 - acc: 0.8220 - val_loss: 0.5562 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.4958 - acc: 0.8390 - val_loss: 0.5430 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4871 - acc: 0.8559 - val_loss: 0.5304 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4789 - acc: 0.8814 - val_loss: 0.5186 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4708 - acc: 0.8983 - val_loss: 0.5072 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4630 - acc: 0.8983 - val_loss: 0.4962 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4556 - acc: 0.9153 - val_loss: 0.4857 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4484 - acc: 0.9492 - val_loss: 0.4760 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4413 - acc: 0.9576 - val_loss: 0.4661 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4345 - acc: 0.9661 - val_loss: 0.4568 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.4278 - acc: 0.9661 - val_loss: 0.4478 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.4212 - acc: 0.9661 - val_loss: 0.4390 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4149 - acc: 0.9746 - val_loss: 0.4305 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4087 - acc: 0.9746 - val_loss: 0.4224 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4027 - acc: 0.9831 - val_loss: 0.4143 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3967 - acc: 0.9831 - val_loss: 0.4067 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3909 - acc: 0.9915 - val_loss: 0.3992 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3852 - acc: 0.9915 - val_loss: 0.3920 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3796 - acc: 0.9915 - val_loss: 0.3848 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3740 - acc: 0.9915 - val_loss: 0.3780 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3686 - acc: 0.9915 - val_loss: 0.3711 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3633 - acc: 0.9915 - val_loss: 0.3646 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3579 - acc: 0.9915 - val_loss: 0.3581 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.3526 - acc: 0.9915 - val_loss: 0.3519 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3474 - acc: 0.9915 - val_loss: 0.3456 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.3422 - acc: 0.9915 - val_loss: 0.3396 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3372 - acc: 0.9915 - val_loss: 0.3336 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3322 - acc: 0.9915 - val_loss: 0.3280 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3273 - acc: 0.9915 - val_loss: 0.3222 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3224 - acc: 0.9915 - val_loss: 0.3167 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3176 - acc: 0.9915 - val_loss: 0.3112 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.3128 - acc: 0.9915 - val_loss: 0.3058 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3081 - acc: 0.9915 - val_loss: 0.3006 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3035 - acc: 0.9915 - val_loss: 0.2956 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2989 - acc: 0.9915 - val_loss: 0.2906 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2944 - acc: 0.9915 - val_loss: 0.2857 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2899 - acc: 0.9915 - val_loss: 0.2808 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2855 - acc: 0.9915 - val_loss: 0.2762 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2811 - acc: 0.9915 - val_loss: 0.2717 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2768 - acc: 0.9915 - val_loss: 0.2671 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.2725 - acc: 0.9915 - val_loss: 0.2627 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2683 - acc: 0.9915 - val_loss: 0.2584 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.2642 - acc: 0.9915 - val_loss: 0.2541 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2601 - acc: 0.9915 - val_loss: 0.2501 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2561 - acc: 0.9915 - val_loss: 0.2460 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.2520 - acc: 0.9915 - val_loss: 0.2421 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2481 - acc: 0.9915 - val_loss: 0.2383 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.2443 - acc: 0.9915 - val_loss: 0.2345 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2406 - acc: 0.9915 - val_loss: 0.2310 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.2369 - acc: 0.9915 - val_loss: 0.2274 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2332 - acc: 0.9915 - val_loss: 0.2239 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2296 - acc: 0.9915 - val_loss: 0.2205 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.2261 - acc: 0.9915 - val_loss: 0.2173 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2227 - acc: 0.9915 - val_loss: 0.2141 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2193 - acc: 0.9915 - val_loss: 0.2109 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.2159 - acc: 0.9915 - val_loss: 0.2079 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2127 - acc: 0.9915 - val_loss: 0.2049 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2095 - acc: 0.9915 - val_loss: 0.2020 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2063 - acc: 0.9915 - val_loss: 0.1992 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2033 - acc: 0.9915 - val_loss: 0.1963 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2002 - acc: 0.9915 - val_loss: 0.1938 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1972 - acc: 0.9915 - val_loss: 0.1911 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1943 - acc: 0.9915 - val_loss: 0.1886 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1914 - acc: 0.9915 - val_loss: 0.1859 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1885 - acc: 0.9915 - val_loss: 0.1835 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1858 - acc: 0.9915 - val_loss: 0.1812 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1831 - acc: 0.9915 - val_loss: 0.1788 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1805 - acc: 0.9915 - val_loss: 0.1765 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1779 - acc: 0.9915 - val_loss: 0.1744 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.1754 - acc: 0.9915 - val_loss: 0.1722 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 2.6224 - acc: 0.1271 - val_loss: 2.1658 - val_acc: 0.1538\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 2.4391 - acc: 0.1441 - val_loss: 2.0280 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 2.2721 - acc: 0.1441 - val_loss: 1.9059 - val_acc: 0.1538\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 2.1216 - acc: 0.1441 - val_loss: 1.7953 - val_acc: 0.1538\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.9867 - acc: 0.1441 - val_loss: 1.6955 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.8659 - acc: 0.1441 - val_loss: 1.6080 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.7569 - acc: 0.1441 - val_loss: 1.5306 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.6568 - acc: 0.1441 - val_loss: 1.4643 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.5665 - acc: 0.1695 - val_loss: 1.4032 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.4835 - acc: 0.1695 - val_loss: 1.3468 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.4079 - acc: 0.1780 - val_loss: 1.2952 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.3404 - acc: 0.2119 - val_loss: 1.2482 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 1.2786 - acc: 0.2203 - val_loss: 1.2043 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.2216 - acc: 0.2203 - val_loss: 1.1639 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.1693 - acc: 0.2288 - val_loss: 1.1267 - val_acc: 0.2308\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1214 - acc: 0.2458 - val_loss: 1.0923 - val_acc: 0.2308\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0769 - acc: 0.2627 - val_loss: 1.0585 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0349 - acc: 0.3051 - val_loss: 1.0266 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9954 - acc: 0.5678 - val_loss: 0.9970 - val_acc: 0.6923\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9586 - acc: 0.6864 - val_loss: 0.9682 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9242 - acc: 0.7034 - val_loss: 0.9412 - val_acc: 0.7692\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8917 - acc: 0.7203 - val_loss: 0.9167 - val_acc: 0.7692\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8611 - acc: 0.7627 - val_loss: 0.8941 - val_acc: 0.7692\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8324 - acc: 0.8051 - val_loss: 0.8716 - val_acc: 0.7692\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8061 - acc: 0.8305 - val_loss: 0.8504 - val_acc: 0.7692\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.7807 - acc: 0.8390 - val_loss: 0.8304 - val_acc: 0.7692\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7565 - acc: 0.8559 - val_loss: 0.8119 - val_acc: 0.7692\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7332 - acc: 0.8814 - val_loss: 0.7939 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.7106 - acc: 0.9068 - val_loss: 0.7762 - val_acc: 0.7692\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6890 - acc: 0.9068 - val_loss: 0.7590 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6679 - acc: 0.9068 - val_loss: 0.7418 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.6471 - acc: 0.9237 - val_loss: 0.7254 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.6272 - acc: 0.9322 - val_loss: 0.7089 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6081 - acc: 0.9407 - val_loss: 0.6931 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.5898 - acc: 0.9407 - val_loss: 0.6778 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.5722 - acc: 0.9492 - val_loss: 0.6626 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5551 - acc: 0.9661 - val_loss: 0.6480 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5392 - acc: 0.9661 - val_loss: 0.6339 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5235 - acc: 0.9746 - val_loss: 0.6200 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.5083 - acc: 0.9746 - val_loss: 0.6062 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4935 - acc: 0.9831 - val_loss: 0.5930 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4789 - acc: 0.9831 - val_loss: 0.5796 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.4651 - acc: 0.9831 - val_loss: 0.5670 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4516 - acc: 0.9831 - val_loss: 0.5547 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.4386 - acc: 0.9831 - val_loss: 0.5425 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4260 - acc: 0.9831 - val_loss: 0.5308 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4138 - acc: 0.9831 - val_loss: 0.5193 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.4021 - acc: 0.9831 - val_loss: 0.5081 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.3909 - acc: 0.9831 - val_loss: 0.4974 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3802 - acc: 0.9831 - val_loss: 0.4869 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3698 - acc: 0.9831 - val_loss: 0.4768 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3599 - acc: 0.9831 - val_loss: 0.4671 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3504 - acc: 0.9831 - val_loss: 0.4575 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3412 - acc: 0.9831 - val_loss: 0.4483 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3323 - acc: 0.9831 - val_loss: 0.4395 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3238 - acc: 0.9831 - val_loss: 0.4307 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3155 - acc: 0.9915 - val_loss: 0.4223 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3076 - acc: 0.9915 - val_loss: 0.4141 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3000 - acc: 0.9915 - val_loss: 0.4061 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2926 - acc: 0.9915 - val_loss: 0.3985 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.2855 - acc: 0.9915 - val_loss: 0.3910 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2787 - acc: 0.9915 - val_loss: 0.3837 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.2722 - acc: 0.9915 - val_loss: 0.3767 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2658 - acc: 0.9915 - val_loss: 0.3698 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.2597 - acc: 0.9915 - val_loss: 0.3632 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2538 - acc: 0.9915 - val_loss: 0.3568 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2481 - acc: 0.9915 - val_loss: 0.3506 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2427 - acc: 0.9915 - val_loss: 0.3446 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2375 - acc: 0.9915 - val_loss: 0.3388 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2324 - acc: 0.9915 - val_loss: 0.3331 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2275 - acc: 0.9915 - val_loss: 0.3277 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2228 - acc: 0.9915 - val_loss: 0.3223 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.2182 - acc: 0.9915 - val_loss: 0.3173 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2138 - acc: 0.9915 - val_loss: 0.3122 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2096 - acc: 0.9915 - val_loss: 0.3073 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2055 - acc: 0.9915 - val_loss: 0.3026 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2015 - acc: 0.9915 - val_loss: 0.2979 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1977 - acc: 0.9915 - val_loss: 0.2935 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1940 - acc: 0.9915 - val_loss: 0.2891 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1905 - acc: 0.9915 - val_loss: 0.2849 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1870 - acc: 0.9915 - val_loss: 0.2808 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1837 - acc: 0.9915 - val_loss: 0.2767 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.1804 - acc: 0.9915 - val_loss: 0.2729 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1773 - acc: 0.9915 - val_loss: 0.2690 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1743 - acc: 0.9915 - val_loss: 0.2653 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1714 - acc: 0.9915 - val_loss: 0.2617 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.1685 - acc: 0.9915 - val_loss: 0.2582 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1658 - acc: 0.9915 - val_loss: 0.2547 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1631 - acc: 0.9915 - val_loss: 0.2514 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1605 - acc: 0.9915 - val_loss: 0.2482 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1580 - acc: 0.9915 - val_loss: 0.2450 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.1556 - acc: 0.9915 - val_loss: 0.2419 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1532 - acc: 0.9915 - val_loss: 0.2389 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1509 - acc: 0.9915 - val_loss: 0.2360 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1487 - acc: 0.9915 - val_loss: 0.2330 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1465 - acc: 0.9915 - val_loss: 0.2302 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1444 - acc: 0.9915 - val_loss: 0.2275 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.1423 - acc: 0.9915 - val_loss: 0.2247 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1403 - acc: 0.9915 - val_loss: 0.2221 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1384 - acc: 0.9915 - val_loss: 0.2196 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 2.4187 - acc: 0.0000e+00 - val_loss: 2.5429 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 2.2868 - acc: 0.0000e+00 - val_loss: 2.4066 - val_acc: 0.0769\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 2.1688 - acc: 0.0000e+00 - val_loss: 2.2836 - val_acc: 0.0769\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 2.0617 - acc: 0.0000e+00 - val_loss: 2.1652 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.9625 - acc: 0.0000e+00 - val_loss: 2.0635 - val_acc: 0.0769\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.8737 - acc: 0.0000e+00 - val_loss: 1.9661 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.7927 - acc: 0.0000e+00 - val_loss: 1.8788 - val_acc: 0.0769\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.7197 - acc: 0.0000e+00 - val_loss: 1.8003 - val_acc: 0.0769\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.6534 - acc: 0.0000e+00 - val_loss: 1.7311 - val_acc: 0.0769\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.5929 - acc: 0.0000e+00 - val_loss: 1.6650 - val_acc: 0.0769\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.5388 - acc: 0.0000e+00 - val_loss: 1.6063 - val_acc: 0.0769\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.4902 - acc: 0.0085 - val_loss: 1.5526 - val_acc: 0.0769\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.4463 - acc: 0.0085 - val_loss: 1.5039 - val_acc: 0.0769\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 1.4071 - acc: 0.0254 - val_loss: 1.4606 - val_acc: 0.0769\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.3726 - acc: 0.0424 - val_loss: 1.4222 - val_acc: 0.0769\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.3411 - acc: 0.0508 - val_loss: 1.3879 - val_acc: 0.0769\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.3118 - acc: 0.0678 - val_loss: 1.3563 - val_acc: 0.1538\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2846 - acc: 0.0763 - val_loss: 1.3268 - val_acc: 0.1538\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.2596 - acc: 0.1017 - val_loss: 1.2988 - val_acc: 0.1538\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2367 - acc: 0.1186 - val_loss: 1.2731 - val_acc: 0.1538\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.2157 - acc: 0.1271 - val_loss: 1.2499 - val_acc: 0.1538\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1965 - acc: 0.1441 - val_loss: 1.2288 - val_acc: 0.1538\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1784 - acc: 0.1525 - val_loss: 1.2086 - val_acc: 0.1538\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1621 - acc: 0.1610 - val_loss: 1.1900 - val_acc: 0.1538\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1470 - acc: 0.1695 - val_loss: 1.1732 - val_acc: 0.1538\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.1329 - acc: 0.1949 - val_loss: 1.1568 - val_acc: 0.1538\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1194 - acc: 0.2119 - val_loss: 1.1421 - val_acc: 0.1538\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1069 - acc: 0.2203 - val_loss: 1.1278 - val_acc: 0.2308\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0950 - acc: 0.2203 - val_loss: 1.1148 - val_acc: 0.2308\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0836 - acc: 0.2203 - val_loss: 1.1019 - val_acc: 0.2308\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0729 - acc: 0.2458 - val_loss: 1.0893 - val_acc: 0.2308\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0627 - acc: 0.2542 - val_loss: 1.0777 - val_acc: 0.2308\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0529 - acc: 0.2881 - val_loss: 1.0665 - val_acc: 0.3077\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0434 - acc: 0.3220 - val_loss: 1.0556 - val_acc: 0.3077\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0344 - acc: 0.3559 - val_loss: 1.0455 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0253 - acc: 0.3644 - val_loss: 1.0352 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.0166 - acc: 0.3983 - val_loss: 1.0254 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0083 - acc: 0.4068 - val_loss: 1.0159 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0000 - acc: 0.4492 - val_loss: 1.0065 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9917 - acc: 0.5000 - val_loss: 0.9975 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9834 - acc: 0.5169 - val_loss: 0.9879 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9748 - acc: 0.5339 - val_loss: 0.9789 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9662 - acc: 0.5508 - val_loss: 0.9701 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9579 - acc: 0.5593 - val_loss: 0.9614 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9495 - acc: 0.5593 - val_loss: 0.9526 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9413 - acc: 0.5593 - val_loss: 0.9439 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9329 - acc: 0.5763 - val_loss: 0.9351 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9246 - acc: 0.5763 - val_loss: 0.9267 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9164 - acc: 0.5763 - val_loss: 0.9179 - val_acc: 0.6154\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9085 - acc: 0.5763 - val_loss: 0.9096 - val_acc: 0.6154\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9005 - acc: 0.5763 - val_loss: 0.9010 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8925 - acc: 0.5763 - val_loss: 0.8927 - val_acc: 0.6154\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.8846 - acc: 0.5763 - val_loss: 0.8843 - val_acc: 0.6154\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 290us/step - loss: 0.8767 - acc: 0.5763 - val_loss: 0.8761 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8689 - acc: 0.5763 - val_loss: 0.8679 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8611 - acc: 0.5763 - val_loss: 0.8597 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8531 - acc: 0.5763 - val_loss: 0.8518 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8453 - acc: 0.5847 - val_loss: 0.8437 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8375 - acc: 0.5847 - val_loss: 0.8358 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8298 - acc: 0.5847 - val_loss: 0.8278 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8220 - acc: 0.5847 - val_loss: 0.8198 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8143 - acc: 0.5847 - val_loss: 0.8120 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8066 - acc: 0.5847 - val_loss: 0.8043 - val_acc: 0.6154\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7988 - acc: 0.5847 - val_loss: 0.7959 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7909 - acc: 0.5847 - val_loss: 0.7883 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7832 - acc: 0.5847 - val_loss: 0.7805 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.7754 - acc: 0.5847 - val_loss: 0.7725 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.7677 - acc: 0.5847 - val_loss: 0.7648 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7601 - acc: 0.5847 - val_loss: 0.7570 - val_acc: 0.6154\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7524 - acc: 0.5847 - val_loss: 0.7494 - val_acc: 0.6154\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7447 - acc: 0.5847 - val_loss: 0.7416 - val_acc: 0.6154\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7373 - acc: 0.5847 - val_loss: 0.7340 - val_acc: 0.6154\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7296 - acc: 0.5847 - val_loss: 0.7265 - val_acc: 0.6154\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7221 - acc: 0.5847 - val_loss: 0.7191 - val_acc: 0.6154\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7147 - acc: 0.5847 - val_loss: 0.7114 - val_acc: 0.6154\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7073 - acc: 0.5847 - val_loss: 0.7041 - val_acc: 0.6154\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6999 - acc: 0.5847 - val_loss: 0.6967 - val_acc: 0.6154\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 259us/step - loss: 0.6928 - acc: 0.5847 - val_loss: 0.6896 - val_acc: 0.6154\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6854 - acc: 0.5847 - val_loss: 0.6826 - val_acc: 0.6154\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 278us/step - loss: 0.6783 - acc: 0.5847 - val_loss: 0.6756 - val_acc: 0.6154\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.6712 - acc: 0.5847 - val_loss: 0.6686 - val_acc: 0.6154\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6642 - acc: 0.5847 - val_loss: 0.6617 - val_acc: 0.6154\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6573 - acc: 0.5847 - val_loss: 0.6550 - val_acc: 0.6154\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6504 - acc: 0.5847 - val_loss: 0.6484 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6437 - acc: 0.9322 - val_loss: 0.6419 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6369 - acc: 0.9831 - val_loss: 0.6354 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6303 - acc: 0.9915 - val_loss: 0.6292 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6237 - acc: 0.9915 - val_loss: 0.6229 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.6173 - acc: 0.9915 - val_loss: 0.6169 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.6110 - acc: 0.9915 - val_loss: 0.6107 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6047 - acc: 0.9915 - val_loss: 0.6048 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5986 - acc: 0.9915 - val_loss: 0.5990 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5925 - acc: 0.9915 - val_loss: 0.5933 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.5866 - acc: 0.9915 - val_loss: 0.5878 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.5807 - acc: 0.9915 - val_loss: 0.5822 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5749 - acc: 0.9915 - val_loss: 0.5768 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5692 - acc: 0.9915 - val_loss: 0.5713 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5636 - acc: 0.9915 - val_loss: 0.5662 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5582 - acc: 0.9915 - val_loss: 0.5610 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5527 - acc: 0.9915 - val_loss: 0.5560 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 0.6164 - acc: 0.7119 - val_loss: 0.5663 - val_acc: 0.9231\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6030 - acc: 0.8983 - val_loss: 0.5563 - val_acc: 0.9231\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.5904 - acc: 0.9153 - val_loss: 0.5464 - val_acc: 1.0000\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5781 - acc: 0.9237 - val_loss: 0.5367 - val_acc: 1.0000\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5662 - acc: 0.9407 - val_loss: 0.5273 - val_acc: 1.0000\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.5549 - acc: 0.9407 - val_loss: 0.5181 - val_acc: 1.0000\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5442 - acc: 0.9407 - val_loss: 0.5091 - val_acc: 1.0000\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5336 - acc: 0.9407 - val_loss: 0.5003 - val_acc: 1.0000\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.5234 - acc: 0.9407 - val_loss: 0.4920 - val_acc: 1.0000\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5136 - acc: 0.9407 - val_loss: 0.4833 - val_acc: 1.0000\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5042 - acc: 0.9407 - val_loss: 0.4754 - val_acc: 1.0000\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4948 - acc: 0.9407 - val_loss: 0.4670 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4858 - acc: 0.9407 - val_loss: 0.4590 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4770 - acc: 0.9407 - val_loss: 0.4512 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.4685 - acc: 0.9407 - val_loss: 0.4435 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4602 - acc: 0.9492 - val_loss: 0.4359 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4522 - acc: 0.9407 - val_loss: 0.4284 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4442 - acc: 0.9407 - val_loss: 0.4213 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.4365 - acc: 0.9407 - val_loss: 0.4139 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.4291 - acc: 0.9407 - val_loss: 0.4070 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4217 - acc: 0.9407 - val_loss: 0.4000 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.4144 - acc: 0.9407 - val_loss: 0.3931 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4073 - acc: 0.9492 - val_loss: 0.3864 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4005 - acc: 0.9492 - val_loss: 0.3798 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3937 - acc: 0.9492 - val_loss: 0.3731 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3871 - acc: 0.9492 - val_loss: 0.3669 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3807 - acc: 0.9492 - val_loss: 0.3606 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3744 - acc: 0.9492 - val_loss: 0.3543 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3682 - acc: 0.9576 - val_loss: 0.3482 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3622 - acc: 0.9576 - val_loss: 0.3423 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3563 - acc: 0.9576 - val_loss: 0.3364 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3506 - acc: 0.9576 - val_loss: 0.3308 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3449 - acc: 0.9746 - val_loss: 0.3251 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.3394 - acc: 0.9746 - val_loss: 0.3195 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.3340 - acc: 0.9746 - val_loss: 0.3143 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.3288 - acc: 0.9746 - val_loss: 0.3089 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.3237 - acc: 0.9746 - val_loss: 0.3037 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.3187 - acc: 0.9746 - val_loss: 0.2988 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3138 - acc: 0.9746 - val_loss: 0.2938 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3091 - acc: 0.9746 - val_loss: 0.2890 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3044 - acc: 0.9746 - val_loss: 0.2843 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2999 - acc: 0.9746 - val_loss: 0.2796 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2955 - acc: 0.9746 - val_loss: 0.2751 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2912 - acc: 0.9746 - val_loss: 0.2707 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2870 - acc: 0.9746 - val_loss: 0.2664 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2830 - acc: 0.9746 - val_loss: 0.2622 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2790 - acc: 0.9746 - val_loss: 0.2581 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2751 - acc: 0.9746 - val_loss: 0.2542 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2713 - acc: 0.9746 - val_loss: 0.2503 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2677 - acc: 0.9746 - val_loss: 0.2465 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2641 - acc: 0.9746 - val_loss: 0.2427 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.2605 - acc: 0.9746 - val_loss: 0.2387 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2570 - acc: 0.9746 - val_loss: 0.2347 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2535 - acc: 0.9746 - val_loss: 0.2308 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2501 - acc: 0.9746 - val_loss: 0.2269 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2468 - acc: 0.9746 - val_loss: 0.2230 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2435 - acc: 0.9746 - val_loss: 0.2192 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2403 - acc: 0.9746 - val_loss: 0.2155 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.2372 - acc: 0.9831 - val_loss: 0.2119 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2341 - acc: 0.9831 - val_loss: 0.2084 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2311 - acc: 0.9831 - val_loss: 0.2050 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2282 - acc: 0.9831 - val_loss: 0.2017 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2254 - acc: 0.9831 - val_loss: 0.1984 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2227 - acc: 0.9831 - val_loss: 0.1951 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2200 - acc: 0.9831 - val_loss: 0.1921 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2174 - acc: 0.9831 - val_loss: 0.1890 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2148 - acc: 0.9831 - val_loss: 0.1859 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2123 - acc: 0.9831 - val_loss: 0.1831 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2099 - acc: 0.9831 - val_loss: 0.1802 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.2075 - acc: 0.9831 - val_loss: 0.1774 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2052 - acc: 0.9831 - val_loss: 0.1747 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2029 - acc: 0.9831 - val_loss: 0.1721 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.2007 - acc: 0.9831 - val_loss: 0.1694 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1986 - acc: 0.9831 - val_loss: 0.1669 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1964 - acc: 0.9831 - val_loss: 0.1644 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.1944 - acc: 0.9831 - val_loss: 0.1619 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.1924 - acc: 0.9831 - val_loss: 0.1595 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1904 - acc: 0.9831 - val_loss: 0.1572 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.1885 - acc: 0.9831 - val_loss: 0.1550 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1866 - acc: 0.9831 - val_loss: 0.1528 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.1848 - acc: 0.9831 - val_loss: 0.1506 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1830 - acc: 0.9831 - val_loss: 0.1484 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1812 - acc: 0.9831 - val_loss: 0.1463 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.1795 - acc: 0.9831 - val_loss: 0.1443 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1779 - acc: 0.9831 - val_loss: 0.1423 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1762 - acc: 0.9831 - val_loss: 0.1403 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.1746 - acc: 0.9831 - val_loss: 0.1383 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.1731 - acc: 0.9831 - val_loss: 0.1365 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1715 - acc: 0.9831 - val_loss: 0.1346 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1700 - acc: 0.9831 - val_loss: 0.1328 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1686 - acc: 0.9831 - val_loss: 0.1310 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.1671 - acc: 0.9831 - val_loss: 0.1293 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1657 - acc: 0.9831 - val_loss: 0.1276 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1644 - acc: 0.9831 - val_loss: 0.1259 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1630 - acc: 0.9831 - val_loss: 0.1243 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1617 - acc: 0.9831 - val_loss: 0.1227 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1604 - acc: 0.9831 - val_loss: 0.1211 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1592 - acc: 0.9831 - val_loss: 0.1196 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1579 - acc: 0.9831 - val_loss: 0.1180 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.1566 - acc: 0.9831 - val_loss: 0.1165 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 0.9088 - acc: 0.5508 - val_loss: 0.7981 - val_acc: 0.5385\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.8837 - acc: 0.5508 - val_loss: 0.7739 - val_acc: 0.5385\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.8603 - acc: 0.5847 - val_loss: 0.7519 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8387 - acc: 0.6017 - val_loss: 0.7315 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8182 - acc: 0.6102 - val_loss: 0.7113 - val_acc: 0.6154\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7989 - acc: 0.6102 - val_loss: 0.6927 - val_acc: 0.6154\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7805 - acc: 0.6271 - val_loss: 0.6768 - val_acc: 0.6154\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7626 - acc: 0.6525 - val_loss: 0.6613 - val_acc: 0.6154\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7465 - acc: 0.6610 - val_loss: 0.6464 - val_acc: 0.6923\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7312 - acc: 0.6610 - val_loss: 0.6328 - val_acc: 0.6923\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7168 - acc: 0.6610 - val_loss: 0.6194 - val_acc: 0.6923\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7023 - acc: 0.6695 - val_loss: 0.6059 - val_acc: 0.6923\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6883 - acc: 0.6780 - val_loss: 0.5932 - val_acc: 0.6923\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6751 - acc: 0.6949 - val_loss: 0.5810 - val_acc: 0.6923\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6625 - acc: 0.7034 - val_loss: 0.5693 - val_acc: 0.7692\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6503 - acc: 0.7119 - val_loss: 0.5580 - val_acc: 0.7692\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6383 - acc: 0.7458 - val_loss: 0.5463 - val_acc: 0.8462\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6264 - acc: 0.7542 - val_loss: 0.5357 - val_acc: 0.8462\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6148 - acc: 0.7542 - val_loss: 0.5248 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6036 - acc: 0.7627 - val_loss: 0.5146 - val_acc: 0.8462\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5928 - acc: 0.7627 - val_loss: 0.5044 - val_acc: 0.8462\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 299us/step - loss: 0.5825 - acc: 0.7712 - val_loss: 0.4946 - val_acc: 0.8462\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.5722 - acc: 0.7966 - val_loss: 0.4849 - val_acc: 0.8462\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5620 - acc: 0.8136 - val_loss: 0.4755 - val_acc: 0.8462\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.5520 - acc: 0.8136 - val_loss: 0.4667 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5423 - acc: 0.8220 - val_loss: 0.4580 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5329 - acc: 0.8220 - val_loss: 0.4498 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.5236 - acc: 0.8305 - val_loss: 0.4414 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5143 - acc: 0.8305 - val_loss: 0.4330 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.5054 - acc: 0.8305 - val_loss: 0.4249 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4965 - acc: 0.8305 - val_loss: 0.4170 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4880 - acc: 0.8305 - val_loss: 0.4093 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.4797 - acc: 0.8390 - val_loss: 0.4017 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.4715 - acc: 0.8475 - val_loss: 0.3942 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4634 - acc: 0.8559 - val_loss: 0.3867 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4554 - acc: 0.8559 - val_loss: 0.3794 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4477 - acc: 0.8559 - val_loss: 0.3722 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4401 - acc: 0.8729 - val_loss: 0.3654 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4326 - acc: 0.8814 - val_loss: 0.3587 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4253 - acc: 0.8898 - val_loss: 0.3523 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4183 - acc: 0.8983 - val_loss: 0.3462 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4113 - acc: 0.8983 - val_loss: 0.3399 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.4042 - acc: 0.8983 - val_loss: 0.3337 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3973 - acc: 0.8983 - val_loss: 0.3275 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.3905 - acc: 0.9068 - val_loss: 0.3215 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3838 - acc: 0.9153 - val_loss: 0.3154 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3775 - acc: 0.9153 - val_loss: 0.3096 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3710 - acc: 0.9237 - val_loss: 0.3038 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3646 - acc: 0.9237 - val_loss: 0.2982 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3583 - acc: 0.9322 - val_loss: 0.2926 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3522 - acc: 0.9322 - val_loss: 0.2871 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3461 - acc: 0.9407 - val_loss: 0.2816 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3403 - acc: 0.9492 - val_loss: 0.2764 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3346 - acc: 0.9492 - val_loss: 0.2712 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.3290 - acc: 0.9492 - val_loss: 0.2663 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.3237 - acc: 0.9492 - val_loss: 0.2614 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.3184 - acc: 0.9492 - val_loss: 0.2566 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.3133 - acc: 0.9492 - val_loss: 0.2519 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3084 - acc: 0.9492 - val_loss: 0.2466 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3037 - acc: 0.9492 - val_loss: 0.2412 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2990 - acc: 0.9492 - val_loss: 0.2360 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.2945 - acc: 0.9492 - val_loss: 0.2309 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.2902 - acc: 0.9492 - val_loss: 0.2259 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2860 - acc: 0.9492 - val_loss: 0.2212 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.2818 - acc: 0.9492 - val_loss: 0.2163 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2776 - acc: 0.9576 - val_loss: 0.2116 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2734 - acc: 0.9576 - val_loss: 0.2070 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.2693 - acc: 0.9661 - val_loss: 0.2023 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2653 - acc: 0.9661 - val_loss: 0.1979 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2614 - acc: 0.9661 - val_loss: 0.1937 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2576 - acc: 0.9661 - val_loss: 0.1893 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2539 - acc: 0.9661 - val_loss: 0.1853 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2503 - acc: 0.9661 - val_loss: 0.1813 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.2469 - acc: 0.9661 - val_loss: 0.1774 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2434 - acc: 0.9661 - val_loss: 0.1737 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2401 - acc: 0.9661 - val_loss: 0.1699 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2369 - acc: 0.9661 - val_loss: 0.1663 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2338 - acc: 0.9661 - val_loss: 0.1630 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2307 - acc: 0.9661 - val_loss: 0.1594 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2277 - acc: 0.9661 - val_loss: 0.1563 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2248 - acc: 0.9661 - val_loss: 0.1530 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2219 - acc: 0.9661 - val_loss: 0.1499 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2191 - acc: 0.9661 - val_loss: 0.1469 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2164 - acc: 0.9661 - val_loss: 0.1439 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2138 - acc: 0.9661 - val_loss: 0.1410 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2112 - acc: 0.9661 - val_loss: 0.1382 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.2087 - acc: 0.9661 - val_loss: 0.1355 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.2062 - acc: 0.9661 - val_loss: 0.1328 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2038 - acc: 0.9661 - val_loss: 0.1302 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2014 - acc: 0.9661 - val_loss: 0.1277 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1991 - acc: 0.9661 - val_loss: 0.1253 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1969 - acc: 0.9746 - val_loss: 0.1229 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1947 - acc: 0.9746 - val_loss: 0.1206 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.1925 - acc: 0.9746 - val_loss: 0.1183 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1904 - acc: 0.9746 - val_loss: 0.1161 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1883 - acc: 0.9746 - val_loss: 0.1140 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1863 - acc: 0.9746 - val_loss: 0.1119 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1843 - acc: 0.9746 - val_loss: 0.1099 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.1824 - acc: 0.9746 - val_loss: 0.1078 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1805 - acc: 0.9746 - val_loss: 0.1059 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 2.7319 - acc: 0.0000e+00 - val_loss: 2.6583 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 2.5305 - acc: 0.0000e+00 - val_loss: 2.4561 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 2.3494 - acc: 0.0000e+00 - val_loss: 2.2753 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 2.1880 - acc: 0.0000e+00 - val_loss: 2.1159 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 2.0452 - acc: 0.0000e+00 - val_loss: 1.9717 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.9179 - acc: 0.0000e+00 - val_loss: 1.8400 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.8028 - acc: 0.0000e+00 - val_loss: 1.7287 - val_acc: 0.0000e+00\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.7015 - acc: 0.0000e+00 - val_loss: 1.6275 - val_acc: 0.0000e+00\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.6105 - acc: 0.0000e+00 - val_loss: 1.5342 - val_acc: 0.0000e+00\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.5290 - acc: 0.0000e+00 - val_loss: 1.4522 - val_acc: 0.0000e+00\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.4552 - acc: 0.0169 - val_loss: 1.3765 - val_acc: 0.0000e+00\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.3872 - acc: 0.0254 - val_loss: 1.3075 - val_acc: 0.0000e+00\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.3258 - acc: 0.0424 - val_loss: 1.2447 - val_acc: 0.0000e+00\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2700 - acc: 0.0932 - val_loss: 1.1892 - val_acc: 0.1538\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.2183 - acc: 0.2373 - val_loss: 1.1384 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.1709 - acc: 0.3814 - val_loss: 1.0902 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 1.1274 - acc: 0.4831 - val_loss: 1.0469 - val_acc: 0.7692\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 1.0873 - acc: 0.5593 - val_loss: 1.0066 - val_acc: 0.7692\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0499 - acc: 0.6017 - val_loss: 0.9685 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0142 - acc: 0.6780 - val_loss: 0.9319 - val_acc: 0.8462\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9808 - acc: 0.7288 - val_loss: 0.8968 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.9493 - acc: 0.7712 - val_loss: 0.8655 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9197 - acc: 0.7712 - val_loss: 0.8353 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8914 - acc: 0.7966 - val_loss: 0.8060 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8642 - acc: 0.8136 - val_loss: 0.7792 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8390 - acc: 0.8305 - val_loss: 0.7533 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.8148 - acc: 0.8390 - val_loss: 0.7275 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7914 - acc: 0.8475 - val_loss: 0.7042 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.7696 - acc: 0.8475 - val_loss: 0.6814 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7487 - acc: 0.8475 - val_loss: 0.6591 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7288 - acc: 0.8475 - val_loss: 0.6386 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7096 - acc: 0.8559 - val_loss: 0.6189 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6918 - acc: 0.8559 - val_loss: 0.6002 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6743 - acc: 0.8559 - val_loss: 0.5822 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6576 - acc: 0.8644 - val_loss: 0.5650 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6415 - acc: 0.8644 - val_loss: 0.5490 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6262 - acc: 0.8644 - val_loss: 0.5332 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6113 - acc: 0.8644 - val_loss: 0.5184 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.5971 - acc: 0.8644 - val_loss: 0.5037 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5835 - acc: 0.8644 - val_loss: 0.4898 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.5703 - acc: 0.8644 - val_loss: 0.4767 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.5577 - acc: 0.8644 - val_loss: 0.4640 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5453 - acc: 0.8644 - val_loss: 0.4519 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.5333 - acc: 0.8644 - val_loss: 0.4403 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.5219 - acc: 0.8644 - val_loss: 0.4289 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.5108 - acc: 0.8644 - val_loss: 0.4181 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4999 - acc: 0.8644 - val_loss: 0.4077 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4895 - acc: 0.8644 - val_loss: 0.3977 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.4794 - acc: 0.8644 - val_loss: 0.3880 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.4696 - acc: 0.8644 - val_loss: 0.3787 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4600 - acc: 0.8644 - val_loss: 0.3697 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4508 - acc: 0.8644 - val_loss: 0.3610 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4417 - acc: 0.8644 - val_loss: 0.3527 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.4329 - acc: 0.8729 - val_loss: 0.3446 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 290us/step - loss: 0.4243 - acc: 0.8898 - val_loss: 0.3368 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4160 - acc: 0.8898 - val_loss: 0.3294 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4079 - acc: 0.8898 - val_loss: 0.3220 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3999 - acc: 0.9068 - val_loss: 0.3150 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.3921 - acc: 0.9237 - val_loss: 0.3082 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.3847 - acc: 0.9237 - val_loss: 0.3015 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3773 - acc: 0.9237 - val_loss: 0.2952 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3700 - acc: 0.9322 - val_loss: 0.2890 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3629 - acc: 0.9407 - val_loss: 0.2829 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3560 - acc: 0.9576 - val_loss: 0.2771 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3493 - acc: 0.9746 - val_loss: 0.2714 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3427 - acc: 0.9746 - val_loss: 0.2659 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3362 - acc: 0.9746 - val_loss: 0.2605 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3298 - acc: 0.9661 - val_loss: 0.2553 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.3236 - acc: 0.9661 - val_loss: 0.2502 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3176 - acc: 0.9661 - val_loss: 0.2452 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3116 - acc: 0.9661 - val_loss: 0.2404 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3058 - acc: 0.9661 - val_loss: 0.2357 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3001 - acc: 0.9661 - val_loss: 0.2311 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2945 - acc: 0.9661 - val_loss: 0.2267 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2889 - acc: 0.9661 - val_loss: 0.2223 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2837 - acc: 0.9746 - val_loss: 0.2181 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.2784 - acc: 0.9746 - val_loss: 0.2140 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2733 - acc: 0.9831 - val_loss: 0.2100 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.2683 - acc: 0.9831 - val_loss: 0.2060 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.2635 - acc: 0.9831 - val_loss: 0.2022 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.2587 - acc: 0.9831 - val_loss: 0.1985 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2541 - acc: 0.9831 - val_loss: 0.1948 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2495 - acc: 0.9831 - val_loss: 0.1913 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2450 - acc: 0.9831 - val_loss: 0.1878 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2406 - acc: 0.9831 - val_loss: 0.1844 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.2364 - acc: 0.9831 - val_loss: 0.1811 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.2322 - acc: 0.9831 - val_loss: 0.1779 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2281 - acc: 0.9831 - val_loss: 0.1747 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2242 - acc: 0.9831 - val_loss: 0.1716 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.2204 - acc: 0.9831 - val_loss: 0.1686 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2166 - acc: 0.9831 - val_loss: 0.1657 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.2129 - acc: 0.9831 - val_loss: 0.1628 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2093 - acc: 0.9831 - val_loss: 0.1600 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2059 - acc: 0.9831 - val_loss: 0.1572 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2025 - acc: 0.9831 - val_loss: 0.1546 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1992 - acc: 0.9831 - val_loss: 0.1520 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1960 - acc: 0.9831 - val_loss: 0.1494 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1929 - acc: 0.9831 - val_loss: 0.1469 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.1898 - acc: 0.9831 - val_loss: 0.1445 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1869 - acc: 0.9831 - val_loss: 0.1422 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "ed2e10db-1c89-4ab2-f9f1-8b60c306ca46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.46815712,  2.41613978],\n",
              "       [-2.04213808, -1.20055142],\n",
              "       [-2.26598031, -2.51797746],\n",
              "       [-1.62561816, -1.97201226],\n",
              "       [-2.12949607, -1.76064808],\n",
              "       [-1.32853889, -2.05415965],\n",
              "       [-1.86366758, -1.96105974],\n",
              "       [ 3.1426414 ,  0.23838493],\n",
              "       [ 2.76856724,  0.87507987],\n",
              "       [ 3.33006485,  0.80105572],\n",
              "       [ 3.99255757,  0.1366972 ],\n",
              "       [ 2.79798337,  1.20174003],\n",
              "       [ 1.67824072,  0.59967329]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6258d123-db07-4e0f-b535-a81762d57aaa",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "96b44564-2a57-45c6-b946-d33353e93c59",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1a0dfb93-bd0d-4620-973c-58bbba5e512e",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0bebcc47-98d2-4160-b5b9-d83ecb390fe6",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc7526142e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0421c20d-e162-4522-af93-12beb6d8131a",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc75259be10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff332cea-1abf-49a0-801c-a217ea7306b0",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 1.5186 - acc: 0.4504\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.3946 - acc: 0.4504\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2830 - acc: 0.4504\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 1.1909 - acc: 0.4504\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.1125 - acc: 0.4580\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0460 - acc: 0.4656\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9867 - acc: 0.4656\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.9369 - acc: 0.4656\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8962 - acc: 0.4733\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8603 - acc: 0.4885\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8286 - acc: 0.4885\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8009 - acc: 0.4885\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7763 - acc: 0.4885\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7543 - acc: 0.5038\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.7345 - acc: 0.5496\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.7168 - acc: 0.6183\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.7005 - acc: 0.6641\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6855 - acc: 0.7252\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.6720 - acc: 0.7481\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6595 - acc: 0.7710\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.6475 - acc: 0.8168\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.6366 - acc: 0.8626\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.6264 - acc: 0.9008\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6168 - acc: 0.9237\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.6084 - acc: 0.9313\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6001 - acc: 0.9389\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.5919 - acc: 0.9542\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5842 - acc: 0.9618\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.5768 - acc: 0.9618\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.5695 - acc: 0.9771\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5628 - acc: 0.9771\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5561 - acc: 0.9771\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.5495 - acc: 0.9847\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.5434 - acc: 0.9847\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.5373 - acc: 0.9847\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.5314 - acc: 0.9847\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.5257 - acc: 0.9847\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.5201 - acc: 0.9847\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5148 - acc: 0.9847\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.5095 - acc: 0.9847\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.5044 - acc: 0.9847\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4993 - acc: 0.9847\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4942 - acc: 0.9847\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4892 - acc: 0.9847\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4845 - acc: 0.9847\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4800 - acc: 0.9847\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.4753 - acc: 0.9847\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4708 - acc: 0.9847\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4664 - acc: 0.9847\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4620 - acc: 0.9847\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.4577 - acc: 0.9847\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.4535 - acc: 0.9847\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.4495 - acc: 0.9847\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.4455 - acc: 0.9847\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4415 - acc: 0.9847\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.4378 - acc: 0.9847\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.4339 - acc: 0.9847\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4301 - acc: 0.9847\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4265 - acc: 0.9847\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4229 - acc: 0.9847\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.4193 - acc: 0.9847\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.4158 - acc: 0.9847\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.4124 - acc: 0.9847\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.4091 - acc: 0.9847\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4057 - acc: 0.9847\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4024 - acc: 0.9847\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3992 - acc: 0.9847\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3960 - acc: 0.9847\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3929 - acc: 0.9847\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.3899 - acc: 0.9847\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.3869 - acc: 0.9847\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.3839 - acc: 0.9847\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.3811 - acc: 0.9847\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.3781 - acc: 0.9847\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.3753 - acc: 0.9847\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.3725 - acc: 0.9847\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.3698 - acc: 0.9847\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.3671 - acc: 0.9847\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3644 - acc: 0.9847\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.3618 - acc: 0.9847\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.3592 - acc: 0.9847\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3567 - acc: 0.9847\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.3542 - acc: 0.9847\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3517 - acc: 0.9847\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.3493 - acc: 0.9847\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.3469 - acc: 0.9847\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.3445 - acc: 0.9847\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.3422 - acc: 0.9847\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.3399 - acc: 0.9847\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.3376 - acc: 0.9847\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.3354 - acc: 0.9847\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.3332 - acc: 0.9847\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.3310 - acc: 0.9847\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3288 - acc: 0.9847\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.3266 - acc: 0.9847\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.3245 - acc: 0.9847\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.3225 - acc: 0.9847\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.3204 - acc: 0.9847\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3184 - acc: 0.9847\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.3164 - acc: 0.9847\n",
            "34/34 [==============================] - 0s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6674ae6e-b918-4816-c3b7-03ee68004941",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "24e7993b-d567-45db-84cb-b76989be024e",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11764705882352941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZN9I7JNNUyS",
        "colab_type": "text"
      },
      "source": [
        "# Prova remove correlated features with treshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wACff-R4Ib",
        "colab_type": "text"
      },
      "source": [
        "https://campus.datacamp.com/courses/dimensionality-reduction-in-python/feature-selection-i-selecting-for-feature-information?ex=14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNufM8B3NYs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a positive correlation matrix\n",
        "corr_df = train_data_stand.corr().abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8ShRUvN63m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create and apply mask\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DgfSECpOwPu",
        "colab_type": "code",
        "outputId": "c98d4882-932f-4d61-83fc-163d5fb0c92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [False,  True,  True, ...,  True,  True,  True],\n",
              "       [False, False,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [False, False, False, ...,  True,  True,  True],\n",
              "       [False, False, False, ..., False,  True,  True],\n",
              "       [False, False, False, ..., False, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xQLptVOw6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri_df = corr_df.mask(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDvePEPO8mn",
        "colab_type": "code",
        "outputId": "c6497e18-7ee6-4777-b2d1-c34fa3b0241e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tri_df"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>VoxelVolume</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <td>0.821982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MeshVolume</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <td>0.785606</td>\n",
              "      <td>0.964760</td>\n",
              "      <td>0.785457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sphericity</th>\n",
              "      <td>0.329751</td>\n",
              "      <td>0.678628</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.694906</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coarseness</th>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.586992</td>\n",
              "      <td>0.401370</td>\n",
              "      <td>0.546184</td>\n",
              "      <td>0.356708</td>\n",
              "      <td>0.569919</td>\n",
              "      <td>0.048602</td>\n",
              "      <td>0.813595</td>\n",
              "      <td>0.614989</td>\n",
              "      <td>0.094676</td>\n",
              "      <td>0.471734</td>\n",
              "      <td>0.598815</td>\n",
              "      <td>0.567029</td>\n",
              "      <td>0.554227</td>\n",
              "      <td>0.192052</td>\n",
              "      <td>0.605729</td>\n",
              "      <td>0.246122</td>\n",
              "      <td>0.406714</td>\n",
              "      <td>0.351606</td>\n",
              "      <td>0.669357</td>\n",
              "      <td>0.183179</td>\n",
              "      <td>0.731964</td>\n",
              "      <td>0.417524</td>\n",
              "      <td>0.077465</td>\n",
              "      <td>0.379163</td>\n",
              "      <td>0.437111</td>\n",
              "      <td>0.787425</td>\n",
              "      <td>0.182358</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>0.119708</td>\n",
              "      <td>0.324753</td>\n",
              "      <td>0.661086</td>\n",
              "      <td>0.326758</td>\n",
              "      <td>0.468498</td>\n",
              "      <td>0.481677</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.319663</td>\n",
              "      <td>0.667850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350082</td>\n",
              "      <td>0.292163</td>\n",
              "      <td>0.539925</td>\n",
              "      <td>0.345026</td>\n",
              "      <td>0.148147</td>\n",
              "      <td>0.251303</td>\n",
              "      <td>0.399211</td>\n",
              "      <td>0.336092</td>\n",
              "      <td>0.360524</td>\n",
              "      <td>0.381037</td>\n",
              "      <td>0.556410</td>\n",
              "      <td>0.412333</td>\n",
              "      <td>0.521415</td>\n",
              "      <td>0.480184</td>\n",
              "      <td>0.512788</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.194296</td>\n",
              "      <td>0.595126</td>\n",
              "      <td>0.552550</td>\n",
              "      <td>0.201612</td>\n",
              "      <td>0.272599</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.374723</td>\n",
              "      <td>0.426885</td>\n",
              "      <td>0.417651</td>\n",
              "      <td>0.272696</td>\n",
              "      <td>0.501061</td>\n",
              "      <td>0.667822</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.511486</td>\n",
              "      <td>0.366279</td>\n",
              "      <td>0.704958</td>\n",
              "      <td>0.523589</td>\n",
              "      <td>0.754391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Complexity</th>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.257297</td>\n",
              "      <td>0.277025</td>\n",
              "      <td>0.229141</td>\n",
              "      <td>0.182615</td>\n",
              "      <td>0.268450</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.214902</td>\n",
              "      <td>0.256226</td>\n",
              "      <td>0.019975</td>\n",
              "      <td>0.283738</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>0.441669</td>\n",
              "      <td>0.213055</td>\n",
              "      <td>0.423394</td>\n",
              "      <td>0.392267</td>\n",
              "      <td>0.103854</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.422629</td>\n",
              "      <td>0.058114</td>\n",
              "      <td>0.198407</td>\n",
              "      <td>0.214767</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.125140</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.275899</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.322120</td>\n",
              "      <td>0.410309</td>\n",
              "      <td>0.276311</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>0.275068</td>\n",
              "      <td>0.221594</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.125930</td>\n",
              "      <td>0.298911</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267658</td>\n",
              "      <td>0.198372</td>\n",
              "      <td>0.116352</td>\n",
              "      <td>0.255976</td>\n",
              "      <td>0.464523</td>\n",
              "      <td>0.267090</td>\n",
              "      <td>0.228132</td>\n",
              "      <td>0.195523</td>\n",
              "      <td>0.139835</td>\n",
              "      <td>0.198482</td>\n",
              "      <td>0.296969</td>\n",
              "      <td>0.359280</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>0.168824</td>\n",
              "      <td>0.228273</td>\n",
              "      <td>0.355694</td>\n",
              "      <td>0.220089</td>\n",
              "      <td>0.151630</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.011797</td>\n",
              "      <td>0.469854</td>\n",
              "      <td>0.036549</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.274863</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.351102</td>\n",
              "      <td>0.063147</td>\n",
              "      <td>0.152285</td>\n",
              "      <td>0.074162</td>\n",
              "      <td>0.313890</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.083326</td>\n",
              "      <td>0.166795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strength</th>\n",
              "      <td>0.510155</td>\n",
              "      <td>0.642559</td>\n",
              "      <td>0.509976</td>\n",
              "      <td>0.607597</td>\n",
              "      <td>0.339595</td>\n",
              "      <td>0.645544</td>\n",
              "      <td>0.021965</td>\n",
              "      <td>0.758610</td>\n",
              "      <td>0.670268</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.562243</td>\n",
              "      <td>0.659201</td>\n",
              "      <td>0.624228</td>\n",
              "      <td>0.620794</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.388883</td>\n",
              "      <td>0.341710</td>\n",
              "      <td>0.538344</td>\n",
              "      <td>0.425689</td>\n",
              "      <td>0.458791</td>\n",
              "      <td>0.226763</td>\n",
              "      <td>0.486770</td>\n",
              "      <td>0.290047</td>\n",
              "      <td>0.085559</td>\n",
              "      <td>0.272032</td>\n",
              "      <td>0.325476</td>\n",
              "      <td>0.564383</td>\n",
              "      <td>0.104481</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.099738</td>\n",
              "      <td>0.184149</td>\n",
              "      <td>0.141523</td>\n",
              "      <td>0.432332</td>\n",
              "      <td>0.139576</td>\n",
              "      <td>0.296993</td>\n",
              "      <td>0.271914</td>\n",
              "      <td>0.332061</td>\n",
              "      <td>0.195143</td>\n",
              "      <td>0.422246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126145</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.283099</td>\n",
              "      <td>0.224183</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>0.155122</td>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.240018</td>\n",
              "      <td>0.438144</td>\n",
              "      <td>0.264077</td>\n",
              "      <td>0.333647</td>\n",
              "      <td>0.530074</td>\n",
              "      <td>0.343866</td>\n",
              "      <td>0.337177</td>\n",
              "      <td>0.347487</td>\n",
              "      <td>0.027798</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.372899</td>\n",
              "      <td>0.365016</td>\n",
              "      <td>0.083608</td>\n",
              "      <td>0.329544</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.390861</td>\n",
              "      <td>0.567592</td>\n",
              "      <td>0.552634</td>\n",
              "      <td>0.329531</td>\n",
              "      <td>0.225342</td>\n",
              "      <td>0.451164</td>\n",
              "      <td>0.206294</td>\n",
              "      <td>0.221538</td>\n",
              "      <td>0.253885</td>\n",
              "      <td>0.383804</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.544355</td>\n",
              "      <td>0.611316</td>\n",
              "      <td>0.746738</td>\n",
              "      <td>0.074615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contrast.1</th>\n",
              "      <td>0.458069</td>\n",
              "      <td>0.592542</td>\n",
              "      <td>0.457835</td>\n",
              "      <td>0.555373</td>\n",
              "      <td>0.380133</td>\n",
              "      <td>0.588236</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>0.639843</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.065998</td>\n",
              "      <td>0.515229</td>\n",
              "      <td>0.623406</td>\n",
              "      <td>0.576338</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.699380</td>\n",
              "      <td>0.654218</td>\n",
              "      <td>0.165224</td>\n",
              "      <td>0.399133</td>\n",
              "      <td>0.462206</td>\n",
              "      <td>0.847990</td>\n",
              "      <td>0.404501</td>\n",
              "      <td>0.807251</td>\n",
              "      <td>0.503620</td>\n",
              "      <td>0.040094</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>0.507856</td>\n",
              "      <td>0.811825</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.674231</td>\n",
              "      <td>0.184830</td>\n",
              "      <td>0.427215</td>\n",
              "      <td>0.958897</td>\n",
              "      <td>0.507007</td>\n",
              "      <td>0.941239</td>\n",
              "      <td>0.822280</td>\n",
              "      <td>0.836388</td>\n",
              "      <td>0.858476</td>\n",
              "      <td>0.916872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777488</td>\n",
              "      <td>0.592929</td>\n",
              "      <td>0.882098</td>\n",
              "      <td>0.561351</td>\n",
              "      <td>0.652744</td>\n",
              "      <td>0.457706</td>\n",
              "      <td>0.693751</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.473433</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.601669</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.679507</td>\n",
              "      <td>0.566054</td>\n",
              "      <td>0.635554</td>\n",
              "      <td>0.154862</td>\n",
              "      <td>0.617903</td>\n",
              "      <td>0.665488</td>\n",
              "      <td>0.705727</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.390378</td>\n",
              "      <td>0.398000</td>\n",
              "      <td>0.502327</td>\n",
              "      <td>0.354174</td>\n",
              "      <td>0.437122</td>\n",
              "      <td>0.390572</td>\n",
              "      <td>0.610009</td>\n",
              "      <td>0.852276</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.242269</td>\n",
              "      <td>0.640887</td>\n",
              "      <td>0.494642</td>\n",
              "      <td>0.794082</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.759051</td>\n",
              "      <td>0.551508</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.374697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Busyness</th>\n",
              "      <td>0.937894</td>\n",
              "      <td>0.810919</td>\n",
              "      <td>0.937881</td>\n",
              "      <td>0.773599</td>\n",
              "      <td>0.339451</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.682675</td>\n",
              "      <td>0.843581</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>0.913619</td>\n",
              "      <td>0.867644</td>\n",
              "      <td>0.805965</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>0.176933</td>\n",
              "      <td>0.469093</td>\n",
              "      <td>0.023465</td>\n",
              "      <td>0.890974</td>\n",
              "      <td>0.884836</td>\n",
              "      <td>0.556691</td>\n",
              "      <td>0.356571</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.253053</td>\n",
              "      <td>0.425932</td>\n",
              "      <td>0.366938</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.364459</td>\n",
              "      <td>0.187027</td>\n",
              "      <td>0.262478</td>\n",
              "      <td>0.474823</td>\n",
              "      <td>0.313668</td>\n",
              "      <td>0.411803</td>\n",
              "      <td>0.469366</td>\n",
              "      <td>0.490211</td>\n",
              "      <td>0.354087</td>\n",
              "      <td>0.516006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294360</td>\n",
              "      <td>0.307158</td>\n",
              "      <td>0.382443</td>\n",
              "      <td>0.108961</td>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>0.428333</td>\n",
              "      <td>0.320272</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.365808</td>\n",
              "      <td>0.400702</td>\n",
              "      <td>0.915079</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.448260</td>\n",
              "      <td>0.462777</td>\n",
              "      <td>0.126498</td>\n",
              "      <td>0.276150</td>\n",
              "      <td>0.466663</td>\n",
              "      <td>0.497241</td>\n",
              "      <td>0.130804</td>\n",
              "      <td>0.770513</td>\n",
              "      <td>0.135920</td>\n",
              "      <td>0.372307</td>\n",
              "      <td>0.858147</td>\n",
              "      <td>0.912460</td>\n",
              "      <td>0.770458</td>\n",
              "      <td>0.381146</td>\n",
              "      <td>0.557089</td>\n",
              "      <td>0.516059</td>\n",
              "      <td>0.592938</td>\n",
              "      <td>0.403923</td>\n",
              "      <td>0.370737</td>\n",
              "      <td>0.363137</td>\n",
              "      <td>0.403091</td>\n",
              "      <td>0.382465</td>\n",
              "      <td>0.434561</td>\n",
              "      <td>0.093528</td>\n",
              "      <td>0.602417</td>\n",
              "      <td>0.418697</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "VoxelVolume                NaN                NaN  ...         NaN       NaN\n",
              "Maximum3DDiameter     0.821982                NaN  ...         NaN       NaN\n",
              "MeshVolume            0.999999           0.821800  ...         NaN       NaN\n",
              "MajorAxisLength       0.785606           0.964760  ...         NaN       NaN\n",
              "Sphericity            0.329751           0.678628  ...         NaN       NaN\n",
              "...                        ...                ...  ...         ...       ...\n",
              "Coarseness            0.401634           0.586992  ...         NaN       NaN\n",
              "Complexity            0.277302           0.257297  ...         NaN       NaN\n",
              "Strength              0.510155           0.642559  ...         NaN       NaN\n",
              "Contrast.1            0.458069           0.592542  ...         NaN       NaN\n",
              "Busyness              0.937894           0.810919  ...    0.418697       NaN\n",
              "\n",
              "[107 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1MkWP2O9hU",
        "colab_type": "code",
        "outputId": "309ee04b-1434-49d0-eab3-d31dda519d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Find columns that meet threshold\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.70)]\n",
        "to_drop"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VoxelVolume',\n",
              " 'Maximum3DDiameter',\n",
              " 'MeshVolume',\n",
              " 'MajorAxisLength',\n",
              " 'LeastAxisLength',\n",
              " 'Elongation',\n",
              " 'SurfaceVolumeRatio',\n",
              " 'Maximum2DDiameterSlice',\n",
              " 'SurfaceArea',\n",
              " 'MinorAxisLength',\n",
              " 'Maximum2DDiameterColumn',\n",
              " 'Maximum2DDiameterRow',\n",
              " 'GrayLevelVariance',\n",
              " 'HighGrayLevelEmphasis',\n",
              " 'DependenceEntropy',\n",
              " 'DependenceNonUniformity',\n",
              " 'GrayLevelNonUniformity',\n",
              " 'SmallDependenceEmphasis',\n",
              " 'SmallDependenceHighGrayLevelEmphasis',\n",
              " 'DependenceNonUniformityNormalized',\n",
              " 'LargeDependenceEmphasis',\n",
              " 'LargeDependenceLowGrayLevelEmphasis',\n",
              " 'DependenceVariance',\n",
              " 'LargeDependenceHighGrayLevelEmphasis',\n",
              " 'SmallDependenceLowGrayLevelEmphasis',\n",
              " 'LowGrayLevelEmphasis',\n",
              " 'JointAverage',\n",
              " 'SumAverage',\n",
              " 'JointEntropy',\n",
              " 'ClusterShade',\n",
              " 'MaximumProbability',\n",
              " 'Idmn',\n",
              " 'JointEnergy',\n",
              " 'Contrast',\n",
              " 'DifferenceEntropy',\n",
              " 'InverseVariance',\n",
              " 'DifferenceVariance',\n",
              " 'Idn',\n",
              " 'Idm',\n",
              " 'Correlation',\n",
              " 'Autocorrelation',\n",
              " 'SumEntropy',\n",
              " 'SumSquares',\n",
              " 'ClusterProminence',\n",
              " 'Imc2',\n",
              " 'DifferenceAverage',\n",
              " 'Id',\n",
              " 'ClusterTendency',\n",
              " 'InterquartileRange',\n",
              " 'Skewness',\n",
              " 'Uniformity',\n",
              " 'Median',\n",
              " 'Energy',\n",
              " 'RobustMeanAbsoluteDeviation',\n",
              " 'MeanAbsoluteDeviation',\n",
              " 'Maximum',\n",
              " 'RootMeanSquared',\n",
              " 'Minimum',\n",
              " 'Entropy',\n",
              " 'Range',\n",
              " 'Variance',\n",
              " '10Percentile',\n",
              " 'Kurtosis',\n",
              " 'Mean',\n",
              " 'ShortRunLowGrayLevelEmphasis',\n",
              " 'GrayLevelVariance.1',\n",
              " 'LowGrayLevelRunEmphasis',\n",
              " 'GrayLevelNonUniformityNormalized',\n",
              " 'RunVariance',\n",
              " 'GrayLevelNonUniformity.1',\n",
              " 'LongRunEmphasis',\n",
              " 'ShortRunHighGrayLevelEmphasis',\n",
              " 'RunLengthNonUniformity',\n",
              " 'ShortRunEmphasis',\n",
              " 'LongRunHighGrayLevelEmphasis',\n",
              " 'RunPercentage',\n",
              " 'LongRunLowGrayLevelEmphasis',\n",
              " 'RunEntropy',\n",
              " 'HighGrayLevelRunEmphasis',\n",
              " 'RunLengthNonUniformityNormalized',\n",
              " 'ZoneVariance',\n",
              " 'SizeZoneNonUniformityNormalized',\n",
              " 'SizeZoneNonUniformity',\n",
              " 'GrayLevelNonUniformity.2',\n",
              " 'LargeAreaEmphasis',\n",
              " 'SmallAreaHighGrayLevelEmphasis',\n",
              " 'ZonePercentage',\n",
              " 'LowGrayLevelZoneEmphasis',\n",
              " 'SmallAreaLowGrayLevelEmphasis',\n",
              " 'Coarseness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKeereQoci",
        "colab_type": "text"
      },
      "source": [
        "The reason we used the mask to set half of the matrix to NA value is that we ewnt to avoid removing both features when thay have a strong correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R-tG-WPNXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "train_data_stand_reduced = train_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNUiUoZ0Vdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "test_data_stand_reduced = test_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-uuBifRO8G",
        "colab_type": "code",
        "outputId": "e26ac720-a506-41e0-f291-516147d6dc63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wuGcLV0acQ",
        "colab_type": "code",
        "outputId": "ee0a33ce-37a9-47b5-850a-b03e637bedd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data_stand_reduced.shape"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1qR3re7QwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_reduced = train_data_stand_reduced.to_numpy()\n",
        "test_data_stand_reduced = test_data_stand_reduced.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUG3a0o5W1O",
        "colab_type": "code",
        "outputId": "163e8154-886a-42aa-b243-55093b35a5fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8wkFES71GJ",
        "colab_type": "code",
        "outputId": "9d715ae3-0cee-4859-d9e4-c9ed96fcef51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljMaQeuSHe1",
        "colab_type": "text"
      },
      "source": [
        "funziona bene, però bisogna stare attenti a basarsi unicamente sul coefficiente di correlazione. Se y = x^2, x e y risulteranno scorrelate secondo il coeffiente di correlazione di Pearson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxuH4u741mLR",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(17,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmeGHBV1xs_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5hBUph6149E"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7NwNpEO149J"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MD1VB0S149O",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "22233dcd-3de6-4bcc-f6d6-c1efeef72f03",
        "id": "VkpRnrey149Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_reduced, train_labels_dec)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bff58c91-6a90-4db2-de20-624a8c37146b",
        "id": "F3zK4E6o149g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9cedccbb-ab3b-4793-91b6-78c2151e52dc",
        "id": "BDX_0MCd149o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9QlfChU149v",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvCKZfjj1490",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bjrjhWo1497"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Du7DFfm1498",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qsz_jnVasri",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "78c39b83-6e4c-4dac-805f-bb545bfc058c"
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8920408e-cb93-4a79-b196-6ab02a8d4316",
        "id": "kHNpJxBL14-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_reduced = []\n",
        "all_loss_histories_reduced = []\n",
        "all_val_acc_histories_reduced = []\n",
        "all_val_loss_histories_reduced = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_reduced[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_reduced[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history_reduced = history.history['acc']\n",
        "  all_acc_histories_reduced.append(acc_history)\n",
        "\n",
        "  loss_history_reduced = history.history['loss']\n",
        "  all_loss_histories_reduced.append(loss_history)\n",
        "\n",
        "  acc_val_history_reduced = history.history['val_acc']\n",
        "  all_val_acc_histories_reduced.append(acc_val_history)\n",
        "\n",
        "  loss_val_history_reduced = history.history['val_loss']\n",
        "  all_val_loss_histories_reduced.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 1s 9ms/step - loss: 1.3005 - acc: 0.3504 - val_loss: 1.1231 - val_acc: 0.3571\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 1.1510 - acc: 0.3504 - val_loss: 1.0852 - val_acc: 0.5714\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 1.0750 - acc: 0.4188 - val_loss: 1.0689 - val_acc: 0.5714\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 213us/step - loss: 1.0374 - acc: 0.4274 - val_loss: 1.0604 - val_acc: 0.5714\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 1.0102 - acc: 0.4615 - val_loss: 1.0525 - val_acc: 0.5714\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.9960 - acc: 0.4957 - val_loss: 1.0514 - val_acc: 0.5714\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9810 - acc: 0.5214 - val_loss: 1.0499 - val_acc: 0.6429\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.9721 - acc: 0.5299 - val_loss: 1.0482 - val_acc: 0.6429\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9638 - acc: 0.5556 - val_loss: 1.0481 - val_acc: 0.6429\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.9519 - acc: 0.5299 - val_loss: 1.0468 - val_acc: 0.6429\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9456 - acc: 0.5726 - val_loss: 1.0472 - val_acc: 0.6429\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.9407 - acc: 0.5726 - val_loss: 1.0475 - val_acc: 0.6429\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.9324 - acc: 0.5556 - val_loss: 1.0483 - val_acc: 0.6429\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.9275 - acc: 0.5726 - val_loss: 1.0482 - val_acc: 0.6429\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 284us/step - loss: 0.9206 - acc: 0.5726 - val_loss: 1.0490 - val_acc: 0.7143\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 284us/step - loss: 0.9162 - acc: 0.5726 - val_loss: 1.0481 - val_acc: 0.6429\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.9115 - acc: 0.5726 - val_loss: 1.0496 - val_acc: 0.5714\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 232us/step - loss: 0.9099 - acc: 0.5726 - val_loss: 1.0494 - val_acc: 0.5714\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 225us/step - loss: 0.9031 - acc: 0.5726 - val_loss: 1.0485 - val_acc: 0.5714\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.8985 - acc: 0.5726 - val_loss: 1.0490 - val_acc: 0.5714\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.8961 - acc: 0.5726 - val_loss: 1.0497 - val_acc: 0.5714\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.8906 - acc: 0.5641 - val_loss: 1.0502 - val_acc: 0.5714\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.8864 - acc: 0.5641 - val_loss: 1.0507 - val_acc: 0.5714\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.8825 - acc: 0.5726 - val_loss: 1.0521 - val_acc: 0.5714\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 225us/step - loss: 0.8783 - acc: 0.5726 - val_loss: 1.0542 - val_acc: 0.5714\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 0.8773 - acc: 0.5641 - val_loss: 1.0540 - val_acc: 0.5714\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 219us/step - loss: 0.8737 - acc: 0.5726 - val_loss: 1.0537 - val_acc: 0.5714\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.8701 - acc: 0.5812 - val_loss: 1.0551 - val_acc: 0.5714\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.8681 - acc: 0.5897 - val_loss: 1.0539 - val_acc: 0.5714\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.8645 - acc: 0.5897 - val_loss: 1.0541 - val_acc: 0.5714\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.8616 - acc: 0.5812 - val_loss: 1.0557 - val_acc: 0.5714\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.8563 - acc: 0.5812 - val_loss: 1.0585 - val_acc: 0.5714\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.8536 - acc: 0.5812 - val_loss: 1.0591 - val_acc: 0.5714\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.8516 - acc: 0.5812 - val_loss: 1.0620 - val_acc: 0.5714\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 227us/step - loss: 0.8495 - acc: 0.5897 - val_loss: 1.0650 - val_acc: 0.5714\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 236us/step - loss: 0.8452 - acc: 0.5556 - val_loss: 1.0672 - val_acc: 0.5714\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.8406 - acc: 0.5897 - val_loss: 1.0681 - val_acc: 0.5714\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.8408 - acc: 0.5983 - val_loss: 1.0688 - val_acc: 0.5714\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 216us/step - loss: 0.8362 - acc: 0.5812 - val_loss: 1.0725 - val_acc: 0.5714\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 234us/step - loss: 0.8335 - acc: 0.5726 - val_loss: 1.0752 - val_acc: 0.5714\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 221us/step - loss: 0.8301 - acc: 0.5897 - val_loss: 1.0768 - val_acc: 0.5714\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.8278 - acc: 0.5983 - val_loss: 1.0781 - val_acc: 0.5714\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.8222 - acc: 0.5812 - val_loss: 1.0799 - val_acc: 0.5714\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.8211 - acc: 0.5897 - val_loss: 1.0815 - val_acc: 0.5714\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 246us/step - loss: 0.8176 - acc: 0.5812 - val_loss: 1.0852 - val_acc: 0.5714\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.8139 - acc: 0.5897 - val_loss: 1.0853 - val_acc: 0.5714\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 220us/step - loss: 0.8099 - acc: 0.5983 - val_loss: 1.0858 - val_acc: 0.5714\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.8066 - acc: 0.6154 - val_loss: 1.0905 - val_acc: 0.5714\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.8046 - acc: 0.6154 - val_loss: 1.0921 - val_acc: 0.5714\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.8019 - acc: 0.5983 - val_loss: 1.0932 - val_acc: 0.5714\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.7986 - acc: 0.6068 - val_loss: 1.0932 - val_acc: 0.5714\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.7964 - acc: 0.6154 - val_loss: 1.0915 - val_acc: 0.5714\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.7944 - acc: 0.6239 - val_loss: 1.0954 - val_acc: 0.5714\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.7924 - acc: 0.6068 - val_loss: 1.0929 - val_acc: 0.5714\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.7919 - acc: 0.6154 - val_loss: 1.0923 - val_acc: 0.5714\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.7853 - acc: 0.6154 - val_loss: 1.0943 - val_acc: 0.5000\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 215us/step - loss: 0.7838 - acc: 0.6239 - val_loss: 1.0954 - val_acc: 0.5000\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.7797 - acc: 0.6154 - val_loss: 1.0966 - val_acc: 0.5000\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.7770 - acc: 0.6154 - val_loss: 1.0960 - val_acc: 0.5000\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.7770 - acc: 0.6068 - val_loss: 1.0974 - val_acc: 0.5000\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.7740 - acc: 0.6239 - val_loss: 1.0954 - val_acc: 0.5000\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.7701 - acc: 0.6154 - val_loss: 1.0969 - val_acc: 0.5000\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.7699 - acc: 0.6325 - val_loss: 1.1001 - val_acc: 0.5000\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 214us/step - loss: 0.7667 - acc: 0.6154 - val_loss: 1.0995 - val_acc: 0.5000\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 189us/step - loss: 0.7654 - acc: 0.6325 - val_loss: 1.1035 - val_acc: 0.5000\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.7612 - acc: 0.6410 - val_loss: 1.1037 - val_acc: 0.5000\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.7572 - acc: 0.6410 - val_loss: 1.1050 - val_acc: 0.5000\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.7583 - acc: 0.6410 - val_loss: 1.1086 - val_acc: 0.5000\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.7549 - acc: 0.6410 - val_loss: 1.1115 - val_acc: 0.5000\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 258us/step - loss: 0.7511 - acc: 0.6410 - val_loss: 1.1090 - val_acc: 0.5000\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.7498 - acc: 0.6239 - val_loss: 1.1135 - val_acc: 0.5000\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.7465 - acc: 0.6496 - val_loss: 1.1140 - val_acc: 0.5000\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.7443 - acc: 0.6581 - val_loss: 1.1126 - val_acc: 0.5000\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 234us/step - loss: 0.7433 - acc: 0.6496 - val_loss: 1.1201 - val_acc: 0.5000\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.7414 - acc: 0.6581 - val_loss: 1.1201 - val_acc: 0.5000\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.7384 - acc: 0.6581 - val_loss: 1.1242 - val_acc: 0.5000\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.7382 - acc: 0.6581 - val_loss: 1.1274 - val_acc: 0.5000\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.7358 - acc: 0.6752 - val_loss: 1.1350 - val_acc: 0.5000\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.7331 - acc: 0.6581 - val_loss: 1.1356 - val_acc: 0.5000\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.7328 - acc: 0.6581 - val_loss: 1.1359 - val_acc: 0.5000\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.7314 - acc: 0.6496 - val_loss: 1.1385 - val_acc: 0.5000\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.7272 - acc: 0.6667 - val_loss: 1.1430 - val_acc: 0.5000\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.7231 - acc: 0.6581 - val_loss: 1.1454 - val_acc: 0.5000\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.7228 - acc: 0.6667 - val_loss: 1.1528 - val_acc: 0.5000\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.7209 - acc: 0.6667 - val_loss: 1.1537 - val_acc: 0.5000\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 231us/step - loss: 0.7171 - acc: 0.6667 - val_loss: 1.1582 - val_acc: 0.5000\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.7160 - acc: 0.6667 - val_loss: 1.1604 - val_acc: 0.5000\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 0.7136 - acc: 0.6752 - val_loss: 1.1621 - val_acc: 0.5000\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.7152 - acc: 0.6667 - val_loss: 1.1649 - val_acc: 0.5000\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.7103 - acc: 0.6752 - val_loss: 1.1705 - val_acc: 0.5000\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.7102 - acc: 0.6667 - val_loss: 1.1750 - val_acc: 0.5000\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 205us/step - loss: 0.7063 - acc: 0.6752 - val_loss: 1.1767 - val_acc: 0.5000\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.7043 - acc: 0.6667 - val_loss: 1.1807 - val_acc: 0.5000\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 226us/step - loss: 0.7002 - acc: 0.6752 - val_loss: 1.1896 - val_acc: 0.5000\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.7014 - acc: 0.6667 - val_loss: 1.1920 - val_acc: 0.5000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.6972 - acc: 0.6581 - val_loss: 1.1934 - val_acc: 0.5000\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 217us/step - loss: 0.6934 - acc: 0.6667 - val_loss: 1.2009 - val_acc: 0.5000\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.6956 - acc: 0.6667 - val_loss: 1.2017 - val_acc: 0.5000\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.6923 - acc: 0.6581 - val_loss: 1.2062 - val_acc: 0.5000\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.6908 - acc: 0.6752 - val_loss: 1.2138 - val_acc: 0.5000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 9ms/step - loss: 1.2796 - acc: 0.3644 - val_loss: 1.0153 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 278us/step - loss: 1.1570 - acc: 0.4661 - val_loss: 0.9531 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 1.0848 - acc: 0.5508 - val_loss: 0.9242 - val_acc: 0.6154\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 268us/step - loss: 1.0359 - acc: 0.5847 - val_loss: 0.9067 - val_acc: 0.6154\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0027 - acc: 0.6102 - val_loss: 0.8991 - val_acc: 0.6154\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9754 - acc: 0.6186 - val_loss: 0.8937 - val_acc: 0.6154\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9601 - acc: 0.5932 - val_loss: 0.8950 - val_acc: 0.6154\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9449 - acc: 0.5763 - val_loss: 0.8921 - val_acc: 0.6154\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9322 - acc: 0.5847 - val_loss: 0.8888 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9215 - acc: 0.5763 - val_loss: 0.8897 - val_acc: 0.6154\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9103 - acc: 0.5763 - val_loss: 0.8912 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9054 - acc: 0.5932 - val_loss: 0.8955 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8960 - acc: 0.5932 - val_loss: 0.8978 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8907 - acc: 0.5932 - val_loss: 0.9035 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.8833 - acc: 0.6017 - val_loss: 0.9060 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8765 - acc: 0.6017 - val_loss: 0.9097 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8723 - acc: 0.6186 - val_loss: 0.9111 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.8680 - acc: 0.6271 - val_loss: 0.9140 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8610 - acc: 0.6271 - val_loss: 0.9180 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.8557 - acc: 0.6271 - val_loss: 0.9223 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8533 - acc: 0.6271 - val_loss: 0.9254 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8483 - acc: 0.6271 - val_loss: 0.9322 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8440 - acc: 0.6186 - val_loss: 0.9375 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8427 - acc: 0.6102 - val_loss: 0.9411 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.8377 - acc: 0.6271 - val_loss: 0.9419 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8342 - acc: 0.6186 - val_loss: 0.9480 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8316 - acc: 0.6271 - val_loss: 0.9495 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8274 - acc: 0.6525 - val_loss: 0.9545 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8262 - acc: 0.6441 - val_loss: 0.9567 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8218 - acc: 0.6525 - val_loss: 0.9608 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8168 - acc: 0.6441 - val_loss: 0.9586 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8132 - acc: 0.6525 - val_loss: 0.9647 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8109 - acc: 0.6610 - val_loss: 0.9640 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8095 - acc: 0.6610 - val_loss: 0.9667 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8059 - acc: 0.6610 - val_loss: 0.9688 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8052 - acc: 0.6525 - val_loss: 0.9774 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7997 - acc: 0.6695 - val_loss: 0.9746 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.7976 - acc: 0.6695 - val_loss: 0.9818 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7940 - acc: 0.6695 - val_loss: 0.9825 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7930 - acc: 0.6610 - val_loss: 0.9893 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7871 - acc: 0.6695 - val_loss: 0.9918 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7874 - acc: 0.6610 - val_loss: 0.9942 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7845 - acc: 0.6780 - val_loss: 0.9977 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7817 - acc: 0.6610 - val_loss: 0.9986 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7783 - acc: 0.6695 - val_loss: 1.0031 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7770 - acc: 0.6695 - val_loss: 1.0019 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.7749 - acc: 0.6695 - val_loss: 1.0089 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7719 - acc: 0.6695 - val_loss: 1.0124 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7689 - acc: 0.6695 - val_loss: 1.0090 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7663 - acc: 0.6695 - val_loss: 1.0134 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7639 - acc: 0.6864 - val_loss: 1.0166 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7631 - acc: 0.6610 - val_loss: 1.0230 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7598 - acc: 0.6780 - val_loss: 1.0270 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7579 - acc: 0.6695 - val_loss: 1.0305 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.7544 - acc: 0.6780 - val_loss: 1.0306 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7523 - acc: 0.6780 - val_loss: 1.0377 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7502 - acc: 0.6695 - val_loss: 1.0365 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7482 - acc: 0.6695 - val_loss: 1.0388 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7503 - acc: 0.6949 - val_loss: 1.0399 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7436 - acc: 0.6610 - val_loss: 1.0471 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7399 - acc: 0.6780 - val_loss: 1.0489 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7409 - acc: 0.6695 - val_loss: 1.0518 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7391 - acc: 0.6610 - val_loss: 1.0551 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7350 - acc: 0.6610 - val_loss: 1.0589 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7348 - acc: 0.6610 - val_loss: 1.0685 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7308 - acc: 0.6780 - val_loss: 1.0786 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7299 - acc: 0.6610 - val_loss: 1.0763 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7281 - acc: 0.6780 - val_loss: 1.0773 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7264 - acc: 0.6780 - val_loss: 1.0867 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7245 - acc: 0.6949 - val_loss: 1.0902 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.7224 - acc: 0.6780 - val_loss: 1.0994 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7196 - acc: 0.6695 - val_loss: 1.0953 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7144 - acc: 0.6780 - val_loss: 1.1005 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7151 - acc: 0.6949 - val_loss: 1.1045 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7155 - acc: 0.6864 - val_loss: 1.1037 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7086 - acc: 0.6949 - val_loss: 1.1085 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7073 - acc: 0.6864 - val_loss: 1.1144 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7040 - acc: 0.6780 - val_loss: 1.1192 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7049 - acc: 0.6864 - val_loss: 1.1149 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7034 - acc: 0.6864 - val_loss: 1.1175 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7005 - acc: 0.6780 - val_loss: 1.1261 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6987 - acc: 0.7034 - val_loss: 1.1298 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6944 - acc: 0.6780 - val_loss: 1.1396 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6941 - acc: 0.6949 - val_loss: 1.1367 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.6927 - acc: 0.6864 - val_loss: 1.1387 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6901 - acc: 0.6780 - val_loss: 1.1488 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6892 - acc: 0.6864 - val_loss: 1.1478 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6866 - acc: 0.7034 - val_loss: 1.1506 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6817 - acc: 0.7034 - val_loss: 1.1565 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6815 - acc: 0.6949 - val_loss: 1.1529 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.6792 - acc: 0.7119 - val_loss: 1.1668 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.6760 - acc: 0.7203 - val_loss: 1.1694 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6732 - acc: 0.7203 - val_loss: 1.1745 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6703 - acc: 0.7288 - val_loss: 1.1783 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6702 - acc: 0.7119 - val_loss: 1.1805 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6677 - acc: 0.7288 - val_loss: 1.1834 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6649 - acc: 0.7288 - val_loss: 1.1876 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6612 - acc: 0.7288 - val_loss: 1.1946 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6600 - acc: 0.7288 - val_loss: 1.1909 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.6600 - acc: 0.7288 - val_loss: 1.1933 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 10ms/step - loss: 1.5091 - acc: 0.4237 - val_loss: 1.1870 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2378 - acc: 0.4068 - val_loss: 1.1176 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0998 - acc: 0.4237 - val_loss: 1.1062 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0343 - acc: 0.4322 - val_loss: 1.1194 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9957 - acc: 0.4915 - val_loss: 1.1452 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9704 - acc: 0.5000 - val_loss: 1.1693 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9536 - acc: 0.4915 - val_loss: 1.1921 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9408 - acc: 0.5424 - val_loss: 1.2166 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9280 - acc: 0.5593 - val_loss: 1.2432 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9208 - acc: 0.5593 - val_loss: 1.2663 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9090 - acc: 0.5847 - val_loss: 1.2889 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9009 - acc: 0.5932 - val_loss: 1.3144 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8931 - acc: 0.5847 - val_loss: 1.3270 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8847 - acc: 0.5932 - val_loss: 1.3496 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8775 - acc: 0.5847 - val_loss: 1.3650 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.8705 - acc: 0.5932 - val_loss: 1.3873 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8646 - acc: 0.6271 - val_loss: 1.4046 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8594 - acc: 0.6186 - val_loss: 1.4134 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8539 - acc: 0.6102 - val_loss: 1.4287 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8474 - acc: 0.6017 - val_loss: 1.4413 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8423 - acc: 0.5932 - val_loss: 1.4570 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8387 - acc: 0.6017 - val_loss: 1.4651 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8339 - acc: 0.5932 - val_loss: 1.4772 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8289 - acc: 0.6017 - val_loss: 1.4881 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.8236 - acc: 0.6102 - val_loss: 1.5021 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8213 - acc: 0.6102 - val_loss: 1.5112 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8158 - acc: 0.6017 - val_loss: 1.5191 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8126 - acc: 0.6186 - val_loss: 1.5278 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8113 - acc: 0.5932 - val_loss: 1.5393 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8047 - acc: 0.6186 - val_loss: 1.5484 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8018 - acc: 0.6102 - val_loss: 1.5596 - val_acc: 0.5385\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7980 - acc: 0.6186 - val_loss: 1.5720 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7963 - acc: 0.6186 - val_loss: 1.5839 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7912 - acc: 0.6271 - val_loss: 1.5873 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7890 - acc: 0.6271 - val_loss: 1.5957 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7862 - acc: 0.6271 - val_loss: 1.6066 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7822 - acc: 0.6271 - val_loss: 1.6239 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7790 - acc: 0.6271 - val_loss: 1.6177 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7763 - acc: 0.6271 - val_loss: 1.6310 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7727 - acc: 0.6271 - val_loss: 1.6377 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7691 - acc: 0.6271 - val_loss: 1.6494 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7630 - acc: 0.6271 - val_loss: 1.6608 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7622 - acc: 0.6356 - val_loss: 1.6661 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7568 - acc: 0.6441 - val_loss: 1.6754 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.7555 - acc: 0.6525 - val_loss: 1.6895 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7509 - acc: 0.6441 - val_loss: 1.6826 - val_acc: 0.5385\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7483 - acc: 0.6525 - val_loss: 1.6889 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7480 - acc: 0.6610 - val_loss: 1.7015 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7443 - acc: 0.6356 - val_loss: 1.7126 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7404 - acc: 0.6695 - val_loss: 1.7212 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7372 - acc: 0.6356 - val_loss: 1.7314 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7347 - acc: 0.6525 - val_loss: 1.7444 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7331 - acc: 0.6695 - val_loss: 1.7424 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7282 - acc: 0.6695 - val_loss: 1.7505 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7262 - acc: 0.6780 - val_loss: 1.7611 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7245 - acc: 0.6610 - val_loss: 1.7716 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7212 - acc: 0.6695 - val_loss: 1.7763 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7191 - acc: 0.6525 - val_loss: 1.7843 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7137 - acc: 0.6695 - val_loss: 1.7955 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.7136 - acc: 0.6525 - val_loss: 1.8056 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7100 - acc: 0.6610 - val_loss: 1.7998 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7056 - acc: 0.6525 - val_loss: 1.8118 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7039 - acc: 0.6610 - val_loss: 1.8247 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7023 - acc: 0.6525 - val_loss: 1.8284 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7005 - acc: 0.6695 - val_loss: 1.8391 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.6967 - acc: 0.6441 - val_loss: 1.8506 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.6925 - acc: 0.6610 - val_loss: 1.8562 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6914 - acc: 0.6610 - val_loss: 1.8665 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6858 - acc: 0.6695 - val_loss: 1.8675 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6851 - acc: 0.6695 - val_loss: 1.8822 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6835 - acc: 0.6695 - val_loss: 1.8823 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6815 - acc: 0.6610 - val_loss: 1.8893 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6785 - acc: 0.6780 - val_loss: 1.8983 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6735 - acc: 0.6780 - val_loss: 1.9047 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6721 - acc: 0.6864 - val_loss: 1.9157 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.6699 - acc: 0.6695 - val_loss: 1.9053 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.6670 - acc: 0.7034 - val_loss: 1.9174 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6632 - acc: 0.6864 - val_loss: 1.9276 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6620 - acc: 0.6780 - val_loss: 1.9282 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6585 - acc: 0.6780 - val_loss: 1.9374 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6560 - acc: 0.6780 - val_loss: 1.9432 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6544 - acc: 0.6864 - val_loss: 1.9530 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6535 - acc: 0.6864 - val_loss: 1.9600 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6500 - acc: 0.7119 - val_loss: 1.9642 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6474 - acc: 0.6949 - val_loss: 1.9690 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.6475 - acc: 0.7119 - val_loss: 1.9750 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6414 - acc: 0.7373 - val_loss: 1.9834 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6399 - acc: 0.7034 - val_loss: 1.9944 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6369 - acc: 0.7119 - val_loss: 2.0017 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6338 - acc: 0.7203 - val_loss: 2.0093 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6344 - acc: 0.7119 - val_loss: 2.0042 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6313 - acc: 0.7119 - val_loss: 2.0143 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6293 - acc: 0.7203 - val_loss: 2.0221 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6260 - acc: 0.7119 - val_loss: 2.0343 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6224 - acc: 0.7203 - val_loss: 2.0342 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6201 - acc: 0.7288 - val_loss: 2.0358 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.6185 - acc: 0.7373 - val_loss: 2.0419 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6160 - acc: 0.7458 - val_loss: 2.0520 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6130 - acc: 0.7288 - val_loss: 2.0640 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6107 - acc: 0.7288 - val_loss: 2.0686 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 9ms/step - loss: 1.2401 - acc: 0.2542 - val_loss: 1.2511 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1464 - acc: 0.3136 - val_loss: 1.1771 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0853 - acc: 0.3983 - val_loss: 1.1286 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0485 - acc: 0.4407 - val_loss: 1.0969 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0203 - acc: 0.4661 - val_loss: 1.0762 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0027 - acc: 0.4831 - val_loss: 1.0610 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9874 - acc: 0.5085 - val_loss: 1.0494 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9756 - acc: 0.5169 - val_loss: 1.0444 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9678 - acc: 0.5085 - val_loss: 1.0370 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9604 - acc: 0.5169 - val_loss: 1.0299 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9543 - acc: 0.5424 - val_loss: 1.0248 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9488 - acc: 0.5424 - val_loss: 1.0218 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9418 - acc: 0.5508 - val_loss: 1.0165 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9372 - acc: 0.5508 - val_loss: 1.0142 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9344 - acc: 0.5508 - val_loss: 1.0115 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9296 - acc: 0.5593 - val_loss: 1.0093 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9247 - acc: 0.5508 - val_loss: 1.0066 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9212 - acc: 0.5593 - val_loss: 1.0040 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9193 - acc: 0.5678 - val_loss: 1.0038 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9154 - acc: 0.5678 - val_loss: 1.0012 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9120 - acc: 0.5678 - val_loss: 0.9995 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9090 - acc: 0.5508 - val_loss: 1.0000 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9053 - acc: 0.5593 - val_loss: 0.9968 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9020 - acc: 0.5763 - val_loss: 0.9922 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8993 - acc: 0.5763 - val_loss: 0.9911 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8968 - acc: 0.5593 - val_loss: 0.9900 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8937 - acc: 0.5847 - val_loss: 0.9857 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8888 - acc: 0.5847 - val_loss: 0.9847 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.8877 - acc: 0.5932 - val_loss: 0.9817 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8853 - acc: 0.5678 - val_loss: 0.9813 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8807 - acc: 0.5932 - val_loss: 0.9802 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8779 - acc: 0.5932 - val_loss: 0.9779 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8744 - acc: 0.5932 - val_loss: 0.9758 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8719 - acc: 0.5932 - val_loss: 0.9716 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8684 - acc: 0.6102 - val_loss: 0.9693 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8655 - acc: 0.5932 - val_loss: 0.9686 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8628 - acc: 0.6017 - val_loss: 0.9669 - val_acc: 0.6154\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.8597 - acc: 0.6102 - val_loss: 0.9660 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.8561 - acc: 0.6102 - val_loss: 0.9659 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8533 - acc: 0.6017 - val_loss: 0.9627 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8500 - acc: 0.6102 - val_loss: 0.9607 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8466 - acc: 0.6356 - val_loss: 0.9610 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.8447 - acc: 0.6186 - val_loss: 0.9594 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8410 - acc: 0.6102 - val_loss: 0.9581 - val_acc: 0.6154\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8376 - acc: 0.6186 - val_loss: 0.9541 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8345 - acc: 0.6356 - val_loss: 0.9528 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8321 - acc: 0.6356 - val_loss: 0.9511 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8276 - acc: 0.6525 - val_loss: 0.9493 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8252 - acc: 0.6441 - val_loss: 0.9478 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8224 - acc: 0.6441 - val_loss: 0.9468 - val_acc: 0.6154\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8181 - acc: 0.6441 - val_loss: 0.9473 - val_acc: 0.6154\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8156 - acc: 0.6441 - val_loss: 0.9495 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.8112 - acc: 0.6525 - val_loss: 0.9456 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8104 - acc: 0.6525 - val_loss: 0.9434 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8074 - acc: 0.6610 - val_loss: 0.9413 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8029 - acc: 0.6695 - val_loss: 0.9406 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8016 - acc: 0.6695 - val_loss: 0.9393 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7979 - acc: 0.6780 - val_loss: 0.9375 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 246us/step - loss: 0.7941 - acc: 0.6695 - val_loss: 0.9369 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 254us/step - loss: 0.7921 - acc: 0.6525 - val_loss: 0.9382 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.7890 - acc: 0.6780 - val_loss: 0.9363 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7847 - acc: 0.6780 - val_loss: 0.9361 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7815 - acc: 0.6780 - val_loss: 0.9372 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7804 - acc: 0.6695 - val_loss: 0.9332 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7763 - acc: 0.6695 - val_loss: 0.9343 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7732 - acc: 0.6780 - val_loss: 0.9350 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7719 - acc: 0.6780 - val_loss: 0.9346 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7676 - acc: 0.6864 - val_loss: 0.9323 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7645 - acc: 0.6780 - val_loss: 0.9318 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7612 - acc: 0.6780 - val_loss: 0.9294 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7587 - acc: 0.6864 - val_loss: 0.9286 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7548 - acc: 0.6695 - val_loss: 0.9347 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7517 - acc: 0.6695 - val_loss: 0.9317 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7498 - acc: 0.6695 - val_loss: 0.9346 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7479 - acc: 0.6780 - val_loss: 0.9298 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7431 - acc: 0.6780 - val_loss: 0.9334 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7402 - acc: 0.6949 - val_loss: 0.9329 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7368 - acc: 0.6864 - val_loss: 0.9352 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7341 - acc: 0.6864 - val_loss: 0.9347 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7317 - acc: 0.7034 - val_loss: 0.9316 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7303 - acc: 0.6949 - val_loss: 0.9353 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7261 - acc: 0.7034 - val_loss: 0.9375 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7236 - acc: 0.7034 - val_loss: 0.9322 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7198 - acc: 0.7034 - val_loss: 0.9332 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7171 - acc: 0.7034 - val_loss: 0.9348 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.7135 - acc: 0.7034 - val_loss: 0.9312 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7092 - acc: 0.7119 - val_loss: 0.9354 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7087 - acc: 0.7119 - val_loss: 0.9328 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7067 - acc: 0.7119 - val_loss: 0.9358 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7026 - acc: 0.7034 - val_loss: 0.9318 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6981 - acc: 0.7203 - val_loss: 0.9313 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.6954 - acc: 0.7288 - val_loss: 0.9257 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6935 - acc: 0.7119 - val_loss: 0.9359 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6910 - acc: 0.7203 - val_loss: 0.9386 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6866 - acc: 0.7203 - val_loss: 0.9394 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6837 - acc: 0.7288 - val_loss: 0.9422 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6814 - acc: 0.7203 - val_loss: 0.9446 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6779 - acc: 0.7288 - val_loss: 0.9492 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6766 - acc: 0.7288 - val_loss: 0.9548 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6742 - acc: 0.7203 - val_loss: 0.9579 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 10ms/step - loss: 1.1344 - acc: 0.4915 - val_loss: 1.1031 - val_acc: 0.3846\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.0856 - acc: 0.4831 - val_loss: 1.0863 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0393 - acc: 0.5085 - val_loss: 1.0780 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0137 - acc: 0.5424 - val_loss: 1.0747 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9918 - acc: 0.5254 - val_loss: 1.0680 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9782 - acc: 0.5254 - val_loss: 1.0695 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.9629 - acc: 0.5254 - val_loss: 1.0636 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9497 - acc: 0.5169 - val_loss: 1.0659 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9413 - acc: 0.5424 - val_loss: 1.0673 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9322 - acc: 0.5424 - val_loss: 1.0669 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9225 - acc: 0.5593 - val_loss: 1.0689 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9171 - acc: 0.5763 - val_loss: 1.0728 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9110 - acc: 0.5932 - val_loss: 1.0732 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9038 - acc: 0.5932 - val_loss: 1.0746 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8995 - acc: 0.5932 - val_loss: 1.0767 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8947 - acc: 0.5932 - val_loss: 1.0780 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8904 - acc: 0.6017 - val_loss: 1.0806 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8838 - acc: 0.5932 - val_loss: 1.0804 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8807 - acc: 0.6102 - val_loss: 1.0807 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8780 - acc: 0.6017 - val_loss: 1.0831 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8725 - acc: 0.6102 - val_loss: 1.0870 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8684 - acc: 0.5847 - val_loss: 1.0874 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8648 - acc: 0.5932 - val_loss: 1.0882 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8617 - acc: 0.6017 - val_loss: 1.0897 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8586 - acc: 0.5932 - val_loss: 1.0928 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8550 - acc: 0.6102 - val_loss: 1.0972 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8518 - acc: 0.6017 - val_loss: 1.0990 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8519 - acc: 0.6017 - val_loss: 1.1035 - val_acc: 0.3077\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8474 - acc: 0.6102 - val_loss: 1.1030 - val_acc: 0.3077\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8402 - acc: 0.6186 - val_loss: 1.1040 - val_acc: 0.3077\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8420 - acc: 0.6102 - val_loss: 1.1045 - val_acc: 0.3077\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8360 - acc: 0.6356 - val_loss: 1.1107 - val_acc: 0.3077\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8342 - acc: 0.6271 - val_loss: 1.1109 - val_acc: 0.3077\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8340 - acc: 0.6186 - val_loss: 1.1123 - val_acc: 0.3077\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8302 - acc: 0.6356 - val_loss: 1.1211 - val_acc: 0.3077\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8267 - acc: 0.6441 - val_loss: 1.1203 - val_acc: 0.3077\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.8233 - acc: 0.6356 - val_loss: 1.1213 - val_acc: 0.3077\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8209 - acc: 0.6356 - val_loss: 1.1245 - val_acc: 0.3077\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8161 - acc: 0.6610 - val_loss: 1.1271 - val_acc: 0.3077\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8149 - acc: 0.6610 - val_loss: 1.1306 - val_acc: 0.3077\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8103 - acc: 0.6525 - val_loss: 1.1320 - val_acc: 0.3077\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8101 - acc: 0.6610 - val_loss: 1.1342 - val_acc: 0.3077\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8076 - acc: 0.6525 - val_loss: 1.1360 - val_acc: 0.3077\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8042 - acc: 0.6864 - val_loss: 1.1367 - val_acc: 0.3077\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7982 - acc: 0.6525 - val_loss: 1.1443 - val_acc: 0.3077\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7964 - acc: 0.6695 - val_loss: 1.1456 - val_acc: 0.3077\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7946 - acc: 0.6695 - val_loss: 1.1460 - val_acc: 0.3077\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7927 - acc: 0.6610 - val_loss: 1.1520 - val_acc: 0.3077\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7874 - acc: 0.6864 - val_loss: 1.1519 - val_acc: 0.3077\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7872 - acc: 0.6525 - val_loss: 1.1550 - val_acc: 0.3077\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7838 - acc: 0.6780 - val_loss: 1.1601 - val_acc: 0.3077\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7816 - acc: 0.6780 - val_loss: 1.1621 - val_acc: 0.3077\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7775 - acc: 0.6949 - val_loss: 1.1692 - val_acc: 0.3077\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7757 - acc: 0.6864 - val_loss: 1.1696 - val_acc: 0.3077\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7729 - acc: 0.6949 - val_loss: 1.1720 - val_acc: 0.3077\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7693 - acc: 0.6949 - val_loss: 1.1737 - val_acc: 0.3077\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7682 - acc: 0.7034 - val_loss: 1.1753 - val_acc: 0.3077\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7659 - acc: 0.6864 - val_loss: 1.1808 - val_acc: 0.3077\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7649 - acc: 0.6864 - val_loss: 1.1831 - val_acc: 0.3077\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7602 - acc: 0.6864 - val_loss: 1.1875 - val_acc: 0.3077\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7577 - acc: 0.6949 - val_loss: 1.1933 - val_acc: 0.3077\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7561 - acc: 0.6864 - val_loss: 1.1938 - val_acc: 0.3077\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7547 - acc: 0.6949 - val_loss: 1.2003 - val_acc: 0.3077\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7504 - acc: 0.6949 - val_loss: 1.2007 - val_acc: 0.3077\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7487 - acc: 0.6864 - val_loss: 1.2037 - val_acc: 0.3077\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7463 - acc: 0.6864 - val_loss: 1.2015 - val_acc: 0.3077\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.7402 - acc: 0.6949 - val_loss: 1.2137 - val_acc: 0.3077\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7418 - acc: 0.6864 - val_loss: 1.2136 - val_acc: 0.3077\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.7404 - acc: 0.6949 - val_loss: 1.2129 - val_acc: 0.3077\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7371 - acc: 0.6949 - val_loss: 1.2228 - val_acc: 0.3077\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7325 - acc: 0.6864 - val_loss: 1.2256 - val_acc: 0.3077\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7312 - acc: 0.6949 - val_loss: 1.2246 - val_acc: 0.3077\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7292 - acc: 0.6780 - val_loss: 1.2273 - val_acc: 0.3077\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7278 - acc: 0.6780 - val_loss: 1.2302 - val_acc: 0.3077\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7264 - acc: 0.6780 - val_loss: 1.2330 - val_acc: 0.3077\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7238 - acc: 0.6780 - val_loss: 1.2321 - val_acc: 0.3077\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7196 - acc: 0.6780 - val_loss: 1.2395 - val_acc: 0.3077\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7138 - acc: 0.6780 - val_loss: 1.2427 - val_acc: 0.3077\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7151 - acc: 0.6780 - val_loss: 1.2439 - val_acc: 0.3077\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7150 - acc: 0.6864 - val_loss: 1.2456 - val_acc: 0.3077\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.7098 - acc: 0.6780 - val_loss: 1.2467 - val_acc: 0.3077\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7072 - acc: 0.6864 - val_loss: 1.2477 - val_acc: 0.3077\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7060 - acc: 0.6780 - val_loss: 1.2546 - val_acc: 0.3077\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7020 - acc: 0.6949 - val_loss: 1.2538 - val_acc: 0.3077\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6998 - acc: 0.6864 - val_loss: 1.2619 - val_acc: 0.3077\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.6988 - acc: 0.6780 - val_loss: 1.2617 - val_acc: 0.3077\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6954 - acc: 0.6780 - val_loss: 1.2617 - val_acc: 0.3077\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 274us/step - loss: 0.6915 - acc: 0.6949 - val_loss: 1.2725 - val_acc: 0.3077\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6886 - acc: 0.7034 - val_loss: 1.2703 - val_acc: 0.3077\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 302us/step - loss: 0.6858 - acc: 0.6949 - val_loss: 1.2733 - val_acc: 0.3077\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6820 - acc: 0.7034 - val_loss: 1.2726 - val_acc: 0.3077\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6828 - acc: 0.6949 - val_loss: 1.2762 - val_acc: 0.3077\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6767 - acc: 0.7034 - val_loss: 1.2820 - val_acc: 0.3077\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.6778 - acc: 0.7034 - val_loss: 1.2843 - val_acc: 0.3077\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6729 - acc: 0.7203 - val_loss: 1.2880 - val_acc: 0.3077\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6683 - acc: 0.6949 - val_loss: 1.2889 - val_acc: 0.3077\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.6688 - acc: 0.7034 - val_loss: 1.2859 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.6661 - acc: 0.7203 - val_loss: 1.2994 - val_acc: 0.3077\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6626 - acc: 0.7119 - val_loss: 1.2997 - val_acc: 0.3077\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.6612 - acc: 0.7288 - val_loss: 1.3063 - val_acc: 0.3077\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 10ms/step - loss: 1.3365 - acc: 0.3220 - val_loss: 1.3710 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 1.1626 - acc: 0.3729 - val_loss: 1.2372 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 1.0648 - acc: 0.4492 - val_loss: 1.1496 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0112 - acc: 0.5000 - val_loss: 1.0869 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9767 - acc: 0.5254 - val_loss: 1.0522 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 259us/step - loss: 0.9526 - acc: 0.5763 - val_loss: 1.0256 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9368 - acc: 0.6017 - val_loss: 1.0028 - val_acc: 0.6154\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9244 - acc: 0.6017 - val_loss: 0.9873 - val_acc: 0.6154\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9142 - acc: 0.6186 - val_loss: 0.9699 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9080 - acc: 0.6102 - val_loss: 0.9631 - val_acc: 0.6154\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.8990 - acc: 0.6271 - val_loss: 0.9525 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.8928 - acc: 0.6356 - val_loss: 0.9450 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8852 - acc: 0.6441 - val_loss: 0.9338 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8819 - acc: 0.6186 - val_loss: 0.9309 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8751 - acc: 0.6356 - val_loss: 0.9246 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8725 - acc: 0.6525 - val_loss: 0.9214 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8666 - acc: 0.6186 - val_loss: 0.9194 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8630 - acc: 0.6186 - val_loss: 0.9147 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8597 - acc: 0.6441 - val_loss: 0.9082 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8554 - acc: 0.6441 - val_loss: 0.9049 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8518 - acc: 0.6610 - val_loss: 0.8998 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8486 - acc: 0.6525 - val_loss: 0.8954 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8463 - acc: 0.6525 - val_loss: 0.8931 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8444 - acc: 0.6525 - val_loss: 0.8890 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.8415 - acc: 0.6525 - val_loss: 0.8862 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.8357 - acc: 0.6695 - val_loss: 0.8849 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8338 - acc: 0.6610 - val_loss: 0.8828 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8310 - acc: 0.6525 - val_loss: 0.8776 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8289 - acc: 0.6525 - val_loss: 0.8782 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.8228 - acc: 0.6525 - val_loss: 0.8743 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8179 - acc: 0.6525 - val_loss: 0.8731 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8201 - acc: 0.6441 - val_loss: 0.8671 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8143 - acc: 0.6525 - val_loss: 0.8617 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8102 - acc: 0.6441 - val_loss: 0.8598 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8073 - acc: 0.6610 - val_loss: 0.8569 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.8049 - acc: 0.6441 - val_loss: 0.8518 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8015 - acc: 0.6610 - val_loss: 0.8505 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7976 - acc: 0.6525 - val_loss: 0.8502 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7981 - acc: 0.6610 - val_loss: 0.8473 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7941 - acc: 0.6441 - val_loss: 0.8454 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7909 - acc: 0.6525 - val_loss: 0.8406 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7883 - acc: 0.6525 - val_loss: 0.8397 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7844 - acc: 0.6441 - val_loss: 0.8353 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7835 - acc: 0.6441 - val_loss: 0.8337 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7791 - acc: 0.6610 - val_loss: 0.8276 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7762 - acc: 0.6610 - val_loss: 0.8290 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7750 - acc: 0.6695 - val_loss: 0.8297 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7700 - acc: 0.6610 - val_loss: 0.8243 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7679 - acc: 0.6695 - val_loss: 0.8241 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7657 - acc: 0.6780 - val_loss: 0.8193 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7628 - acc: 0.6780 - val_loss: 0.8183 - val_acc: 0.6923\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7586 - acc: 0.6695 - val_loss: 0.8141 - val_acc: 0.6923\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7551 - acc: 0.6695 - val_loss: 0.8100 - val_acc: 0.6923\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7538 - acc: 0.6695 - val_loss: 0.8064 - val_acc: 0.6923\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.7506 - acc: 0.6780 - val_loss: 0.8040 - val_acc: 0.6923\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7491 - acc: 0.6864 - val_loss: 0.8029 - val_acc: 0.6923\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7457 - acc: 0.6864 - val_loss: 0.7994 - val_acc: 0.6923\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7432 - acc: 0.6780 - val_loss: 0.7955 - val_acc: 0.6923\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.7416 - acc: 0.6780 - val_loss: 0.7941 - val_acc: 0.6923\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7407 - acc: 0.6864 - val_loss: 0.7955 - val_acc: 0.6923\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.7368 - acc: 0.6864 - val_loss: 0.7947 - val_acc: 0.6923\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7336 - acc: 0.6864 - val_loss: 0.7910 - val_acc: 0.6923\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7327 - acc: 0.6864 - val_loss: 0.7907 - val_acc: 0.6923\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7304 - acc: 0.7034 - val_loss: 0.7874 - val_acc: 0.6923\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7268 - acc: 0.6780 - val_loss: 0.7853 - val_acc: 0.6923\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.7267 - acc: 0.6949 - val_loss: 0.7829 - val_acc: 0.6923\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7234 - acc: 0.7034 - val_loss: 0.7764 - val_acc: 0.6923\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7200 - acc: 0.7034 - val_loss: 0.7788 - val_acc: 0.6923\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7187 - acc: 0.6949 - val_loss: 0.7750 - val_acc: 0.6923\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7166 - acc: 0.7034 - val_loss: 0.7781 - val_acc: 0.6923\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7154 - acc: 0.6949 - val_loss: 0.7786 - val_acc: 0.6923\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7102 - acc: 0.6949 - val_loss: 0.7739 - val_acc: 0.6923\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7077 - acc: 0.6949 - val_loss: 0.7718 - val_acc: 0.6923\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7094 - acc: 0.7119 - val_loss: 0.7697 - val_acc: 0.6923\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.7045 - acc: 0.7119 - val_loss: 0.7686 - val_acc: 0.6923\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6996 - acc: 0.7203 - val_loss: 0.7665 - val_acc: 0.6923\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7001 - acc: 0.7034 - val_loss: 0.7626 - val_acc: 0.6923\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6958 - acc: 0.7119 - val_loss: 0.7669 - val_acc: 0.6923\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6948 - acc: 0.7203 - val_loss: 0.7660 - val_acc: 0.6923\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6953 - acc: 0.7034 - val_loss: 0.7610 - val_acc: 0.6923\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6895 - acc: 0.7203 - val_loss: 0.7622 - val_acc: 0.6923\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6875 - acc: 0.7119 - val_loss: 0.7559 - val_acc: 0.6923\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.6847 - acc: 0.7203 - val_loss: 0.7537 - val_acc: 0.6923\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6844 - acc: 0.7203 - val_loss: 0.7549 - val_acc: 0.6923\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6837 - acc: 0.7119 - val_loss: 0.7551 - val_acc: 0.6923\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6783 - acc: 0.7203 - val_loss: 0.7557 - val_acc: 0.6923\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6775 - acc: 0.7203 - val_loss: 0.7544 - val_acc: 0.6923\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6759 - acc: 0.7203 - val_loss: 0.7535 - val_acc: 0.6923\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6737 - acc: 0.7203 - val_loss: 0.7532 - val_acc: 0.6923\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6702 - acc: 0.7119 - val_loss: 0.7517 - val_acc: 0.6923\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6700 - acc: 0.7119 - val_loss: 0.7488 - val_acc: 0.6923\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6668 - acc: 0.7119 - val_loss: 0.7469 - val_acc: 0.6923\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.6668 - acc: 0.7034 - val_loss: 0.7482 - val_acc: 0.6923\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.6642 - acc: 0.7203 - val_loss: 0.7447 - val_acc: 0.6923\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6624 - acc: 0.7288 - val_loss: 0.7437 - val_acc: 0.6923\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6588 - acc: 0.7203 - val_loss: 0.7458 - val_acc: 0.6923\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6576 - acc: 0.7203 - val_loss: 0.7440 - val_acc: 0.6923\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6545 - acc: 0.7203 - val_loss: 0.7420 - val_acc: 0.6923\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6562 - acc: 0.7203 - val_loss: 0.7425 - val_acc: 0.6923\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6511 - acc: 0.7203 - val_loss: 0.7374 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 10ms/step - loss: 1.5650 - acc: 0.2034 - val_loss: 1.7724 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2443 - acc: 0.3390 - val_loss: 1.5110 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1237 - acc: 0.3983 - val_loss: 1.3760 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0574 - acc: 0.4831 - val_loss: 1.3019 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0161 - acc: 0.5424 - val_loss: 1.2465 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9920 - acc: 0.5254 - val_loss: 1.2123 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9681 - acc: 0.5508 - val_loss: 1.1836 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9532 - acc: 0.5847 - val_loss: 1.1613 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9408 - acc: 0.5763 - val_loss: 1.1458 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9294 - acc: 0.5763 - val_loss: 1.1332 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9175 - acc: 0.5763 - val_loss: 1.1237 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9109 - acc: 0.5678 - val_loss: 1.1131 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9017 - acc: 0.5847 - val_loss: 1.1092 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8967 - acc: 0.5763 - val_loss: 1.1021 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.8912 - acc: 0.5847 - val_loss: 1.0986 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8861 - acc: 0.5847 - val_loss: 1.0941 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8794 - acc: 0.5847 - val_loss: 1.0920 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8729 - acc: 0.5763 - val_loss: 1.0894 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.8704 - acc: 0.5932 - val_loss: 1.0874 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8651 - acc: 0.5932 - val_loss: 1.0866 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8604 - acc: 0.5763 - val_loss: 1.0921 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8562 - acc: 0.5847 - val_loss: 1.0896 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8534 - acc: 0.6017 - val_loss: 1.0883 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.8483 - acc: 0.5932 - val_loss: 1.0938 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8441 - acc: 0.5932 - val_loss: 1.0958 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8434 - acc: 0.5932 - val_loss: 1.0934 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8385 - acc: 0.5678 - val_loss: 1.0929 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.8380 - acc: 0.6017 - val_loss: 1.0951 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8336 - acc: 0.5763 - val_loss: 1.0995 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8288 - acc: 0.5932 - val_loss: 1.0981 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8233 - acc: 0.6017 - val_loss: 1.1019 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8207 - acc: 0.6102 - val_loss: 1.1020 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8173 - acc: 0.6186 - val_loss: 1.1030 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8138 - acc: 0.6186 - val_loss: 1.1067 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8079 - acc: 0.6186 - val_loss: 1.1073 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8062 - acc: 0.6186 - val_loss: 1.1113 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8042 - acc: 0.6271 - val_loss: 1.1129 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7999 - acc: 0.6441 - val_loss: 1.1176 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.7971 - acc: 0.6441 - val_loss: 1.1193 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7938 - acc: 0.6271 - val_loss: 1.1210 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7928 - acc: 0.6441 - val_loss: 1.1247 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7878 - acc: 0.6441 - val_loss: 1.1349 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7840 - acc: 0.6525 - val_loss: 1.1387 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7821 - acc: 0.6525 - val_loss: 1.1437 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7783 - acc: 0.6441 - val_loss: 1.1463 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.7758 - acc: 0.6525 - val_loss: 1.1472 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7705 - acc: 0.6610 - val_loss: 1.1562 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7667 - acc: 0.6610 - val_loss: 1.1568 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.7648 - acc: 0.6610 - val_loss: 1.1560 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7629 - acc: 0.6695 - val_loss: 1.1609 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7578 - acc: 0.6780 - val_loss: 1.1666 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7530 - acc: 0.6864 - val_loss: 1.1691 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7522 - acc: 0.6864 - val_loss: 1.1733 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7490 - acc: 0.6780 - val_loss: 1.1802 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7444 - acc: 0.6864 - val_loss: 1.1816 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7436 - acc: 0.6864 - val_loss: 1.1864 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7400 - acc: 0.6695 - val_loss: 1.1875 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7368 - acc: 0.6780 - val_loss: 1.1871 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.7347 - acc: 0.6949 - val_loss: 1.1966 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.7302 - acc: 0.6949 - val_loss: 1.2006 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.7270 - acc: 0.6780 - val_loss: 1.2084 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7245 - acc: 0.7034 - val_loss: 1.2132 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7219 - acc: 0.6949 - val_loss: 1.2187 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7173 - acc: 0.7119 - val_loss: 1.2244 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.7145 - acc: 0.6949 - val_loss: 1.2301 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.7118 - acc: 0.7034 - val_loss: 1.2337 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.7094 - acc: 0.6949 - val_loss: 1.2414 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.7057 - acc: 0.7119 - val_loss: 1.2475 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7035 - acc: 0.7119 - val_loss: 1.2532 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6991 - acc: 0.7034 - val_loss: 1.2556 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6975 - acc: 0.7203 - val_loss: 1.2600 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6935 - acc: 0.7288 - val_loss: 1.2619 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6938 - acc: 0.7119 - val_loss: 1.2665 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6906 - acc: 0.7119 - val_loss: 1.2736 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.6875 - acc: 0.7203 - val_loss: 1.2821 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6844 - acc: 0.7203 - val_loss: 1.2859 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.6793 - acc: 0.7034 - val_loss: 1.2912 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6783 - acc: 0.7203 - val_loss: 1.2935 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6740 - acc: 0.7203 - val_loss: 1.3006 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6745 - acc: 0.7119 - val_loss: 1.3176 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6702 - acc: 0.7373 - val_loss: 1.3222 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6695 - acc: 0.7203 - val_loss: 1.3220 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6650 - acc: 0.7119 - val_loss: 1.3319 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6647 - acc: 0.7203 - val_loss: 1.3407 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6625 - acc: 0.7203 - val_loss: 1.3438 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6585 - acc: 0.7288 - val_loss: 1.3450 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.6524 - acc: 0.7373 - val_loss: 1.3538 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6522 - acc: 0.7119 - val_loss: 1.3673 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6503 - acc: 0.7034 - val_loss: 1.3709 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6466 - acc: 0.7288 - val_loss: 1.3766 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6442 - acc: 0.7288 - val_loss: 1.3823 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6439 - acc: 0.7373 - val_loss: 1.3904 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.6396 - acc: 0.7458 - val_loss: 1.3965 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6386 - acc: 0.7627 - val_loss: 1.4055 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6358 - acc: 0.7458 - val_loss: 1.4032 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6340 - acc: 0.7542 - val_loss: 1.4078 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6325 - acc: 0.7458 - val_loss: 1.4134 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6275 - acc: 0.7542 - val_loss: 1.4181 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6275 - acc: 0.7542 - val_loss: 1.4282 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6253 - acc: 0.7712 - val_loss: 1.4321 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 11ms/step - loss: 1.0741 - acc: 0.4322 - val_loss: 1.0964 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0157 - acc: 0.4661 - val_loss: 1.0659 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9790 - acc: 0.4915 - val_loss: 1.0524 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9551 - acc: 0.5169 - val_loss: 1.0349 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9391 - acc: 0.5424 - val_loss: 1.0226 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9232 - acc: 0.5508 - val_loss: 1.0164 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9099 - acc: 0.5508 - val_loss: 1.0163 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9013 - acc: 0.5593 - val_loss: 1.0162 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8911 - acc: 0.5593 - val_loss: 1.0135 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8830 - acc: 0.5593 - val_loss: 1.0182 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8737 - acc: 0.5763 - val_loss: 1.0164 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.8683 - acc: 0.5763 - val_loss: 1.0172 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8617 - acc: 0.5847 - val_loss: 1.0195 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8550 - acc: 0.5763 - val_loss: 1.0203 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8499 - acc: 0.6017 - val_loss: 1.0207 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8449 - acc: 0.6102 - val_loss: 1.0244 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.8387 - acc: 0.5847 - val_loss: 1.0259 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.8334 - acc: 0.5678 - val_loss: 1.0297 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8276 - acc: 0.5763 - val_loss: 1.0346 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 252us/step - loss: 0.8220 - acc: 0.6017 - val_loss: 1.0371 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8180 - acc: 0.6102 - val_loss: 1.0427 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.8150 - acc: 0.6186 - val_loss: 1.0465 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.8093 - acc: 0.6186 - val_loss: 1.0532 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8027 - acc: 0.6695 - val_loss: 1.0599 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 0.7981 - acc: 0.6610 - val_loss: 1.0659 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.7949 - acc: 0.6610 - val_loss: 1.0716 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.7900 - acc: 0.6525 - val_loss: 1.0730 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7862 - acc: 0.6525 - val_loss: 1.0746 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7793 - acc: 0.6695 - val_loss: 1.0826 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7767 - acc: 0.6695 - val_loss: 1.0899 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7708 - acc: 0.6695 - val_loss: 1.0982 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7666 - acc: 0.6695 - val_loss: 1.1047 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7627 - acc: 0.6780 - val_loss: 1.1106 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.7573 - acc: 0.6695 - val_loss: 1.1156 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.7543 - acc: 0.6864 - val_loss: 1.1248 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.7512 - acc: 0.6864 - val_loss: 1.1295 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7450 - acc: 0.7034 - val_loss: 1.1384 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.7428 - acc: 0.6864 - val_loss: 1.1419 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7390 - acc: 0.7034 - val_loss: 1.1573 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 262us/step - loss: 0.7347 - acc: 0.7203 - val_loss: 1.1638 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7329 - acc: 0.7034 - val_loss: 1.1756 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7277 - acc: 0.7034 - val_loss: 1.1761 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7226 - acc: 0.7119 - val_loss: 1.1819 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7211 - acc: 0.7034 - val_loss: 1.1938 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.7162 - acc: 0.7034 - val_loss: 1.2017 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7139 - acc: 0.7119 - val_loss: 1.2053 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.7098 - acc: 0.7119 - val_loss: 1.2189 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.7064 - acc: 0.7034 - val_loss: 1.2293 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7034 - acc: 0.7203 - val_loss: 1.2411 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7006 - acc: 0.7203 - val_loss: 1.2455 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6959 - acc: 0.7288 - val_loss: 1.2557 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6937 - acc: 0.7203 - val_loss: 1.2695 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.6892 - acc: 0.7203 - val_loss: 1.2827 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.6920 - acc: 0.7203 - val_loss: 1.2883 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6820 - acc: 0.7203 - val_loss: 1.2988 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.6802 - acc: 0.7203 - val_loss: 1.3028 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6778 - acc: 0.7119 - val_loss: 1.3090 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6739 - acc: 0.7119 - val_loss: 1.3223 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6710 - acc: 0.7203 - val_loss: 1.3228 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6696 - acc: 0.7288 - val_loss: 1.3294 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6675 - acc: 0.7119 - val_loss: 1.3474 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.6641 - acc: 0.7373 - val_loss: 1.3554 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6607 - acc: 0.7373 - val_loss: 1.3661 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6598 - acc: 0.7203 - val_loss: 1.3708 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.6522 - acc: 0.7288 - val_loss: 1.3776 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6497 - acc: 0.7373 - val_loss: 1.3937 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6490 - acc: 0.7119 - val_loss: 1.4017 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6445 - acc: 0.7288 - val_loss: 1.3995 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6456 - acc: 0.7288 - val_loss: 1.4053 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6411 - acc: 0.7458 - val_loss: 1.4151 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6402 - acc: 0.7203 - val_loss: 1.4236 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6369 - acc: 0.7373 - val_loss: 1.4370 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6352 - acc: 0.7203 - val_loss: 1.4382 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.6316 - acc: 0.7373 - val_loss: 1.4483 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6285 - acc: 0.7288 - val_loss: 1.4631 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.6264 - acc: 0.7203 - val_loss: 1.4660 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6301 - acc: 0.7373 - val_loss: 1.4615 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6217 - acc: 0.7288 - val_loss: 1.4833 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6189 - acc: 0.7458 - val_loss: 1.4961 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6181 - acc: 0.7288 - val_loss: 1.5017 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6141 - acc: 0.7288 - val_loss: 1.5052 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6136 - acc: 0.7203 - val_loss: 1.5122 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6103 - acc: 0.7203 - val_loss: 1.5240 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6084 - acc: 0.7119 - val_loss: 1.5355 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.6069 - acc: 0.7288 - val_loss: 1.5395 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.6040 - acc: 0.7288 - val_loss: 1.5505 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6010 - acc: 0.7373 - val_loss: 1.5481 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.5999 - acc: 0.7288 - val_loss: 1.5667 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5977 - acc: 0.7458 - val_loss: 1.5860 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5938 - acc: 0.7458 - val_loss: 1.5902 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5962 - acc: 0.7288 - val_loss: 1.5894 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.5933 - acc: 0.7458 - val_loss: 1.6214 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5882 - acc: 0.7373 - val_loss: 1.6320 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.5860 - acc: 0.7373 - val_loss: 1.6380 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 0.5824 - acc: 0.7542 - val_loss: 1.6474 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.5813 - acc: 0.7288 - val_loss: 1.6772 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.5771 - acc: 0.7373 - val_loss: 1.6720 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.5727 - acc: 0.7458 - val_loss: 1.6760 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.5705 - acc: 0.7712 - val_loss: 1.7118 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5711 - acc: 0.7458 - val_loss: 1.7172 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 11ms/step - loss: 1.2512 - acc: 0.2966 - val_loss: 1.1197 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 1.1289 - acc: 0.3814 - val_loss: 1.0553 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0580 - acc: 0.4407 - val_loss: 1.0153 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0113 - acc: 0.4831 - val_loss: 0.9936 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9851 - acc: 0.4746 - val_loss: 0.9770 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9611 - acc: 0.4746 - val_loss: 0.9721 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9457 - acc: 0.4746 - val_loss: 0.9599 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9318 - acc: 0.4661 - val_loss: 0.9549 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9202 - acc: 0.4746 - val_loss: 0.9510 - val_acc: 0.6154\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9101 - acc: 0.5085 - val_loss: 0.9469 - val_acc: 0.6154\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.9030 - acc: 0.4915 - val_loss: 0.9452 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.8947 - acc: 0.5000 - val_loss: 0.9504 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 258us/step - loss: 0.8865 - acc: 0.5339 - val_loss: 0.9490 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.8779 - acc: 0.5339 - val_loss: 0.9520 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8732 - acc: 0.5593 - val_loss: 0.9537 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8661 - acc: 0.5847 - val_loss: 0.9546 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8595 - acc: 0.5847 - val_loss: 0.9583 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8560 - acc: 0.5763 - val_loss: 0.9619 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.8501 - acc: 0.5763 - val_loss: 0.9611 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8435 - acc: 0.5678 - val_loss: 0.9627 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.8385 - acc: 0.5847 - val_loss: 0.9631 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8321 - acc: 0.5847 - val_loss: 0.9667 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8295 - acc: 0.5932 - val_loss: 0.9701 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8228 - acc: 0.5932 - val_loss: 0.9715 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8195 - acc: 0.5932 - val_loss: 0.9720 - val_acc: 0.6154\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8139 - acc: 0.6186 - val_loss: 0.9728 - val_acc: 0.6154\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8075 - acc: 0.6356 - val_loss: 0.9728 - val_acc: 0.6154\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.8036 - acc: 0.6271 - val_loss: 0.9782 - val_acc: 0.6154\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8005 - acc: 0.6186 - val_loss: 0.9782 - val_acc: 0.6154\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7945 - acc: 0.6271 - val_loss: 0.9809 - val_acc: 0.6154\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7912 - acc: 0.6356 - val_loss: 0.9806 - val_acc: 0.6154\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7880 - acc: 0.6186 - val_loss: 0.9800 - val_acc: 0.6154\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.7827 - acc: 0.6356 - val_loss: 0.9809 - val_acc: 0.6154\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7773 - acc: 0.6441 - val_loss: 0.9862 - val_acc: 0.6154\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7739 - acc: 0.6610 - val_loss: 0.9871 - val_acc: 0.6154\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7684 - acc: 0.6610 - val_loss: 0.9946 - val_acc: 0.6154\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7633 - acc: 0.6441 - val_loss: 0.9916 - val_acc: 0.6154\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7610 - acc: 0.6695 - val_loss: 0.9892 - val_acc: 0.6154\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7546 - acc: 0.6610 - val_loss: 0.9921 - val_acc: 0.6154\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7506 - acc: 0.6695 - val_loss: 0.9946 - val_acc: 0.6154\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7473 - acc: 0.6610 - val_loss: 0.9977 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.7435 - acc: 0.6864 - val_loss: 1.0001 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7384 - acc: 0.6695 - val_loss: 1.0017 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7356 - acc: 0.6864 - val_loss: 1.0124 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7304 - acc: 0.6949 - val_loss: 1.0118 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7254 - acc: 0.6864 - val_loss: 1.0156 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7226 - acc: 0.6949 - val_loss: 1.0240 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.7194 - acc: 0.7203 - val_loss: 1.0255 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 250us/step - loss: 0.7159 - acc: 0.6949 - val_loss: 1.0312 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7110 - acc: 0.6949 - val_loss: 1.0452 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7071 - acc: 0.6949 - val_loss: 1.0467 - val_acc: 0.6923\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.7056 - acc: 0.7203 - val_loss: 1.0415 - val_acc: 0.6923\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.7007 - acc: 0.7034 - val_loss: 1.0418 - val_acc: 0.6923\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6982 - acc: 0.7119 - val_loss: 1.0482 - val_acc: 0.6154\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6936 - acc: 0.7034 - val_loss: 1.0577 - val_acc: 0.6154\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6907 - acc: 0.6949 - val_loss: 1.0553 - val_acc: 0.6154\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6879 - acc: 0.7034 - val_loss: 1.0623 - val_acc: 0.6154\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6838 - acc: 0.7034 - val_loss: 1.0590 - val_acc: 0.6154\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6837 - acc: 0.7203 - val_loss: 1.0569 - val_acc: 0.6154\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6799 - acc: 0.7203 - val_loss: 1.0676 - val_acc: 0.6154\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6748 - acc: 0.7203 - val_loss: 1.0729 - val_acc: 0.6154\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6732 - acc: 0.7119 - val_loss: 1.0823 - val_acc: 0.6154\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6716 - acc: 0.7458 - val_loss: 1.0899 - val_acc: 0.6154\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6687 - acc: 0.7373 - val_loss: 1.0904 - val_acc: 0.6154\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.6648 - acc: 0.7288 - val_loss: 1.0852 - val_acc: 0.6154\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6627 - acc: 0.7458 - val_loss: 1.0961 - val_acc: 0.6154\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.6601 - acc: 0.7203 - val_loss: 1.0950 - val_acc: 0.6154\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6558 - acc: 0.7458 - val_loss: 1.1069 - val_acc: 0.6154\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6552 - acc: 0.7288 - val_loss: 1.1069 - val_acc: 0.6154\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6521 - acc: 0.7373 - val_loss: 1.1064 - val_acc: 0.6154\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6505 - acc: 0.7288 - val_loss: 1.1147 - val_acc: 0.6154\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6461 - acc: 0.7458 - val_loss: 1.1116 - val_acc: 0.6154\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6426 - acc: 0.7373 - val_loss: 1.1210 - val_acc: 0.6154\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6407 - acc: 0.7288 - val_loss: 1.1216 - val_acc: 0.6154\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6409 - acc: 0.7458 - val_loss: 1.1289 - val_acc: 0.6154\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6387 - acc: 0.7458 - val_loss: 1.1264 - val_acc: 0.6154\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.6348 - acc: 0.7373 - val_loss: 1.1228 - val_acc: 0.6154\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.6317 - acc: 0.7458 - val_loss: 1.1340 - val_acc: 0.6154\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.6299 - acc: 0.7373 - val_loss: 1.1405 - val_acc: 0.6154\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.6280 - acc: 0.7458 - val_loss: 1.1459 - val_acc: 0.6154\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.6262 - acc: 0.7458 - val_loss: 1.1496 - val_acc: 0.6154\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6245 - acc: 0.7542 - val_loss: 1.1637 - val_acc: 0.6154\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6208 - acc: 0.7627 - val_loss: 1.1550 - val_acc: 0.6154\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6210 - acc: 0.7458 - val_loss: 1.1590 - val_acc: 0.6154\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.6168 - acc: 0.7542 - val_loss: 1.1658 - val_acc: 0.6154\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6173 - acc: 0.7373 - val_loss: 1.1691 - val_acc: 0.6154\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6138 - acc: 0.7712 - val_loss: 1.1740 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6113 - acc: 0.7542 - val_loss: 1.1717 - val_acc: 0.6154\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6088 - acc: 0.7627 - val_loss: 1.1806 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.6068 - acc: 0.7627 - val_loss: 1.1712 - val_acc: 0.6154\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6049 - acc: 0.7627 - val_loss: 1.1863 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.6021 - acc: 0.7627 - val_loss: 1.1991 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.5987 - acc: 0.7627 - val_loss: 1.1942 - val_acc: 0.6154\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.5997 - acc: 0.7627 - val_loss: 1.1855 - val_acc: 0.6154\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.5965 - acc: 0.7542 - val_loss: 1.1933 - val_acc: 0.6154\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.5937 - acc: 0.7458 - val_loss: 1.1984 - val_acc: 0.6154\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5930 - acc: 0.7627 - val_loss: 1.2105 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5891 - acc: 0.7712 - val_loss: 1.2212 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.5879 - acc: 0.7712 - val_loss: 1.2192 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5880 - acc: 0.7627 - val_loss: 1.2281 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 12ms/step - loss: 1.3962 - acc: 0.3051 - val_loss: 1.6154 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.2028 - acc: 0.3644 - val_loss: 1.4448 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.0875 - acc: 0.4407 - val_loss: 1.3569 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0219 - acc: 0.5000 - val_loss: 1.3215 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9777 - acc: 0.5254 - val_loss: 1.3109 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9463 - acc: 0.5678 - val_loss: 1.2997 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9253 - acc: 0.5847 - val_loss: 1.3077 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9103 - acc: 0.5932 - val_loss: 1.3137 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8994 - acc: 0.5847 - val_loss: 1.3217 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.8893 - acc: 0.5847 - val_loss: 1.3229 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8818 - acc: 0.6017 - val_loss: 1.3307 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.8755 - acc: 0.6017 - val_loss: 1.3357 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8677 - acc: 0.6186 - val_loss: 1.3588 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.8635 - acc: 0.6102 - val_loss: 1.3627 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8609 - acc: 0.6102 - val_loss: 1.3735 - val_acc: 0.4615\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8547 - acc: 0.6186 - val_loss: 1.3819 - val_acc: 0.4615\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8504 - acc: 0.6271 - val_loss: 1.3785 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.8434 - acc: 0.6186 - val_loss: 1.4027 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8405 - acc: 0.6186 - val_loss: 1.4160 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8347 - acc: 0.6271 - val_loss: 1.4198 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.8320 - acc: 0.6271 - val_loss: 1.4323 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.8281 - acc: 0.6271 - val_loss: 1.4392 - val_acc: 0.3846\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.8247 - acc: 0.6356 - val_loss: 1.4596 - val_acc: 0.3846\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.8223 - acc: 0.6017 - val_loss: 1.4657 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8173 - acc: 0.6102 - val_loss: 1.4723 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.8141 - acc: 0.6356 - val_loss: 1.4921 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.8105 - acc: 0.6271 - val_loss: 1.4962 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.8094 - acc: 0.6356 - val_loss: 1.5090 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8055 - acc: 0.6271 - val_loss: 1.5140 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.8049 - acc: 0.6441 - val_loss: 1.5318 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8001 - acc: 0.6441 - val_loss: 1.5353 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7976 - acc: 0.6356 - val_loss: 1.5542 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.7931 - acc: 0.6441 - val_loss: 1.5736 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7900 - acc: 0.6441 - val_loss: 1.5723 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7874 - acc: 0.6441 - val_loss: 1.5938 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.7846 - acc: 0.6441 - val_loss: 1.5979 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7832 - acc: 0.6525 - val_loss: 1.6243 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7770 - acc: 0.6610 - val_loss: 1.6397 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7744 - acc: 0.6441 - val_loss: 1.6440 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.7719 - acc: 0.6610 - val_loss: 1.6525 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7676 - acc: 0.6695 - val_loss: 1.6691 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7650 - acc: 0.6780 - val_loss: 1.6619 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7611 - acc: 0.6695 - val_loss: 1.6955 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.7581 - acc: 0.6695 - val_loss: 1.6934 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.7557 - acc: 0.6864 - val_loss: 1.7154 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.7516 - acc: 0.6610 - val_loss: 1.7284 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7493 - acc: 0.6780 - val_loss: 1.7508 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.7449 - acc: 0.6610 - val_loss: 1.7577 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.7427 - acc: 0.6780 - val_loss: 1.7510 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7393 - acc: 0.6864 - val_loss: 1.7815 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7351 - acc: 0.6864 - val_loss: 1.8034 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7334 - acc: 0.6780 - val_loss: 1.8082 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7279 - acc: 0.6864 - val_loss: 1.8215 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.7279 - acc: 0.6949 - val_loss: 1.8116 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7247 - acc: 0.6864 - val_loss: 1.8274 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.7210 - acc: 0.7034 - val_loss: 1.8437 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7185 - acc: 0.6949 - val_loss: 1.8717 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7148 - acc: 0.7034 - val_loss: 1.8813 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 239us/step - loss: 0.7139 - acc: 0.6949 - val_loss: 1.8951 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 0.7109 - acc: 0.6949 - val_loss: 1.9069 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7091 - acc: 0.6949 - val_loss: 1.9243 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.7054 - acc: 0.7119 - val_loss: 1.9406 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7031 - acc: 0.7034 - val_loss: 1.9493 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.6999 - acc: 0.7034 - val_loss: 1.9571 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6951 - acc: 0.7119 - val_loss: 1.9724 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.6935 - acc: 0.7034 - val_loss: 1.9872 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6901 - acc: 0.7034 - val_loss: 2.0040 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6865 - acc: 0.7119 - val_loss: 2.0027 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6846 - acc: 0.7119 - val_loss: 2.0105 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.6797 - acc: 0.7119 - val_loss: 2.0248 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6776 - acc: 0.7288 - val_loss: 2.0270 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6754 - acc: 0.7288 - val_loss: 2.0381 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6722 - acc: 0.7203 - val_loss: 2.0523 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 0.6707 - acc: 0.7373 - val_loss: 2.0575 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6675 - acc: 0.7458 - val_loss: 2.0843 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.6648 - acc: 0.7373 - val_loss: 2.0922 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6615 - acc: 0.7542 - val_loss: 2.0982 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6579 - acc: 0.7458 - val_loss: 2.1201 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6560 - acc: 0.7373 - val_loss: 2.1148 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6552 - acc: 0.7458 - val_loss: 2.1410 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6501 - acc: 0.7542 - val_loss: 2.1494 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.6482 - acc: 0.7627 - val_loss: 2.1537 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.6449 - acc: 0.7542 - val_loss: 2.1562 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6431 - acc: 0.7627 - val_loss: 2.1690 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.6403 - acc: 0.7542 - val_loss: 2.1886 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6381 - acc: 0.7627 - val_loss: 2.1948 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.6330 - acc: 0.7627 - val_loss: 2.2013 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6307 - acc: 0.7712 - val_loss: 2.2041 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.6292 - acc: 0.7712 - val_loss: 2.2234 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.6260 - acc: 0.7797 - val_loss: 2.2322 - val_acc: 0.3846\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.6255 - acc: 0.7542 - val_loss: 2.2479 - val_acc: 0.3846\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6207 - acc: 0.7712 - val_loss: 2.2356 - val_acc: 0.3846\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.6189 - acc: 0.7627 - val_loss: 2.2504 - val_acc: 0.3846\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6153 - acc: 0.7627 - val_loss: 2.2577 - val_acc: 0.3846\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6140 - acc: 0.7627 - val_loss: 2.2761 - val_acc: 0.3846\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6110 - acc: 0.7797 - val_loss: 2.2867 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6073 - acc: 0.7797 - val_loss: 2.3103 - val_acc: 0.3846\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.6058 - acc: 0.7966 - val_loss: 2.3074 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.6042 - acc: 0.7797 - val_loss: 2.3130 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6009 - acc: 0.7881 - val_loss: 2.3190 - val_acc: 0.3846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3b367b32-1a40-436e-e87e-9be4de75a5f4",
        "id": "6OH7qa6a14-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7.34257050e-01,  9.68988268e-02,  3.85769839e-01,\n",
              "         1.61200050e-01, -5.40961150e-01, -3.83658358e-01,\n",
              "        -1.13750969e-01, -2.28591476e-01, -1.15055286e-01,\n",
              "        -3.30299955e-01,  1.35982249e-01,  3.71533408e-01,\n",
              "        -1.21254356e-01, -4.84378805e-01, -5.30458542e-01,\n",
              "        -4.87994766e-01,  1.88481002e-01],\n",
              "       [ 1.54957687e+00,  1.90539890e+00, -2.22052320e-01,\n",
              "         3.87307146e-01, -5.45340485e-01, -1.01385115e+00,\n",
              "        -2.73626628e-01, -6.67618363e-01, -3.33713506e-01,\n",
              "        -3.30970545e-01, -1.05552128e+00, -1.16554466e+00,\n",
              "         7.92773556e-01, -6.04756035e-03, -1.36103435e-01,\n",
              "         2.84618093e-01, -7.68000271e-01],\n",
              "       [ 1.00150735e+00, -8.58201787e-01,  3.11646964e-01,\n",
              "        -1.43442573e+00, -5.49929348e-01, -9.04252402e-01,\n",
              "        -5.28211743e-01, -1.20002808e-01, -3.35769175e-01,\n",
              "        -3.30973760e-01, -4.42456637e-01,  2.35041057e+00,\n",
              "        -1.82929991e+00,  2.68994980e-01,  2.63286133e+00,\n",
              "         1.29973450e+00, -1.07303619e+00],\n",
              "       [ 5.22267532e-01, -1.06875098e-01, -6.47157410e-01,\n",
              "        -2.81002626e-02, -5.47216346e-01,  5.47366256e-02,\n",
              "        -9.22810177e-01,  2.97018333e-01, -1.85535435e-01,\n",
              "        -3.30751288e-01, -1.01051841e+00,  5.33572761e-01,\n",
              "        -8.41591880e-01, -1.03863818e+00, -5.91370564e-01,\n",
              "        -4.78529164e-01,  7.65523623e-02],\n",
              "       [ 2.31051081e-01, -3.20246036e-01,  3.33606515e-01,\n",
              "        -1.92448965e+00, -5.47077623e-01, -1.39470679e+00,\n",
              "         1.71658188e-01, -8.49813810e-01, -3.35016786e-01,\n",
              "        -3.30973339e-01, -1.13057908e+00, -3.96647241e-02,\n",
              "        -7.76402925e-02, -1.46431122e-01,  1.57365298e+00,\n",
              "         2.06583169e+00, -9.92270817e-01],\n",
              "       [-6.44453936e-01, -8.78723589e-03, -1.04418142e-01,\n",
              "        -5.22952367e-01,  9.09313099e-01,  1.69871781e+00,\n",
              "        -1.21474566e+00,  1.75610763e+00, -9.44599440e-02,\n",
              "        -3.30471019e-01,  8.05783791e-01, -1.11896137e+00,\n",
              "        -5.71419421e-01, -1.13904175e+00,  2.36554102e-01,\n",
              "        -7.98738149e-01, -5.48945706e-01],\n",
              "       [-7.06431308e-01, -9.89748699e-03,  1.32801751e+00,\n",
              "        -1.37618450e+00, -5.39625628e-01,  6.60326170e+00,\n",
              "         3.83648904e+00, -1.69002919e+00, -2.97148140e-01,\n",
              "        -3.30858604e-01,  2.98396859e+00, -8.38653288e-01,\n",
              "         2.67323855e+00,  1.97331940e+00,  4.59022522e-01,\n",
              "        -5.16599725e-01, -7.19356677e-01],\n",
              "       [ 2.06934501e+00,  1.21207305e+00, -8.02529761e-01,\n",
              "        -3.23665850e+00,  3.52435672e-01, -1.01385115e+00,\n",
              "         5.82901521e-02, -7.18648977e-01, -3.35432838e-01,\n",
              "        -3.30974284e-01, -1.66414662e+00,  2.66798240e+00,\n",
              "        -1.79401751e+00,  3.67356718e-01,  2.40487403e+00,\n",
              "         4.02913212e+00, -1.04661333e+00],\n",
              "       [ 1.78305420e+00,  1.37711411e+00,  1.49916132e-01,\n",
              "        -5.15432158e-01, -5.46664013e-01, -1.17824927e+00,\n",
              "        -7.14148531e-01, -3.26597763e-01, -3.31802117e-01,\n",
              "        -3.30964935e-01, -1.10153207e+00, -3.25076660e-01,\n",
              "        -2.76899548e-01, -4.81278689e-01,  1.63951096e-01,\n",
              "         7.64810335e-01, -8.13461099e-01],\n",
              "       [ 1.33916856e+00,  1.74729366e+00,  9.32576683e-01,\n",
              "        -3.92248540e-01, -5.42436939e-01, -5.48056477e-01,\n",
              "         5.42853861e-01, -3.37319011e-01,  1.32217300e-01,\n",
              "        -3.30650739e-01,  9.93278185e-02,  2.18568209e+00,\n",
              "        -7.52734919e-01, -5.58159325e-01, -2.94368551e-01,\n",
              "         3.00978986e-04, -1.51329794e-01],\n",
              "       [ 4.98694935e-01,  7.35612793e-01,  6.35586964e-01,\n",
              "         6.54250049e-01,  3.16485533e-02, -1.64460866e-01,\n",
              "        -3.77661612e-01,  2.09671116e+00, -2.80318175e-01,\n",
              "        -3.30830305e-01,  6.43998628e-01,  5.19144703e-01,\n",
              "        -1.77373780e+00, -1.05512656e+00,  3.58224772e+00,\n",
              "        -7.01535374e-01, -9.40195936e-01],\n",
              "       [-1.70779933e+00, -1.82177506e+00,  6.30653686e-01,\n",
              "         7.72815870e-02, -5.41667522e-01, -7.94653656e-01,\n",
              "        -2.36091860e-01, -5.49058455e-01, -3.32324336e-01,\n",
              "        -3.30964704e-01, -2.78998649e-01,  7.31967045e-01,\n",
              "        -5.20673201e-02,  1.47627164e+00, -2.60634812e-01,\n",
              "         1.91408748e-01, -6.63483974e-01],\n",
              "       [-2.61293931e-01, -8.16561598e-01,  1.04338706e+00,\n",
              "        -1.15650705e+00, -5.41614180e-01,  2.60290747e+00,\n",
              "         1.30285401e+00, -7.54923071e-01, -2.82469107e-01,\n",
              "        -3.30793928e-01,  5.28076978e-01, -1.69820568e-01,\n",
              "         8.39421745e-01, -4.61928261e-01, -2.21762703e-01,\n",
              "        -2.37855717e-01, -5.14552169e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WoqA4rT14-Q",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ab81c88b-12b2-4928-a414-9ecf27b67314",
        "id": "YEyjhsg414-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1066871e-b732-49ed-cf7e-7c82306f636a",
        "id": "9WylvzXG14-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw9GTwCC14-f",
        "colab": {}
      },
      "source": [
        "average_acc_history_reduced = [np.mean([x[i] for x in all_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_reduced = [np.mean([x[i] for x in all_loss_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_acc_history_reduced = [np.mean([x[i] for x in all_val_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_loss_history_reduced = [np.mean([x[i] for x in all_val_loss_histories_reduced]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7b9d5b3f-c7d2-4ae9-9a7e-1b6bfb09a613",
        "id": "MdMC3doS14-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history_reduced)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "outputId": "68ce9b4f-677a-429a-b207-8806a00b20c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_32\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_63 (Dense)             (None, 10)                180       \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrcg4Bx625fC"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9_gKwYk25fK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCcaAgTi25fZ",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "563709de-d959-4960-a747-de3f56ff654e",
        "id": "6H8nOl_X25fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_reduced, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_reduced, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc751d58390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nicAYY6Lc/wNF8XEdqzZn8gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMh2BG152967"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "86624a9f-cbf2-4b3b-e896-7f1a77f75326",
        "id": "WNEq_n08297J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_reduced, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_reduced, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc751cc2cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5BQnjMi93Dgn"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a387b094-cdd7-45a7-e36c-534ffa0fcff4",
        "id": "yANP0XPF3Dg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_reduced, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_reduced, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 9ms/step - loss: 1.1641 - acc: 0.4122\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0482 - acc: 0.4427\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9928 - acc: 0.5267\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9662 - acc: 0.5725\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9504 - acc: 0.5802\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9380 - acc: 0.5573\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9281 - acc: 0.5954\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9203 - acc: 0.5954\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9110 - acc: 0.5802\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9018 - acc: 0.5954\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8971 - acc: 0.5725\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8903 - acc: 0.5573\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8835 - acc: 0.5725\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8788 - acc: 0.5878\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8731 - acc: 0.5878\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8675 - acc: 0.5878\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8625 - acc: 0.5725\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8600 - acc: 0.5954\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 234us/step - loss: 0.8578 - acc: 0.5954\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.8508 - acc: 0.5802\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8459 - acc: 0.5802\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8424 - acc: 0.5878\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8408 - acc: 0.5802\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8383 - acc: 0.5954\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.8320 - acc: 0.5802\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 230us/step - loss: 0.8285 - acc: 0.5878\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.8241 - acc: 0.5954\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8241 - acc: 0.5802\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.8186 - acc: 0.5878\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.8168 - acc: 0.5878\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8130 - acc: 0.5802\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.8115 - acc: 0.6031\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.8073 - acc: 0.6031\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.8052 - acc: 0.6107\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.8008 - acc: 0.6183\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.7981 - acc: 0.5954\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 249us/step - loss: 0.7972 - acc: 0.6031\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.7947 - acc: 0.6031\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.7908 - acc: 0.6031\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.7880 - acc: 0.6183\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.7843 - acc: 0.6183\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.7813 - acc: 0.6107\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 0.7797 - acc: 0.6336\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.7761 - acc: 0.6183\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.7731 - acc: 0.6412\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.7708 - acc: 0.6336\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.7670 - acc: 0.6412\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.7666 - acc: 0.6336\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.7643 - acc: 0.6183\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 236us/step - loss: 0.7604 - acc: 0.6565\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.7565 - acc: 0.6489\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.7545 - acc: 0.6489\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.7509 - acc: 0.6412\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.7473 - acc: 0.6565\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.7439 - acc: 0.6489\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.7471 - acc: 0.6718\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.7412 - acc: 0.6718\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.7389 - acc: 0.6718\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.7383 - acc: 0.6641\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.7371 - acc: 0.6641\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.7349 - acc: 0.6565\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.7302 - acc: 0.6641\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 275us/step - loss: 0.7297 - acc: 0.6794\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.7283 - acc: 0.6718\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.7256 - acc: 0.6718\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.7215 - acc: 0.6794\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.7181 - acc: 0.6718\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.7173 - acc: 0.6794\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.7149 - acc: 0.6794\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.7145 - acc: 0.6718\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.7093 - acc: 0.6794\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.7099 - acc: 0.6718\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.7068 - acc: 0.6870\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.7035 - acc: 0.6794\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.7021 - acc: 0.6718\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.7011 - acc: 0.6947\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 229us/step - loss: 0.7012 - acc: 0.6947\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.6964 - acc: 0.6718\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.6944 - acc: 0.6641\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.6917 - acc: 0.6794\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 234us/step - loss: 0.6906 - acc: 0.6870\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 222us/step - loss: 0.6889 - acc: 0.6794\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6884 - acc: 0.6718\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.6829 - acc: 0.6718\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 222us/step - loss: 0.6832 - acc: 0.6718\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.6793 - acc: 0.7023\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.6821 - acc: 0.6718\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.6792 - acc: 0.6794\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.6767 - acc: 0.6794\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.6750 - acc: 0.6947\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6723 - acc: 0.7099\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.6711 - acc: 0.6870\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.6696 - acc: 0.6870\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.6694 - acc: 0.7099\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.6644 - acc: 0.7099\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.6658 - acc: 0.6870\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.6635 - acc: 0.6870\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.6618 - acc: 0.7023\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6603 - acc: 0.6870\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.6580 - acc: 0.7099\n",
            "34/34 [==============================] - 0s 14ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "83ac8c61-d2a1-4bfc-d8e5-a740e1f8759f",
        "id": "kQLgWQtS3DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b90552b3-6838-47c7-fbff-82b8080d9932",
        "id": "_c5TGT4H3DhR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17647058823529413"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMbKL5Yw3DhZ"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}
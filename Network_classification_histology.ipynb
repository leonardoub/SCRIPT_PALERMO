{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "c7567976-4ef6-4e28-8876-2e7610a7f20b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "edc04c7b-888d-4254-ef3d-71aa19bffa8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "e4cc34ea-f8e9-4d98-b963-c3439c9e4fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "4c70d71e-011e-4916-ea72-ef2af42ca79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4NQA5c48ju7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "dd053e31-f72d-4b9c-f3e9-89d87ce26cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "85da60f2-b773-47e6-d70d-40fcad1adaf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "c1e2fcbb-cb08-412c-e667-dda4efef0e91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "7959c16a-3377-44cb-ab2f-69347a57dc1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 1.5873 - acc: 0.3761 - val_loss: 1.5852 - val_acc: 0.3571\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 151us/step - loss: 1.5135 - acc: 0.3932 - val_loss: 1.5376 - val_acc: 0.3571\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 1.4540 - acc: 0.4188 - val_loss: 1.5014 - val_acc: 0.3571\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 1.4007 - acc: 0.4615 - val_loss: 1.4647 - val_acc: 0.3571\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 1.3584 - acc: 0.4530 - val_loss: 1.4282 - val_acc: 0.3571\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 1.3225 - acc: 0.4701 - val_loss: 1.4016 - val_acc: 0.3571\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 154us/step - loss: 1.2899 - acc: 0.4701 - val_loss: 1.3742 - val_acc: 0.3571\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 154us/step - loss: 1.2619 - acc: 0.4786 - val_loss: 1.3491 - val_acc: 0.3571\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 1.2362 - acc: 0.4786 - val_loss: 1.3283 - val_acc: 0.4286\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 196us/step - loss: 1.2134 - acc: 0.4701 - val_loss: 1.3096 - val_acc: 0.4286\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 158us/step - loss: 1.1944 - acc: 0.4530 - val_loss: 1.2921 - val_acc: 0.3571\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 1.1756 - acc: 0.4615 - val_loss: 1.2766 - val_acc: 0.3571\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 1.1590 - acc: 0.4615 - val_loss: 1.2591 - val_acc: 0.3571\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 1.1439 - acc: 0.4444 - val_loss: 1.2442 - val_acc: 0.3571\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 225us/step - loss: 1.1298 - acc: 0.4444 - val_loss: 1.2318 - val_acc: 0.3571\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 1.1178 - acc: 0.4444 - val_loss: 1.2188 - val_acc: 0.3571\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 187us/step - loss: 1.1058 - acc: 0.4444 - val_loss: 1.2080 - val_acc: 0.3571\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 1.0942 - acc: 0.4444 - val_loss: 1.1968 - val_acc: 0.3571\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 223us/step - loss: 1.0842 - acc: 0.4530 - val_loss: 1.1868 - val_acc: 0.3571\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 1.0745 - acc: 0.4615 - val_loss: 1.1785 - val_acc: 0.3571\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 1.0671 - acc: 0.4530 - val_loss: 1.1690 - val_acc: 0.3571\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.0585 - acc: 0.4615 - val_loss: 1.1606 - val_acc: 0.3571\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 245us/step - loss: 1.0510 - acc: 0.4701 - val_loss: 1.1507 - val_acc: 0.4286\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 242us/step - loss: 1.0438 - acc: 0.4872 - val_loss: 1.1433 - val_acc: 0.4286\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 294us/step - loss: 1.0372 - acc: 0.4701 - val_loss: 1.1376 - val_acc: 0.4286\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 1.0312 - acc: 0.4786 - val_loss: 1.1304 - val_acc: 0.4286\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 1.0259 - acc: 0.4957 - val_loss: 1.1240 - val_acc: 0.4286\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 161us/step - loss: 1.0207 - acc: 0.4701 - val_loss: 1.1151 - val_acc: 0.4286\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.0159 - acc: 0.4957 - val_loss: 1.1107 - val_acc: 0.4286\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 1.0114 - acc: 0.5128 - val_loss: 1.1070 - val_acc: 0.4286\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 259us/step - loss: 1.0076 - acc: 0.4872 - val_loss: 1.0978 - val_acc: 0.4286\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 1.0032 - acc: 0.4957 - val_loss: 1.0940 - val_acc: 0.4286\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9995 - acc: 0.5043 - val_loss: 1.0911 - val_acc: 0.4286\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.9969 - acc: 0.5043 - val_loss: 1.0867 - val_acc: 0.4286\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9935 - acc: 0.4957 - val_loss: 1.0833 - val_acc: 0.4286\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9906 - acc: 0.5128 - val_loss: 1.0804 - val_acc: 0.4286\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9876 - acc: 0.5128 - val_loss: 1.0775 - val_acc: 0.4286\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9863 - acc: 0.5214 - val_loss: 1.0731 - val_acc: 0.4286\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 156us/step - loss: 0.9829 - acc: 0.5128 - val_loss: 1.0701 - val_acc: 0.4286\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 0.9804 - acc: 0.5214 - val_loss: 1.0672 - val_acc: 0.4286\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 156us/step - loss: 0.9786 - acc: 0.5214 - val_loss: 1.0654 - val_acc: 0.4286\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 191us/step - loss: 0.9765 - acc: 0.5214 - val_loss: 1.0633 - val_acc: 0.4286\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 194us/step - loss: 0.9750 - acc: 0.5214 - val_loss: 1.0616 - val_acc: 0.4286\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.9730 - acc: 0.5214 - val_loss: 1.0583 - val_acc: 0.4286\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9714 - acc: 0.5214 - val_loss: 1.0555 - val_acc: 0.4286\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9696 - acc: 0.5214 - val_loss: 1.0538 - val_acc: 0.4286\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9684 - acc: 0.5214 - val_loss: 1.0517 - val_acc: 0.4286\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.9671 - acc: 0.5214 - val_loss: 1.0498 - val_acc: 0.4286\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 222us/step - loss: 0.9656 - acc: 0.5299 - val_loss: 1.0472 - val_acc: 0.4286\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 217us/step - loss: 0.9642 - acc: 0.5299 - val_loss: 1.0456 - val_acc: 0.4286\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.9629 - acc: 0.5299 - val_loss: 1.0445 - val_acc: 0.4286\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9618 - acc: 0.5299 - val_loss: 1.0442 - val_acc: 0.4286\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9616 - acc: 0.5299 - val_loss: 1.0414 - val_acc: 0.4286\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 154us/step - loss: 0.9606 - acc: 0.5214 - val_loss: 1.0401 - val_acc: 0.4286\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 153us/step - loss: 0.9594 - acc: 0.5214 - val_loss: 1.0399 - val_acc: 0.4286\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.9587 - acc: 0.5214 - val_loss: 1.0386 - val_acc: 0.4286\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9580 - acc: 0.5299 - val_loss: 1.0373 - val_acc: 0.4286\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9571 - acc: 0.5214 - val_loss: 1.0368 - val_acc: 0.4286\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9564 - acc: 0.5214 - val_loss: 1.0356 - val_acc: 0.4286\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9556 - acc: 0.5299 - val_loss: 1.0352 - val_acc: 0.4286\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9546 - acc: 0.5214 - val_loss: 1.0348 - val_acc: 0.4286\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9550 - acc: 0.5385 - val_loss: 1.0325 - val_acc: 0.4286\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9530 - acc: 0.5214 - val_loss: 1.0330 - val_acc: 0.4286\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9531 - acc: 0.5556 - val_loss: 1.0311 - val_acc: 0.4286\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.9522 - acc: 0.5556 - val_loss: 1.0300 - val_acc: 0.4286\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9519 - acc: 0.5556 - val_loss: 1.0300 - val_acc: 0.4286\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9506 - acc: 0.5556 - val_loss: 1.0304 - val_acc: 0.4286\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.9502 - acc: 0.5556 - val_loss: 1.0289 - val_acc: 0.4286\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.9491 - acc: 0.5556 - val_loss: 1.0285 - val_acc: 0.4286\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9491 - acc: 0.5556 - val_loss: 1.0292 - val_acc: 0.4286\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9483 - acc: 0.5556 - val_loss: 1.0274 - val_acc: 0.4286\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9481 - acc: 0.5556 - val_loss: 1.0267 - val_acc: 0.4286\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.9472 - acc: 0.5556 - val_loss: 1.0270 - val_acc: 0.4286\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9470 - acc: 0.5556 - val_loss: 1.0262 - val_acc: 0.4286\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9463 - acc: 0.5556 - val_loss: 1.0266 - val_acc: 0.4286\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.9461 - acc: 0.5556 - val_loss: 1.0254 - val_acc: 0.4286\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9453 - acc: 0.5556 - val_loss: 1.0251 - val_acc: 0.4286\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 199us/step - loss: 0.9449 - acc: 0.5470 - val_loss: 1.0250 - val_acc: 0.4286\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9446 - acc: 0.5556 - val_loss: 1.0250 - val_acc: 0.4286\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9439 - acc: 0.5556 - val_loss: 1.0246 - val_acc: 0.4286\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.9437 - acc: 0.5556 - val_loss: 1.0249 - val_acc: 0.4286\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9434 - acc: 0.5470 - val_loss: 1.0243 - val_acc: 0.4286\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9437 - acc: 0.5470 - val_loss: 1.0247 - val_acc: 0.4286\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 0.9422 - acc: 0.5470 - val_loss: 1.0235 - val_acc: 0.4286\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9419 - acc: 0.5470 - val_loss: 1.0249 - val_acc: 0.4286\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9414 - acc: 0.5470 - val_loss: 1.0247 - val_acc: 0.4286\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 176us/step - loss: 0.9412 - acc: 0.5470 - val_loss: 1.0241 - val_acc: 0.4286\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9406 - acc: 0.5470 - val_loss: 1.0239 - val_acc: 0.4286\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9401 - acc: 0.5470 - val_loss: 1.0240 - val_acc: 0.4286\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 153us/step - loss: 0.9398 - acc: 0.5470 - val_loss: 1.0242 - val_acc: 0.4286\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 154us/step - loss: 0.9397 - acc: 0.5470 - val_loss: 1.0245 - val_acc: 0.4286\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 156us/step - loss: 0.9394 - acc: 0.5470 - val_loss: 1.0235 - val_acc: 0.4286\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 0.9401 - acc: 0.5470 - val_loss: 1.0238 - val_acc: 0.4286\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9390 - acc: 0.5470 - val_loss: 1.0241 - val_acc: 0.4286\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.9387 - acc: 0.5470 - val_loss: 1.0247 - val_acc: 0.4286\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 201us/step - loss: 0.9381 - acc: 0.5470 - val_loss: 1.0249 - val_acc: 0.4286\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.9380 - acc: 0.5470 - val_loss: 1.0257 - val_acc: 0.4286\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.9375 - acc: 0.5470 - val_loss: 1.0251 - val_acc: 0.4286\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.9372 - acc: 0.5470 - val_loss: 1.0261 - val_acc: 0.4286\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.9375 - acc: 0.5470 - val_loss: 1.0251 - val_acc: 0.4286\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 1ms/step - loss: 1.4081 - acc: 0.4153 - val_loss: 1.0320 - val_acc: 0.5385\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.3412 - acc: 0.4153 - val_loss: 0.9897 - val_acc: 0.5385\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.2886 - acc: 0.4237 - val_loss: 0.9570 - val_acc: 0.6154\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.2440 - acc: 0.4068 - val_loss: 0.9342 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.2071 - acc: 0.3983 - val_loss: 0.9227 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1798 - acc: 0.4153 - val_loss: 0.9153 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1565 - acc: 0.3983 - val_loss: 0.9088 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1385 - acc: 0.3983 - val_loss: 0.9051 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 1.1242 - acc: 0.3898 - val_loss: 0.9040 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 148us/step - loss: 1.1122 - acc: 0.4068 - val_loss: 0.9036 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1038 - acc: 0.4068 - val_loss: 0.9056 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0943 - acc: 0.4153 - val_loss: 0.9070 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0880 - acc: 0.4068 - val_loss: 0.9084 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0828 - acc: 0.3983 - val_loss: 0.9090 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0763 - acc: 0.4153 - val_loss: 0.9097 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0709 - acc: 0.4068 - val_loss: 0.9124 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0661 - acc: 0.4068 - val_loss: 0.9122 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0602 - acc: 0.4068 - val_loss: 0.9172 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0547 - acc: 0.4068 - val_loss: 0.9235 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0508 - acc: 0.4661 - val_loss: 0.9275 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0459 - acc: 0.5085 - val_loss: 0.9346 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 1.0415 - acc: 0.5085 - val_loss: 0.9408 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0377 - acc: 0.5085 - val_loss: 0.9460 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0341 - acc: 0.5169 - val_loss: 0.9500 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0318 - acc: 0.5085 - val_loss: 0.9552 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0283 - acc: 0.5169 - val_loss: 0.9613 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0254 - acc: 0.5000 - val_loss: 0.9662 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0223 - acc: 0.5000 - val_loss: 0.9704 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0196 - acc: 0.5169 - val_loss: 0.9743 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0182 - acc: 0.5085 - val_loss: 0.9786 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 1.0153 - acc: 0.5085 - val_loss: 0.9821 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0129 - acc: 0.5000 - val_loss: 0.9898 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0108 - acc: 0.5085 - val_loss: 0.9918 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0088 - acc: 0.5169 - val_loss: 0.9955 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0069 - acc: 0.5085 - val_loss: 1.0022 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0047 - acc: 0.5169 - val_loss: 1.0039 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0033 - acc: 0.5254 - val_loss: 1.0114 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 152us/step - loss: 1.0007 - acc: 0.5169 - val_loss: 1.0164 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9985 - acc: 0.5254 - val_loss: 1.0199 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9965 - acc: 0.5169 - val_loss: 1.0243 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9953 - acc: 0.5169 - val_loss: 1.0281 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9932 - acc: 0.5254 - val_loss: 1.0318 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9910 - acc: 0.5254 - val_loss: 1.0340 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9898 - acc: 0.5254 - val_loss: 1.0385 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9878 - acc: 0.5254 - val_loss: 1.0432 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9858 - acc: 0.5254 - val_loss: 1.0449 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9845 - acc: 0.5254 - val_loss: 1.0487 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9824 - acc: 0.5254 - val_loss: 1.0512 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9809 - acc: 0.5169 - val_loss: 1.0572 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9798 - acc: 0.5169 - val_loss: 1.0605 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9770 - acc: 0.5339 - val_loss: 1.0625 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9757 - acc: 0.5085 - val_loss: 1.0629 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9747 - acc: 0.5339 - val_loss: 1.0670 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9740 - acc: 0.5169 - val_loss: 1.0666 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9730 - acc: 0.5169 - val_loss: 1.0684 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9720 - acc: 0.5085 - val_loss: 1.0706 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9714 - acc: 0.5000 - val_loss: 1.0726 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9696 - acc: 0.5254 - val_loss: 1.0764 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9684 - acc: 0.5254 - val_loss: 1.0779 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9671 - acc: 0.5254 - val_loss: 1.0813 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9663 - acc: 0.5339 - val_loss: 1.0835 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9651 - acc: 0.5254 - val_loss: 1.0881 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9639 - acc: 0.5339 - val_loss: 1.0900 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9632 - acc: 0.5254 - val_loss: 1.0935 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9623 - acc: 0.5254 - val_loss: 1.0947 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9611 - acc: 0.5424 - val_loss: 1.0970 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9605 - acc: 0.5339 - val_loss: 1.1006 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9590 - acc: 0.5424 - val_loss: 1.1012 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9578 - acc: 0.5424 - val_loss: 1.1049 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9569 - acc: 0.5424 - val_loss: 1.1075 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9569 - acc: 0.5339 - val_loss: 1.1080 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9545 - acc: 0.5424 - val_loss: 1.1105 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9537 - acc: 0.5424 - val_loss: 1.1115 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9528 - acc: 0.5424 - val_loss: 1.1141 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9516 - acc: 0.5424 - val_loss: 1.1178 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9509 - acc: 0.5424 - val_loss: 1.1162 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9508 - acc: 0.5424 - val_loss: 1.1206 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9491 - acc: 0.5424 - val_loss: 1.1209 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9484 - acc: 0.5424 - val_loss: 1.1234 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9475 - acc: 0.5424 - val_loss: 1.1239 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9472 - acc: 0.5424 - val_loss: 1.1261 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9459 - acc: 0.5424 - val_loss: 1.1268 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9453 - acc: 0.5424 - val_loss: 1.1292 - val_acc: 0.4615\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 234us/step - loss: 0.9447 - acc: 0.5593 - val_loss: 1.1311 - val_acc: 0.4615\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9434 - acc: 0.5424 - val_loss: 1.1307 - val_acc: 0.4615\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.9428 - acc: 0.5424 - val_loss: 1.1324 - val_acc: 0.4615\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9418 - acc: 0.5424 - val_loss: 1.1342 - val_acc: 0.4615\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9414 - acc: 0.5508 - val_loss: 1.1398 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 151us/step - loss: 0.9403 - acc: 0.5593 - val_loss: 1.1404 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9390 - acc: 0.5678 - val_loss: 1.1412 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9386 - acc: 0.5678 - val_loss: 1.1417 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9378 - acc: 0.5678 - val_loss: 1.1441 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9372 - acc: 0.5678 - val_loss: 1.1429 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9366 - acc: 0.5508 - val_loss: 1.1428 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9360 - acc: 0.5678 - val_loss: 1.1445 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9357 - acc: 0.5678 - val_loss: 1.1446 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9346 - acc: 0.5678 - val_loss: 1.1453 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9346 - acc: 0.5678 - val_loss: 1.1480 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9339 - acc: 0.5678 - val_loss: 1.1453 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9335 - acc: 0.5678 - val_loss: 1.1465 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.9368 - acc: 0.3305 - val_loss: 1.1356 - val_acc: 0.6154\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.8047 - acc: 0.3305 - val_loss: 1.1384 - val_acc: 0.6154\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.6921 - acc: 0.3305 - val_loss: 1.1382 - val_acc: 0.6154\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.5994 - acc: 0.3220 - val_loss: 1.1383 - val_acc: 0.6154\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.5235 - acc: 0.3305 - val_loss: 1.1413 - val_acc: 0.6154\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.4586 - acc: 0.3220 - val_loss: 1.1416 - val_acc: 0.6154\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.4051 - acc: 0.3475 - val_loss: 1.1476 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.3581 - acc: 0.3475 - val_loss: 1.1502 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.3179 - acc: 0.3559 - val_loss: 1.1533 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2838 - acc: 0.3644 - val_loss: 1.1557 - val_acc: 0.6154\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.2532 - acc: 0.3559 - val_loss: 1.1588 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.2266 - acc: 0.3729 - val_loss: 1.1598 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 1.2046 - acc: 0.3644 - val_loss: 1.1618 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1808 - acc: 0.3644 - val_loss: 1.1657 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.1612 - acc: 0.3729 - val_loss: 1.1661 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1449 - acc: 0.3898 - val_loss: 1.1686 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.1281 - acc: 0.3983 - val_loss: 1.1702 - val_acc: 0.6154\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.1157 - acc: 0.4068 - val_loss: 1.1716 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.1015 - acc: 0.4153 - val_loss: 1.1721 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0896 - acc: 0.4407 - val_loss: 1.1730 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0795 - acc: 0.4661 - val_loss: 1.1733 - val_acc: 0.6923\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0689 - acc: 0.4915 - val_loss: 1.1747 - val_acc: 0.6923\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0600 - acc: 0.4831 - val_loss: 1.1744 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0493 - acc: 0.4915 - val_loss: 1.1748 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0416 - acc: 0.5339 - val_loss: 1.1732 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0334 - acc: 0.5424 - val_loss: 1.1728 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0261 - acc: 0.5424 - val_loss: 1.1730 - val_acc: 0.6923\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0190 - acc: 0.5593 - val_loss: 1.1741 - val_acc: 0.6923\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0127 - acc: 0.5593 - val_loss: 1.1727 - val_acc: 0.6923\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0059 - acc: 0.5593 - val_loss: 1.1707 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 143us/step - loss: 1.0006 - acc: 0.5678 - val_loss: 1.1701 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9955 - acc: 0.5763 - val_loss: 1.1702 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9915 - acc: 0.5763 - val_loss: 1.1687 - val_acc: 0.6923\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9852 - acc: 0.5763 - val_loss: 1.1672 - val_acc: 0.6923\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9815 - acc: 0.5763 - val_loss: 1.1663 - val_acc: 0.6923\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9764 - acc: 0.5763 - val_loss: 1.1659 - val_acc: 0.6923\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9745 - acc: 0.5932 - val_loss: 1.1645 - val_acc: 0.6923\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9672 - acc: 0.5847 - val_loss: 1.1648 - val_acc: 0.6923\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9647 - acc: 0.5763 - val_loss: 1.1637 - val_acc: 0.6923\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9593 - acc: 0.5847 - val_loss: 1.1631 - val_acc: 0.6923\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9564 - acc: 0.5847 - val_loss: 1.1622 - val_acc: 0.6923\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9536 - acc: 0.5847 - val_loss: 1.1613 - val_acc: 0.6923\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9498 - acc: 0.5847 - val_loss: 1.1599 - val_acc: 0.6923\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9473 - acc: 0.5932 - val_loss: 1.1597 - val_acc: 0.6923\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9448 - acc: 0.5847 - val_loss: 1.1590 - val_acc: 0.6923\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9400 - acc: 0.5847 - val_loss: 1.1591 - val_acc: 0.6923\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9384 - acc: 0.5763 - val_loss: 1.1579 - val_acc: 0.6923\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9356 - acc: 0.5847 - val_loss: 1.1566 - val_acc: 0.6923\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9323 - acc: 0.5847 - val_loss: 1.1562 - val_acc: 0.6923\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9306 - acc: 0.5847 - val_loss: 1.1561 - val_acc: 0.6923\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9290 - acc: 0.5847 - val_loss: 1.1549 - val_acc: 0.6923\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9248 - acc: 0.5847 - val_loss: 1.1547 - val_acc: 0.6923\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9226 - acc: 0.5847 - val_loss: 1.1543 - val_acc: 0.6923\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9202 - acc: 0.5847 - val_loss: 1.1525 - val_acc: 0.6923\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9193 - acc: 0.5847 - val_loss: 1.1527 - val_acc: 0.6923\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9158 - acc: 0.5847 - val_loss: 1.1510 - val_acc: 0.6923\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9136 - acc: 0.5847 - val_loss: 1.1501 - val_acc: 0.6923\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9123 - acc: 0.5847 - val_loss: 1.1496 - val_acc: 0.6923\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9104 - acc: 0.5847 - val_loss: 1.1493 - val_acc: 0.6923\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9084 - acc: 0.5847 - val_loss: 1.1485 - val_acc: 0.6923\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9059 - acc: 0.5847 - val_loss: 1.1475 - val_acc: 0.6923\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9053 - acc: 0.5932 - val_loss: 1.1476 - val_acc: 0.6923\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9029 - acc: 0.5932 - val_loss: 1.1470 - val_acc: 0.6923\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9011 - acc: 0.5932 - val_loss: 1.1469 - val_acc: 0.6923\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9001 - acc: 0.5932 - val_loss: 1.1465 - val_acc: 0.6923\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.8976 - acc: 0.5847 - val_loss: 1.1458 - val_acc: 0.6923\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8966 - acc: 0.5847 - val_loss: 1.1461 - val_acc: 0.6923\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8955 - acc: 0.5847 - val_loss: 1.1458 - val_acc: 0.6923\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.8934 - acc: 0.5847 - val_loss: 1.1450 - val_acc: 0.6923\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.8920 - acc: 0.5847 - val_loss: 1.1453 - val_acc: 0.6923\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.8914 - acc: 0.5847 - val_loss: 1.1453 - val_acc: 0.6923\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.8894 - acc: 0.5847 - val_loss: 1.1448 - val_acc: 0.6923\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.8891 - acc: 0.5847 - val_loss: 1.1443 - val_acc: 0.6923\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.8885 - acc: 0.5847 - val_loss: 1.1446 - val_acc: 0.6923\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8864 - acc: 0.5847 - val_loss: 1.1431 - val_acc: 0.6923\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.8850 - acc: 0.5847 - val_loss: 1.1443 - val_acc: 0.6923\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 0.8841 - acc: 0.5932 - val_loss: 1.1434 - val_acc: 0.6923\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.8832 - acc: 0.5763 - val_loss: 1.1432 - val_acc: 0.6923\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.8815 - acc: 0.5847 - val_loss: 1.1424 - val_acc: 0.6923\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8803 - acc: 0.5847 - val_loss: 1.1425 - val_acc: 0.6923\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.8802 - acc: 0.5847 - val_loss: 1.1418 - val_acc: 0.6923\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.8787 - acc: 0.5847 - val_loss: 1.1424 - val_acc: 0.6923\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.8773 - acc: 0.5847 - val_loss: 1.1420 - val_acc: 0.6923\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.8772 - acc: 0.5847 - val_loss: 1.1426 - val_acc: 0.6923\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8762 - acc: 0.5847 - val_loss: 1.1415 - val_acc: 0.6923\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.8756 - acc: 0.5847 - val_loss: 1.1419 - val_acc: 0.6923\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.8746 - acc: 0.5847 - val_loss: 1.1426 - val_acc: 0.6923\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8736 - acc: 0.5847 - val_loss: 1.1423 - val_acc: 0.6923\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.8744 - acc: 0.5847 - val_loss: 1.1430 - val_acc: 0.6923\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.8723 - acc: 0.5847 - val_loss: 1.1424 - val_acc: 0.6923\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8709 - acc: 0.5847 - val_loss: 1.1429 - val_acc: 0.6923\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.8703 - acc: 0.5847 - val_loss: 1.1440 - val_acc: 0.6923\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.8705 - acc: 0.5847 - val_loss: 1.1436 - val_acc: 0.6923\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8694 - acc: 0.5847 - val_loss: 1.1444 - val_acc: 0.6923\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.8678 - acc: 0.5847 - val_loss: 1.1454 - val_acc: 0.6923\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.8673 - acc: 0.5847 - val_loss: 1.1467 - val_acc: 0.6923\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.8672 - acc: 0.5847 - val_loss: 1.1469 - val_acc: 0.6923\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.8666 - acc: 0.5847 - val_loss: 1.1491 - val_acc: 0.6923\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 152us/step - loss: 0.8667 - acc: 0.5847 - val_loss: 1.1495 - val_acc: 0.6923\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.8642 - acc: 0.5847 - val_loss: 1.1498 - val_acc: 0.6923\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.8903 - acc: 0.3390 - val_loss: 1.7030 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.7908 - acc: 0.3644 - val_loss: 1.6266 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.6978 - acc: 0.3559 - val_loss: 1.5542 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.6196 - acc: 0.3898 - val_loss: 1.4953 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.5513 - acc: 0.3898 - val_loss: 1.4422 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.4915 - acc: 0.4068 - val_loss: 1.3899 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.4384 - acc: 0.4237 - val_loss: 1.3485 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.3928 - acc: 0.4322 - val_loss: 1.3125 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.3523 - acc: 0.4237 - val_loss: 1.2767 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.3183 - acc: 0.4407 - val_loss: 1.2479 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.2877 - acc: 0.4492 - val_loss: 1.2224 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2615 - acc: 0.4322 - val_loss: 1.2010 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2391 - acc: 0.4237 - val_loss: 1.1811 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2187 - acc: 0.4322 - val_loss: 1.1657 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.2021 - acc: 0.4322 - val_loss: 1.1518 - val_acc: 0.2308\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.1866 - acc: 0.4576 - val_loss: 1.1404 - val_acc: 0.2308\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.1725 - acc: 0.4407 - val_loss: 1.1293 - val_acc: 0.2308\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1622 - acc: 0.4407 - val_loss: 1.1212 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.1514 - acc: 0.4407 - val_loss: 1.1132 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1431 - acc: 0.4746 - val_loss: 1.1057 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1339 - acc: 0.4492 - val_loss: 1.0989 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1256 - acc: 0.4492 - val_loss: 1.0943 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1199 - acc: 0.4746 - val_loss: 1.0889 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1123 - acc: 0.4831 - val_loss: 1.0844 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.1075 - acc: 0.4831 - val_loss: 1.0789 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 263us/step - loss: 1.1015 - acc: 0.4915 - val_loss: 1.0741 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0978 - acc: 0.4915 - val_loss: 1.0704 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0932 - acc: 0.4915 - val_loss: 1.0664 - val_acc: 0.3077\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0878 - acc: 0.5000 - val_loss: 1.0635 - val_acc: 0.3077\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0836 - acc: 0.5085 - val_loss: 1.0600 - val_acc: 0.3077\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0797 - acc: 0.4915 - val_loss: 1.0575 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0757 - acc: 0.5000 - val_loss: 1.0547 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0727 - acc: 0.4915 - val_loss: 1.0522 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0688 - acc: 0.4915 - val_loss: 1.0490 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0657 - acc: 0.4831 - val_loss: 1.0458 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0621 - acc: 0.5085 - val_loss: 1.0430 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0594 - acc: 0.5085 - val_loss: 1.0399 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0561 - acc: 0.5085 - val_loss: 1.0374 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0526 - acc: 0.5000 - val_loss: 1.0350 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0508 - acc: 0.5000 - val_loss: 1.0329 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0479 - acc: 0.5085 - val_loss: 1.0302 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0456 - acc: 0.4915 - val_loss: 1.0290 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0434 - acc: 0.5085 - val_loss: 1.0278 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0413 - acc: 0.5169 - val_loss: 1.0258 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0389 - acc: 0.5169 - val_loss: 1.0242 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 1.0366 - acc: 0.5254 - val_loss: 1.0230 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0350 - acc: 0.5085 - val_loss: 1.0215 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0341 - acc: 0.5085 - val_loss: 1.0199 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0313 - acc: 0.5254 - val_loss: 1.0193 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0298 - acc: 0.5169 - val_loss: 1.0182 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0274 - acc: 0.5254 - val_loss: 1.0166 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0261 - acc: 0.5254 - val_loss: 1.0161 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0246 - acc: 0.5085 - val_loss: 1.0159 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0234 - acc: 0.5169 - val_loss: 1.0147 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0220 - acc: 0.5169 - val_loss: 1.0148 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0212 - acc: 0.5000 - val_loss: 1.0131 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0191 - acc: 0.5000 - val_loss: 1.0126 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0174 - acc: 0.5085 - val_loss: 1.0132 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0161 - acc: 0.5169 - val_loss: 1.0128 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.0141 - acc: 0.5169 - val_loss: 1.0122 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0131 - acc: 0.5339 - val_loss: 1.0117 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0128 - acc: 0.5254 - val_loss: 1.0110 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0107 - acc: 0.5254 - val_loss: 1.0099 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0094 - acc: 0.5339 - val_loss: 1.0095 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0087 - acc: 0.5339 - val_loss: 1.0088 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0072 - acc: 0.5339 - val_loss: 1.0082 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0062 - acc: 0.5254 - val_loss: 1.0085 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0047 - acc: 0.5339 - val_loss: 1.0074 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0036 - acc: 0.5424 - val_loss: 1.0071 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0025 - acc: 0.5508 - val_loss: 1.0060 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0016 - acc: 0.5508 - val_loss: 1.0053 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9999 - acc: 0.5508 - val_loss: 1.0047 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9995 - acc: 0.5508 - val_loss: 1.0039 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9986 - acc: 0.5424 - val_loss: 1.0030 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9974 - acc: 0.5508 - val_loss: 1.0021 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9952 - acc: 0.5508 - val_loss: 1.0014 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9949 - acc: 0.5508 - val_loss: 1.0011 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9943 - acc: 0.5508 - val_loss: 1.0005 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9926 - acc: 0.5508 - val_loss: 1.0009 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.9918 - acc: 0.5508 - val_loss: 1.0008 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9910 - acc: 0.5593 - val_loss: 1.0011 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9899 - acc: 0.5508 - val_loss: 1.0009 - val_acc: 0.4615\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9889 - acc: 0.5508 - val_loss: 1.0010 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9885 - acc: 0.5678 - val_loss: 1.0013 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9873 - acc: 0.5678 - val_loss: 1.0017 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9863 - acc: 0.5678 - val_loss: 1.0017 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9854 - acc: 0.5678 - val_loss: 1.0016 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9847 - acc: 0.5678 - val_loss: 1.0023 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9836 - acc: 0.5678 - val_loss: 1.0021 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9828 - acc: 0.5678 - val_loss: 1.0029 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9822 - acc: 0.5678 - val_loss: 1.0035 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9816 - acc: 0.5678 - val_loss: 1.0031 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9803 - acc: 0.5678 - val_loss: 1.0041 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9799 - acc: 0.5678 - val_loss: 1.0046 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9784 - acc: 0.5678 - val_loss: 1.0047 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9781 - acc: 0.5678 - val_loss: 1.0052 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9768 - acc: 0.5763 - val_loss: 1.0055 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9769 - acc: 0.5763 - val_loss: 1.0057 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9759 - acc: 0.5678 - val_loss: 1.0062 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 244us/step - loss: 0.9750 - acc: 0.5678 - val_loss: 1.0067 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.8607 - acc: 0.2797 - val_loss: 2.5821 - val_acc: 0.2308\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.6832 - acc: 0.3305 - val_loss: 2.3219 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.5548 - acc: 0.3729 - val_loss: 2.1160 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.4835 - acc: 0.3644 - val_loss: 1.9696 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.4326 - acc: 0.3898 - val_loss: 1.8854 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.3929 - acc: 0.4068 - val_loss: 1.8012 - val_acc: 0.3077\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.3676 - acc: 0.4068 - val_loss: 1.7421 - val_acc: 0.3077\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.3436 - acc: 0.4068 - val_loss: 1.6939 - val_acc: 0.3077\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.3182 - acc: 0.4068 - val_loss: 1.6567 - val_acc: 0.3077\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.3006 - acc: 0.4153 - val_loss: 1.6207 - val_acc: 0.3077\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.2840 - acc: 0.3983 - val_loss: 1.5928 - val_acc: 0.3077\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.2670 - acc: 0.3983 - val_loss: 1.5637 - val_acc: 0.3077\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.2517 - acc: 0.3983 - val_loss: 1.5437 - val_acc: 0.3077\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2376 - acc: 0.4068 - val_loss: 1.5194 - val_acc: 0.3077\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2230 - acc: 0.4153 - val_loss: 1.4964 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.2092 - acc: 0.4153 - val_loss: 1.4746 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1963 - acc: 0.4153 - val_loss: 1.4547 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.1861 - acc: 0.4153 - val_loss: 1.4409 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.1788 - acc: 0.4237 - val_loss: 1.4246 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1650 - acc: 0.4068 - val_loss: 1.4169 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.1591 - acc: 0.4068 - val_loss: 1.4011 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.1477 - acc: 0.4068 - val_loss: 1.3939 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1423 - acc: 0.4068 - val_loss: 1.3824 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1343 - acc: 0.4237 - val_loss: 1.3780 - val_acc: 0.3077\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1292 - acc: 0.4237 - val_loss: 1.3700 - val_acc: 0.3077\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.1220 - acc: 0.4322 - val_loss: 1.3611 - val_acc: 0.3077\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1203 - acc: 0.4407 - val_loss: 1.3568 - val_acc: 0.3077\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.1129 - acc: 0.4492 - val_loss: 1.3509 - val_acc: 0.3077\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1079 - acc: 0.4492 - val_loss: 1.3450 - val_acc: 0.2308\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.1044 - acc: 0.4576 - val_loss: 1.3397 - val_acc: 0.2308\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0999 - acc: 0.4407 - val_loss: 1.3315 - val_acc: 0.2308\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0952 - acc: 0.4492 - val_loss: 1.3269 - val_acc: 0.2308\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0958 - acc: 0.4576 - val_loss: 1.3210 - val_acc: 0.2308\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0876 - acc: 0.4576 - val_loss: 1.3136 - val_acc: 0.2308\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0846 - acc: 0.4661 - val_loss: 1.3114 - val_acc: 0.2308\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0804 - acc: 0.4661 - val_loss: 1.3057 - val_acc: 0.2308\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0799 - acc: 0.4661 - val_loss: 1.3018 - val_acc: 0.2308\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 1.0756 - acc: 0.4576 - val_loss: 1.2955 - val_acc: 0.2308\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0720 - acc: 0.4661 - val_loss: 1.2862 - val_acc: 0.2308\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0695 - acc: 0.4661 - val_loss: 1.2861 - val_acc: 0.2308\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0665 - acc: 0.4661 - val_loss: 1.2798 - val_acc: 0.2308\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0634 - acc: 0.4746 - val_loss: 1.2729 - val_acc: 0.2308\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0615 - acc: 0.4661 - val_loss: 1.2662 - val_acc: 0.2308\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0587 - acc: 0.4661 - val_loss: 1.2606 - val_acc: 0.2308\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0551 - acc: 0.4661 - val_loss: 1.2597 - val_acc: 0.2308\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0527 - acc: 0.4661 - val_loss: 1.2542 - val_acc: 0.2308\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0511 - acc: 0.4492 - val_loss: 1.2490 - val_acc: 0.2308\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0486 - acc: 0.4661 - val_loss: 1.2430 - val_acc: 0.2308\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0497 - acc: 0.4661 - val_loss: 1.2352 - val_acc: 0.2308\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 1.0430 - acc: 0.4407 - val_loss: 1.2361 - val_acc: 0.2308\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0441 - acc: 0.4407 - val_loss: 1.2319 - val_acc: 0.2308\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0394 - acc: 0.4492 - val_loss: 1.2267 - val_acc: 0.2308\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0394 - acc: 0.4492 - val_loss: 1.2228 - val_acc: 0.2308\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0361 - acc: 0.4492 - val_loss: 1.2179 - val_acc: 0.2308\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0336 - acc: 0.4576 - val_loss: 1.2118 - val_acc: 0.2308\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0306 - acc: 0.4576 - val_loss: 1.2073 - val_acc: 0.2308\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0326 - acc: 0.4576 - val_loss: 1.2041 - val_acc: 0.2308\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0288 - acc: 0.4576 - val_loss: 1.1971 - val_acc: 0.2308\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0248 - acc: 0.4492 - val_loss: 1.1966 - val_acc: 0.2308\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0260 - acc: 0.4492 - val_loss: 1.1945 - val_acc: 0.2308\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0224 - acc: 0.4407 - val_loss: 1.1902 - val_acc: 0.2308\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0207 - acc: 0.4407 - val_loss: 1.1854 - val_acc: 0.2308\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0187 - acc: 0.4492 - val_loss: 1.1833 - val_acc: 0.2308\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0163 - acc: 0.4576 - val_loss: 1.1790 - val_acc: 0.2308\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0186 - acc: 0.4576 - val_loss: 1.1765 - val_acc: 0.2308\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.0139 - acc: 0.4576 - val_loss: 1.1721 - val_acc: 0.2308\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.0138 - acc: 0.4661 - val_loss: 1.1689 - val_acc: 0.2308\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0109 - acc: 0.4746 - val_loss: 1.1666 - val_acc: 0.2308\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0089 - acc: 0.4576 - val_loss: 1.1631 - val_acc: 0.2308\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 1.0098 - acc: 0.4492 - val_loss: 1.1601 - val_acc: 0.2308\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0097 - acc: 0.4661 - val_loss: 1.1609 - val_acc: 0.2308\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0093 - acc: 0.4746 - val_loss: 1.1565 - val_acc: 0.2308\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0083 - acc: 0.4576 - val_loss: 1.1584 - val_acc: 0.2308\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0075 - acc: 0.4492 - val_loss: 1.1568 - val_acc: 0.2308\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 1.0068 - acc: 0.4576 - val_loss: 1.1539 - val_acc: 0.2308\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0026 - acc: 0.4492 - val_loss: 1.1514 - val_acc: 0.2308\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0028 - acc: 0.4492 - val_loss: 1.1504 - val_acc: 0.2308\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0009 - acc: 0.4492 - val_loss: 1.1475 - val_acc: 0.2308\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0027 - acc: 0.4576 - val_loss: 1.1462 - val_acc: 0.2308\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0031 - acc: 0.4407 - val_loss: 1.1444 - val_acc: 0.2308\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 1.0000 - acc: 0.4576 - val_loss: 1.1426 - val_acc: 0.2308\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9984 - acc: 0.4661 - val_loss: 1.1409 - val_acc: 0.2308\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9985 - acc: 0.4492 - val_loss: 1.1398 - val_acc: 0.2308\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9979 - acc: 0.4661 - val_loss: 1.1381 - val_acc: 0.2308\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.9961 - acc: 0.4661 - val_loss: 1.1371 - val_acc: 0.2308\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9968 - acc: 0.4576 - val_loss: 1.1369 - val_acc: 0.2308\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9957 - acc: 0.4492 - val_loss: 1.1363 - val_acc: 0.2308\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9947 - acc: 0.4661 - val_loss: 1.1343 - val_acc: 0.2308\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9940 - acc: 0.4407 - val_loss: 1.1331 - val_acc: 0.2308\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9936 - acc: 0.4492 - val_loss: 1.1303 - val_acc: 0.3077\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9934 - acc: 0.4407 - val_loss: 1.1297 - val_acc: 0.3077\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9947 - acc: 0.4576 - val_loss: 1.1282 - val_acc: 0.3077\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9931 - acc: 0.4407 - val_loss: 1.1274 - val_acc: 0.3077\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9914 - acc: 0.4407 - val_loss: 1.1271 - val_acc: 0.2308\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9921 - acc: 0.4746 - val_loss: 1.1262 - val_acc: 0.2308\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9905 - acc: 0.4492 - val_loss: 1.1235 - val_acc: 0.3846\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9905 - acc: 0.4661 - val_loss: 1.1227 - val_acc: 0.3077\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9896 - acc: 0.4576 - val_loss: 1.1210 - val_acc: 0.3846\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9929 - acc: 0.4492 - val_loss: 1.1210 - val_acc: 0.3846\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 257us/step - loss: 0.9889 - acc: 0.4492 - val_loss: 1.1200 - val_acc: 0.3846\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 3.8979 - acc: 0.2881 - val_loss: 2.7701 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 3.5357 - acc: 0.2881 - val_loss: 2.4652 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 3.2121 - acc: 0.2881 - val_loss: 2.2015 - val_acc: 0.3077\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.9014 - acc: 0.2712 - val_loss: 1.9972 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 2.6578 - acc: 0.2797 - val_loss: 1.8500 - val_acc: 0.3077\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 2.4807 - acc: 0.2881 - val_loss: 1.7334 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 2.3375 - acc: 0.3390 - val_loss: 1.6289 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 2.2229 - acc: 0.3390 - val_loss: 1.5413 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.1256 - acc: 0.3390 - val_loss: 1.4647 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 2.0406 - acc: 0.3390 - val_loss: 1.3947 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.9638 - acc: 0.3390 - val_loss: 1.3308 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.8950 - acc: 0.3814 - val_loss: 1.2787 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.8284 - acc: 0.3898 - val_loss: 1.2200 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.7737 - acc: 0.4068 - val_loss: 1.1762 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.7235 - acc: 0.4153 - val_loss: 1.1345 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.6787 - acc: 0.4068 - val_loss: 1.1020 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.6341 - acc: 0.4322 - val_loss: 1.0737 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.5930 - acc: 0.4237 - val_loss: 1.0454 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.5587 - acc: 0.4407 - val_loss: 1.0200 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.5214 - acc: 0.4407 - val_loss: 0.9982 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.4932 - acc: 0.4661 - val_loss: 0.9806 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.4566 - acc: 0.4831 - val_loss: 0.9633 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4188 - acc: 0.4831 - val_loss: 0.9508 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 1.3863 - acc: 0.4831 - val_loss: 0.9338 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.3571 - acc: 0.4746 - val_loss: 0.9201 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.3287 - acc: 0.4831 - val_loss: 0.9084 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.3032 - acc: 0.4831 - val_loss: 0.8988 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.2796 - acc: 0.4915 - val_loss: 0.8887 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2571 - acc: 0.5085 - val_loss: 0.8803 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.2385 - acc: 0.5169 - val_loss: 0.8721 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.2181 - acc: 0.5085 - val_loss: 0.8645 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1997 - acc: 0.5000 - val_loss: 0.8589 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.1861 - acc: 0.5085 - val_loss: 0.8545 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1672 - acc: 0.5085 - val_loss: 0.8491 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1529 - acc: 0.5085 - val_loss: 0.8452 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1405 - acc: 0.5085 - val_loss: 0.8418 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.1278 - acc: 0.5169 - val_loss: 0.8391 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1166 - acc: 0.5169 - val_loss: 0.8361 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.1065 - acc: 0.5254 - val_loss: 0.8334 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0974 - acc: 0.5169 - val_loss: 0.8309 - val_acc: 0.5385\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0861 - acc: 0.5169 - val_loss: 0.8294 - val_acc: 0.5385\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.0783 - acc: 0.5169 - val_loss: 0.8281 - val_acc: 0.5385\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0712 - acc: 0.5000 - val_loss: 0.8273 - val_acc: 0.5385\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0641 - acc: 0.5085 - val_loss: 0.8263 - val_acc: 0.5385\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0577 - acc: 0.5085 - val_loss: 0.8256 - val_acc: 0.5385\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0520 - acc: 0.5085 - val_loss: 0.8253 - val_acc: 0.6154\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0455 - acc: 0.5085 - val_loss: 0.8251 - val_acc: 0.5385\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0402 - acc: 0.5169 - val_loss: 0.8251 - val_acc: 0.5385\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0364 - acc: 0.5169 - val_loss: 0.8250 - val_acc: 0.5385\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0312 - acc: 0.5169 - val_loss: 0.8248 - val_acc: 0.5385\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0278 - acc: 0.5254 - val_loss: 0.8248 - val_acc: 0.5385\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0239 - acc: 0.5254 - val_loss: 0.8248 - val_acc: 0.5385\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0199 - acc: 0.5339 - val_loss: 0.8247 - val_acc: 0.5385\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 232us/step - loss: 1.0167 - acc: 0.5169 - val_loss: 0.8251 - val_acc: 0.5385\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0130 - acc: 0.5339 - val_loss: 0.8251 - val_acc: 0.5385\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.0102 - acc: 0.5254 - val_loss: 0.8258 - val_acc: 0.5385\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0061 - acc: 0.5169 - val_loss: 0.8261 - val_acc: 0.5385\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.0040 - acc: 0.5085 - val_loss: 0.8267 - val_acc: 0.5385\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0013 - acc: 0.5085 - val_loss: 0.8270 - val_acc: 0.5385\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9985 - acc: 0.5169 - val_loss: 0.8269 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9968 - acc: 0.5339 - val_loss: 0.8272 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9939 - acc: 0.5254 - val_loss: 0.8279 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9926 - acc: 0.5508 - val_loss: 0.8280 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9912 - acc: 0.5424 - val_loss: 0.8287 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9887 - acc: 0.5678 - val_loss: 0.8296 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9858 - acc: 0.5339 - val_loss: 0.8300 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9838 - acc: 0.5339 - val_loss: 0.8303 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9822 - acc: 0.5508 - val_loss: 0.8305 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9808 - acc: 0.5339 - val_loss: 0.8311 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9790 - acc: 0.5339 - val_loss: 0.8316 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9784 - acc: 0.5508 - val_loss: 0.8323 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9763 - acc: 0.5424 - val_loss: 0.8328 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9748 - acc: 0.5678 - val_loss: 0.8330 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9737 - acc: 0.5508 - val_loss: 0.8333 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9719 - acc: 0.5593 - val_loss: 0.8334 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9704 - acc: 0.5678 - val_loss: 0.8339 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9692 - acc: 0.5678 - val_loss: 0.8347 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9686 - acc: 0.5678 - val_loss: 0.8350 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9672 - acc: 0.5763 - val_loss: 0.8356 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9656 - acc: 0.5593 - val_loss: 0.8362 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9644 - acc: 0.5763 - val_loss: 0.8370 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9646 - acc: 0.5678 - val_loss: 0.8377 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9629 - acc: 0.5593 - val_loss: 0.8377 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9619 - acc: 0.5678 - val_loss: 0.8380 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9609 - acc: 0.5678 - val_loss: 0.8384 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9600 - acc: 0.5593 - val_loss: 0.8387 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9595 - acc: 0.5424 - val_loss: 0.8392 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9583 - acc: 0.5424 - val_loss: 0.8398 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9572 - acc: 0.5424 - val_loss: 0.8403 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9569 - acc: 0.5593 - val_loss: 0.8408 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9560 - acc: 0.5678 - val_loss: 0.8409 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9548 - acc: 0.5424 - val_loss: 0.8414 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9543 - acc: 0.5593 - val_loss: 0.8422 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9530 - acc: 0.5593 - val_loss: 0.8430 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9530 - acc: 0.5678 - val_loss: 0.8436 - val_acc: 0.6154\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9522 - acc: 0.5763 - val_loss: 0.8434 - val_acc: 0.6154\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9515 - acc: 0.5678 - val_loss: 0.8440 - val_acc: 0.6154\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9507 - acc: 0.5593 - val_loss: 0.8442 - val_acc: 0.6154\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9499 - acc: 0.5593 - val_loss: 0.8445 - val_acc: 0.6154\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9495 - acc: 0.5508 - val_loss: 0.8452 - val_acc: 0.6154\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 1.4925 - acc: 0.4746 - val_loss: 1.4769 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 248us/step - loss: 1.4321 - acc: 0.4831 - val_loss: 1.4456 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.3814 - acc: 0.5085 - val_loss: 1.4190 - val_acc: 0.3846\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.3389 - acc: 0.5085 - val_loss: 1.3986 - val_acc: 0.3846\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.3022 - acc: 0.5000 - val_loss: 1.3767 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.2705 - acc: 0.5000 - val_loss: 1.3549 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.2445 - acc: 0.5000 - val_loss: 1.3327 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.2202 - acc: 0.5000 - val_loss: 1.3168 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1999 - acc: 0.5000 - val_loss: 1.2992 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1823 - acc: 0.5000 - val_loss: 1.2825 - val_acc: 0.3846\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1660 - acc: 0.4915 - val_loss: 1.2685 - val_acc: 0.3846\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 1.1527 - acc: 0.5000 - val_loss: 1.2579 - val_acc: 0.3846\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 155us/step - loss: 1.1407 - acc: 0.5085 - val_loss: 1.2437 - val_acc: 0.3846\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.1304 - acc: 0.5085 - val_loss: 1.2344 - val_acc: 0.3846\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.1219 - acc: 0.5085 - val_loss: 1.2220 - val_acc: 0.3846\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.1124 - acc: 0.4915 - val_loss: 1.2113 - val_acc: 0.3846\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1068 - acc: 0.4915 - val_loss: 1.2004 - val_acc: 0.3846\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0973 - acc: 0.4915 - val_loss: 1.1932 - val_acc: 0.3846\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.0906 - acc: 0.4831 - val_loss: 1.1843 - val_acc: 0.3846\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0865 - acc: 0.4831 - val_loss: 1.1726 - val_acc: 0.3846\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0785 - acc: 0.4831 - val_loss: 1.1640 - val_acc: 0.3846\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0720 - acc: 0.4831 - val_loss: 1.1510 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0640 - acc: 0.4661 - val_loss: 1.1423 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0572 - acc: 0.4746 - val_loss: 1.1333 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0518 - acc: 0.4746 - val_loss: 1.1240 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0454 - acc: 0.4831 - val_loss: 1.1166 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 1.0403 - acc: 0.4746 - val_loss: 1.1099 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0349 - acc: 0.4746 - val_loss: 1.1027 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.0302 - acc: 0.4746 - val_loss: 1.0971 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0251 - acc: 0.4831 - val_loss: 1.0902 - val_acc: 0.5385\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0234 - acc: 0.4831 - val_loss: 1.0842 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0173 - acc: 0.4746 - val_loss: 1.0795 - val_acc: 0.4615\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0141 - acc: 0.4746 - val_loss: 1.0729 - val_acc: 0.4615\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.0095 - acc: 0.4831 - val_loss: 1.0687 - val_acc: 0.4615\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.0082 - acc: 0.4746 - val_loss: 1.0650 - val_acc: 0.4615\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.0048 - acc: 0.4746 - val_loss: 1.0626 - val_acc: 0.4615\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0004 - acc: 0.4746 - val_loss: 1.0601 - val_acc: 0.4615\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9990 - acc: 0.4746 - val_loss: 1.0563 - val_acc: 0.4615\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9955 - acc: 0.4746 - val_loss: 1.0543 - val_acc: 0.4615\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9922 - acc: 0.4746 - val_loss: 1.0502 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9906 - acc: 0.4746 - val_loss: 1.0484 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.9877 - acc: 0.4746 - val_loss: 1.0473 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9853 - acc: 0.4831 - val_loss: 1.0441 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9831 - acc: 0.4831 - val_loss: 1.0409 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 272us/step - loss: 0.9814 - acc: 0.4831 - val_loss: 1.0385 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.9806 - acc: 0.4831 - val_loss: 1.0374 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9774 - acc: 0.4915 - val_loss: 1.0366 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9754 - acc: 0.4915 - val_loss: 1.0356 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9744 - acc: 0.4915 - val_loss: 1.0352 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9722 - acc: 0.4915 - val_loss: 1.0332 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9719 - acc: 0.4915 - val_loss: 1.0320 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9693 - acc: 0.4915 - val_loss: 1.0321 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9687 - acc: 0.4915 - val_loss: 1.0314 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9687 - acc: 0.4915 - val_loss: 1.0304 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9650 - acc: 0.4915 - val_loss: 1.0301 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9666 - acc: 0.4915 - val_loss: 1.0287 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9657 - acc: 0.4915 - val_loss: 1.0264 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9628 - acc: 0.4915 - val_loss: 1.0261 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9622 - acc: 0.4915 - val_loss: 1.0249 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9597 - acc: 0.4915 - val_loss: 1.0229 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9594 - acc: 0.4915 - val_loss: 1.0227 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9583 - acc: 0.4915 - val_loss: 1.0218 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9588 - acc: 0.5000 - val_loss: 1.0206 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9550 - acc: 0.5000 - val_loss: 1.0207 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9552 - acc: 0.5000 - val_loss: 1.0203 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9536 - acc: 0.5000 - val_loss: 1.0188 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9527 - acc: 0.5000 - val_loss: 1.0195 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.9509 - acc: 0.5000 - val_loss: 1.0198 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9503 - acc: 0.5000 - val_loss: 1.0193 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9491 - acc: 0.5085 - val_loss: 1.0176 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9487 - acc: 0.5085 - val_loss: 1.0169 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9481 - acc: 0.5000 - val_loss: 1.0168 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9477 - acc: 0.5000 - val_loss: 1.0163 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9463 - acc: 0.5000 - val_loss: 1.0155 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9450 - acc: 0.5000 - val_loss: 1.0167 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9433 - acc: 0.5085 - val_loss: 1.0154 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9453 - acc: 0.5085 - val_loss: 1.0166 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9420 - acc: 0.5000 - val_loss: 1.0154 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.9416 - acc: 0.5000 - val_loss: 1.0157 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9401 - acc: 0.5169 - val_loss: 1.0156 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9394 - acc: 0.5169 - val_loss: 1.0140 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9392 - acc: 0.5169 - val_loss: 1.0143 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9370 - acc: 0.5000 - val_loss: 1.0149 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.9360 - acc: 0.5000 - val_loss: 1.0150 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9357 - acc: 0.5169 - val_loss: 1.0148 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.9341 - acc: 0.5169 - val_loss: 1.0141 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9351 - acc: 0.5169 - val_loss: 1.0133 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9333 - acc: 0.5169 - val_loss: 1.0134 - val_acc: 0.3846\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9330 - acc: 0.5169 - val_loss: 1.0157 - val_acc: 0.3846\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9313 - acc: 0.5169 - val_loss: 1.0138 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9318 - acc: 0.5169 - val_loss: 1.0143 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9312 - acc: 0.5169 - val_loss: 1.0130 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9309 - acc: 0.5085 - val_loss: 1.0116 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9304 - acc: 0.5169 - val_loss: 1.0128 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9295 - acc: 0.5169 - val_loss: 1.0140 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9287 - acc: 0.5085 - val_loss: 1.0158 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9273 - acc: 0.5169 - val_loss: 1.0156 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9262 - acc: 0.5169 - val_loss: 1.0155 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9253 - acc: 0.5169 - val_loss: 1.0155 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.9258 - acc: 0.5169 - val_loss: 1.0165 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 3ms/step - loss: 2.4792 - acc: 0.4322 - val_loss: 3.1218 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 2.2996 - acc: 0.4576 - val_loss: 2.8290 - val_acc: 0.2308\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 2.1541 - acc: 0.4576 - val_loss: 2.5684 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 2.0223 - acc: 0.4746 - val_loss: 2.3365 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 153us/step - loss: 1.9025 - acc: 0.4915 - val_loss: 2.1089 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.7886 - acc: 0.4915 - val_loss: 1.9204 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.6916 - acc: 0.5169 - val_loss: 1.7590 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.6094 - acc: 0.5085 - val_loss: 1.6039 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.5393 - acc: 0.5000 - val_loss: 1.4953 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.4809 - acc: 0.5169 - val_loss: 1.4004 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 267us/step - loss: 1.4273 - acc: 0.5169 - val_loss: 1.3220 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.3823 - acc: 0.5169 - val_loss: 1.2549 - val_acc: 0.2308\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.3412 - acc: 0.5169 - val_loss: 1.2015 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.3047 - acc: 0.5169 - val_loss: 1.1575 - val_acc: 0.2308\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.2738 - acc: 0.5000 - val_loss: 1.1191 - val_acc: 0.3077\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 1.2493 - acc: 0.5085 - val_loss: 1.0898 - val_acc: 0.3077\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.2214 - acc: 0.5085 - val_loss: 1.0655 - val_acc: 0.3077\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.2013 - acc: 0.4915 - val_loss: 1.0432 - val_acc: 0.3077\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.1824 - acc: 0.5000 - val_loss: 1.0294 - val_acc: 0.3077\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.1669 - acc: 0.5169 - val_loss: 1.0151 - val_acc: 0.3077\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1529 - acc: 0.5254 - val_loss: 1.0030 - val_acc: 0.3077\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.1407 - acc: 0.5169 - val_loss: 0.9945 - val_acc: 0.3077\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1297 - acc: 0.5339 - val_loss: 0.9852 - val_acc: 0.3077\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.1200 - acc: 0.5339 - val_loss: 0.9775 - val_acc: 0.3846\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1110 - acc: 0.5424 - val_loss: 0.9711 - val_acc: 0.3846\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 1.1041 - acc: 0.5339 - val_loss: 0.9652 - val_acc: 0.3846\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0953 - acc: 0.5254 - val_loss: 0.9584 - val_acc: 0.3846\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.0891 - acc: 0.5339 - val_loss: 0.9534 - val_acc: 0.3846\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0806 - acc: 0.5508 - val_loss: 0.9478 - val_acc: 0.3846\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0746 - acc: 0.5508 - val_loss: 0.9453 - val_acc: 0.3846\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0681 - acc: 0.5508 - val_loss: 0.9410 - val_acc: 0.3846\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0630 - acc: 0.5593 - val_loss: 0.9376 - val_acc: 0.3846\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0572 - acc: 0.5508 - val_loss: 0.9375 - val_acc: 0.3846\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0537 - acc: 0.5424 - val_loss: 0.9342 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0478 - acc: 0.5339 - val_loss: 0.9318 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0432 - acc: 0.5424 - val_loss: 0.9273 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0406 - acc: 0.5339 - val_loss: 0.9257 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 1.0361 - acc: 0.5339 - val_loss: 0.9227 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 1.0315 - acc: 0.5508 - val_loss: 0.9225 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 154us/step - loss: 1.0285 - acc: 0.5339 - val_loss: 0.9202 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0255 - acc: 0.5424 - val_loss: 0.9189 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0222 - acc: 0.5339 - val_loss: 0.9180 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0191 - acc: 0.5424 - val_loss: 0.9171 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0163 - acc: 0.5424 - val_loss: 0.9145 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0133 - acc: 0.5508 - val_loss: 0.9150 - val_acc: 0.3846\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0111 - acc: 0.5424 - val_loss: 0.9142 - val_acc: 0.3846\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0083 - acc: 0.5339 - val_loss: 0.9124 - val_acc: 0.3846\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.0051 - acc: 0.5508 - val_loss: 0.9109 - val_acc: 0.3846\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.0035 - acc: 0.5424 - val_loss: 0.9107 - val_acc: 0.3846\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.0017 - acc: 0.5508 - val_loss: 0.9110 - val_acc: 0.3846\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0000 - acc: 0.5508 - val_loss: 0.9102 - val_acc: 0.3846\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9978 - acc: 0.5593 - val_loss: 0.9102 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9963 - acc: 0.5508 - val_loss: 0.9105 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9947 - acc: 0.5424 - val_loss: 0.9107 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9927 - acc: 0.5508 - val_loss: 0.9105 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9902 - acc: 0.5508 - val_loss: 0.9104 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9910 - acc: 0.5424 - val_loss: 0.9104 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.9868 - acc: 0.5424 - val_loss: 0.9088 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9868 - acc: 0.5508 - val_loss: 0.9091 - val_acc: 0.3846\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.9844 - acc: 0.5508 - val_loss: 0.9089 - val_acc: 0.3846\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9824 - acc: 0.5508 - val_loss: 0.9091 - val_acc: 0.3846\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9817 - acc: 0.5339 - val_loss: 0.9081 - val_acc: 0.3846\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9789 - acc: 0.5424 - val_loss: 0.9089 - val_acc: 0.3846\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9783 - acc: 0.5424 - val_loss: 0.9079 - val_acc: 0.3846\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9763 - acc: 0.5424 - val_loss: 0.9087 - val_acc: 0.3846\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9758 - acc: 0.5424 - val_loss: 0.9084 - val_acc: 0.3846\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 226us/step - loss: 0.9746 - acc: 0.5424 - val_loss: 0.9086 - val_acc: 0.3846\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9724 - acc: 0.5424 - val_loss: 0.9083 - val_acc: 0.3846\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.9705 - acc: 0.5424 - val_loss: 0.9092 - val_acc: 0.3846\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9696 - acc: 0.5508 - val_loss: 0.9092 - val_acc: 0.3846\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9681 - acc: 0.5508 - val_loss: 0.9102 - val_acc: 0.3846\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9667 - acc: 0.5508 - val_loss: 0.9102 - val_acc: 0.3846\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9658 - acc: 0.5508 - val_loss: 0.9100 - val_acc: 0.3846\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9652 - acc: 0.5508 - val_loss: 0.9107 - val_acc: 0.3846\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.9646 - acc: 0.5508 - val_loss: 0.9108 - val_acc: 0.3846\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9645 - acc: 0.5508 - val_loss: 0.9105 - val_acc: 0.3846\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9627 - acc: 0.5508 - val_loss: 0.9108 - val_acc: 0.3846\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9616 - acc: 0.5508 - val_loss: 0.9109 - val_acc: 0.3846\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9608 - acc: 0.5508 - val_loss: 0.9122 - val_acc: 0.3846\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9607 - acc: 0.5508 - val_loss: 0.9125 - val_acc: 0.3846\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9602 - acc: 0.5508 - val_loss: 0.9113 - val_acc: 0.3846\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9590 - acc: 0.5508 - val_loss: 0.9110 - val_acc: 0.3846\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9573 - acc: 0.5508 - val_loss: 0.9137 - val_acc: 0.3846\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9563 - acc: 0.5508 - val_loss: 0.9135 - val_acc: 0.3846\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9554 - acc: 0.5424 - val_loss: 0.9143 - val_acc: 0.3846\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9562 - acc: 0.5424 - val_loss: 0.9148 - val_acc: 0.3846\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9545 - acc: 0.5508 - val_loss: 0.9151 - val_acc: 0.3846\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9536 - acc: 0.5508 - val_loss: 0.9157 - val_acc: 0.4615\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9537 - acc: 0.5508 - val_loss: 0.9171 - val_acc: 0.4615\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9531 - acc: 0.5424 - val_loss: 0.9144 - val_acc: 0.4615\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9525 - acc: 0.5508 - val_loss: 0.9154 - val_acc: 0.4615\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9512 - acc: 0.5508 - val_loss: 0.9153 - val_acc: 0.4615\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9501 - acc: 0.5508 - val_loss: 0.9154 - val_acc: 0.4615\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9493 - acc: 0.5508 - val_loss: 0.9157 - val_acc: 0.4615\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.9499 - acc: 0.5508 - val_loss: 0.9154 - val_acc: 0.4615\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9487 - acc: 0.5508 - val_loss: 0.9160 - val_acc: 0.4615\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9478 - acc: 0.5508 - val_loss: 0.9164 - val_acc: 0.4615\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9469 - acc: 0.5508 - val_loss: 0.9162 - val_acc: 0.4615\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9470 - acc: 0.5508 - val_loss: 0.9170 - val_acc: 0.4615\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9465 - acc: 0.5508 - val_loss: 0.9171 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 0s 4ms/step - loss: 1.6188 - acc: 0.3644 - val_loss: 1.5228 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 1.5595 - acc: 0.3729 - val_loss: 1.4749 - val_acc: 0.3077\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.5117 - acc: 0.3898 - val_loss: 1.4389 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.4706 - acc: 0.3729 - val_loss: 1.4065 - val_acc: 0.2308\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4334 - acc: 0.3729 - val_loss: 1.3745 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.3995 - acc: 0.3814 - val_loss: 1.3493 - val_acc: 0.2308\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.3689 - acc: 0.3729 - val_loss: 1.3305 - val_acc: 0.2308\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.3422 - acc: 0.3814 - val_loss: 1.3098 - val_acc: 0.2308\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 1.3186 - acc: 0.3729 - val_loss: 1.2953 - val_acc: 0.2308\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.2966 - acc: 0.3898 - val_loss: 1.2793 - val_acc: 0.2308\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2762 - acc: 0.3898 - val_loss: 1.2661 - val_acc: 0.2308\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.2595 - acc: 0.3898 - val_loss: 1.2582 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.2409 - acc: 0.3983 - val_loss: 1.2457 - val_acc: 0.1538\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.2244 - acc: 0.3983 - val_loss: 1.2400 - val_acc: 0.1538\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.2100 - acc: 0.3983 - val_loss: 1.2320 - val_acc: 0.1538\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.1973 - acc: 0.3983 - val_loss: 1.2279 - val_acc: 0.1538\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1855 - acc: 0.3983 - val_loss: 1.2198 - val_acc: 0.1538\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.1722 - acc: 0.3983 - val_loss: 1.2164 - val_acc: 0.1538\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1619 - acc: 0.3983 - val_loss: 1.2113 - val_acc: 0.1538\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 1.1523 - acc: 0.4068 - val_loss: 1.2041 - val_acc: 0.1538\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 1.1413 - acc: 0.4068 - val_loss: 1.2035 - val_acc: 0.1538\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 1.1327 - acc: 0.4068 - val_loss: 1.1992 - val_acc: 0.1538\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.1236 - acc: 0.4237 - val_loss: 1.1958 - val_acc: 0.1538\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1165 - acc: 0.4153 - val_loss: 1.1903 - val_acc: 0.1538\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1089 - acc: 0.4237 - val_loss: 1.1870 - val_acc: 0.1538\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.1012 - acc: 0.4237 - val_loss: 1.1821 - val_acc: 0.1538\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0952 - acc: 0.4237 - val_loss: 1.1821 - val_acc: 0.2308\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0875 - acc: 0.4237 - val_loss: 1.1773 - val_acc: 0.2308\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0818 - acc: 0.4322 - val_loss: 1.1748 - val_acc: 0.2308\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0759 - acc: 0.4237 - val_loss: 1.1726 - val_acc: 0.2308\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0697 - acc: 0.4322 - val_loss: 1.1673 - val_acc: 0.2308\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0632 - acc: 0.4322 - val_loss: 1.1625 - val_acc: 0.2308\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0600 - acc: 0.4322 - val_loss: 1.1604 - val_acc: 0.2308\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0539 - acc: 0.4322 - val_loss: 1.1563 - val_acc: 0.3846\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0493 - acc: 0.4407 - val_loss: 1.1517 - val_acc: 0.3846\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0445 - acc: 0.4407 - val_loss: 1.1492 - val_acc: 0.3846\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.0404 - acc: 0.4576 - val_loss: 1.1447 - val_acc: 0.3846\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0370 - acc: 0.4746 - val_loss: 1.1433 - val_acc: 0.3846\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 1.0336 - acc: 0.4746 - val_loss: 1.1392 - val_acc: 0.3846\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0291 - acc: 0.4831 - val_loss: 1.1387 - val_acc: 0.3846\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0259 - acc: 0.4746 - val_loss: 1.1364 - val_acc: 0.3846\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.0224 - acc: 0.4831 - val_loss: 1.1330 - val_acc: 0.3846\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0204 - acc: 0.4831 - val_loss: 1.1321 - val_acc: 0.3846\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.0158 - acc: 0.5000 - val_loss: 1.1300 - val_acc: 0.3846\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0134 - acc: 0.5000 - val_loss: 1.1269 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0101 - acc: 0.5339 - val_loss: 1.1251 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0076 - acc: 0.5254 - val_loss: 1.1235 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0056 - acc: 0.5339 - val_loss: 1.1233 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0035 - acc: 0.5254 - val_loss: 1.1219 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.0022 - acc: 0.5339 - val_loss: 1.1205 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 1.0009 - acc: 0.5169 - val_loss: 1.1203 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9982 - acc: 0.5085 - val_loss: 1.1176 - val_acc: 0.4615\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9963 - acc: 0.5169 - val_loss: 1.1191 - val_acc: 0.4615\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9940 - acc: 0.5254 - val_loss: 1.1174 - val_acc: 0.4615\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.9925 - acc: 0.5169 - val_loss: 1.1139 - val_acc: 0.4615\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9903 - acc: 0.5169 - val_loss: 1.1150 - val_acc: 0.4615\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.9902 - acc: 0.5254 - val_loss: 1.1157 - val_acc: 0.4615\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 295us/step - loss: 0.9889 - acc: 0.5169 - val_loss: 1.1139 - val_acc: 0.4615\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9869 - acc: 0.5085 - val_loss: 1.1124 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9845 - acc: 0.5085 - val_loss: 1.1125 - val_acc: 0.5385\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9828 - acc: 0.5085 - val_loss: 1.1107 - val_acc: 0.5385\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9815 - acc: 0.5085 - val_loss: 1.1111 - val_acc: 0.5385\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9804 - acc: 0.4915 - val_loss: 1.1096 - val_acc: 0.5385\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9789 - acc: 0.5000 - val_loss: 1.1078 - val_acc: 0.5385\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.9778 - acc: 0.4915 - val_loss: 1.1062 - val_acc: 0.5385\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9762 - acc: 0.5085 - val_loss: 1.1048 - val_acc: 0.5385\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.9752 - acc: 0.5085 - val_loss: 1.1057 - val_acc: 0.5385\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9732 - acc: 0.5085 - val_loss: 1.1064 - val_acc: 0.5385\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9719 - acc: 0.5254 - val_loss: 1.1066 - val_acc: 0.5385\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9714 - acc: 0.5254 - val_loss: 1.1061 - val_acc: 0.5385\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9709 - acc: 0.5254 - val_loss: 1.1060 - val_acc: 0.5385\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9691 - acc: 0.5508 - val_loss: 1.1040 - val_acc: 0.5385\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9680 - acc: 0.5508 - val_loss: 1.1040 - val_acc: 0.5385\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9674 - acc: 0.5339 - val_loss: 1.1034 - val_acc: 0.5385\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9659 - acc: 0.5424 - val_loss: 1.1018 - val_acc: 0.5385\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9641 - acc: 0.5424 - val_loss: 1.1018 - val_acc: 0.5385\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9644 - acc: 0.5254 - val_loss: 1.1016 - val_acc: 0.5385\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9627 - acc: 0.5424 - val_loss: 1.0997 - val_acc: 0.5385\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9614 - acc: 0.5339 - val_loss: 1.1010 - val_acc: 0.5385\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.9603 - acc: 0.5508 - val_loss: 1.1005 - val_acc: 0.5385\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9593 - acc: 0.5424 - val_loss: 1.1006 - val_acc: 0.5385\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9597 - acc: 0.5424 - val_loss: 1.1002 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.9574 - acc: 0.5424 - val_loss: 1.1008 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.9565 - acc: 0.5339 - val_loss: 1.1018 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9554 - acc: 0.5508 - val_loss: 1.0996 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9553 - acc: 0.5424 - val_loss: 1.1007 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.9542 - acc: 0.5424 - val_loss: 1.0995 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.9527 - acc: 0.5424 - val_loss: 1.0995 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9518 - acc: 0.5424 - val_loss: 1.1005 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9514 - acc: 0.5339 - val_loss: 1.1009 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9497 - acc: 0.5424 - val_loss: 1.0988 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9495 - acc: 0.5339 - val_loss: 1.1007 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.9490 - acc: 0.5424 - val_loss: 1.1004 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.9479 - acc: 0.5424 - val_loss: 1.1017 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9477 - acc: 0.5424 - val_loss: 1.1004 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.9468 - acc: 0.5593 - val_loss: 1.0996 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9463 - acc: 0.5678 - val_loss: 1.0997 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.9445 - acc: 0.5508 - val_loss: 1.1000 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.9445 - acc: 0.5508 - val_loss: 1.0991 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.9433 - acc: 0.5678 - val_loss: 1.0973 - val_acc: 0.5385\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 4ms/step - loss: 2.6607 - acc: 0.4153 - val_loss: 3.0586 - val_acc: 0.3077\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 2.3953 - acc: 0.4322 - val_loss: 3.0350 - val_acc: 0.3846\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 2.1925 - acc: 0.4153 - val_loss: 2.9244 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 2.0245 - acc: 0.4153 - val_loss: 2.7842 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.8842 - acc: 0.4237 - val_loss: 2.6622 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.7605 - acc: 0.4153 - val_loss: 2.5419 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.6591 - acc: 0.4068 - val_loss: 2.4434 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.5702 - acc: 0.4153 - val_loss: 2.3452 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.4970 - acc: 0.4068 - val_loss: 2.2670 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.4310 - acc: 0.4153 - val_loss: 2.1870 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 1.3783 - acc: 0.4068 - val_loss: 2.1223 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.3375 - acc: 0.4322 - val_loss: 2.0562 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 1.3041 - acc: 0.4492 - val_loss: 2.0132 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 1.2792 - acc: 0.4661 - val_loss: 1.9594 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2561 - acc: 0.4831 - val_loss: 1.9229 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 1.2348 - acc: 0.4831 - val_loss: 1.8784 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2184 - acc: 0.4746 - val_loss: 1.8480 - val_acc: 0.4615\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.2029 - acc: 0.4746 - val_loss: 1.8198 - val_acc: 0.4615\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.1888 - acc: 0.4746 - val_loss: 1.7902 - val_acc: 0.4615\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 1.1791 - acc: 0.4746 - val_loss: 1.7649 - val_acc: 0.4615\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1660 - acc: 0.4746 - val_loss: 1.7394 - val_acc: 0.4615\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 1.1545 - acc: 0.4915 - val_loss: 1.7224 - val_acc: 0.4615\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.1446 - acc: 0.4831 - val_loss: 1.7005 - val_acc: 0.4615\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.1359 - acc: 0.4831 - val_loss: 1.6703 - val_acc: 0.4615\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.1262 - acc: 0.4831 - val_loss: 1.6535 - val_acc: 0.4615\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 224us/step - loss: 1.1179 - acc: 0.4915 - val_loss: 1.6343 - val_acc: 0.4615\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1103 - acc: 0.4915 - val_loss: 1.6230 - val_acc: 0.4615\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 1.1034 - acc: 0.4915 - val_loss: 1.6140 - val_acc: 0.4615\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.0979 - acc: 0.4915 - val_loss: 1.6033 - val_acc: 0.4615\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 236us/step - loss: 1.0923 - acc: 0.5000 - val_loss: 1.5926 - val_acc: 0.4615\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 1.0874 - acc: 0.5169 - val_loss: 1.5841 - val_acc: 0.4615\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 1.0826 - acc: 0.5169 - val_loss: 1.5735 - val_acc: 0.5385\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.0783 - acc: 0.5254 - val_loss: 1.5641 - val_acc: 0.5385\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0733 - acc: 0.5254 - val_loss: 1.5566 - val_acc: 0.5385\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0684 - acc: 0.5169 - val_loss: 1.5331 - val_acc: 0.5385\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0655 - acc: 0.5254 - val_loss: 1.5218 - val_acc: 0.5385\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.0598 - acc: 0.5254 - val_loss: 1.5150 - val_acc: 0.5385\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.0581 - acc: 0.5254 - val_loss: 1.5009 - val_acc: 0.5385\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.0533 - acc: 0.5254 - val_loss: 1.4920 - val_acc: 0.5385\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0493 - acc: 0.5339 - val_loss: 1.4727 - val_acc: 0.4615\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 1.0433 - acc: 0.5339 - val_loss: 1.4661 - val_acc: 0.4615\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 1.0399 - acc: 0.5424 - val_loss: 1.4511 - val_acc: 0.4615\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 1.0380 - acc: 0.5424 - val_loss: 1.4458 - val_acc: 0.4615\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.0317 - acc: 0.5424 - val_loss: 1.4190 - val_acc: 0.4615\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.0308 - acc: 0.5339 - val_loss: 1.4138 - val_acc: 0.4615\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 266us/step - loss: 1.0259 - acc: 0.5339 - val_loss: 1.4078 - val_acc: 0.4615\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.0233 - acc: 0.5339 - val_loss: 1.3992 - val_acc: 0.4615\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0208 - acc: 0.5508 - val_loss: 1.3910 - val_acc: 0.4615\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 1.0178 - acc: 0.5508 - val_loss: 1.3843 - val_acc: 0.4615\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 1.0144 - acc: 0.5424 - val_loss: 1.3802 - val_acc: 0.4615\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 1.0118 - acc: 0.5508 - val_loss: 1.3778 - val_acc: 0.4615\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0096 - acc: 0.5508 - val_loss: 1.3709 - val_acc: 0.3846\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0062 - acc: 0.5508 - val_loss: 1.3655 - val_acc: 0.3846\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0038 - acc: 0.5508 - val_loss: 1.3589 - val_acc: 0.3846\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.0001 - acc: 0.5508 - val_loss: 1.3539 - val_acc: 0.3846\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.9989 - acc: 0.5508 - val_loss: 1.3474 - val_acc: 0.3846\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.9952 - acc: 0.5593 - val_loss: 1.3411 - val_acc: 0.3846\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9928 - acc: 0.5678 - val_loss: 1.3356 - val_acc: 0.3846\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9909 - acc: 0.5593 - val_loss: 1.3210 - val_acc: 0.4615\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.9875 - acc: 0.5508 - val_loss: 1.3153 - val_acc: 0.4615\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9857 - acc: 0.5593 - val_loss: 1.3109 - val_acc: 0.4615\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9825 - acc: 0.5593 - val_loss: 1.3034 - val_acc: 0.4615\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9806 - acc: 0.5678 - val_loss: 1.2991 - val_acc: 0.4615\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9784 - acc: 0.5678 - val_loss: 1.2965 - val_acc: 0.4615\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.9779 - acc: 0.5678 - val_loss: 1.2908 - val_acc: 0.4615\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9742 - acc: 0.5678 - val_loss: 1.2827 - val_acc: 0.4615\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9715 - acc: 0.5593 - val_loss: 1.2816 - val_acc: 0.4615\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9710 - acc: 0.5763 - val_loss: 1.2714 - val_acc: 0.4615\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9681 - acc: 0.5678 - val_loss: 1.2665 - val_acc: 0.4615\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.9675 - acc: 0.5678 - val_loss: 1.2655 - val_acc: 0.4615\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 255us/step - loss: 0.9658 - acc: 0.5678 - val_loss: 1.2642 - val_acc: 0.4615\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.9650 - acc: 0.5678 - val_loss: 1.2664 - val_acc: 0.4615\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.9640 - acc: 0.5678 - val_loss: 1.2557 - val_acc: 0.4615\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.9617 - acc: 0.5678 - val_loss: 1.2584 - val_acc: 0.4615\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9609 - acc: 0.5678 - val_loss: 1.2541 - val_acc: 0.4615\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.9591 - acc: 0.5678 - val_loss: 1.2519 - val_acc: 0.4615\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9582 - acc: 0.5678 - val_loss: 1.2479 - val_acc: 0.4615\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9576 - acc: 0.5678 - val_loss: 1.2455 - val_acc: 0.4615\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9550 - acc: 0.5678 - val_loss: 1.2417 - val_acc: 0.4615\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9569 - acc: 0.5763 - val_loss: 1.2400 - val_acc: 0.4615\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.9542 - acc: 0.5847 - val_loss: 1.2376 - val_acc: 0.4615\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.9532 - acc: 0.5678 - val_loss: 1.2331 - val_acc: 0.5385\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9517 - acc: 0.5763 - val_loss: 1.2328 - val_acc: 0.5385\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.9515 - acc: 0.5678 - val_loss: 1.2268 - val_acc: 0.5385\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.9511 - acc: 0.5763 - val_loss: 1.2253 - val_acc: 0.5385\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.9493 - acc: 0.5763 - val_loss: 1.2221 - val_acc: 0.5385\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9484 - acc: 0.5763 - val_loss: 1.2213 - val_acc: 0.5385\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9479 - acc: 0.5763 - val_loss: 1.2161 - val_acc: 0.5385\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9475 - acc: 0.5847 - val_loss: 1.2105 - val_acc: 0.5385\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.9467 - acc: 0.5763 - val_loss: 1.2112 - val_acc: 0.5385\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.9470 - acc: 0.5763 - val_loss: 1.2013 - val_acc: 0.5385\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9439 - acc: 0.5847 - val_loss: 1.1971 - val_acc: 0.5385\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9440 - acc: 0.5847 - val_loss: 1.1962 - val_acc: 0.5385\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9431 - acc: 0.5847 - val_loss: 1.1921 - val_acc: 0.5385\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9414 - acc: 0.5847 - val_loss: 1.1903 - val_acc: 0.5385\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9418 - acc: 0.5847 - val_loss: 1.1887 - val_acc: 0.5385\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.9406 - acc: 0.5932 - val_loss: 1.1855 - val_acc: 0.5385\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.9424 - acc: 0.5847 - val_loss: 1.1822 - val_acc: 0.5385\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9400 - acc: 0.5847 - val_loss: 1.1810 - val_acc: 0.5385\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9400 - acc: 0.5932 - val_loss: 1.1779 - val_acc: 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "ccc1c331-c8d1-4bc3-85ee-074cbc06e9b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "9dec9549-5648-4d82-ac09-d39a6121d723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "d321dbea-f001-47ce-d16f-4730713e8357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "2ce2cbdd-1306-4cb4-fec8-6218f8bdbc8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f07131d5080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "4e707774-83c4-42bf-dde7-3aa74b85eeb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f0712cfdf98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nleLAYWZmpThwmJlZKf8DkworeDqYph0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "9298d323-0347-4171-86d3-5e926c80efb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 2.2548 - acc: 0.4198\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 2.0648 - acc: 0.4122\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 145us/step - loss: 1.9046 - acc: 0.4580\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.7834 - acc: 0.4504\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.6753 - acc: 0.4427\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.6001 - acc: 0.4198\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.5283 - acc: 0.4198\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.4817 - acc: 0.3969\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.4472 - acc: 0.3664\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.4114 - acc: 0.3817\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.3855 - acc: 0.3893\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 218us/step - loss: 1.3626 - acc: 0.3893\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.3409 - acc: 0.3893\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.3217 - acc: 0.3893\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.3088 - acc: 0.4122\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 1.2956 - acc: 0.4046\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 1.2761 - acc: 0.4198\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.2650 - acc: 0.4122\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 1.2512 - acc: 0.4122\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.2392 - acc: 0.4122\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.2278 - acc: 0.4122\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 1.2173 - acc: 0.4122\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.2052 - acc: 0.4122\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1959 - acc: 0.4198\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.1857 - acc: 0.4122\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.1780 - acc: 0.4198\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.1681 - acc: 0.4275\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1603 - acc: 0.4275\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 1.1522 - acc: 0.4351\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 1.1441 - acc: 0.4198\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.1391 - acc: 0.4198\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.1329 - acc: 0.4427\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.1256 - acc: 0.4427\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 143us/step - loss: 1.1170 - acc: 0.4656\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.1115 - acc: 0.4656\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 140us/step - loss: 1.1062 - acc: 0.4580\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.1006 - acc: 0.4656\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0977 - acc: 0.4656\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.0935 - acc: 0.4580\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0868 - acc: 0.4733\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0825 - acc: 0.4656\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.0774 - acc: 0.4733\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0741 - acc: 0.4885\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0679 - acc: 0.4733\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0639 - acc: 0.4809\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.0596 - acc: 0.4809\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 145us/step - loss: 1.0574 - acc: 0.5038\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 219us/step - loss: 1.0519 - acc: 0.4733\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.0455 - acc: 0.4809\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.0424 - acc: 0.4809\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.0389 - acc: 0.4962\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.0370 - acc: 0.4962\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0348 - acc: 0.5115\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.0276 - acc: 0.5115\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0240 - acc: 0.5038\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 1.0212 - acc: 0.5038\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0172 - acc: 0.5115\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.0132 - acc: 0.5115\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.0122 - acc: 0.5267\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 1.0075 - acc: 0.5191\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0044 - acc: 0.5344\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0013 - acc: 0.5420\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 1.0016 - acc: 0.5496\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9984 - acc: 0.5344\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.9959 - acc: 0.5420\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9946 - acc: 0.5344\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9936 - acc: 0.5420\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9924 - acc: 0.5420\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9887 - acc: 0.5267\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9854 - acc: 0.5344\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9847 - acc: 0.5344\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9828 - acc: 0.5420\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9807 - acc: 0.5573\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9797 - acc: 0.5573\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9775 - acc: 0.5725\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9753 - acc: 0.5878\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9738 - acc: 0.5802\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9722 - acc: 0.5802\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9719 - acc: 0.5725\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9700 - acc: 0.5878\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9688 - acc: 0.5954\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9675 - acc: 0.5878\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9684 - acc: 0.5802\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9661 - acc: 0.5954\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9648 - acc: 0.5878\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9635 - acc: 0.6031\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9631 - acc: 0.5878\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9624 - acc: 0.5954\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9628 - acc: 0.5802\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9616 - acc: 0.5878\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9609 - acc: 0.5878\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9590 - acc: 0.5878\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9585 - acc: 0.5878\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9587 - acc: 0.5954\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9578 - acc: 0.5878\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9562 - acc: 0.5878\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9561 - acc: 0.5878\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9545 - acc: 0.5802\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9537 - acc: 0.5878\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9539 - acc: 0.5878\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "00d4c4d7-1372-49ab-fc02-05ae3c0cef36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "52f6327f-2383-4710-80e3-bfe75d971622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11764705882352941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "938fa249-aedc-4fa8-b5f6-fbcb7faf29d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "3ea99dec-d146-47f2-f330-daec5446413b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dvJzYLTzmiQa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ae32197f-ca2f-413a-9a09-f2460d11da4b",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ca47d985-de5f-4e3b-f0ed-2e31ff765a4c",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   8  10  11  12  13  14  15  16  17  18  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  46  47  48  49  50  51  52  55  56  57  58  59  60\n",
            "  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 113 115 116 117 118 119 120 121\n",
            " 122 123 124 125 126 127 128 129 130] TEST: [  7   9  19  28  45  53  54  66  80  95 105 111 112 114]\n",
            "TRAIN: [  1   2   4   5   6   7   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  45  46  47  48  49  50  52  53  54  55  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 111 112 113 114 115 116 117\n",
            " 118 119 121 122 123 124 126 127 129 130] TEST: [  0   3  18  44  51  56  57  72  90 109 120 125 128]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  33  34  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  73  74  75  76  77  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 103 105 106 107 108 109 110 111 112 114 115 116 117 118 119\n",
            " 120 121 122 124 125 126 127 128 129 130] TEST: [  6  15  32  35  36  52  59  68  78 102 104 113 123]\n",
            "TRAIN: [  0   1   3   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  57\n",
            "  58  59  61  62  63  64  66  67  68  69  70  71  72  73  75  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  92  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129] TEST: [  2   4  16  43  60  65  74  76  86  91  93 108 130]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  27  28  29  30  31  32  33  34  35  36  37  39\n",
            "  40  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 117 118\n",
            " 119 120 122 123 125 126 127 128 129 130] TEST: [ 11  12  26  38  41  50  69  70  83 107 116 121 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  15  16  17  18  19\n",
            "  20  22  23  24  25  26  28  29  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  63  64  65  66  67  68  69  70  71  72  73  74  75  76  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  98  99 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  5  14  21  27  34  48  61  62  77  96  97 103 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  14  15  16  17  18\n",
            "  19  20  21  23  24  25  26  27  28  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  43  44  45  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  64  65  66  67  68  69  70  72  73  74  76  77  78  80\n",
            "  81  82  83  84  85  86  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 124 125 128 129 130] TEST: [ 13  22  29  42  46  63  71  75  79  87 100 126 127]\n",
            "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  48  50  51  52  53  54  56  57  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  83  84  86  87  88  89  90  91  92  93  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 130] TEST: [  1  24  30  47  49  55  58  82  85  94 106 110 129]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  18\n",
            "  19  21  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  89  90  91  92  93  94  95  96  97  99\n",
            " 100 102 103 104 105 106 107 108 109 110 111 112 113 114 116 117 118 119\n",
            " 120 121 123 124 125 126 127 128 129 130] TEST: [  8  20  23  39  40  64  67  84  88  98 101 115 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  11  12  13  14  15  16  18  19\n",
            "  20  21  22  23  24  26  27  28  29  30  32  34  35  36  38  39  40  41\n",
            "  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  82  83  84  85  86  87  88  90  91  93  94  95  96  97  98 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [ 10  17  25  31  33  37  73  81  89  92  99 117 118]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a4b403c5-d635-485e-e682-6e890c5b6bd5",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d88bf024-21a1-4dde-e7d6-af30305c097a",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 100\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/100\n",
            "117/117 [==============================] - 1s 5ms/step - loss: 2.2468 - acc: 0.3932 - val_loss: 2.1754 - val_acc: 0.4286\n",
            "Epoch 2/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 2.0864 - acc: 0.3932 - val_loss: 2.0241 - val_acc: 0.4286\n",
            "Epoch 3/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 1.9422 - acc: 0.3932 - val_loss: 1.8866 - val_acc: 0.4286\n",
            "Epoch 4/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 1.8107 - acc: 0.3932 - val_loss: 1.7646 - val_acc: 0.4286\n",
            "Epoch 5/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 1.6927 - acc: 0.3932 - val_loss: 1.6516 - val_acc: 0.4286\n",
            "Epoch 6/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 1.5866 - acc: 0.3932 - val_loss: 1.5515 - val_acc: 0.4286\n",
            "Epoch 7/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 1.4907 - acc: 0.3932 - val_loss: 1.4622 - val_acc: 0.4286\n",
            "Epoch 8/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 1.4047 - acc: 0.3932 - val_loss: 1.3811 - val_acc: 0.4286\n",
            "Epoch 9/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 1.3265 - acc: 0.4103 - val_loss: 1.3062 - val_acc: 0.4286\n",
            "Epoch 10/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 1.2545 - acc: 0.4103 - val_loss: 1.2375 - val_acc: 0.4286\n",
            "Epoch 11/100\n",
            "117/117 [==============================] - 0s 206us/step - loss: 1.1895 - acc: 0.4103 - val_loss: 1.1780 - val_acc: 0.4286\n",
            "Epoch 12/100\n",
            "117/117 [==============================] - 0s 250us/step - loss: 1.1304 - acc: 0.4103 - val_loss: 1.1226 - val_acc: 0.4286\n",
            "Epoch 13/100\n",
            "117/117 [==============================] - 0s 159us/step - loss: 1.0763 - acc: 0.4359 - val_loss: 1.0715 - val_acc: 0.4286\n",
            "Epoch 14/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 1.0272 - acc: 0.4530 - val_loss: 1.0253 - val_acc: 0.4286\n",
            "Epoch 15/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.9802 - acc: 0.4615 - val_loss: 0.9810 - val_acc: 0.4286\n",
            "Epoch 16/100\n",
            "117/117 [==============================] - 0s 188us/step - loss: 0.9362 - acc: 0.4615 - val_loss: 0.9405 - val_acc: 0.4286\n",
            "Epoch 17/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.8940 - acc: 0.4957 - val_loss: 0.8967 - val_acc: 0.5714\n",
            "Epoch 18/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.8539 - acc: 0.5043 - val_loss: 0.8563 - val_acc: 0.5714\n",
            "Epoch 19/100\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.8162 - acc: 0.5214 - val_loss: 0.8186 - val_acc: 0.5714\n",
            "Epoch 20/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.7816 - acc: 0.5726 - val_loss: 0.7826 - val_acc: 0.5714\n",
            "Epoch 21/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.7496 - acc: 0.5812 - val_loss: 0.7498 - val_acc: 0.6429\n",
            "Epoch 22/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.7196 - acc: 0.6410 - val_loss: 0.7192 - val_acc: 0.6429\n",
            "Epoch 23/100\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.6913 - acc: 0.6752 - val_loss: 0.6905 - val_acc: 0.7143\n",
            "Epoch 24/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.6654 - acc: 0.7265 - val_loss: 0.6638 - val_acc: 0.7857\n",
            "Epoch 25/100\n",
            "117/117 [==============================] - 0s 200us/step - loss: 0.6402 - acc: 0.7607 - val_loss: 0.6381 - val_acc: 0.7857\n",
            "Epoch 26/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.6165 - acc: 0.7863 - val_loss: 0.6139 - val_acc: 0.8571\n",
            "Epoch 27/100\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.5943 - acc: 0.8034 - val_loss: 0.5917 - val_acc: 0.8571\n",
            "Epoch 28/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.5736 - acc: 0.8291 - val_loss: 0.5704 - val_acc: 0.8571\n",
            "Epoch 29/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.5539 - acc: 0.8291 - val_loss: 0.5506 - val_acc: 0.8571\n",
            "Epoch 30/100\n",
            "117/117 [==============================] - 0s 184us/step - loss: 0.5355 - acc: 0.8462 - val_loss: 0.5318 - val_acc: 0.8571\n",
            "Epoch 31/100\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.5185 - acc: 0.8632 - val_loss: 0.5141 - val_acc: 0.8571\n",
            "Epoch 32/100\n",
            "117/117 [==============================] - 0s 181us/step - loss: 0.5023 - acc: 0.8632 - val_loss: 0.4970 - val_acc: 0.8571\n",
            "Epoch 33/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.4866 - acc: 0.8718 - val_loss: 0.4811 - val_acc: 0.8571\n",
            "Epoch 34/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.4718 - acc: 0.8889 - val_loss: 0.4657 - val_acc: 0.9286\n",
            "Epoch 35/100\n",
            "117/117 [==============================] - 0s 167us/step - loss: 0.4577 - acc: 0.9231 - val_loss: 0.4513 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.4446 - acc: 0.9316 - val_loss: 0.4377 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "117/117 [==============================] - 0s 169us/step - loss: 0.4321 - acc: 0.9573 - val_loss: 0.4243 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.4198 - acc: 0.9573 - val_loss: 0.4114 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.4079 - acc: 0.9573 - val_loss: 0.3994 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.3968 - acc: 0.9658 - val_loss: 0.3877 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "117/117 [==============================] - 0s 233us/step - loss: 0.3862 - acc: 0.9744 - val_loss: 0.3764 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "117/117 [==============================] - 0s 175us/step - loss: 0.3759 - acc: 0.9744 - val_loss: 0.3655 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "117/117 [==============================] - 0s 211us/step - loss: 0.3662 - acc: 0.9744 - val_loss: 0.3552 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "117/117 [==============================] - 0s 183us/step - loss: 0.3568 - acc: 0.9744 - val_loss: 0.3455 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.3479 - acc: 0.9744 - val_loss: 0.3362 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "117/117 [==============================] - 0s 237us/step - loss: 0.3393 - acc: 0.9744 - val_loss: 0.3270 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.3310 - acc: 0.9744 - val_loss: 0.3181 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.3231 - acc: 0.9829 - val_loss: 0.3097 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.3155 - acc: 0.9915 - val_loss: 0.3018 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.3081 - acc: 0.9915 - val_loss: 0.2938 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.3011 - acc: 0.9915 - val_loss: 0.2863 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "117/117 [==============================] - 0s 193us/step - loss: 0.2943 - acc: 0.9915 - val_loss: 0.2792 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "117/117 [==============================] - 0s 229us/step - loss: 0.2877 - acc: 0.9915 - val_loss: 0.2722 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "117/117 [==============================] - 0s 203us/step - loss: 0.2815 - acc: 0.9915 - val_loss: 0.2655 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.2755 - acc: 0.9915 - val_loss: 0.2590 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "117/117 [==============================] - 0s 207us/step - loss: 0.2695 - acc: 0.9915 - val_loss: 0.2527 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.2638 - acc: 0.9915 - val_loss: 0.2467 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.2584 - acc: 0.9915 - val_loss: 0.2408 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "117/117 [==============================] - 0s 174us/step - loss: 0.2531 - acc: 0.9915 - val_loss: 0.2352 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.2481 - acc: 0.9915 - val_loss: 0.2297 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.2432 - acc: 0.9915 - val_loss: 0.2245 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.2384 - acc: 0.9915 - val_loss: 0.2195 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "117/117 [==============================] - 0s 192us/step - loss: 0.2338 - acc: 0.9915 - val_loss: 0.2146 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.2294 - acc: 0.9915 - val_loss: 0.2099 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.2250 - acc: 0.9915 - val_loss: 0.2052 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.2208 - acc: 0.9915 - val_loss: 0.2007 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.2168 - acc: 0.9915 - val_loss: 0.1963 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.2128 - acc: 0.9915 - val_loss: 0.1920 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "117/117 [==============================] - 0s 226us/step - loss: 0.2090 - acc: 0.9915 - val_loss: 0.1880 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "117/117 [==============================] - 0s 185us/step - loss: 0.2053 - acc: 0.9915 - val_loss: 0.1840 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.2018 - acc: 0.9915 - val_loss: 0.1802 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.1984 - acc: 0.9915 - val_loss: 0.1766 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "117/117 [==============================] - 0s 198us/step - loss: 0.1951 - acc: 0.9915 - val_loss: 0.1731 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "117/117 [==============================] - 0s 204us/step - loss: 0.1919 - acc: 0.9915 - val_loss: 0.1696 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.1888 - acc: 0.9915 - val_loss: 0.1662 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "117/117 [==============================] - 0s 213us/step - loss: 0.1858 - acc: 0.9915 - val_loss: 0.1630 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.1829 - acc: 0.9915 - val_loss: 0.1599 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.1800 - acc: 0.9915 - val_loss: 0.1568 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.1772 - acc: 0.9915 - val_loss: 0.1539 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "117/117 [==============================] - 0s 171us/step - loss: 0.1746 - acc: 0.9915 - val_loss: 0.1510 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "117/117 [==============================] - 0s 182us/step - loss: 0.1719 - acc: 0.9915 - val_loss: 0.1482 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.1694 - acc: 0.9915 - val_loss: 0.1455 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "117/117 [==============================] - 0s 235us/step - loss: 0.1669 - acc: 0.9915 - val_loss: 0.1429 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "117/117 [==============================] - 0s 209us/step - loss: 0.1646 - acc: 0.9915 - val_loss: 0.1403 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "117/117 [==============================] - 0s 244us/step - loss: 0.1622 - acc: 0.9915 - val_loss: 0.1379 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "117/117 [==============================] - 0s 208us/step - loss: 0.1600 - acc: 0.9915 - val_loss: 0.1355 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.1577 - acc: 0.9915 - val_loss: 0.1330 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.1556 - acc: 0.9915 - val_loss: 0.1308 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "117/117 [==============================] - 0s 197us/step - loss: 0.1535 - acc: 0.9915 - val_loss: 0.1286 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.1515 - acc: 0.9915 - val_loss: 0.1264 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.1495 - acc: 0.9915 - val_loss: 0.1243 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.1475 - acc: 0.9915 - val_loss: 0.1222 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "117/117 [==============================] - 0s 179us/step - loss: 0.1456 - acc: 0.9915 - val_loss: 0.1203 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.1438 - acc: 0.9915 - val_loss: 0.1183 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "117/117 [==============================] - 0s 190us/step - loss: 0.1420 - acc: 0.9915 - val_loss: 0.1164 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "117/117 [==============================] - 0s 224us/step - loss: 0.1402 - acc: 0.9915 - val_loss: 0.1146 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.1385 - acc: 0.9915 - val_loss: 0.1127 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "117/117 [==============================] - 0s 210us/step - loss: 0.1368 - acc: 0.9915 - val_loss: 0.1110 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "117/117 [==============================] - 0s 202us/step - loss: 0.1352 - acc: 0.9915 - val_loss: 0.1093 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.1336 - acc: 0.9915 - val_loss: 0.1076 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 5ms/step - loss: 2.2703 - acc: 0.0593 - val_loss: 2.3771 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 2.0944 - acc: 0.0763 - val_loss: 2.1862 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 1.9317 - acc: 0.0847 - val_loss: 2.0200 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.7865 - acc: 0.0847 - val_loss: 1.8550 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 1.6492 - acc: 0.1017 - val_loss: 1.7098 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.5283 - acc: 0.1441 - val_loss: 1.5735 - val_acc: 0.0769\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.4141 - acc: 0.1695 - val_loss: 1.4510 - val_acc: 0.0769\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.3084 - acc: 0.3644 - val_loss: 1.3311 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2137 - acc: 0.4746 - val_loss: 1.2241 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1240 - acc: 0.5085 - val_loss: 1.1258 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 1.0437 - acc: 0.5169 - val_loss: 1.0366 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9724 - acc: 0.5763 - val_loss: 0.9543 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9052 - acc: 0.6780 - val_loss: 0.8837 - val_acc: 0.7692\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.8470 - acc: 0.7288 - val_loss: 0.8206 - val_acc: 0.7692\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.7935 - acc: 0.7881 - val_loss: 0.7662 - val_acc: 0.7692\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.7460 - acc: 0.8051 - val_loss: 0.7199 - val_acc: 0.7692\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.7033 - acc: 0.8136 - val_loss: 0.6799 - val_acc: 0.8462\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.6659 - acc: 0.8305 - val_loss: 0.6437 - val_acc: 0.8462\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.6321 - acc: 0.8475 - val_loss: 0.6121 - val_acc: 0.8462\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.6020 - acc: 0.8898 - val_loss: 0.5840 - val_acc: 0.9231\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5747 - acc: 0.9068 - val_loss: 0.5584 - val_acc: 0.9231\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5498 - acc: 0.9237 - val_loss: 0.5349 - val_acc: 0.9231\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5268 - acc: 0.9322 - val_loss: 0.5138 - val_acc: 0.9231\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5057 - acc: 0.9576 - val_loss: 0.4945 - val_acc: 0.9231\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4861 - acc: 0.9576 - val_loss: 0.4764 - val_acc: 0.9231\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4678 - acc: 0.9746 - val_loss: 0.4595 - val_acc: 0.9231\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.4507 - acc: 0.9746 - val_loss: 0.4438 - val_acc: 0.9231\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4345 - acc: 0.9831 - val_loss: 0.4291 - val_acc: 0.9231\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4193 - acc: 0.9831 - val_loss: 0.4154 - val_acc: 0.9231\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 227us/step - loss: 0.4050 - acc: 0.9831 - val_loss: 0.4025 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3914 - acc: 0.9831 - val_loss: 0.3904 - val_acc: 0.9231\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.3786 - acc: 0.9831 - val_loss: 0.3789 - val_acc: 0.9231\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3663 - acc: 0.9831 - val_loss: 0.3681 - val_acc: 0.9231\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3549 - acc: 0.9831 - val_loss: 0.3578 - val_acc: 0.9231\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.3439 - acc: 0.9831 - val_loss: 0.3481 - val_acc: 0.9231\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3335 - acc: 0.9831 - val_loss: 0.3390 - val_acc: 0.9231\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.3236 - acc: 0.9915 - val_loss: 0.3302 - val_acc: 0.9231\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.3142 - acc: 0.9915 - val_loss: 0.3219 - val_acc: 0.9231\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.3053 - acc: 0.9915 - val_loss: 0.3141 - val_acc: 0.9231\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2968 - acc: 0.9915 - val_loss: 0.3066 - val_acc: 0.9231\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2887 - acc: 0.9915 - val_loss: 0.2995 - val_acc: 0.9231\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2810 - acc: 0.9915 - val_loss: 0.2927 - val_acc: 0.9231\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2736 - acc: 0.9915 - val_loss: 0.2862 - val_acc: 0.9231\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2666 - acc: 0.9915 - val_loss: 0.2801 - val_acc: 0.9231\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2598 - acc: 0.9915 - val_loss: 0.2741 - val_acc: 0.9231\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2534 - acc: 0.9915 - val_loss: 0.2685 - val_acc: 0.9231\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2473 - acc: 0.9915 - val_loss: 0.2631 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2414 - acc: 0.9915 - val_loss: 0.2580 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.2357 - acc: 0.9915 - val_loss: 0.2531 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2303 - acc: 0.9915 - val_loss: 0.2483 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.2251 - acc: 0.9915 - val_loss: 0.2438 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.2201 - acc: 0.9915 - val_loss: 0.2394 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2154 - acc: 0.9915 - val_loss: 0.2353 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2108 - acc: 0.9915 - val_loss: 0.2312 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2064 - acc: 0.9915 - val_loss: 0.2273 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2021 - acc: 0.9915 - val_loss: 0.2236 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1980 - acc: 0.9915 - val_loss: 0.2200 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1941 - acc: 0.9915 - val_loss: 0.2165 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1904 - acc: 0.9915 - val_loss: 0.2132 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1867 - acc: 0.9915 - val_loss: 0.2100 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1832 - acc: 0.9915 - val_loss: 0.2069 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1799 - acc: 0.9915 - val_loss: 0.2040 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1766 - acc: 0.9915 - val_loss: 0.2011 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1734 - acc: 0.9915 - val_loss: 0.1983 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1704 - acc: 0.9915 - val_loss: 0.1956 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1675 - acc: 0.9915 - val_loss: 0.1930 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1646 - acc: 0.9915 - val_loss: 0.1905 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1619 - acc: 0.9915 - val_loss: 0.1880 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1592 - acc: 0.9915 - val_loss: 0.1856 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1567 - acc: 0.9915 - val_loss: 0.1833 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1542 - acc: 0.9915 - val_loss: 0.1811 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1518 - acc: 0.9915 - val_loss: 0.1789 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1495 - acc: 0.9915 - val_loss: 0.1768 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1472 - acc: 0.9915 - val_loss: 0.1748 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1450 - acc: 0.9915 - val_loss: 0.1728 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1429 - acc: 0.9915 - val_loss: 0.1709 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1408 - acc: 0.9915 - val_loss: 0.1691 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1389 - acc: 0.9915 - val_loss: 0.1673 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1369 - acc: 0.9915 - val_loss: 0.1655 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1350 - acc: 0.9915 - val_loss: 0.1638 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1332 - acc: 0.9915 - val_loss: 0.1622 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.1314 - acc: 0.9915 - val_loss: 0.1606 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1297 - acc: 0.9915 - val_loss: 0.1590 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1280 - acc: 0.9915 - val_loss: 0.1574 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1264 - acc: 0.9915 - val_loss: 0.1559 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1248 - acc: 0.9915 - val_loss: 0.1545 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1232 - acc: 0.9915 - val_loss: 0.1531 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.1217 - acc: 1.0000 - val_loss: 0.1517 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1203 - acc: 1.0000 - val_loss: 0.1503 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1189 - acc: 1.0000 - val_loss: 0.1490 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1175 - acc: 1.0000 - val_loss: 0.1477 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1161 - acc: 1.0000 - val_loss: 0.1465 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1148 - acc: 1.0000 - val_loss: 0.1453 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1135 - acc: 1.0000 - val_loss: 0.1441 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1122 - acc: 1.0000 - val_loss: 0.1429 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1110 - acc: 1.0000 - val_loss: 0.1417 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1098 - acc: 1.0000 - val_loss: 0.1406 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1086 - acc: 1.0000 - val_loss: 0.1395 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1075 - acc: 1.0000 - val_loss: 0.1384 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1064 - acc: 1.0000 - val_loss: 0.1374 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.3276 - acc: 0.4492 - val_loss: 1.2536 - val_acc: 0.5385\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.2579 - acc: 0.5000 - val_loss: 1.2012 - val_acc: 0.5385\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.1962 - acc: 0.5339 - val_loss: 1.1525 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.1405 - acc: 0.5339 - val_loss: 1.1086 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.0907 - acc: 0.5339 - val_loss: 1.0677 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 1.0449 - acc: 0.5339 - val_loss: 1.0313 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 230us/step - loss: 1.0043 - acc: 0.5339 - val_loss: 0.9974 - val_acc: 0.5385\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.9673 - acc: 0.5339 - val_loss: 0.9666 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.9342 - acc: 0.5339 - val_loss: 0.9384 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.9040 - acc: 0.5339 - val_loss: 0.9124 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.8759 - acc: 0.5339 - val_loss: 0.8877 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.8501 - acc: 0.5339 - val_loss: 0.8647 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.8257 - acc: 0.5424 - val_loss: 0.8434 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8037 - acc: 0.5424 - val_loss: 0.8234 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.7830 - acc: 0.5424 - val_loss: 0.8047 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.7635 - acc: 0.5424 - val_loss: 0.7869 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.7450 - acc: 0.5593 - val_loss: 0.7700 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.7277 - acc: 0.5593 - val_loss: 0.7540 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.7113 - acc: 0.5593 - val_loss: 0.7390 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.6957 - acc: 0.5593 - val_loss: 0.7248 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.6810 - acc: 0.5593 - val_loss: 0.7111 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.6670 - acc: 0.5678 - val_loss: 0.6982 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.6537 - acc: 0.5847 - val_loss: 0.6857 - val_acc: 0.5385\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.6411 - acc: 0.6186 - val_loss: 0.6738 - val_acc: 0.5385\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.6291 - acc: 0.6525 - val_loss: 0.6628 - val_acc: 0.5385\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6176 - acc: 0.6610 - val_loss: 0.6517 - val_acc: 0.5385\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 245us/step - loss: 0.6068 - acc: 0.6864 - val_loss: 0.6411 - val_acc: 0.5385\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5966 - acc: 0.7288 - val_loss: 0.6316 - val_acc: 0.5385\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5866 - acc: 0.7542 - val_loss: 0.6218 - val_acc: 0.5385\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5770 - acc: 0.7966 - val_loss: 0.6129 - val_acc: 0.6923\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.5677 - acc: 0.8136 - val_loss: 0.6039 - val_acc: 0.6923\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5588 - acc: 0.8390 - val_loss: 0.5952 - val_acc: 0.6923\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.5503 - acc: 0.8390 - val_loss: 0.5870 - val_acc: 0.7692\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.5421 - acc: 0.8898 - val_loss: 0.5789 - val_acc: 0.7692\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5342 - acc: 0.8898 - val_loss: 0.5711 - val_acc: 0.7692\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.5262 - acc: 0.9153 - val_loss: 0.5634 - val_acc: 0.7692\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5184 - acc: 0.9237 - val_loss: 0.5562 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5110 - acc: 0.9322 - val_loss: 0.5492 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5040 - acc: 0.9322 - val_loss: 0.5424 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4971 - acc: 0.9322 - val_loss: 0.5359 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.4904 - acc: 0.9322 - val_loss: 0.5294 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4838 - acc: 0.9322 - val_loss: 0.5233 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4774 - acc: 0.9237 - val_loss: 0.5171 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.4710 - acc: 0.9237 - val_loss: 0.5114 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4648 - acc: 0.9237 - val_loss: 0.5063 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4588 - acc: 0.9237 - val_loss: 0.5013 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4529 - acc: 0.9237 - val_loss: 0.4966 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.4473 - acc: 0.9237 - val_loss: 0.4919 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.4418 - acc: 0.9237 - val_loss: 0.4873 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4364 - acc: 0.9322 - val_loss: 0.4830 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 243us/step - loss: 0.4313 - acc: 0.9407 - val_loss: 0.4789 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4263 - acc: 0.9407 - val_loss: 0.4748 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4213 - acc: 0.9407 - val_loss: 0.4708 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.4165 - acc: 0.9407 - val_loss: 0.4669 - val_acc: 0.8462\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.4118 - acc: 0.9407 - val_loss: 0.4631 - val_acc: 0.8462\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.4072 - acc: 0.9407 - val_loss: 0.4592 - val_acc: 0.8462\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.4026 - acc: 0.9407 - val_loss: 0.4555 - val_acc: 0.8462\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3980 - acc: 0.9492 - val_loss: 0.4519 - val_acc: 0.8462\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3936 - acc: 0.9492 - val_loss: 0.4483 - val_acc: 0.8462\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.3892 - acc: 0.9576 - val_loss: 0.4448 - val_acc: 0.8462\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3849 - acc: 0.9576 - val_loss: 0.4413 - val_acc: 0.8462\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3806 - acc: 0.9661 - val_loss: 0.4379 - val_acc: 0.8462\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3764 - acc: 0.9661 - val_loss: 0.4345 - val_acc: 0.8462\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.3722 - acc: 0.9661 - val_loss: 0.4313 - val_acc: 0.8462\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3681 - acc: 0.9661 - val_loss: 0.4280 - val_acc: 0.8462\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3640 - acc: 0.9746 - val_loss: 0.4248 - val_acc: 0.8462\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3600 - acc: 0.9746 - val_loss: 0.4216 - val_acc: 0.8462\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3561 - acc: 0.9746 - val_loss: 0.4186 - val_acc: 0.8462\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3521 - acc: 0.9746 - val_loss: 0.4155 - val_acc: 0.8462\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3483 - acc: 0.9746 - val_loss: 0.4126 - val_acc: 0.8462\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3445 - acc: 0.9746 - val_loss: 0.4097 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.3407 - acc: 0.9746 - val_loss: 0.4067 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.3370 - acc: 0.9746 - val_loss: 0.4040 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.3333 - acc: 0.9746 - val_loss: 0.4012 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3297 - acc: 0.9746 - val_loss: 0.3984 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3260 - acc: 0.9746 - val_loss: 0.3957 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.3224 - acc: 0.9746 - val_loss: 0.3931 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3188 - acc: 0.9746 - val_loss: 0.3905 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3152 - acc: 0.9746 - val_loss: 0.3879 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3118 - acc: 0.9746 - val_loss: 0.3853 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 233us/step - loss: 0.3083 - acc: 0.9746 - val_loss: 0.3828 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.3049 - acc: 0.9746 - val_loss: 0.3803 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.3015 - acc: 0.9746 - val_loss: 0.3777 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2981 - acc: 0.9746 - val_loss: 0.3752 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.2948 - acc: 0.9746 - val_loss: 0.3727 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2915 - acc: 0.9746 - val_loss: 0.3703 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2882 - acc: 0.9746 - val_loss: 0.3679 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2850 - acc: 0.9746 - val_loss: 0.3655 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2819 - acc: 0.9746 - val_loss: 0.3633 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2787 - acc: 0.9746 - val_loss: 0.3610 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2757 - acc: 0.9831 - val_loss: 0.3588 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2727 - acc: 0.9831 - val_loss: 0.3565 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2697 - acc: 0.9831 - val_loss: 0.3544 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2669 - acc: 0.9831 - val_loss: 0.3523 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 225us/step - loss: 0.2640 - acc: 0.9831 - val_loss: 0.3503 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.2613 - acc: 0.9831 - val_loss: 0.3483 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2585 - acc: 0.9831 - val_loss: 0.3463 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2557 - acc: 0.9831 - val_loss: 0.3444 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.2530 - acc: 0.9831 - val_loss: 0.3425 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2504 - acc: 0.9831 - val_loss: 0.3407 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.7137 - acc: 0.0593 - val_loss: 1.4427 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 1.5839 - acc: 0.0763 - val_loss: 1.3500 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 1.4682 - acc: 0.1017 - val_loss: 1.2685 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 1.3691 - acc: 0.1356 - val_loss: 1.1975 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.2801 - acc: 0.1695 - val_loss: 1.1362 - val_acc: 0.2308\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.2030 - acc: 0.1780 - val_loss: 1.0804 - val_acc: 0.3846\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1346 - acc: 0.1780 - val_loss: 1.0307 - val_acc: 0.3846\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0744 - acc: 0.2203 - val_loss: 0.9878 - val_acc: 0.3846\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 1.0212 - acc: 0.2458 - val_loss: 0.9486 - val_acc: 0.3846\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9728 - acc: 0.2966 - val_loss: 0.9136 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.9293 - acc: 0.3729 - val_loss: 0.8806 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8900 - acc: 0.5000 - val_loss: 0.8507 - val_acc: 0.6154\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.8542 - acc: 0.5339 - val_loss: 0.8225 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.8215 - acc: 0.5763 - val_loss: 0.7961 - val_acc: 0.6154\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.7914 - acc: 0.6356 - val_loss: 0.7722 - val_acc: 0.6154\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.7640 - acc: 0.7119 - val_loss: 0.7495 - val_acc: 0.6154\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.7385 - acc: 0.7288 - val_loss: 0.7285 - val_acc: 0.7692\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.7147 - acc: 0.7373 - val_loss: 0.7084 - val_acc: 0.7692\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.6924 - acc: 0.7712 - val_loss: 0.6899 - val_acc: 0.7692\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.6715 - acc: 0.7966 - val_loss: 0.6720 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.6516 - acc: 0.7966 - val_loss: 0.6552 - val_acc: 0.7692\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6332 - acc: 0.8136 - val_loss: 0.6394 - val_acc: 0.7692\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6157 - acc: 0.8136 - val_loss: 0.6241 - val_acc: 0.7692\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5992 - acc: 0.8305 - val_loss: 0.6096 - val_acc: 0.7692\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5836 - acc: 0.8305 - val_loss: 0.5958 - val_acc: 0.8462\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.5689 - acc: 0.8305 - val_loss: 0.5829 - val_acc: 0.8462\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5550 - acc: 0.8305 - val_loss: 0.5706 - val_acc: 0.8462\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5417 - acc: 0.8305 - val_loss: 0.5588 - val_acc: 0.8462\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5291 - acc: 0.8390 - val_loss: 0.5479 - val_acc: 0.8462\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.5172 - acc: 0.8390 - val_loss: 0.5372 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.5058 - acc: 0.8390 - val_loss: 0.5270 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4949 - acc: 0.8390 - val_loss: 0.5173 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.4844 - acc: 0.8475 - val_loss: 0.5082 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4744 - acc: 0.8475 - val_loss: 0.4994 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4648 - acc: 0.8475 - val_loss: 0.4909 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.4557 - acc: 0.8475 - val_loss: 0.4830 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.4469 - acc: 0.8475 - val_loss: 0.4754 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4387 - acc: 0.8475 - val_loss: 0.4681 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.4306 - acc: 0.8559 - val_loss: 0.4612 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4231 - acc: 0.8559 - val_loss: 0.4546 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.4158 - acc: 0.8559 - val_loss: 0.4482 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.4088 - acc: 0.8644 - val_loss: 0.4422 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.4022 - acc: 0.8644 - val_loss: 0.4365 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3958 - acc: 0.8644 - val_loss: 0.4310 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3896 - acc: 0.8644 - val_loss: 0.4257 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3838 - acc: 0.8644 - val_loss: 0.4207 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3781 - acc: 0.8729 - val_loss: 0.4158 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3727 - acc: 0.8729 - val_loss: 0.4112 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3675 - acc: 0.8729 - val_loss: 0.4066 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3625 - acc: 0.8729 - val_loss: 0.4024 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3576 - acc: 0.8729 - val_loss: 0.3982 - val_acc: 0.8462\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.3530 - acc: 0.8729 - val_loss: 0.3943 - val_acc: 0.8462\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3485 - acc: 0.8729 - val_loss: 0.3905 - val_acc: 0.8462\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.3442 - acc: 0.8729 - val_loss: 0.3868 - val_acc: 0.8462\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 251us/step - loss: 0.3401 - acc: 0.8729 - val_loss: 0.3832 - val_acc: 0.8462\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3361 - acc: 0.8814 - val_loss: 0.3798 - val_acc: 0.8462\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3322 - acc: 0.8814 - val_loss: 0.3766 - val_acc: 0.8462\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3285 - acc: 0.8814 - val_loss: 0.3734 - val_acc: 0.8462\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3250 - acc: 0.8814 - val_loss: 0.3703 - val_acc: 0.8462\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3215 - acc: 0.8814 - val_loss: 0.3673 - val_acc: 0.8462\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.3182 - acc: 0.8814 - val_loss: 0.3645 - val_acc: 0.8462\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3149 - acc: 0.8814 - val_loss: 0.3617 - val_acc: 0.8462\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3118 - acc: 0.8814 - val_loss: 0.3590 - val_acc: 0.8462\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3088 - acc: 0.8814 - val_loss: 0.3564 - val_acc: 0.8462\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3059 - acc: 0.8814 - val_loss: 0.3538 - val_acc: 0.8462\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.3030 - acc: 0.8814 - val_loss: 0.3513 - val_acc: 0.8462\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3002 - acc: 0.8814 - val_loss: 0.3490 - val_acc: 0.8462\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.2976 - acc: 0.8814 - val_loss: 0.3467 - val_acc: 0.8462\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2950 - acc: 0.8814 - val_loss: 0.3444 - val_acc: 0.8462\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2925 - acc: 0.8814 - val_loss: 0.3423 - val_acc: 0.8462\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2901 - acc: 0.8814 - val_loss: 0.3401 - val_acc: 0.8462\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2878 - acc: 0.8814 - val_loss: 0.3380 - val_acc: 0.8462\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2855 - acc: 0.8814 - val_loss: 0.3361 - val_acc: 0.8462\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2832 - acc: 0.8814 - val_loss: 0.3341 - val_acc: 0.8462\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.2811 - acc: 0.8814 - val_loss: 0.3322 - val_acc: 0.8462\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2790 - acc: 0.8814 - val_loss: 0.3303 - val_acc: 0.8462\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2769 - acc: 0.8814 - val_loss: 0.3285 - val_acc: 0.8462\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2749 - acc: 0.8814 - val_loss: 0.3267 - val_acc: 0.8462\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2729 - acc: 0.8814 - val_loss: 0.3249 - val_acc: 0.8462\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2709 - acc: 0.8814 - val_loss: 0.3232 - val_acc: 0.8462\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2690 - acc: 0.8814 - val_loss: 0.3214 - val_acc: 0.8462\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2671 - acc: 0.8814 - val_loss: 0.3198 - val_acc: 0.8462\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.2654 - acc: 0.8814 - val_loss: 0.3182 - val_acc: 0.8462\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2636 - acc: 0.8814 - val_loss: 0.3166 - val_acc: 0.8462\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.2619 - acc: 0.8814 - val_loss: 0.3151 - val_acc: 0.8462\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2602 - acc: 0.8814 - val_loss: 0.3135 - val_acc: 0.8462\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2586 - acc: 0.8814 - val_loss: 0.3120 - val_acc: 0.8462\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2570 - acc: 0.8814 - val_loss: 0.3106 - val_acc: 0.8462\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2554 - acc: 0.8814 - val_loss: 0.3092 - val_acc: 0.8462\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2539 - acc: 0.8814 - val_loss: 0.3078 - val_acc: 0.8462\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2524 - acc: 0.8814 - val_loss: 0.3064 - val_acc: 0.8462\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2509 - acc: 0.8814 - val_loss: 0.3050 - val_acc: 0.8462\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2495 - acc: 0.8814 - val_loss: 0.3037 - val_acc: 0.8462\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.2481 - acc: 0.8814 - val_loss: 0.3024 - val_acc: 0.8462\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2467 - acc: 0.8814 - val_loss: 0.3011 - val_acc: 0.8462\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.2454 - acc: 0.8814 - val_loss: 0.2999 - val_acc: 0.8462\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 0.2441 - acc: 0.8814 - val_loss: 0.2986 - val_acc: 0.8462\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 222us/step - loss: 0.2428 - acc: 0.8814 - val_loss: 0.2974 - val_acc: 0.8462\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.2415 - acc: 0.8814 - val_loss: 0.2962 - val_acc: 0.8462\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2403 - acc: 0.8814 - val_loss: 0.2951 - val_acc: 0.8462\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 6ms/step - loss: 1.8111 - acc: 0.1186 - val_loss: 2.0553 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 1.7117 - acc: 0.2373 - val_loss: 1.9437 - val_acc: 0.0769\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 1.6211 - acc: 0.0593 - val_loss: 1.8443 - val_acc: 0.0769\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 1.5400 - acc: 0.1102 - val_loss: 1.7548 - val_acc: 0.0769\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 1.4667 - acc: 0.0593 - val_loss: 1.6729 - val_acc: 0.1538\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 241us/step - loss: 1.4003 - acc: 0.3051 - val_loss: 1.6016 - val_acc: 0.1538\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 1.3394 - acc: 0.3136 - val_loss: 1.5375 - val_acc: 0.1538\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 1.2835 - acc: 0.3220 - val_loss: 1.4770 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.2329 - acc: 0.3220 - val_loss: 1.4223 - val_acc: 0.1538\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.1863 - acc: 0.3475 - val_loss: 1.3733 - val_acc: 0.1538\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 1.1437 - acc: 0.3644 - val_loss: 1.3269 - val_acc: 0.1538\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 1.1052 - acc: 0.3814 - val_loss: 1.2859 - val_acc: 0.1538\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0699 - acc: 0.4407 - val_loss: 1.2469 - val_acc: 0.2308\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0365 - acc: 0.6356 - val_loss: 1.2122 - val_acc: 0.4615\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 1.0061 - acc: 0.7542 - val_loss: 1.1793 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.9776 - acc: 0.7627 - val_loss: 1.1489 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.9515 - acc: 0.7712 - val_loss: 1.1212 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.9271 - acc: 0.7797 - val_loss: 1.0947 - val_acc: 0.5385\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.9043 - acc: 0.7797 - val_loss: 1.0702 - val_acc: 0.5385\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.8825 - acc: 0.7881 - val_loss: 1.0470 - val_acc: 0.5385\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8624 - acc: 0.8051 - val_loss: 1.0256 - val_acc: 0.5385\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8434 - acc: 0.8051 - val_loss: 1.0055 - val_acc: 0.5385\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8252 - acc: 0.8475 - val_loss: 0.9863 - val_acc: 0.6923\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.8078 - acc: 0.8898 - val_loss: 0.9683 - val_acc: 0.6923\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.7915 - acc: 0.9153 - val_loss: 0.9510 - val_acc: 0.7692\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.7760 - acc: 0.9153 - val_loss: 0.9354 - val_acc: 0.7692\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.7612 - acc: 0.9153 - val_loss: 0.9201 - val_acc: 0.7692\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.7470 - acc: 0.9322 - val_loss: 0.9060 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7336 - acc: 0.9322 - val_loss: 0.8922 - val_acc: 0.7692\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.7204 - acc: 0.9322 - val_loss: 0.8787 - val_acc: 0.7692\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.7077 - acc: 0.9322 - val_loss: 0.8658 - val_acc: 0.7692\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.6956 - acc: 0.9322 - val_loss: 0.8537 - val_acc: 0.7692\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.6841 - acc: 0.9322 - val_loss: 0.8414 - val_acc: 0.7692\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.6728 - acc: 0.9407 - val_loss: 0.8300 - val_acc: 0.7692\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.6618 - acc: 0.9407 - val_loss: 0.8189 - val_acc: 0.7692\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6515 - acc: 0.9576 - val_loss: 0.8084 - val_acc: 0.7692\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.6415 - acc: 0.9576 - val_loss: 0.7980 - val_acc: 0.7692\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6317 - acc: 0.9576 - val_loss: 0.7879 - val_acc: 0.7692\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.6222 - acc: 0.9576 - val_loss: 0.7784 - val_acc: 0.7692\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.6130 - acc: 0.9576 - val_loss: 0.7689 - val_acc: 0.7692\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.6041 - acc: 0.9576 - val_loss: 0.7596 - val_acc: 0.7692\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.5955 - acc: 0.9661 - val_loss: 0.7506 - val_acc: 0.7692\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.5872 - acc: 0.9661 - val_loss: 0.7420 - val_acc: 0.7692\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5791 - acc: 0.9661 - val_loss: 0.7344 - val_acc: 0.7692\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5713 - acc: 0.9661 - val_loss: 0.7270 - val_acc: 0.7692\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5637 - acc: 0.9661 - val_loss: 0.7195 - val_acc: 0.7692\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5564 - acc: 0.9661 - val_loss: 0.7124 - val_acc: 0.7692\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5494 - acc: 0.9661 - val_loss: 0.7056 - val_acc: 0.7692\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.5428 - acc: 0.9661 - val_loss: 0.6987 - val_acc: 0.7692\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5363 - acc: 0.9661 - val_loss: 0.6923 - val_acc: 0.7692\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5300 - acc: 0.9661 - val_loss: 0.6862 - val_acc: 0.7692\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.5240 - acc: 0.9661 - val_loss: 0.6806 - val_acc: 0.7692\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.5181 - acc: 0.9746 - val_loss: 0.6750 - val_acc: 0.7692\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.5124 - acc: 0.9746 - val_loss: 0.6694 - val_acc: 0.7692\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5069 - acc: 0.9831 - val_loss: 0.6641 - val_acc: 0.7692\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5015 - acc: 0.9831 - val_loss: 0.6585 - val_acc: 0.7692\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4962 - acc: 0.9915 - val_loss: 0.6533 - val_acc: 0.7692\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4911 - acc: 0.9915 - val_loss: 0.6481 - val_acc: 0.7692\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.4860 - acc: 0.9915 - val_loss: 0.6425 - val_acc: 0.7692\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4810 - acc: 0.9915 - val_loss: 0.6372 - val_acc: 0.7692\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4762 - acc: 0.9915 - val_loss: 0.6322 - val_acc: 0.7692\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4716 - acc: 0.9915 - val_loss: 0.6272 - val_acc: 0.7692\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4670 - acc: 0.9915 - val_loss: 0.6224 - val_acc: 0.7692\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4625 - acc: 0.9915 - val_loss: 0.6176 - val_acc: 0.7692\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4581 - acc: 0.9915 - val_loss: 0.6128 - val_acc: 0.7692\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4539 - acc: 0.9915 - val_loss: 0.6081 - val_acc: 0.7692\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.4497 - acc: 0.9915 - val_loss: 0.6035 - val_acc: 0.7692\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4456 - acc: 0.9915 - val_loss: 0.5988 - val_acc: 0.7692\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4415 - acc: 0.9915 - val_loss: 0.5942 - val_acc: 0.7692\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.4375 - acc: 0.9915 - val_loss: 0.5898 - val_acc: 0.7692\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.4336 - acc: 0.9915 - val_loss: 0.5854 - val_acc: 0.7692\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4298 - acc: 0.9915 - val_loss: 0.5812 - val_acc: 0.7692\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.4260 - acc: 0.9915 - val_loss: 0.5768 - val_acc: 0.7692\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4223 - acc: 0.9915 - val_loss: 0.5725 - val_acc: 0.7692\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4186 - acc: 0.9915 - val_loss: 0.5685 - val_acc: 0.7692\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4151 - acc: 0.9915 - val_loss: 0.5645 - val_acc: 0.7692\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4116 - acc: 0.9915 - val_loss: 0.5605 - val_acc: 0.7692\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4081 - acc: 0.9915 - val_loss: 0.5568 - val_acc: 0.7692\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.4048 - acc: 0.9915 - val_loss: 0.5530 - val_acc: 0.7692\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.4015 - acc: 0.9915 - val_loss: 0.5492 - val_acc: 0.7692\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.3982 - acc: 0.9915 - val_loss: 0.5454 - val_acc: 0.7692\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3949 - acc: 0.9915 - val_loss: 0.5418 - val_acc: 0.7692\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3918 - acc: 0.9915 - val_loss: 0.5384 - val_acc: 0.7692\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3886 - acc: 0.9915 - val_loss: 0.5349 - val_acc: 0.7692\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3855 - acc: 0.9915 - val_loss: 0.5315 - val_acc: 0.7692\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3824 - acc: 0.9915 - val_loss: 0.5280 - val_acc: 0.7692\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.3794 - acc: 0.9915 - val_loss: 0.5246 - val_acc: 0.7692\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3764 - acc: 0.9915 - val_loss: 0.5212 - val_acc: 0.7692\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3735 - acc: 0.9915 - val_loss: 0.5179 - val_acc: 0.7692\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3706 - acc: 0.9915 - val_loss: 0.5147 - val_acc: 0.7692\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3678 - acc: 0.9915 - val_loss: 0.5114 - val_acc: 0.7692\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3649 - acc: 0.9915 - val_loss: 0.5081 - val_acc: 0.7692\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3622 - acc: 0.9915 - val_loss: 0.5049 - val_acc: 0.7692\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.3594 - acc: 0.9915 - val_loss: 0.5019 - val_acc: 0.7692\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3568 - acc: 0.9915 - val_loss: 0.4989 - val_acc: 0.7692\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.3542 - acc: 0.9915 - val_loss: 0.4959 - val_acc: 0.7692\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3516 - acc: 0.9915 - val_loss: 0.4930 - val_acc: 0.7692\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3490 - acc: 0.9915 - val_loss: 0.4902 - val_acc: 0.8462\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3465 - acc: 0.9915 - val_loss: 0.4873 - val_acc: 0.8462\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.3441 - acc: 0.9915 - val_loss: 0.4846 - val_acc: 0.8462\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 1.2032 - acc: 0.4661 - val_loss: 1.2177 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.1275 - acc: 0.4661 - val_loss: 1.1567 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 1.0589 - acc: 0.4746 - val_loss: 1.1000 - val_acc: 0.4615\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.9968 - acc: 0.4746 - val_loss: 1.0514 - val_acc: 0.4615\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9403 - acc: 0.4831 - val_loss: 1.0045 - val_acc: 0.4615\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8884 - acc: 0.4831 - val_loss: 0.9613 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8423 - acc: 0.4831 - val_loss: 0.9232 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.8000 - acc: 0.4831 - val_loss: 0.8872 - val_acc: 0.5385\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.7607 - acc: 0.4915 - val_loss: 0.8533 - val_acc: 0.5385\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7243 - acc: 0.5000 - val_loss: 0.8217 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6907 - acc: 0.5169 - val_loss: 0.7932 - val_acc: 0.5385\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6595 - acc: 0.5424 - val_loss: 0.7662 - val_acc: 0.5385\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.6309 - acc: 0.5593 - val_loss: 0.7418 - val_acc: 0.6154\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.6036 - acc: 0.5932 - val_loss: 0.7182 - val_acc: 0.6923\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5786 - acc: 0.6525 - val_loss: 0.6962 - val_acc: 0.6923\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.5553 - acc: 0.7034 - val_loss: 0.6750 - val_acc: 0.6923\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.5339 - acc: 0.7627 - val_loss: 0.6551 - val_acc: 0.6923\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.5136 - acc: 0.8220 - val_loss: 0.6365 - val_acc: 0.7692\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4945 - acc: 0.8644 - val_loss: 0.6184 - val_acc: 0.7692\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.4767 - acc: 0.8644 - val_loss: 0.6009 - val_acc: 0.7692\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.4600 - acc: 0.8644 - val_loss: 0.5848 - val_acc: 0.8462\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4445 - acc: 0.8729 - val_loss: 0.5694 - val_acc: 0.8462\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4300 - acc: 0.8729 - val_loss: 0.5545 - val_acc: 0.8462\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.4163 - acc: 0.8729 - val_loss: 0.5403 - val_acc: 0.8462\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.4035 - acc: 0.8729 - val_loss: 0.5272 - val_acc: 0.8462\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3915 - acc: 0.8729 - val_loss: 0.5150 - val_acc: 0.8462\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.3806 - acc: 0.8729 - val_loss: 0.5035 - val_acc: 0.8462\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3701 - acc: 0.8729 - val_loss: 0.4923 - val_acc: 0.8462\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.3603 - acc: 0.8729 - val_loss: 0.4818 - val_acc: 0.8462\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.3510 - acc: 0.8729 - val_loss: 0.4713 - val_acc: 0.8462\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.3423 - acc: 0.8729 - val_loss: 0.4616 - val_acc: 0.8462\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 240us/step - loss: 0.3340 - acc: 0.8729 - val_loss: 0.4521 - val_acc: 0.8462\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3260 - acc: 0.8729 - val_loss: 0.4432 - val_acc: 0.8462\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.3184 - acc: 0.9237 - val_loss: 0.4343 - val_acc: 0.8462\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3111 - acc: 0.9407 - val_loss: 0.4256 - val_acc: 0.8462\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.3040 - acc: 0.9407 - val_loss: 0.4174 - val_acc: 0.8462\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2973 - acc: 0.9407 - val_loss: 0.4094 - val_acc: 0.8462\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2907 - acc: 0.9492 - val_loss: 0.4013 - val_acc: 0.8462\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.2845 - acc: 0.9576 - val_loss: 0.3938 - val_acc: 0.8462\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.2786 - acc: 0.9576 - val_loss: 0.3867 - val_acc: 0.8462\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.2728 - acc: 0.9576 - val_loss: 0.3794 - val_acc: 0.8462\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2674 - acc: 0.9576 - val_loss: 0.3728 - val_acc: 0.8462\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2622 - acc: 0.9576 - val_loss: 0.3664 - val_acc: 0.8462\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2571 - acc: 0.9661 - val_loss: 0.3602 - val_acc: 0.8462\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2522 - acc: 0.9661 - val_loss: 0.3539 - val_acc: 0.8462\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2474 - acc: 0.9661 - val_loss: 0.3477 - val_acc: 0.8462\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2429 - acc: 0.9661 - val_loss: 0.3417 - val_acc: 0.8462\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.2383 - acc: 0.9661 - val_loss: 0.3363 - val_acc: 0.8462\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2340 - acc: 0.9661 - val_loss: 0.3306 - val_acc: 0.8462\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.2298 - acc: 0.9746 - val_loss: 0.3254 - val_acc: 0.8462\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2258 - acc: 0.9746 - val_loss: 0.3199 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.2217 - acc: 0.9746 - val_loss: 0.3148 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2179 - acc: 0.9746 - val_loss: 0.3099 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2141 - acc: 0.9746 - val_loss: 0.3050 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2104 - acc: 0.9746 - val_loss: 0.3001 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 229us/step - loss: 0.2069 - acc: 0.9831 - val_loss: 0.2954 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2034 - acc: 0.9831 - val_loss: 0.2907 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2000 - acc: 0.9831 - val_loss: 0.2862 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1968 - acc: 0.9831 - val_loss: 0.2818 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1936 - acc: 0.9831 - val_loss: 0.2776 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1905 - acc: 0.9831 - val_loss: 0.2735 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1875 - acc: 0.9831 - val_loss: 0.2695 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1845 - acc: 0.9831 - val_loss: 0.2652 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1816 - acc: 0.9915 - val_loss: 0.2613 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1787 - acc: 0.9915 - val_loss: 0.2573 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1759 - acc: 0.9915 - val_loss: 0.2536 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1732 - acc: 0.9915 - val_loss: 0.2498 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1706 - acc: 0.9915 - val_loss: 0.2463 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 231us/step - loss: 0.1681 - acc: 0.9915 - val_loss: 0.2428 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 207us/step - loss: 0.1656 - acc: 0.9915 - val_loss: 0.2392 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1632 - acc: 0.9915 - val_loss: 0.2357 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1608 - acc: 0.9915 - val_loss: 0.2325 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1585 - acc: 0.9915 - val_loss: 0.2291 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1562 - acc: 0.9915 - val_loss: 0.2261 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1541 - acc: 0.9915 - val_loss: 0.2229 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1519 - acc: 0.9915 - val_loss: 0.2199 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1498 - acc: 0.9915 - val_loss: 0.2169 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1478 - acc: 0.9915 - val_loss: 0.2141 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1459 - acc: 0.9915 - val_loss: 0.2114 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1440 - acc: 0.9915 - val_loss: 0.2087 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1421 - acc: 0.9915 - val_loss: 0.2061 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1403 - acc: 0.9915 - val_loss: 0.2034 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1385 - acc: 0.9915 - val_loss: 0.2009 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1368 - acc: 0.9915 - val_loss: 0.1984 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1351 - acc: 0.9915 - val_loss: 0.1960 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1335 - acc: 0.9915 - val_loss: 0.1937 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1318 - acc: 0.9915 - val_loss: 0.1913 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1302 - acc: 0.9915 - val_loss: 0.1891 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1287 - acc: 0.9915 - val_loss: 0.1867 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1272 - acc: 0.9915 - val_loss: 0.1846 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1257 - acc: 0.9915 - val_loss: 0.1825 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.1243 - acc: 0.9915 - val_loss: 0.1802 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1229 - acc: 0.9915 - val_loss: 0.1783 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1215 - acc: 0.9915 - val_loss: 0.1763 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1202 - acc: 0.9915 - val_loss: 0.1743 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1189 - acc: 0.9915 - val_loss: 0.1724 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1176 - acc: 0.9915 - val_loss: 0.1705 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1164 - acc: 0.9915 - val_loss: 0.1687 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1152 - acc: 0.9915 - val_loss: 0.1669 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1140 - acc: 0.9915 - val_loss: 0.1650 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 1.1328 - acc: 0.5678 - val_loss: 1.2385 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 247us/step - loss: 0.9991 - acc: 0.5847 - val_loss: 1.0934 - val_acc: 0.4615\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.8795 - acc: 0.5932 - val_loss: 0.9623 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.7760 - acc: 0.6356 - val_loss: 0.8505 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.6879 - acc: 0.7458 - val_loss: 0.7541 - val_acc: 0.6923\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.6149 - acc: 0.8305 - val_loss: 0.6741 - val_acc: 0.7692\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5537 - acc: 0.8983 - val_loss: 0.6086 - val_acc: 0.7692\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.5037 - acc: 0.9322 - val_loss: 0.5535 - val_acc: 0.7692\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4615 - acc: 0.9322 - val_loss: 0.5095 - val_acc: 0.8462\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4263 - acc: 0.9576 - val_loss: 0.4723 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3966 - acc: 0.9661 - val_loss: 0.4411 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.3714 - acc: 0.9746 - val_loss: 0.4142 - val_acc: 0.9231\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3496 - acc: 0.9831 - val_loss: 0.3920 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.3308 - acc: 0.9915 - val_loss: 0.3718 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.3144 - acc: 0.9915 - val_loss: 0.3548 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2998 - acc: 0.9915 - val_loss: 0.3393 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2867 - acc: 0.9915 - val_loss: 0.3260 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2751 - acc: 0.9915 - val_loss: 0.3136 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.2645 - acc: 0.9915 - val_loss: 0.3027 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2549 - acc: 0.9915 - val_loss: 0.2926 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.2461 - acc: 0.9915 - val_loss: 0.2834 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2380 - acc: 0.9915 - val_loss: 0.2749 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2306 - acc: 0.9915 - val_loss: 0.2672 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2237 - acc: 0.9915 - val_loss: 0.2600 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2174 - acc: 0.9915 - val_loss: 0.2534 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2114 - acc: 0.9915 - val_loss: 0.2474 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2058 - acc: 0.9915 - val_loss: 0.2414 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.2005 - acc: 0.9915 - val_loss: 0.2362 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1956 - acc: 0.9915 - val_loss: 0.2312 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1910 - acc: 0.9915 - val_loss: 0.2265 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1867 - acc: 0.9915 - val_loss: 0.2220 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1825 - acc: 0.9915 - val_loss: 0.2177 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1786 - acc: 0.9915 - val_loss: 0.2138 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1748 - acc: 0.9915 - val_loss: 0.2101 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1712 - acc: 0.9915 - val_loss: 0.2065 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1678 - acc: 0.9915 - val_loss: 0.2032 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1645 - acc: 0.9915 - val_loss: 0.2000 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1614 - acc: 0.9915 - val_loss: 0.1969 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1584 - acc: 0.9915 - val_loss: 0.1939 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1555 - acc: 0.9915 - val_loss: 0.1910 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1528 - acc: 0.9915 - val_loss: 0.1884 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1501 - acc: 0.9915 - val_loss: 0.1858 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1476 - acc: 0.9915 - val_loss: 0.1834 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1451 - acc: 0.9915 - val_loss: 0.1810 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1427 - acc: 0.9915 - val_loss: 0.1788 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1404 - acc: 0.9915 - val_loss: 0.1766 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1382 - acc: 0.9915 - val_loss: 0.1745 - val_acc: 0.9231\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1361 - acc: 0.9915 - val_loss: 0.1725 - val_acc: 0.9231\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1340 - acc: 0.9915 - val_loss: 0.1706 - val_acc: 0.9231\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.1320 - acc: 0.9915 - val_loss: 0.1687 - val_acc: 0.9231\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1301 - acc: 0.9915 - val_loss: 0.1668 - val_acc: 0.9231\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1282 - acc: 0.9915 - val_loss: 0.1651 - val_acc: 0.9231\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1264 - acc: 0.9915 - val_loss: 0.1635 - val_acc: 0.9231\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1246 - acc: 0.9915 - val_loss: 0.1618 - val_acc: 0.9231\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1229 - acc: 0.9915 - val_loss: 0.1602 - val_acc: 0.9231\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1212 - acc: 0.9915 - val_loss: 0.1587 - val_acc: 0.9231\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1196 - acc: 0.9915 - val_loss: 0.1573 - val_acc: 0.9231\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1180 - acc: 0.9915 - val_loss: 0.1559 - val_acc: 0.9231\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1165 - acc: 0.9915 - val_loss: 0.1546 - val_acc: 0.9231\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.1150 - acc: 0.9915 - val_loss: 0.1533 - val_acc: 0.9231\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1136 - acc: 0.9915 - val_loss: 0.1520 - val_acc: 0.9231\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1122 - acc: 0.9915 - val_loss: 0.1507 - val_acc: 0.9231\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1108 - acc: 0.9915 - val_loss: 0.1495 - val_acc: 0.9231\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1095 - acc: 0.9915 - val_loss: 0.1484 - val_acc: 0.9231\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1082 - acc: 0.9915 - val_loss: 0.1473 - val_acc: 0.9231\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1069 - acc: 0.9915 - val_loss: 0.1462 - val_acc: 0.9231\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1057 - acc: 0.9915 - val_loss: 0.1451 - val_acc: 0.9231\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1045 - acc: 0.9915 - val_loss: 0.1441 - val_acc: 0.9231\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1033 - acc: 0.9915 - val_loss: 0.1431 - val_acc: 0.9231\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1021 - acc: 0.9915 - val_loss: 0.1421 - val_acc: 0.9231\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1010 - acc: 0.9915 - val_loss: 0.1411 - val_acc: 0.9231\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.0999 - acc: 0.9915 - val_loss: 0.1402 - val_acc: 0.9231\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.0988 - acc: 0.9915 - val_loss: 0.1393 - val_acc: 0.9231\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.0978 - acc: 0.9915 - val_loss: 0.1384 - val_acc: 0.9231\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.0968 - acc: 0.9915 - val_loss: 0.1376 - val_acc: 0.9231\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.0958 - acc: 0.9915 - val_loss: 0.1367 - val_acc: 0.9231\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.0948 - acc: 0.9915 - val_loss: 0.1359 - val_acc: 0.9231\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.0938 - acc: 0.9915 - val_loss: 0.1351 - val_acc: 0.9231\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.0929 - acc: 0.9915 - val_loss: 0.1344 - val_acc: 0.9231\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.0920 - acc: 0.9915 - val_loss: 0.1336 - val_acc: 0.9231\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.0911 - acc: 0.9915 - val_loss: 0.1329 - val_acc: 0.9231\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.0902 - acc: 0.9915 - val_loss: 0.1322 - val_acc: 0.9231\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.0894 - acc: 0.9915 - val_loss: 0.1315 - val_acc: 0.9231\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.0885 - acc: 0.9915 - val_loss: 0.1308 - val_acc: 0.9231\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.0877 - acc: 0.9915 - val_loss: 0.1301 - val_acc: 0.9231\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.0869 - acc: 0.9915 - val_loss: 0.1295 - val_acc: 0.9231\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.0861 - acc: 0.9915 - val_loss: 0.1288 - val_acc: 0.9231\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.0853 - acc: 0.9915 - val_loss: 0.1282 - val_acc: 0.9231\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.0846 - acc: 0.9915 - val_loss: 0.1277 - val_acc: 0.9231\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.0838 - acc: 0.9915 - val_loss: 0.1271 - val_acc: 0.9231\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.0831 - acc: 0.9915 - val_loss: 0.1265 - val_acc: 0.9231\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.0824 - acc: 0.9915 - val_loss: 0.1259 - val_acc: 0.9231\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.0817 - acc: 0.9915 - val_loss: 0.1254 - val_acc: 0.9231\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.0810 - acc: 0.9915 - val_loss: 0.1249 - val_acc: 0.9231\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.0803 - acc: 0.9915 - val_loss: 0.1244 - val_acc: 0.9231\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.0796 - acc: 0.9915 - val_loss: 0.1238 - val_acc: 0.9231\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.0790 - acc: 0.9915 - val_loss: 0.1233 - val_acc: 0.9231\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.0783 - acc: 0.9915 - val_loss: 0.1228 - val_acc: 0.9231\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.0777 - acc: 0.9915 - val_loss: 0.1223 - val_acc: 0.9231\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.0771 - acc: 0.9915 - val_loss: 0.1219 - val_acc: 0.9231\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 7ms/step - loss: 2.2670 - acc: 0.0000e+00 - val_loss: 2.3321 - val_acc: 0.0000e+00\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 2.0288 - acc: 0.0000e+00 - val_loss: 2.0665 - val_acc: 0.0000e+00\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.8128 - acc: 0.0085 - val_loss: 1.8219 - val_acc: 0.0000e+00\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.6180 - acc: 0.0254 - val_loss: 1.6021 - val_acc: 0.0000e+00\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.4493 - acc: 0.0424 - val_loss: 1.4067 - val_acc: 0.0000e+00\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 1.3001 - acc: 0.0508 - val_loss: 1.2405 - val_acc: 0.0000e+00\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1710 - acc: 0.0847 - val_loss: 1.0969 - val_acc: 0.0769\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 1.0603 - acc: 0.1102 - val_loss: 0.9765 - val_acc: 0.1538\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.9662 - acc: 0.1441 - val_loss: 0.8769 - val_acc: 0.1538\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.8861 - acc: 0.3136 - val_loss: 0.7942 - val_acc: 0.5385\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.8181 - acc: 0.4661 - val_loss: 0.7247 - val_acc: 0.6154\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.7606 - acc: 0.8051 - val_loss: 0.6687 - val_acc: 1.0000\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.7110 - acc: 0.8559 - val_loss: 0.6215 - val_acc: 1.0000\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.6684 - acc: 0.9237 - val_loss: 0.5821 - val_acc: 1.0000\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.6316 - acc: 0.9322 - val_loss: 0.5477 - val_acc: 1.0000\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5993 - acc: 0.9407 - val_loss: 0.5184 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.5703 - acc: 0.9576 - val_loss: 0.4920 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.5440 - acc: 0.9746 - val_loss: 0.4686 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.5203 - acc: 0.9831 - val_loss: 0.4487 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.4986 - acc: 0.9831 - val_loss: 0.4304 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.4789 - acc: 0.9831 - val_loss: 0.4138 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.4609 - acc: 0.9831 - val_loss: 0.3981 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.4440 - acc: 0.9831 - val_loss: 0.3837 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4286 - acc: 0.9831 - val_loss: 0.3700 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.4143 - acc: 0.9831 - val_loss: 0.3572 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.4008 - acc: 0.9831 - val_loss: 0.3451 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3880 - acc: 0.9831 - val_loss: 0.3336 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.3759 - acc: 0.9831 - val_loss: 0.3227 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3645 - acc: 0.9831 - val_loss: 0.3124 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.3535 - acc: 0.9831 - val_loss: 0.3026 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.3433 - acc: 0.9831 - val_loss: 0.2932 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3336 - acc: 0.9831 - val_loss: 0.2842 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3243 - acc: 0.9831 - val_loss: 0.2758 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3155 - acc: 0.9831 - val_loss: 0.2674 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3070 - acc: 0.9831 - val_loss: 0.2596 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2989 - acc: 0.9831 - val_loss: 0.2522 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2911 - acc: 0.9831 - val_loss: 0.2450 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.2838 - acc: 0.9831 - val_loss: 0.2382 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 242us/step - loss: 0.2767 - acc: 0.9831 - val_loss: 0.2315 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2698 - acc: 0.9831 - val_loss: 0.2251 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2633 - acc: 0.9831 - val_loss: 0.2192 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2571 - acc: 0.9831 - val_loss: 0.2132 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2510 - acc: 0.9831 - val_loss: 0.2077 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.2452 - acc: 0.9831 - val_loss: 0.2023 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 223us/step - loss: 0.2397 - acc: 0.9831 - val_loss: 0.1972 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2343 - acc: 0.9831 - val_loss: 0.1922 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 210us/step - loss: 0.2292 - acc: 0.9831 - val_loss: 0.1874 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.2242 - acc: 0.9831 - val_loss: 0.1829 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 205us/step - loss: 0.2194 - acc: 0.9831 - val_loss: 0.1784 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2148 - acc: 0.9831 - val_loss: 0.1741 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.2104 - acc: 0.9831 - val_loss: 0.1700 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2061 - acc: 0.9831 - val_loss: 0.1661 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2020 - acc: 0.9831 - val_loss: 0.1623 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1981 - acc: 0.9831 - val_loss: 0.1586 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.1943 - acc: 0.9831 - val_loss: 0.1550 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1906 - acc: 0.9831 - val_loss: 0.1517 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1870 - acc: 0.9831 - val_loss: 0.1484 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1836 - acc: 0.9831 - val_loss: 0.1452 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1803 - acc: 0.9831 - val_loss: 0.1421 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1771 - acc: 0.9831 - val_loss: 0.1392 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 220us/step - loss: 0.1740 - acc: 0.9831 - val_loss: 0.1364 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1710 - acc: 0.9831 - val_loss: 0.1336 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.1681 - acc: 0.9831 - val_loss: 0.1309 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1653 - acc: 0.9831 - val_loss: 0.1284 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1626 - acc: 0.9831 - val_loss: 0.1259 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 0.1600 - acc: 0.9831 - val_loss: 0.1235 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1575 - acc: 0.9831 - val_loss: 0.1212 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1551 - acc: 0.9831 - val_loss: 0.1189 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1527 - acc: 0.9831 - val_loss: 0.1167 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.1504 - acc: 0.9831 - val_loss: 0.1147 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.1481 - acc: 0.9831 - val_loss: 0.1126 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.1460 - acc: 0.9831 - val_loss: 0.1106 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.1439 - acc: 0.9831 - val_loss: 0.1087 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1418 - acc: 0.9831 - val_loss: 0.1068 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1399 - acc: 0.9831 - val_loss: 0.1050 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1380 - acc: 0.9831 - val_loss: 0.1033 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1361 - acc: 0.9831 - val_loss: 0.1016 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1343 - acc: 0.9831 - val_loss: 0.1000 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.1326 - acc: 0.9831 - val_loss: 0.0984 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.1309 - acc: 0.9831 - val_loss: 0.0968 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.1292 - acc: 0.9831 - val_loss: 0.0953 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.1276 - acc: 0.9831 - val_loss: 0.0939 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1260 - acc: 0.9831 - val_loss: 0.0925 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1245 - acc: 0.9831 - val_loss: 0.0911 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.1230 - acc: 0.9831 - val_loss: 0.0897 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 253us/step - loss: 0.1215 - acc: 0.9831 - val_loss: 0.0884 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1201 - acc: 0.9831 - val_loss: 0.0872 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1188 - acc: 0.9831 - val_loss: 0.0859 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.1174 - acc: 0.9831 - val_loss: 0.0847 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1161 - acc: 0.9831 - val_loss: 0.0836 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.1149 - acc: 0.9831 - val_loss: 0.0824 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.1136 - acc: 0.9831 - val_loss: 0.0813 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1124 - acc: 0.9831 - val_loss: 0.0802 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.1113 - acc: 0.9831 - val_loss: 0.0791 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1101 - acc: 0.9831 - val_loss: 0.0781 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1090 - acc: 0.9831 - val_loss: 0.0771 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1079 - acc: 0.9831 - val_loss: 0.0761 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1068 - acc: 0.9831 - val_loss: 0.0751 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1057 - acc: 0.9831 - val_loss: 0.0742 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1047 - acc: 0.9831 - val_loss: 0.0733 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 2.1674 - acc: 0.0508 - val_loss: 2.0971 - val_acc: 0.0769\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.9649 - acc: 0.1186 - val_loss: 1.8589 - val_acc: 0.1538\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 1.7809 - acc: 0.2203 - val_loss: 1.6544 - val_acc: 0.2308\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 1.6179 - acc: 0.3051 - val_loss: 1.4776 - val_acc: 0.3077\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 1.4704 - acc: 0.3220 - val_loss: 1.3219 - val_acc: 0.3846\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 1.3419 - acc: 0.3390 - val_loss: 1.1864 - val_acc: 0.4615\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 167us/step - loss: 1.2350 - acc: 0.3644 - val_loss: 1.0811 - val_acc: 0.4615\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 160us/step - loss: 1.1539 - acc: 0.3898 - val_loss: 1.0002 - val_acc: 0.4615\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 1.0896 - acc: 0.3898 - val_loss: 0.9368 - val_acc: 0.4615\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 1.0348 - acc: 0.4068 - val_loss: 0.8885 - val_acc: 0.4615\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 206us/step - loss: 0.9879 - acc: 0.4068 - val_loss: 0.8471 - val_acc: 0.4615\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.9469 - acc: 0.4322 - val_loss: 0.8097 - val_acc: 0.4615\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.9088 - acc: 0.4322 - val_loss: 0.7737 - val_acc: 0.5385\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.8746 - acc: 0.4492 - val_loss: 0.7428 - val_acc: 0.5385\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.8437 - acc: 0.4576 - val_loss: 0.7135 - val_acc: 0.5385\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.8148 - acc: 0.4661 - val_loss: 0.6871 - val_acc: 0.5385\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.7886 - acc: 0.4661 - val_loss: 0.6629 - val_acc: 0.5385\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.7637 - acc: 0.4746 - val_loss: 0.6398 - val_acc: 0.6154\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.7405 - acc: 0.5169 - val_loss: 0.6182 - val_acc: 0.6154\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.7188 - acc: 0.5254 - val_loss: 0.5983 - val_acc: 0.6154\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.6983 - acc: 0.5508 - val_loss: 0.5795 - val_acc: 0.6154\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.6791 - acc: 0.5593 - val_loss: 0.5623 - val_acc: 0.6154\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.6614 - acc: 0.5932 - val_loss: 0.5458 - val_acc: 0.6154\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.6447 - acc: 0.6102 - val_loss: 0.5298 - val_acc: 0.6154\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.6287 - acc: 0.6356 - val_loss: 0.5152 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.6137 - acc: 0.6864 - val_loss: 0.5010 - val_acc: 0.6923\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.5995 - acc: 0.7288 - val_loss: 0.4874 - val_acc: 0.7692\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.5857 - acc: 0.7542 - val_loss: 0.4748 - val_acc: 0.7692\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.5724 - acc: 0.7712 - val_loss: 0.4628 - val_acc: 0.8462\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.5598 - acc: 0.9068 - val_loss: 0.4509 - val_acc: 0.9231\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5477 - acc: 0.9661 - val_loss: 0.4394 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 211us/step - loss: 0.5360 - acc: 0.9831 - val_loss: 0.4286 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.5250 - acc: 0.9831 - val_loss: 0.4181 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.5142 - acc: 0.9831 - val_loss: 0.4083 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 238us/step - loss: 0.5038 - acc: 0.9831 - val_loss: 0.3988 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 214us/step - loss: 0.4940 - acc: 0.9831 - val_loss: 0.3893 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.4842 - acc: 0.9831 - val_loss: 0.3803 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4748 - acc: 0.9831 - val_loss: 0.3716 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.4657 - acc: 0.9831 - val_loss: 0.3629 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.4569 - acc: 0.9831 - val_loss: 0.3548 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.4484 - acc: 0.9831 - val_loss: 0.3466 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.4401 - acc: 0.9831 - val_loss: 0.3391 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.4319 - acc: 0.9831 - val_loss: 0.3313 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.4240 - acc: 0.9915 - val_loss: 0.3238 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.4163 - acc: 0.9915 - val_loss: 0.3164 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.4087 - acc: 0.9915 - val_loss: 0.3093 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.4013 - acc: 0.9915 - val_loss: 0.3021 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3941 - acc: 0.9915 - val_loss: 0.2954 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3870 - acc: 0.9915 - val_loss: 0.2886 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.3800 - acc: 0.9915 - val_loss: 0.2820 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.3732 - acc: 0.9915 - val_loss: 0.2755 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3665 - acc: 0.9915 - val_loss: 0.2693 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3600 - acc: 0.9915 - val_loss: 0.2630 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.3536 - acc: 0.9915 - val_loss: 0.2571 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.3475 - acc: 0.9915 - val_loss: 0.2511 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.3414 - acc: 0.9915 - val_loss: 0.2454 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.3354 - acc: 0.9915 - val_loss: 0.2395 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3295 - acc: 0.9915 - val_loss: 0.2340 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.3237 - acc: 0.9915 - val_loss: 0.2286 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.3181 - acc: 0.9915 - val_loss: 0.2232 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3125 - acc: 0.9915 - val_loss: 0.2179 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.3071 - acc: 0.9915 - val_loss: 0.2129 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3018 - acc: 0.9915 - val_loss: 0.2078 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2965 - acc: 0.9915 - val_loss: 0.2029 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 192us/step - loss: 0.2914 - acc: 0.9915 - val_loss: 0.1981 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2864 - acc: 0.9915 - val_loss: 0.1934 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2815 - acc: 0.9915 - val_loss: 0.1889 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.2767 - acc: 0.9915 - val_loss: 0.1844 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2720 - acc: 0.9915 - val_loss: 0.1800 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.2674 - acc: 0.9915 - val_loss: 0.1756 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2628 - acc: 0.9915 - val_loss: 0.1715 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2584 - acc: 0.9915 - val_loss: 0.1674 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.2540 - acc: 0.9915 - val_loss: 0.1634 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.2498 - acc: 0.9915 - val_loss: 0.1595 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.2457 - acc: 0.9915 - val_loss: 0.1557 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.2417 - acc: 0.9915 - val_loss: 0.1520 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2377 - acc: 0.9915 - val_loss: 0.1484 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2338 - acc: 0.9915 - val_loss: 0.1449 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2300 - acc: 0.9915 - val_loss: 0.1414 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2264 - acc: 0.9915 - val_loss: 0.1382 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.2228 - acc: 0.9915 - val_loss: 0.1348 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2193 - acc: 0.9915 - val_loss: 0.1316 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2158 - acc: 0.9915 - val_loss: 0.1286 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.2125 - acc: 0.9915 - val_loss: 0.1256 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2092 - acc: 0.9915 - val_loss: 0.1227 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2060 - acc: 0.9915 - val_loss: 0.1199 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.2029 - acc: 0.9915 - val_loss: 0.1171 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.1998 - acc: 0.9915 - val_loss: 0.1145 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1969 - acc: 0.9915 - val_loss: 0.1119 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1940 - acc: 0.9915 - val_loss: 0.1094 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 174us/step - loss: 0.1911 - acc: 0.9915 - val_loss: 0.1069 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.1884 - acc: 0.9915 - val_loss: 0.1045 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1856 - acc: 0.9915 - val_loss: 0.1022 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.1830 - acc: 0.9915 - val_loss: 0.0999 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1804 - acc: 0.9915 - val_loss: 0.0977 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1779 - acc: 0.9915 - val_loss: 0.0956 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1755 - acc: 0.9915 - val_loss: 0.0936 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1731 - acc: 0.9915 - val_loss: 0.0915 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1707 - acc: 0.9915 - val_loss: 0.0896 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1684 - acc: 0.9915 - val_loss: 0.0876 - val_acc: 1.0000\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/100\n",
            "118/118 [==============================] - 1s 8ms/step - loss: 1.1822 - acc: 0.4576 - val_loss: 1.0826 - val_acc: 0.4615\n",
            "Epoch 2/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 1.1062 - acc: 0.4576 - val_loss: 0.9987 - val_acc: 0.5385\n",
            "Epoch 3/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 1.0403 - acc: 0.4492 - val_loss: 0.9237 - val_acc: 0.5385\n",
            "Epoch 4/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.9824 - acc: 0.4576 - val_loss: 0.8593 - val_acc: 0.5385\n",
            "Epoch 5/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.9315 - acc: 0.4661 - val_loss: 0.8045 - val_acc: 0.5385\n",
            "Epoch 6/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8873 - acc: 0.4746 - val_loss: 0.7561 - val_acc: 0.5385\n",
            "Epoch 7/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.8479 - acc: 0.4915 - val_loss: 0.7163 - val_acc: 0.6154\n",
            "Epoch 8/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.8142 - acc: 0.5932 - val_loss: 0.6803 - val_acc: 0.7692\n",
            "Epoch 9/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.7835 - acc: 0.6780 - val_loss: 0.6493 - val_acc: 0.7692\n",
            "Epoch 10/100\n",
            "118/118 [==============================] - 0s 215us/step - loss: 0.7558 - acc: 0.7034 - val_loss: 0.6197 - val_acc: 0.8462\n",
            "Epoch 11/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.7293 - acc: 0.7034 - val_loss: 0.5933 - val_acc: 0.8462\n",
            "Epoch 12/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.7053 - acc: 0.7373 - val_loss: 0.5683 - val_acc: 0.8462\n",
            "Epoch 13/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.6823 - acc: 0.7542 - val_loss: 0.5458 - val_acc: 0.8462\n",
            "Epoch 14/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.6610 - acc: 0.7797 - val_loss: 0.5247 - val_acc: 0.9231\n",
            "Epoch 15/100\n",
            "118/118 [==============================] - 0s 208us/step - loss: 0.6407 - acc: 0.7966 - val_loss: 0.5039 - val_acc: 0.9231\n",
            "Epoch 16/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.6213 - acc: 0.8051 - val_loss: 0.4847 - val_acc: 1.0000\n",
            "Epoch 17/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.6023 - acc: 0.8051 - val_loss: 0.4659 - val_acc: 1.0000\n",
            "Epoch 18/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.5840 - acc: 0.8136 - val_loss: 0.4488 - val_acc: 1.0000\n",
            "Epoch 19/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.5672 - acc: 0.8220 - val_loss: 0.4322 - val_acc: 1.0000\n",
            "Epoch 20/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.5510 - acc: 0.8220 - val_loss: 0.4171 - val_acc: 1.0000\n",
            "Epoch 21/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.5357 - acc: 0.8220 - val_loss: 0.4025 - val_acc: 1.0000\n",
            "Epoch 22/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.5208 - acc: 0.8390 - val_loss: 0.3888 - val_acc: 1.0000\n",
            "Epoch 23/100\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.5068 - acc: 0.8390 - val_loss: 0.3760 - val_acc: 1.0000\n",
            "Epoch 24/100\n",
            "118/118 [==============================] - 0s 235us/step - loss: 0.4932 - acc: 0.8644 - val_loss: 0.3642 - val_acc: 1.0000\n",
            "Epoch 25/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.4801 - acc: 0.8729 - val_loss: 0.3525 - val_acc: 1.0000\n",
            "Epoch 26/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.4672 - acc: 0.8898 - val_loss: 0.3411 - val_acc: 1.0000\n",
            "Epoch 27/100\n",
            "118/118 [==============================] - 0s 212us/step - loss: 0.4544 - acc: 0.8898 - val_loss: 0.3299 - val_acc: 1.0000\n",
            "Epoch 28/100\n",
            "118/118 [==============================] - 0s 201us/step - loss: 0.4419 - acc: 0.8898 - val_loss: 0.3191 - val_acc: 1.0000\n",
            "Epoch 29/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.4299 - acc: 0.8898 - val_loss: 0.3087 - val_acc: 1.0000\n",
            "Epoch 30/100\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.4178 - acc: 0.8983 - val_loss: 0.2984 - val_acc: 1.0000\n",
            "Epoch 31/100\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.4064 - acc: 0.8983 - val_loss: 0.2886 - val_acc: 1.0000\n",
            "Epoch 32/100\n",
            "118/118 [==============================] - 0s 198us/step - loss: 0.3953 - acc: 0.9153 - val_loss: 0.2792 - val_acc: 1.0000\n",
            "Epoch 33/100\n",
            "118/118 [==============================] - 0s 178us/step - loss: 0.3847 - acc: 0.9153 - val_loss: 0.2703 - val_acc: 1.0000\n",
            "Epoch 34/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.3746 - acc: 0.9153 - val_loss: 0.2614 - val_acc: 1.0000\n",
            "Epoch 35/100\n",
            "118/118 [==============================] - 0s 168us/step - loss: 0.3646 - acc: 0.9153 - val_loss: 0.2530 - val_acc: 1.0000\n",
            "Epoch 36/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3550 - acc: 0.9153 - val_loss: 0.2448 - val_acc: 1.0000\n",
            "Epoch 37/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.3458 - acc: 0.9153 - val_loss: 0.2369 - val_acc: 1.0000\n",
            "Epoch 38/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.3369 - acc: 0.9153 - val_loss: 0.2294 - val_acc: 1.0000\n",
            "Epoch 39/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.3283 - acc: 0.9153 - val_loss: 0.2220 - val_acc: 1.0000\n",
            "Epoch 40/100\n",
            "118/118 [==============================] - 0s 189us/step - loss: 0.3201 - acc: 0.9153 - val_loss: 0.2151 - val_acc: 1.0000\n",
            "Epoch 41/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.3123 - acc: 0.9153 - val_loss: 0.2083 - val_acc: 1.0000\n",
            "Epoch 42/100\n",
            "118/118 [==============================] - 0s 273us/step - loss: 0.3048 - acc: 0.9237 - val_loss: 0.2019 - val_acc: 1.0000\n",
            "Epoch 43/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.2976 - acc: 0.9322 - val_loss: 0.1957 - val_acc: 1.0000\n",
            "Epoch 44/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.2908 - acc: 0.9322 - val_loss: 0.1897 - val_acc: 1.0000\n",
            "Epoch 45/100\n",
            "118/118 [==============================] - 0s 199us/step - loss: 0.2840 - acc: 0.9407 - val_loss: 0.1840 - val_acc: 1.0000\n",
            "Epoch 46/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.2775 - acc: 0.9492 - val_loss: 0.1785 - val_acc: 1.0000\n",
            "Epoch 47/100\n",
            "118/118 [==============================] - 0s 176us/step - loss: 0.2714 - acc: 0.9492 - val_loss: 0.1732 - val_acc: 1.0000\n",
            "Epoch 48/100\n",
            "118/118 [==============================] - 0s 190us/step - loss: 0.2654 - acc: 0.9492 - val_loss: 0.1682 - val_acc: 1.0000\n",
            "Epoch 49/100\n",
            "118/118 [==============================] - 0s 196us/step - loss: 0.2597 - acc: 0.9492 - val_loss: 0.1633 - val_acc: 1.0000\n",
            "Epoch 50/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.2541 - acc: 0.9576 - val_loss: 0.1587 - val_acc: 1.0000\n",
            "Epoch 51/100\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.2487 - acc: 0.9576 - val_loss: 0.1541 - val_acc: 1.0000\n",
            "Epoch 52/100\n",
            "118/118 [==============================] - 0s 169us/step - loss: 0.2435 - acc: 0.9576 - val_loss: 0.1498 - val_acc: 1.0000\n",
            "Epoch 53/100\n",
            "118/118 [==============================] - 0s 179us/step - loss: 0.2385 - acc: 0.9576 - val_loss: 0.1456 - val_acc: 1.0000\n",
            "Epoch 54/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2337 - acc: 0.9576 - val_loss: 0.1416 - val_acc: 1.0000\n",
            "Epoch 55/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.2290 - acc: 0.9576 - val_loss: 0.1379 - val_acc: 1.0000\n",
            "Epoch 56/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.2246 - acc: 0.9576 - val_loss: 0.1341 - val_acc: 1.0000\n",
            "Epoch 57/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.2203 - acc: 0.9576 - val_loss: 0.1307 - val_acc: 1.0000\n",
            "Epoch 58/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.2162 - acc: 0.9576 - val_loss: 0.1272 - val_acc: 1.0000\n",
            "Epoch 59/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2123 - acc: 0.9576 - val_loss: 0.1240 - val_acc: 1.0000\n",
            "Epoch 60/100\n",
            "118/118 [==============================] - 0s 187us/step - loss: 0.2084 - acc: 0.9576 - val_loss: 0.1208 - val_acc: 1.0000\n",
            "Epoch 61/100\n",
            "118/118 [==============================] - 0s 209us/step - loss: 0.2048 - acc: 0.9576 - val_loss: 0.1178 - val_acc: 1.0000\n",
            "Epoch 62/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.2012 - acc: 0.9661 - val_loss: 0.1149 - val_acc: 1.0000\n",
            "Epoch 63/100\n",
            "118/118 [==============================] - 0s 228us/step - loss: 0.1978 - acc: 0.9661 - val_loss: 0.1121 - val_acc: 1.0000\n",
            "Epoch 64/100\n",
            "118/118 [==============================] - 0s 249us/step - loss: 0.1945 - acc: 0.9661 - val_loss: 0.1094 - val_acc: 1.0000\n",
            "Epoch 65/100\n",
            "118/118 [==============================] - 0s 182us/step - loss: 0.1913 - acc: 0.9661 - val_loss: 0.1068 - val_acc: 1.0000\n",
            "Epoch 66/100\n",
            "118/118 [==============================] - 0s 170us/step - loss: 0.1882 - acc: 0.9661 - val_loss: 0.1043 - val_acc: 1.0000\n",
            "Epoch 67/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1852 - acc: 0.9661 - val_loss: 0.1018 - val_acc: 1.0000\n",
            "Epoch 68/100\n",
            "118/118 [==============================] - 0s 204us/step - loss: 0.1823 - acc: 0.9661 - val_loss: 0.0995 - val_acc: 1.0000\n",
            "Epoch 69/100\n",
            "118/118 [==============================] - 0s 237us/step - loss: 0.1795 - acc: 0.9661 - val_loss: 0.0973 - val_acc: 1.0000\n",
            "Epoch 70/100\n",
            "118/118 [==============================] - 0s 221us/step - loss: 0.1768 - acc: 0.9661 - val_loss: 0.0952 - val_acc: 1.0000\n",
            "Epoch 71/100\n",
            "118/118 [==============================] - 0s 219us/step - loss: 0.1742 - acc: 0.9661 - val_loss: 0.0931 - val_acc: 1.0000\n",
            "Epoch 72/100\n",
            "118/118 [==============================] - 0s 200us/step - loss: 0.1716 - acc: 0.9661 - val_loss: 0.0912 - val_acc: 1.0000\n",
            "Epoch 73/100\n",
            "118/118 [==============================] - 0s 213us/step - loss: 0.1691 - acc: 0.9661 - val_loss: 0.0894 - val_acc: 1.0000\n",
            "Epoch 74/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1667 - acc: 0.9661 - val_loss: 0.0876 - val_acc: 1.0000\n",
            "Epoch 75/100\n",
            "118/118 [==============================] - 0s 186us/step - loss: 0.1644 - acc: 0.9661 - val_loss: 0.0859 - val_acc: 1.0000\n",
            "Epoch 76/100\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.1621 - acc: 0.9661 - val_loss: 0.0842 - val_acc: 1.0000\n",
            "Epoch 77/100\n",
            "118/118 [==============================] - 0s 216us/step - loss: 0.1599 - acc: 0.9661 - val_loss: 0.0826 - val_acc: 1.0000\n",
            "Epoch 78/100\n",
            "118/118 [==============================] - 0s 260us/step - loss: 0.1578 - acc: 0.9661 - val_loss: 0.0810 - val_acc: 1.0000\n",
            "Epoch 79/100\n",
            "118/118 [==============================] - 0s 217us/step - loss: 0.1558 - acc: 0.9661 - val_loss: 0.0796 - val_acc: 1.0000\n",
            "Epoch 80/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1537 - acc: 0.9661 - val_loss: 0.0781 - val_acc: 1.0000\n",
            "Epoch 81/100\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.1518 - acc: 0.9661 - val_loss: 0.0767 - val_acc: 1.0000\n",
            "Epoch 82/100\n",
            "118/118 [==============================] - 0s 183us/step - loss: 0.1499 - acc: 0.9661 - val_loss: 0.0754 - val_acc: 1.0000\n",
            "Epoch 83/100\n",
            "118/118 [==============================] - 0s 197us/step - loss: 0.1481 - acc: 0.9661 - val_loss: 0.0740 - val_acc: 1.0000\n",
            "Epoch 84/100\n",
            "118/118 [==============================] - 0s 184us/step - loss: 0.1463 - acc: 0.9661 - val_loss: 0.0728 - val_acc: 1.0000\n",
            "Epoch 85/100\n",
            "118/118 [==============================] - 0s 193us/step - loss: 0.1446 - acc: 0.9661 - val_loss: 0.0715 - val_acc: 1.0000\n",
            "Epoch 86/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1429 - acc: 0.9661 - val_loss: 0.0703 - val_acc: 1.0000\n",
            "Epoch 87/100\n",
            "118/118 [==============================] - 0s 194us/step - loss: 0.1412 - acc: 0.9661 - val_loss: 0.0692 - val_acc: 1.0000\n",
            "Epoch 88/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1396 - acc: 0.9661 - val_loss: 0.0681 - val_acc: 1.0000\n",
            "Epoch 89/100\n",
            "118/118 [==============================] - 0s 180us/step - loss: 0.1381 - acc: 0.9661 - val_loss: 0.0669 - val_acc: 1.0000\n",
            "Epoch 90/100\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.1366 - acc: 0.9661 - val_loss: 0.0659 - val_acc: 1.0000\n",
            "Epoch 91/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1351 - acc: 0.9661 - val_loss: 0.0649 - val_acc: 1.0000\n",
            "Epoch 92/100\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.1337 - acc: 0.9661 - val_loss: 0.0639 - val_acc: 1.0000\n",
            "Epoch 93/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1323 - acc: 0.9661 - val_loss: 0.0629 - val_acc: 1.0000\n",
            "Epoch 94/100\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.1309 - acc: 0.9661 - val_loss: 0.0619 - val_acc: 1.0000\n",
            "Epoch 95/100\n",
            "118/118 [==============================] - 0s 181us/step - loss: 0.1295 - acc: 0.9661 - val_loss: 0.0610 - val_acc: 1.0000\n",
            "Epoch 96/100\n",
            "118/118 [==============================] - 0s 218us/step - loss: 0.1283 - acc: 0.9661 - val_loss: 0.0601 - val_acc: 1.0000\n",
            "Epoch 97/100\n",
            "118/118 [==============================] - 0s 177us/step - loss: 0.1270 - acc: 0.9661 - val_loss: 0.0593 - val_acc: 1.0000\n",
            "Epoch 98/100\n",
            "118/118 [==============================] - 0s 195us/step - loss: 0.1257 - acc: 0.9661 - val_loss: 0.0584 - val_acc: 1.0000\n",
            "Epoch 99/100\n",
            "118/118 [==============================] - 0s 173us/step - loss: 0.1245 - acc: 0.9661 - val_loss: 0.0576 - val_acc: 1.0000\n",
            "Epoch 100/100\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.1233 - acc: 0.9661 - val_loss: 0.0568 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "64d8781f-046a-4ab3-cd66-888335f0881e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.46815712,  2.41613978],\n",
              "       [-2.04213808, -1.20055142],\n",
              "       [-2.26598031, -2.51797746],\n",
              "       [-1.62561816, -1.97201226],\n",
              "       [-2.12949607, -1.76064808],\n",
              "       [-1.32853889, -2.05415965],\n",
              "       [-1.86366758, -1.96105974],\n",
              "       [ 3.1426414 ,  0.23838493],\n",
              "       [ 2.76856724,  0.87507987],\n",
              "       [ 3.33006485,  0.80105572],\n",
              "       [ 3.99255757,  0.1366972 ],\n",
              "       [ 2.79798337,  1.20174003],\n",
              "       [ 1.67824072,  0.59967329]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "29d9126e-5229-467e-f67e-17806eb67cf7",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "703f9c61-ee58-49dd-8b35-70c8b0070037",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "32fa3a13-9978-4435-c21a-59e8592f254e",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ea135164-7cd3-4b45-e4d0-2a7fd95ec618",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f07124da278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyFkiMMaYIPf/AdN8xiHSIjKSAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "846f03b5-a8fd-4c42-fda8-344b215c6f00",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f071244b160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBMYYE+UsERhjTJT7f1q9t7x2ZTAJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "018e96f7-f223-4a1a-f451-74f650f91525",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 6ms/step - loss: 1.5088 - acc: 0.0229 \n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.4160 - acc: 0.0382\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 214us/step - loss: 1.3354 - acc: 0.0534\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.2638 - acc: 0.0687\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.2011 - acc: 0.0992\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.1457 - acc: 0.1145\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.0989 - acc: 0.1527\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0564 - acc: 0.2061\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0183 - acc: 0.2137\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9851 - acc: 0.2519\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9544 - acc: 0.2901\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9269 - acc: 0.3282\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9020 - acc: 0.3817\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8802 - acc: 0.3817\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8611 - acc: 0.3969\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8446 - acc: 0.4427\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8310 - acc: 0.4962\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8195 - acc: 0.5191\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8088 - acc: 0.5573\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.7984 - acc: 0.5725\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.7887 - acc: 0.6031\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.7791 - acc: 0.6336\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.7700 - acc: 0.6641\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7615 - acc: 0.7099\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.7532 - acc: 0.7405\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.7446 - acc: 0.7405\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.7367 - acc: 0.7557\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.7289 - acc: 0.7634\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.7215 - acc: 0.7710\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.7142 - acc: 0.7939\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.7071 - acc: 0.8092\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.7001 - acc: 0.8244\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.6932 - acc: 0.8321\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.6864 - acc: 0.8626\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.6797 - acc: 0.8855\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6731 - acc: 0.8855\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.6667 - acc: 0.8855\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.6603 - acc: 0.8855\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.6541 - acc: 0.8855\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.6480 - acc: 0.8931\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6419 - acc: 0.8931\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.6359 - acc: 0.9008\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6300 - acc: 0.9160\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.6241 - acc: 0.9160\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6184 - acc: 0.9160\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.6125 - acc: 0.9237\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6070 - acc: 0.9313\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.6014 - acc: 0.9389\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.5960 - acc: 0.9389\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.5907 - acc: 0.9389\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.5854 - acc: 0.9389\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.5802 - acc: 0.9389\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.5749 - acc: 0.9466\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.5699 - acc: 0.9542\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.5649 - acc: 0.9542\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5599 - acc: 0.9542\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.5550 - acc: 0.9542\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.5503 - acc: 0.9542\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.5455 - acc: 0.9618\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.5408 - acc: 0.9618\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.5361 - acc: 0.9618\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.5315 - acc: 0.9618\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5270 - acc: 0.9618\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.5225 - acc: 0.9618\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5181 - acc: 0.9618\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.5137 - acc: 0.9618\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5094 - acc: 0.9695\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.5053 - acc: 0.9695\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.5011 - acc: 0.9695\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.4970 - acc: 0.9695\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.4930 - acc: 0.9695\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.4890 - acc: 0.9695\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.4851 - acc: 0.9695\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.4810 - acc: 0.9695\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.4772 - acc: 0.9771\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 230us/step - loss: 0.4733 - acc: 0.9771\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.4694 - acc: 0.9771\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4657 - acc: 0.9771\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.4620 - acc: 0.9771\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.4584 - acc: 0.9771\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.4548 - acc: 0.9771\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.4513 - acc: 0.9771\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4478 - acc: 0.9771\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.4443 - acc: 0.9771\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4409 - acc: 0.9771\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.4375 - acc: 0.9771\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.4342 - acc: 0.9847\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.4308 - acc: 0.9847\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.4276 - acc: 0.9847\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.4244 - acc: 0.9847\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4213 - acc: 0.9847\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4181 - acc: 0.9847\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4150 - acc: 0.9847\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4120 - acc: 0.9847\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4090 - acc: 0.9847\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4060 - acc: 0.9847\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4029 - acc: 0.9847\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4000 - acc: 0.9847\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3971 - acc: 0.9847\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3942 - acc: 0.9847\n",
            "34/34 [==============================] - 0s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3d085ffe-f4f6-411b-e25d-4648265ffe9a",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "af9b0fd2-171b-4dd7-ffa7-bf7cebe2316e",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23529411764705882"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iiJh2sP6IiF",
        "colab_type": "text"
      },
      "source": [
        "#Classification without PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(107,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLKjM41Q7mmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0425b4de-75f1-48ca-e496-33c2ebcaafea"
      },
      "source": [
        "train_data_stand.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOdipRlO6pWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8d762ba7-0be6-43b6-92b3-aa9f24d10dfa"
      },
      "source": [
        "model = build_model()\n",
        "model.fit(train_data_stand, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        "test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "131/131 [==============================] - 1s 8ms/step - loss: 1.2783 - acc: 0.3893\n",
            "Epoch 2/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.0939 - acc: 0.4275\n",
            "Epoch 3/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.9960 - acc: 0.5191\n",
            "Epoch 4/100\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9502 - acc: 0.5267\n",
            "Epoch 5/100\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9272 - acc: 0.5573\n",
            "Epoch 6/100\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8870 - acc: 0.6183\n",
            "Epoch 7/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8700 - acc: 0.5802\n",
            "Epoch 8/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8411 - acc: 0.6183\n",
            "Epoch 9/100\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.8361 - acc: 0.5954\n",
            "Epoch 10/100\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.8160 - acc: 0.6336\n",
            "Epoch 11/100\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.8014 - acc: 0.6336\n",
            "Epoch 12/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.7842 - acc: 0.6260\n",
            "Epoch 13/100\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.7826 - acc: 0.6183\n",
            "Epoch 14/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.7663 - acc: 0.6336\n",
            "Epoch 15/100\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.7529 - acc: 0.6412\n",
            "Epoch 16/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.7447 - acc: 0.6565\n",
            "Epoch 17/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.7344 - acc: 0.6794\n",
            "Epoch 18/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.7279 - acc: 0.6641\n",
            "Epoch 19/100\n",
            "131/131 [==============================] - 0s 221us/step - loss: 0.7122 - acc: 0.6794\n",
            "Epoch 20/100\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.7052 - acc: 0.6718\n",
            "Epoch 21/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.6877 - acc: 0.6794\n",
            "Epoch 22/100\n",
            "131/131 [==============================] - 0s 238us/step - loss: 0.6981 - acc: 0.6947\n",
            "Epoch 23/100\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.6847 - acc: 0.6718\n",
            "Epoch 24/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6755 - acc: 0.7023\n",
            "Epoch 25/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.6639 - acc: 0.7099\n",
            "Epoch 26/100\n",
            "131/131 [==============================] - 0s 261us/step - loss: 0.6484 - acc: 0.7099\n",
            "Epoch 27/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.6372 - acc: 0.7176\n",
            "Epoch 28/100\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.6279 - acc: 0.7099\n",
            "Epoch 29/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.6210 - acc: 0.7328\n",
            "Epoch 30/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.6093 - acc: 0.7252\n",
            "Epoch 31/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.6036 - acc: 0.7099\n",
            "Epoch 32/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.6050 - acc: 0.7252\n",
            "Epoch 33/100\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.5974 - acc: 0.7099\n",
            "Epoch 34/100\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5780 - acc: 0.7405\n",
            "Epoch 35/100\n",
            "131/131 [==============================] - 0s 217us/step - loss: 0.5734 - acc: 0.7252\n",
            "Epoch 36/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.5707 - acc: 0.7328\n",
            "Epoch 37/100\n",
            "131/131 [==============================] - 0s 226us/step - loss: 0.5616 - acc: 0.7557\n",
            "Epoch 38/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.5576 - acc: 0.7481\n",
            "Epoch 39/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.5457 - acc: 0.7481\n",
            "Epoch 40/100\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.5418 - acc: 0.7557\n",
            "Epoch 41/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.5427 - acc: 0.7481\n",
            "Epoch 42/100\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.5313 - acc: 0.7481\n",
            "Epoch 43/100\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.5203 - acc: 0.7557\n",
            "Epoch 44/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.5233 - acc: 0.7786\n",
            "Epoch 45/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.5227 - acc: 0.7328\n",
            "Epoch 46/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.5000 - acc: 0.7710\n",
            "Epoch 47/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.5028 - acc: 0.7710\n",
            "Epoch 48/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.5000 - acc: 0.7786\n",
            "Epoch 49/100\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.4884 - acc: 0.7710\n",
            "Epoch 50/100\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.4829 - acc: 0.7786\n",
            "Epoch 51/100\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.4794 - acc: 0.7939\n",
            "Epoch 52/100\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4659 - acc: 0.7939\n",
            "Epoch 53/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.4749 - acc: 0.7786\n",
            "Epoch 54/100\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.4600 - acc: 0.8321\n",
            "Epoch 55/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.4530 - acc: 0.8015\n",
            "Epoch 56/100\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.4381 - acc: 0.8321\n",
            "Epoch 57/100\n",
            "131/131 [==============================] - 0s 208us/step - loss: 0.4409 - acc: 0.8473\n",
            "Epoch 58/100\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4403 - acc: 0.8168\n",
            "Epoch 59/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.4314 - acc: 0.8244\n",
            "Epoch 60/100\n",
            "131/131 [==============================] - 0s 280us/step - loss: 0.4363 - acc: 0.8321\n",
            "Epoch 61/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.4180 - acc: 0.8321\n",
            "Epoch 62/100\n",
            "131/131 [==============================] - 0s 221us/step - loss: 0.4188 - acc: 0.8473\n",
            "Epoch 63/100\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.4124 - acc: 0.8321\n",
            "Epoch 64/100\n",
            "131/131 [==============================] - 0s 218us/step - loss: 0.4119 - acc: 0.8321\n",
            "Epoch 65/100\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.4068 - acc: 0.8550\n",
            "Epoch 66/100\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.4097 - acc: 0.8550\n",
            "Epoch 67/100\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3900 - acc: 0.8550\n",
            "Epoch 68/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.3864 - acc: 0.8550\n",
            "Epoch 69/100\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.3866 - acc: 0.8397\n",
            "Epoch 70/100\n",
            "131/131 [==============================] - 0s 222us/step - loss: 0.3709 - acc: 0.8779\n",
            "Epoch 71/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.3838 - acc: 0.8626\n",
            "Epoch 72/100\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.3813 - acc: 0.8550\n",
            "Epoch 73/100\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.3697 - acc: 0.8779\n",
            "Epoch 74/100\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.3595 - acc: 0.8855\n",
            "Epoch 75/100\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3629 - acc: 0.8626\n",
            "Epoch 76/100\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3579 - acc: 0.8779\n",
            "Epoch 77/100\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.3470 - acc: 0.8855\n",
            "Epoch 78/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.3613 - acc: 0.8626\n",
            "Epoch 79/100\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.3407 - acc: 0.9084\n",
            "Epoch 80/100\n",
            "131/131 [==============================] - 0s 222us/step - loss: 0.3363 - acc: 0.8855\n",
            "Epoch 81/100\n",
            "131/131 [==============================] - 0s 220us/step - loss: 0.3345 - acc: 0.8931\n",
            "Epoch 82/100\n",
            "131/131 [==============================] - 0s 229us/step - loss: 0.3350 - acc: 0.8702\n",
            "Epoch 83/100\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.3209 - acc: 0.9008\n",
            "Epoch 84/100\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.3280 - acc: 0.8855\n",
            "Epoch 85/100\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.3315 - acc: 0.9084\n",
            "Epoch 86/100\n",
            "131/131 [==============================] - 0s 237us/step - loss: 0.3198 - acc: 0.9008\n",
            "Epoch 87/100\n",
            "131/131 [==============================] - 0s 239us/step - loss: 0.3162 - acc: 0.8855\n",
            "Epoch 88/100\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.3093 - acc: 0.8931\n",
            "Epoch 89/100\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.3088 - acc: 0.8855\n",
            "Epoch 90/100\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.3029 - acc: 0.9084\n",
            "Epoch 91/100\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.3013 - acc: 0.9008\n",
            "Epoch 92/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.3039 - acc: 0.9084\n",
            "Epoch 93/100\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.2910 - acc: 0.9160\n",
            "Epoch 94/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.2837 - acc: 0.9237\n",
            "Epoch 95/100\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.2890 - acc: 0.9084\n",
            "Epoch 96/100\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.2891 - acc: 0.9160\n",
            "Epoch 97/100\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.2762 - acc: 0.9237\n",
            "Epoch 98/100\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.2699 - acc: 0.9313\n",
            "Epoch 99/100\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.2822 - acc: 0.9084\n",
            "Epoch 100/100\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.2698 - acc: 0.9313\n",
            "34/34 [==============================] - 0s 11ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "310e70e3-5a12-44a4-8e45-0db31732cedd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_53 (Dense)             (None, 10)                1080      \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 1,113\n",
            "Trainable params: 1,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jKN9cYM8Pip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "3084ab9f-8257-4dbc-c5b7-cb593d2e4e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "ec621f6e-2fb0-4877-ae95-654f4c8aac18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "438b2883-db69-43c7-d678-2db3b1a5cc81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "d6a20d18-b239-4445-e52a-a5fe9c5e17db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(6, activation='relu', input_shape=(9,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.4)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "e9fa629c-28de-4928-c475-6f7a0cc1daeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "f0b554f6-e648-44c1-f563-6ba8e15f3d16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   4   5   6   7   8   9  10  11  12  14  15  16  17  18  19\n",
            "  20  22  23  24  25  27  28  29  30  31  32  33  34  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  69  70  71  72  74  75  76  77  78  79  80\n",
            "  81  83  84  85  86  87  88  89  90  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 107 108 109 110 111 113 114 115 116 117 119 120 121\n",
            " 122 123 124 125 126 127 128 129] TEST: [  3  13  21  26  35  50  54  68  73  82  91 106 112 118 130]\n",
            "TRAIN: [  0   1   3   4   5   6   8   9  10  11  12  13  14  15  16  17  19  20\n",
            "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  44  45  46  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  71  72  73  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98\n",
            " 100 101 102 103 104 105 106 107 109 110 111 112 113 115 116 117 118 120\n",
            " 121 122 124 125 126 127 128 129 130] TEST: [  2   7  18  43  47  61  70  74  93  99 108 114 119 123]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  45  46  47  48  50  51  52  53  54  56  57  58\n",
            "  59  60  61  62  63  65  66  67  68  69  70  72  73  74  76  77  78  79\n",
            "  80  81  82  83  84  85  87  88  89  90  91  92  93  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 117 118 119\n",
            " 120 121 122 123 124 125 127 128 130] TEST: [  6  10  31  49  55  64  71  75  86  94  95 116 126 129]\n",
            "TRAIN: [  0   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  29  30  31  33  34  35  36  37  38  39\n",
            "  40  41  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  62  63  64  65  66  68  70  71  73  74  75  76  77  78  79\n",
            "  80  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 101 102 105 106 107 108 109 111 112 113 114 115 116 117 118 119\n",
            " 120 122 123 124 125 126 127 129 130] TEST: [  1   4  28  32  42  67  69  72  81 103 104 110 121 128]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  15  16  17  18  20\n",
            "  21  23  24  25  26  27  28  29  30  31  32  33  34  35  36  38  39  40\n",
            "  42  43  44  45  46  47  48  49  50  51  53  54  55  57  58  59  60  61\n",
            "  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  80\n",
            "  81  82  83  84  85  86  87  90  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  0  14  19  22  37  41  52  56  79  88  89 107 120]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  16  17  18  19\n",
            "  20  21  22  23  25  26  27  28  29  30  31  32  34  35  36  37  38  39\n",
            "  41  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  60\n",
            "  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  84  85  86  87  88  89  90  91  92  93  94  95  96  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 114 115 116 118 119 120\n",
            " 121 122 123 124 125 126 127 128 129 130] TEST: [  9  15  24  33  40  51  59  62  83  97  98 113 117]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   9  10  11  12  13  14  15  18  19  20\n",
            "  21  22  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  61  62  64  65  66  67  68  69  70  71  72  73  74  75  76  79  81\n",
            "  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 123 124 125 126 127 128 129 130] TEST: [  8  16  17  23  25  60  63  77  78  80  90 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  30  31  32  33  34  35  36  37\n",
            "  39  40  41  42  43  44  45  47  49  50  51  52  53  54  55  56  58  59\n",
            "  60  61  62  63  64  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
            "  98  99 100 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117\n",
            " 118 119 120 121 122 123 124 126 128 129 130] TEST: [ 12  29  38  46  48  57  65  84 101 105 125 127]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
            "  19  21  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
            "  40  41  42  43  46  47  48  49  50  51  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  88  89  90  91  93  94  95  96  97  98\n",
            "  99 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120 121 122 123 125 126 127 128 129 130] TEST: [ 11  20  30  39  44  45  66  87  92 100 102 124]\n",
            "TRAIN: [  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  28  29  30  31  32  33  35  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  50  51  52  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  77  78\n",
            "  79  80  81  82  83  84  86  87  88  89  90  91  92  93  94  95  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 110 112 113 114 116 117 118 119\n",
            " 120 121 122 123 124 125 126 127 128 129 130] TEST: [  5  27  34  36  53  58  76  85  96 109 111 115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "83180c76-b647-4874-dfe4-18b4bb155a8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "e839a1e1-98fa-4bfd-a179-0340925e588c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 20\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 116 samples, validate on 15 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "116/116 [==============================] - 1s 5ms/step - loss: 1.7908 - acc: 0.4483 - val_loss: 1.6421 - val_acc: 0.4667\n",
            "Epoch 2/20\n",
            "116/116 [==============================] - 0s 192us/step - loss: 1.3637 - acc: 0.4569 - val_loss: 1.2679 - val_acc: 0.5333\n",
            "Epoch 3/20\n",
            "116/116 [==============================] - 0s 192us/step - loss: 1.1271 - acc: 0.5259 - val_loss: 1.1386 - val_acc: 0.5333\n",
            "Epoch 4/20\n",
            "116/116 [==============================] - 0s 186us/step - loss: 1.0340 - acc: 0.5603 - val_loss: 1.0479 - val_acc: 0.5333\n",
            "Epoch 5/20\n",
            "116/116 [==============================] - 0s 175us/step - loss: 0.9838 - acc: 0.5776 - val_loss: 1.0553 - val_acc: 0.4667\n",
            "Epoch 6/20\n",
            "116/116 [==============================] - 0s 204us/step - loss: 0.9536 - acc: 0.5603 - val_loss: 1.0352 - val_acc: 0.4667\n",
            "Epoch 7/20\n",
            "116/116 [==============================] - 0s 159us/step - loss: 0.9309 - acc: 0.5948 - val_loss: 1.0125 - val_acc: 0.4667\n",
            "Epoch 8/20\n",
            "116/116 [==============================] - 0s 201us/step - loss: 0.9156 - acc: 0.5603 - val_loss: 0.9298 - val_acc: 0.5333\n",
            "Epoch 9/20\n",
            "116/116 [==============================] - 0s 194us/step - loss: 0.8954 - acc: 0.5862 - val_loss: 0.9361 - val_acc: 0.4667\n",
            "Epoch 10/20\n",
            "116/116 [==============================] - 0s 181us/step - loss: 0.8952 - acc: 0.5690 - val_loss: 0.8782 - val_acc: 0.5333\n",
            "Epoch 11/20\n",
            "116/116 [==============================] - 0s 202us/step - loss: 0.8840 - acc: 0.6034 - val_loss: 0.9345 - val_acc: 0.5333\n",
            "Epoch 12/20\n",
            "116/116 [==============================] - 0s 182us/step - loss: 0.8865 - acc: 0.6034 - val_loss: 0.9362 - val_acc: 0.4667\n",
            "Epoch 13/20\n",
            "116/116 [==============================] - 0s 171us/step - loss: 0.8796 - acc: 0.6034 - val_loss: 0.9016 - val_acc: 0.5333\n",
            "Epoch 14/20\n",
            "116/116 [==============================] - 0s 166us/step - loss: 0.8736 - acc: 0.6034 - val_loss: 0.8960 - val_acc: 0.5333\n",
            "Epoch 15/20\n",
            "116/116 [==============================] - 0s 155us/step - loss: 0.8659 - acc: 0.5862 - val_loss: 0.8904 - val_acc: 0.5333\n",
            "Epoch 16/20\n",
            "116/116 [==============================] - 0s 190us/step - loss: 0.8665 - acc: 0.5776 - val_loss: 0.9122 - val_acc: 0.5333\n",
            "Epoch 17/20\n",
            "116/116 [==============================] - 0s 167us/step - loss: 0.8529 - acc: 0.6121 - val_loss: 0.9338 - val_acc: 0.4667\n",
            "Epoch 18/20\n",
            "116/116 [==============================] - 0s 218us/step - loss: 0.8494 - acc: 0.5862 - val_loss: 0.9445 - val_acc: 0.4667\n",
            "Epoch 19/20\n",
            "116/116 [==============================] - 0s 199us/step - loss: 0.8506 - acc: 0.5948 - val_loss: 0.8810 - val_acc: 0.5333\n",
            "Epoch 20/20\n",
            "116/116 [==============================] - 0s 175us/step - loss: 0.8405 - acc: 0.5776 - val_loss: 0.9315 - val_acc: 0.5333\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 0s 1ms/step - loss: 1.9342 - acc: 0.3248 - val_loss: 1.3455 - val_acc: 0.4286\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 0s 185us/step - loss: 1.3152 - acc: 0.3419 - val_loss: 1.1710 - val_acc: 0.3571\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.1774 - acc: 0.4274 - val_loss: 1.0896 - val_acc: 0.4286\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 0s 186us/step - loss: 1.1181 - acc: 0.4701 - val_loss: 1.0609 - val_acc: 0.4286\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 0s 206us/step - loss: 1.0822 - acc: 0.5299 - val_loss: 1.0562 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 0s 168us/step - loss: 1.0582 - acc: 0.4957 - val_loss: 1.0480 - val_acc: 0.4286\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 0s 151us/step - loss: 1.0348 - acc: 0.5299 - val_loss: 1.0454 - val_acc: 0.4286\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 0s 183us/step - loss: 1.0259 - acc: 0.5470 - val_loss: 1.0475 - val_acc: 0.4286\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 0s 162us/step - loss: 1.0202 - acc: 0.5385 - val_loss: 1.0412 - val_acc: 0.4286\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 0s 188us/step - loss: 1.0068 - acc: 0.5470 - val_loss: 1.0407 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.9976 - acc: 0.5214 - val_loss: 1.0340 - val_acc: 0.4286\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9895 - acc: 0.5556 - val_loss: 1.0336 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9824 - acc: 0.5214 - val_loss: 1.0356 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9822 - acc: 0.5043 - val_loss: 1.0343 - val_acc: 0.4286\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9730 - acc: 0.5726 - val_loss: 1.0382 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9702 - acc: 0.5299 - val_loss: 1.0351 - val_acc: 0.5714\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9640 - acc: 0.5556 - val_loss: 1.0354 - val_acc: 0.5714\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 0s 195us/step - loss: 0.9608 - acc: 0.5385 - val_loss: 1.0325 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9521 - acc: 0.5812 - val_loss: 1.0352 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 0s 186us/step - loss: 0.9526 - acc: 0.5470 - val_loss: 1.0385 - val_acc: 0.5000\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 0s 2ms/step - loss: 2.3036 - acc: 0.4017 - val_loss: 2.0036 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 0s 188us/step - loss: 1.7144 - acc: 0.3846 - val_loss: 1.4761 - val_acc: 0.6429\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 0s 181us/step - loss: 1.2766 - acc: 0.4444 - val_loss: 1.2487 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 0s 187us/step - loss: 1.1097 - acc: 0.4872 - val_loss: 1.1736 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 0s 204us/step - loss: 1.0338 - acc: 0.5299 - val_loss: 1.1611 - val_acc: 0.6429\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 0s 193us/step - loss: 1.0043 - acc: 0.5128 - val_loss: 1.1065 - val_acc: 0.6429\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 0s 172us/step - loss: 0.9717 - acc: 0.5214 - val_loss: 1.0780 - val_acc: 0.6429\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.9573 - acc: 0.5470 - val_loss: 1.0705 - val_acc: 0.6429\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 0s 212us/step - loss: 0.9391 - acc: 0.5556 - val_loss: 1.0423 - val_acc: 0.5714\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 0s 162us/step - loss: 0.9358 - acc: 0.5128 - val_loss: 1.0501 - val_acc: 0.5714\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 0s 157us/step - loss: 0.9198 - acc: 0.5385 - val_loss: 1.0576 - val_acc: 0.5714\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 0s 156us/step - loss: 0.9056 - acc: 0.5641 - val_loss: 1.0471 - val_acc: 0.6429\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 0s 163us/step - loss: 0.9011 - acc: 0.5385 - val_loss: 1.0702 - val_acc: 0.5714\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 0s 196us/step - loss: 0.8945 - acc: 0.5641 - val_loss: 1.0590 - val_acc: 0.5714\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.8793 - acc: 0.5556 - val_loss: 1.0838 - val_acc: 0.5714\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 0s 152us/step - loss: 0.8724 - acc: 0.5641 - val_loss: 1.0553 - val_acc: 0.5714\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 0s 151us/step - loss: 0.8707 - acc: 0.5726 - val_loss: 1.0644 - val_acc: 0.5714\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 0s 150us/step - loss: 0.8565 - acc: 0.5470 - val_loss: 1.0850 - val_acc: 0.5714\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 0s 148us/step - loss: 0.8695 - acc: 0.5812 - val_loss: 1.0727 - val_acc: 0.5714\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 0s 166us/step - loss: 0.8465 - acc: 0.5812 - val_loss: 1.0743 - val_acc: 0.5714\n",
            "Train on 117 samples, validate on 14 samples\n",
            "Epoch 1/20\n",
            "117/117 [==============================] - 0s 2ms/step - loss: 1.7922 - acc: 0.4530 - val_loss: 1.3095 - val_acc: 0.4286\n",
            "Epoch 2/20\n",
            "117/117 [==============================] - 0s 199us/step - loss: 1.1842 - acc: 0.4359 - val_loss: 1.1590 - val_acc: 0.4286\n",
            "Epoch 3/20\n",
            "117/117 [==============================] - 0s 189us/step - loss: 1.0801 - acc: 0.4188 - val_loss: 1.1392 - val_acc: 0.4286\n",
            "Epoch 4/20\n",
            "117/117 [==============================] - 0s 162us/step - loss: 1.0366 - acc: 0.5128 - val_loss: 1.1226 - val_acc: 0.4286\n",
            "Epoch 5/20\n",
            "117/117 [==============================] - 0s 173us/step - loss: 0.9917 - acc: 0.4957 - val_loss: 1.1065 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.9795 - acc: 0.5470 - val_loss: 1.0827 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "117/117 [==============================] - 0s 155us/step - loss: 0.9655 - acc: 0.5214 - val_loss: 1.0831 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "117/117 [==============================] - 0s 160us/step - loss: 0.9567 - acc: 0.5385 - val_loss: 1.0641 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9492 - acc: 0.5556 - val_loss: 1.0518 - val_acc: 0.5714\n",
            "Epoch 10/20\n",
            "117/117 [==============================] - 0s 177us/step - loss: 0.9352 - acc: 0.5641 - val_loss: 1.0519 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9275 - acc: 0.5726 - val_loss: 1.0390 - val_acc: 0.5714\n",
            "Epoch 12/20\n",
            "117/117 [==============================] - 0s 165us/step - loss: 0.9273 - acc: 0.5556 - val_loss: 1.0301 - val_acc: 0.5714\n",
            "Epoch 13/20\n",
            "117/117 [==============================] - 0s 170us/step - loss: 0.9256 - acc: 0.5470 - val_loss: 1.0312 - val_acc: 0.5714\n",
            "Epoch 14/20\n",
            "117/117 [==============================] - 0s 158us/step - loss: 0.9212 - acc: 0.5556 - val_loss: 1.0240 - val_acc: 0.5714\n",
            "Epoch 15/20\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9225 - acc: 0.5556 - val_loss: 1.0221 - val_acc: 0.5714\n",
            "Epoch 16/20\n",
            "117/117 [==============================] - 0s 154us/step - loss: 0.9171 - acc: 0.5812 - val_loss: 1.0146 - val_acc: 0.5714\n",
            "Epoch 17/20\n",
            "117/117 [==============================] - 0s 164us/step - loss: 0.9061 - acc: 0.5812 - val_loss: 1.0124 - val_acc: 0.5714\n",
            "Epoch 18/20\n",
            "117/117 [==============================] - 0s 178us/step - loss: 0.9069 - acc: 0.5556 - val_loss: 1.0036 - val_acc: 0.5714\n",
            "Epoch 19/20\n",
            "117/117 [==============================] - 0s 180us/step - loss: 0.9035 - acc: 0.5214 - val_loss: 1.0106 - val_acc: 0.5714\n",
            "Epoch 20/20\n",
            "117/117 [==============================] - 0s 168us/step - loss: 0.8939 - acc: 0.5726 - val_loss: 0.9983 - val_acc: 0.6429\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.7396 - acc: 0.3390 - val_loss: 1.9478 - val_acc: 0.4615\n",
            "Epoch 2/20\n",
            "118/118 [==============================] - 0s 174us/step - loss: 1.1560 - acc: 0.4407 - val_loss: 2.0392 - val_acc: 0.3077\n",
            "Epoch 3/20\n",
            "118/118 [==============================] - 0s 179us/step - loss: 1.0236 - acc: 0.4915 - val_loss: 2.0601 - val_acc: 0.3077\n",
            "Epoch 4/20\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.9890 - acc: 0.5254 - val_loss: 1.9861 - val_acc: 0.3077\n",
            "Epoch 5/20\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.9467 - acc: 0.5424 - val_loss: 1.9639 - val_acc: 0.3077\n",
            "Epoch 6/20\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9266 - acc: 0.5678 - val_loss: 1.8958 - val_acc: 0.3846\n",
            "Epoch 7/20\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9161 - acc: 0.5593 - val_loss: 1.8482 - val_acc: 0.3846\n",
            "Epoch 8/20\n",
            "118/118 [==============================] - 0s 175us/step - loss: 0.9021 - acc: 0.5763 - val_loss: 1.8317 - val_acc: 0.3846\n",
            "Epoch 9/20\n",
            "118/118 [==============================] - 0s 158us/step - loss: 0.8911 - acc: 0.5847 - val_loss: 1.7960 - val_acc: 0.3846\n",
            "Epoch 10/20\n",
            "118/118 [==============================] - 0s 157us/step - loss: 0.8820 - acc: 0.5932 - val_loss: 1.7906 - val_acc: 0.3846\n",
            "Epoch 11/20\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.8762 - acc: 0.6017 - val_loss: 1.7852 - val_acc: 0.3846\n",
            "Epoch 12/20\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.8666 - acc: 0.5593 - val_loss: 1.7065 - val_acc: 0.3846\n",
            "Epoch 13/20\n",
            "118/118 [==============================] - 0s 185us/step - loss: 0.8670 - acc: 0.6186 - val_loss: 1.7071 - val_acc: 0.4615\n",
            "Epoch 14/20\n",
            "118/118 [==============================] - 0s 188us/step - loss: 0.8574 - acc: 0.6186 - val_loss: 1.6720 - val_acc: 0.4615\n",
            "Epoch 15/20\n",
            "118/118 [==============================] - 0s 172us/step - loss: 0.8514 - acc: 0.5678 - val_loss: 1.6026 - val_acc: 0.3846\n",
            "Epoch 16/20\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.8491 - acc: 0.6271 - val_loss: 1.6184 - val_acc: 0.4615\n",
            "Epoch 17/20\n",
            "118/118 [==============================] - 0s 161us/step - loss: 0.8424 - acc: 0.5932 - val_loss: 1.6304 - val_acc: 0.4615\n",
            "Epoch 18/20\n",
            "118/118 [==============================] - 0s 154us/step - loss: 0.8453 - acc: 0.5847 - val_loss: 1.6063 - val_acc: 0.4615\n",
            "Epoch 19/20\n",
            "118/118 [==============================] - 0s 166us/step - loss: 0.8374 - acc: 0.6102 - val_loss: 1.5775 - val_acc: 0.4615\n",
            "Epoch 20/20\n",
            "118/118 [==============================] - 0s 165us/step - loss: 0.8290 - acc: 0.6186 - val_loss: 1.5540 - val_acc: 0.4615\n",
            "Train on 118 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "118/118 [==============================] - 0s 2ms/step - loss: 1.6406 - acc: 0.2373 - val_loss: 1.4135 - val_acc: 0.3846\n",
            "Epoch 2/20\n",
            "118/118 [==============================] - 0s 161us/step - loss: 1.2795 - acc: 0.2797 - val_loss: 1.2427 - val_acc: 0.2308\n",
            "Epoch 3/20\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.1679 - acc: 0.3644 - val_loss: 1.2218 - val_acc: 0.0769\n",
            "Epoch 4/20\n",
            "118/118 [==============================] - 0s 159us/step - loss: 1.1169 - acc: 0.4068 - val_loss: 1.2078 - val_acc: 0.0769\n",
            "Epoch 5/20\n",
            "118/118 [==============================] - 0s 188us/step - loss: 1.0814 - acc: 0.4237 - val_loss: 1.1749 - val_acc: 0.1538\n",
            "Epoch 6/20\n",
            "118/118 [==============================] - 0s 234us/step - loss: 1.0524 - acc: 0.4576 - val_loss: 1.1602 - val_acc: 0.2308\n",
            "Epoch 7/20\n",
            "118/118 [==============================] - 0s 157us/step - loss: 1.0238 - acc: 0.4746 - val_loss: 1.1579 - val_acc: 0.3846\n",
            "Epoch 8/20\n",
            "118/118 [==============================] - 0s 158us/step - loss: 1.0025 - acc: 0.5085 - val_loss: 1.1735 - val_acc: 0.3846\n",
            "Epoch 9/20\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9880 - acc: 0.5508 - val_loss: 1.2497 - val_acc: 0.3846\n",
            "Epoch 10/20\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9732 - acc: 0.5254 - val_loss: 1.2412 - val_acc: 0.4615\n",
            "Epoch 11/20\n",
            "118/118 [==============================] - 0s 162us/step - loss: 0.9552 - acc: 0.5339 - val_loss: 1.2421 - val_acc: 0.4615\n",
            "Epoch 12/20\n",
            "118/118 [==============================] - 0s 156us/step - loss: 0.9500 - acc: 0.5508 - val_loss: 1.3091 - val_acc: 0.3077\n",
            "Epoch 13/20\n",
            "118/118 [==============================] - 0s 191us/step - loss: 0.9431 - acc: 0.5678 - val_loss: 1.3030 - val_acc: 0.3846\n",
            "Epoch 14/20\n",
            "118/118 [==============================] - 0s 203us/step - loss: 0.9350 - acc: 0.5424 - val_loss: 1.3348 - val_acc: 0.3846\n",
            "Epoch 15/20\n",
            "118/118 [==============================] - 0s 202us/step - loss: 0.9314 - acc: 0.5424 - val_loss: 1.3974 - val_acc: 0.3077\n",
            "Epoch 16/20\n",
            "118/118 [==============================] - 0s 171us/step - loss: 0.9206 - acc: 0.5593 - val_loss: 1.4474 - val_acc: 0.2308\n",
            "Epoch 17/20\n",
            "118/118 [==============================] - 0s 159us/step - loss: 0.9076 - acc: 0.5763 - val_loss: 1.4169 - val_acc: 0.3846\n",
            "Epoch 18/20\n",
            "118/118 [==============================] - 0s 160us/step - loss: 0.9192 - acc: 0.5678 - val_loss: 1.4815 - val_acc: 0.3077\n",
            "Epoch 19/20\n",
            "118/118 [==============================] - 0s 163us/step - loss: 0.9015 - acc: 0.5932 - val_loss: 1.4954 - val_acc: 0.3077\n",
            "Epoch 20/20\n",
            "118/118 [==============================] - 0s 164us/step - loss: 0.9051 - acc: 0.5763 - val_loss: 1.5300 - val_acc: 0.3077\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 1.7313 - acc: 0.4370 - val_loss: 0.9522 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 1.4318 - acc: 0.4202 - val_loss: 0.8960 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 180us/step - loss: 1.3067 - acc: 0.5042 - val_loss: 0.8746 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 160us/step - loss: 1.1849 - acc: 0.4958 - val_loss: 0.8593 - val_acc: 0.5833\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 165us/step - loss: 1.1279 - acc: 0.5378 - val_loss: 0.8623 - val_acc: 0.5833\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 184us/step - loss: 1.0970 - acc: 0.5210 - val_loss: 0.8299 - val_acc: 0.6667\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 153us/step - loss: 1.0449 - acc: 0.5294 - val_loss: 0.7889 - val_acc: 0.6667\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 162us/step - loss: 1.0357 - acc: 0.5210 - val_loss: 0.7774 - val_acc: 0.6667\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 163us/step - loss: 1.0094 - acc: 0.5546 - val_loss: 0.8397 - val_acc: 0.6667\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 182us/step - loss: 1.0001 - acc: 0.5966 - val_loss: 0.8368 - val_acc: 0.6667\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.9919 - acc: 0.5798 - val_loss: 0.8637 - val_acc: 0.6667\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 161us/step - loss: 0.9824 - acc: 0.5798 - val_loss: 0.8823 - val_acc: 0.6667\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 186us/step - loss: 0.9589 - acc: 0.5630 - val_loss: 0.8520 - val_acc: 0.7500\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 164us/step - loss: 0.9627 - acc: 0.5798 - val_loss: 0.8522 - val_acc: 0.7500\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 195us/step - loss: 0.9474 - acc: 0.5882 - val_loss: 0.9328 - val_acc: 0.5833\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 176us/step - loss: 0.9446 - acc: 0.5546 - val_loss: 0.8596 - val_acc: 0.7500\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 209us/step - loss: 0.9355 - acc: 0.5798 - val_loss: 0.8831 - val_acc: 0.7500\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 176us/step - loss: 0.9381 - acc: 0.5714 - val_loss: 0.8521 - val_acc: 0.7500\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 157us/step - loss: 0.9365 - acc: 0.5966 - val_loss: 0.8656 - val_acc: 0.7500\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 248us/step - loss: 0.9261 - acc: 0.5714 - val_loss: 0.8817 - val_acc: 0.7500\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 2.2006 - acc: 0.2017 - val_loss: 1.8586 - val_acc: 0.2500\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 178us/step - loss: 1.2413 - acc: 0.3782 - val_loss: 1.6255 - val_acc: 0.1667\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 176us/step - loss: 1.0768 - acc: 0.4622 - val_loss: 1.4981 - val_acc: 0.2500\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 161us/step - loss: 1.0214 - acc: 0.5378 - val_loss: 1.4165 - val_acc: 0.1667\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 180us/step - loss: 0.9719 - acc: 0.5630 - val_loss: 1.3849 - val_acc: 0.2500\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 185us/step - loss: 0.9570 - acc: 0.5546 - val_loss: 1.3337 - val_acc: 0.2500\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 165us/step - loss: 0.9453 - acc: 0.5630 - val_loss: 1.3022 - val_acc: 0.4167\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 0.9258 - acc: 0.5630 - val_loss: 1.2872 - val_acc: 0.3333\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 161us/step - loss: 0.9152 - acc: 0.5882 - val_loss: 1.2893 - val_acc: 0.4167\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 154us/step - loss: 0.9105 - acc: 0.5882 - val_loss: 1.2929 - val_acc: 0.4167\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 201us/step - loss: 0.9186 - acc: 0.5630 - val_loss: 1.2838 - val_acc: 0.4167\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 173us/step - loss: 0.9045 - acc: 0.5966 - val_loss: 1.2539 - val_acc: 0.4167\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 165us/step - loss: 0.8914 - acc: 0.6050 - val_loss: 1.2717 - val_acc: 0.4167\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 162us/step - loss: 0.8864 - acc: 0.6218 - val_loss: 1.2780 - val_acc: 0.4167\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 179us/step - loss: 0.8890 - acc: 0.5882 - val_loss: 1.2762 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 209us/step - loss: 0.8815 - acc: 0.5966 - val_loss: 1.2730 - val_acc: 0.4167\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 225us/step - loss: 0.8852 - acc: 0.6050 - val_loss: 1.3168 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 171us/step - loss: 0.8795 - acc: 0.6134 - val_loss: 1.3044 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 165us/step - loss: 0.8792 - acc: 0.6134 - val_loss: 1.3317 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 175us/step - loss: 0.8677 - acc: 0.6134 - val_loss: 1.3449 - val_acc: 0.5000\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 3ms/step - loss: 1.6903 - acc: 0.4034 - val_loss: 0.7495 - val_acc: 0.7500\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 186us/step - loss: 1.2736 - acc: 0.4874 - val_loss: 0.7900 - val_acc: 0.6667\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 186us/step - loss: 1.1773 - acc: 0.4874 - val_loss: 0.8839 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 199us/step - loss: 1.1049 - acc: 0.5126 - val_loss: 0.9100 - val_acc: 0.5833\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 194us/step - loss: 1.0415 - acc: 0.5042 - val_loss: 0.9877 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 192us/step - loss: 1.0188 - acc: 0.5546 - val_loss: 0.9235 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 0.9798 - acc: 0.5882 - val_loss: 0.9254 - val_acc: 0.4167\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 0.9504 - acc: 0.5798 - val_loss: 0.8894 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.9655 - acc: 0.5378 - val_loss: 0.9483 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.9372 - acc: 0.6050 - val_loss: 0.9958 - val_acc: 0.4167\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 170us/step - loss: 0.9240 - acc: 0.6303 - val_loss: 0.9530 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 163us/step - loss: 0.9148 - acc: 0.6471 - val_loss: 0.9393 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 245us/step - loss: 0.9005 - acc: 0.6555 - val_loss: 0.9527 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 162us/step - loss: 0.9124 - acc: 0.6471 - val_loss: 1.0045 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 163us/step - loss: 0.8868 - acc: 0.6387 - val_loss: 1.0284 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 193us/step - loss: 0.8894 - acc: 0.6471 - val_loss: 1.0145 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.8843 - acc: 0.6555 - val_loss: 0.9740 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 175us/step - loss: 0.8854 - acc: 0.6555 - val_loss: 1.0331 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 163us/step - loss: 0.8755 - acc: 0.6639 - val_loss: 1.0170 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 149us/step - loss: 0.8723 - acc: 0.6639 - val_loss: 1.0365 - val_acc: 0.5000\n",
            "Train on 119 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "119/119 [==============================] - 0s 4ms/step - loss: 1.6043 - acc: 0.3109 - val_loss: 1.7348 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "119/119 [==============================] - 0s 183us/step - loss: 1.2991 - acc: 0.4202 - val_loss: 1.6554 - val_acc: 0.5833\n",
            "Epoch 3/20\n",
            "119/119 [==============================] - 0s 163us/step - loss: 1.1643 - acc: 0.4706 - val_loss: 1.5141 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "119/119 [==============================] - 0s 166us/step - loss: 1.0663 - acc: 0.5294 - val_loss: 1.4507 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "119/119 [==============================] - 0s 147us/step - loss: 1.0148 - acc: 0.5714 - val_loss: 1.3643 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "119/119 [==============================] - 0s 190us/step - loss: 0.9866 - acc: 0.5966 - val_loss: 1.2299 - val_acc: 0.5833\n",
            "Epoch 7/20\n",
            "119/119 [==============================] - 0s 148us/step - loss: 0.9767 - acc: 0.5546 - val_loss: 1.1965 - val_acc: 0.5833\n",
            "Epoch 8/20\n",
            "119/119 [==============================] - 0s 164us/step - loss: 0.9576 - acc: 0.5714 - val_loss: 1.1516 - val_acc: 0.5833\n",
            "Epoch 9/20\n",
            "119/119 [==============================] - 0s 204us/step - loss: 0.9431 - acc: 0.5798 - val_loss: 1.1675 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "119/119 [==============================] - 0s 192us/step - loss: 0.9359 - acc: 0.5966 - val_loss: 1.1118 - val_acc: 0.5833\n",
            "Epoch 11/20\n",
            "119/119 [==============================] - 0s 177us/step - loss: 0.9380 - acc: 0.5882 - val_loss: 1.1057 - val_acc: 0.5833\n",
            "Epoch 12/20\n",
            "119/119 [==============================] - 0s 165us/step - loss: 0.9287 - acc: 0.6134 - val_loss: 1.1260 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "119/119 [==============================] - 0s 158us/step - loss: 0.9286 - acc: 0.5966 - val_loss: 1.0932 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "119/119 [==============================] - 0s 175us/step - loss: 0.9194 - acc: 0.6303 - val_loss: 1.0618 - val_acc: 0.5833\n",
            "Epoch 15/20\n",
            "119/119 [==============================] - 0s 179us/step - loss: 0.9203 - acc: 0.6218 - val_loss: 1.0161 - val_acc: 0.5833\n",
            "Epoch 16/20\n",
            "119/119 [==============================] - 0s 162us/step - loss: 0.9139 - acc: 0.6134 - val_loss: 1.0136 - val_acc: 0.5833\n",
            "Epoch 17/20\n",
            "119/119 [==============================] - 0s 167us/step - loss: 0.9124 - acc: 0.5966 - val_loss: 1.0230 - val_acc: 0.5833\n",
            "Epoch 18/20\n",
            "119/119 [==============================] - 0s 163us/step - loss: 0.9024 - acc: 0.6134 - val_loss: 0.9784 - val_acc: 0.5833\n",
            "Epoch 19/20\n",
            "119/119 [==============================] - 0s 195us/step - loss: 0.9037 - acc: 0.6050 - val_loss: 0.9871 - val_acc: 0.5833\n",
            "Epoch 20/20\n",
            "119/119 [==============================] - 0s 179us/step - loss: 0.9045 - acc: 0.6218 - val_loss: 0.9450 - val_acc: 0.5833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "845a25b7-397c-493c-b1e7-e6b5af787918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "56c62628-95b6-437f-f14b-8fb2d6f0588d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "51d4180f-bbd5-4118-9d5a-22addfc032fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "d0cd061f-5fa9-4662-882e-8e40d3c5f9d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'bo', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'b', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1cc29b3d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "40ef3923-8b53-4c42-84c8-0ba0a0925d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1cc248edd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3hUZfbA8e8JIL0J2EASREACJBgi\noKCC0myggisYpSiyoiiWFVlhRVnRtVdcF/xZEBRYXRXsqLjoKkpAQAEVBJQuzVCCEMj5/fHexCHM\nJJNkZm7K+TzPPJm59cxkck/uW0VVMcYYY/KK8zsAY4wxJZMlCGOMMUFZgjDGGBOUJQhjjDFBWYIw\nxhgTlCUIY4wxQVmCMGETkQoiskdEGkdyWz+JyMkiEvG23iLSTUTWBrz+QUTODGfbIpzrORG5s6j7\nGxNKRb8DMNEjInsCXlYD9gOHvNd/VtVphTmeqh4CakR62/JAVVtE4jgiMhS4UlW7BBx7aCSObUxe\nliDKMFXNvUB7/6EOVdWPQm0vIhVV9WAsYjOmIPZ99J8VMZVjInKviMwQkVdFZDdwpYicLiLzReQ3\nEdkkIk+KSCVv+4oioiKS4L2e6q1/T0R2i8iXItKksNt6688TkR9FJENEnhKR/4nI4BBxhxPjn0Vk\nlYjsFJEnA/atICKPich2EVkN9Mrn8xkjItPzLJsoIo96z4eKyArv/fzk/Xcf6ljrRaSL97yaiLzs\nxbYMaJdn27Eisto77jIR6e0tbwM8DZzpFd9tC/hs7w7Y/zrvvW8XkTdF5PhwPpvCfM458YjIRyKy\nQ0Q2i8iogPP8zftMdolIuoicEKw4T0Q+z/k9e5/nPO88O4CxItJMROZ659jmfW61A/aP997jVm/9\nEyJSxYu5ZcB2x4tIpojUC/V+TRCqao9y8ADWAt3yLLsXOABchPtnoSpwGtABd3d5EvAjMMLbviKg\nQIL3eiqwDUgFKgEzgKlF2PYYYDfQx1t3K5AFDA7xXsKJ8S2gNpAA7Mh578AIYBnQCKgHzHN/BkHP\ncxKwB6gecOxfgVTv9UXeNgKcA+wDkrx13YC1AcdaD3Txnj8MfArUBeKB5Xm2/RNwvPc7ucKL4Vhv\n3VDg0zxxTgXu9p738GJsC1QBngE+CeezKeTnXBvYAowEKgO1gPbeur8CS4Bm3ntoCxwNnJz3swY+\nz/k9e+/tIDAcqID7PjYHzgWO8r4n/wMeDng/33mfZ3Vv+07euknAhIDz3Aa84fffYWl7+B6APWL0\niw6dID4pYL+/AP/2nge76D8bsG1v4LsibHs18FnAOgE2ESJBhBljx4D1/wH+4j2fhytqy1l3ft6L\nVp5jzweu8J6fB/yQz7ZvAzd4z/NLEL8E/i6A6wO3DXLc74ALvOcFJYiXgPsC1tXC1Ts1KuizKeTn\nfBWwIMR2P+XEm2d5OAlidQEx9Ms5L3AmsBmoEGS7TsAaQLzXi4FLI/13VdYfVsRk1gW+EJFTROQd\nr8hgFzAeqJ/P/psDnmeSf8V0qG1PCIxD3V/0+lAHCTPGsM4F/JxPvACvAAO851d4r3PiuFBEvvKK\nP37D/fee32eV4/j8YhCRwSKyxCsm+Q04Jczjgnt/ucdT1V3ATqBhwDZh/c4K+JxPxCWCYPJbV5C8\n38fjRGSmiGzwYngxTwxr1TWIOIyq/g93N9JZRFoDjYF3ihhTuWUJwuRt4vkv3H+sJ6tqLeAu3H/0\n0bQJ9x8uACIiHH5By6s4MW7CXVhyFNQMdybQTUQa4orAXvFirAq8BtyPK/6pA3wYZhybQ8UgIicB\n/8QVs9Tzjvt9wHELapK7EVdslXO8mriirA1hxJVXfp/zOqBpiP1CrdvrxVQtYNlxebbJ+/4ewLW+\na+PFMDhPDPEiUiFEHFOAK3F3OzNVdX+I7UwIliBMXjWBDGCvV8n35xic820gRUQuEpGKuHLtBlGK\ncSZws4g09Cos78hvY1XdjCsGeRFXvLTSW1UZVy6+FTgkIhfiysrDjeFOEakjrp/IiIB1NXAXya24\nXHkt7g4ixxagUWBlcR6vAteISJKIVMYlsM9UNeQdWT7y+5xnAY1FZISIVBaRWiLS3lv3HHCviDQV\np62IHI1LjJtxjSEqiMgwApJZPjHsBTJE5ERcMVeOL4HtwH3iKv6rikingPUv44qkrsAlC1NIliBM\nXrcBg3CVxv/CVSZHlapuAS4HHsX9wTcFvsH95xjpGP8JfAx8CyzA3QUU5BVcnUJu8ZKq/gbcAryB\nq+jth0t04RiHu5NZC7xHwMVLVZcCTwFfe9u0AL4K2HcOsBLYIiKBRUU5+7+PKwp6w9u/MZAWZlx5\nhfycVTUD6A70xSWtH4GzvdUPAW/iPudduArjKl7R4bXAnbgGCyfneW/BjAPa4xLVLOD1gBgOAhcC\nLXF3E7/gfg8569fifs/7VfWLQr53wx8VOMaUGF6RwUagn6p+5nc8pvQSkSm4iu+7/Y6lNLKOcqZE\nEJFeuBZD+3DNJLNw/0UbUyRefU4foI3fsZRWVsRkSorOwGpc2XtP4BKrVDRFJSL34/pi3Keqv/gd\nT2llRUzGGGOCsjsIY4wxQZWZOoj69etrQkKC32EYY0ypsnDhwm2qGrRZeZlJEAkJCaSnp/sdhjHG\nlCoiEnI0AStiMsYYE5QlCGOMMUFFNUGISC9xUy2uEpHRIbb5k4gsFzfufeBAaINEZKX3GBTNOI0x\nxhwpanUQXm/Yibju+OuBBSIyS1WXB2zTDNcpqpOq7hSRY7zlR+O62KfixqVZ6O27szAxZGVlsX79\nen7//ffIvClTJlSpUoVGjRpRqVKo4YyMMRDdSur2wCpVXQ0gbmauPrjJUXJcC0zMufCr6q/e8p7A\nHFXd4e07Bzfz16uFCWD9+vXUrFmThIQE3AChprxTVbZv38769etp0qRJwTsYU45Fs4ipIYeP7b6e\nI4dwbg40Fze95HxvuIVw90VEhnnTGaZv3br1iAB+//136tWrZ8nB5BIR6tWrZ3eVpkSYNg0SEiAu\nzv2cNs3viA7ndzPXirhpCbvg5gOYJ27e3bCo6iTcSJGkpqYG7RJuycHkZd8JUxJMmwbDhkFmpnv9\n88/uNUBaUcffjbBo3kFs4PBJURpx5KQl64FZqpqlqmtwQwY3C3NfY4wptcaM+SM55MjMdMvDFe07\nkGgmiAVAMxFpIiJHAf1x47kHehN394CI1McVOa0GPgB6iEhdEamLm8rxgyjGGhXbt2+nbdu2tG3b\nluOOO46GDRvmvj5w4EBYxxgyZAg//PBDvttMnDiRaSXt3tSYUsDPIp5fQgwhGGp5Xjl3ID//DKp/\n3IFE9D1Ec8Jr3ITwP+Lmpx3jLRsP9PaeC26SmOW4iT36B+x7NbDKewwp6Fzt2rXTvJYvX37EsvxM\nnaoaH68q4n5OnVqo3fM1btw4feihh45Ynp2drYcOHYrciUqJrKwsX89f2O+GKXumTlWtVk3VXV7d\no1q1yP7d5yc+/vBz5zzi42Ozfw4gXUNcV6PaD0JV31XV5qraVFUneMvuUtVZ3nNV1VtVNVFV26jq\n9IB9n1fVk73HC9GME2KUjT2rVq0iMTGRtLQ0WrVqxaZNmxg2bBipqam0atWK8ePH527buXNnFi9e\nzMGDB6lTpw6jR48mOTmZ008/nV9/dY2+xo4dy+OPP567/ejRo2nfvj0tWrTgiy/cRFp79+6lb9++\nJCYm0q9fP1JTU1m8ePERsY0bN47TTjuN1q1bc9111+Uka3788UfOOecckpOTSUlJYe3atQDcd999\ntGnThuTkZMZ498Y5MQNs3ryZk08+GYDnnnuOiy++mK5du9KzZ0927drFOeecQ0pKCklJSbz99h8T\nsr3wwgskJSWRnJzMkCFDyMjI4KSTTuLgwYMA7Ny587DXxhRWJIp4imPCBKhW7fBl1aq55eEo7h1I\nWEJljtL2KO4dRKSycSiBdxArV65UEdEFCxbkrt++fbuquv+sO3furMuWLVNV1U6dOuk333yjWVlZ\nCui7776rqqq33HKL3n///aqqOmbMGH3sscdytx81apSqqr711lvas2dPVVW9//779frrr1dV1cWL\nF2tcXJx+8803R8SZE0d2drb2798/93wpKSk6a9YsVVXdt2+f7t27V2fNmqWdO3fWzMzMw/bNiVlV\nddOmTdq0aVNVVZ08ebI2btxYd+zYoaqqBw4c0IyMDFVV3bJli5588sm58bVo0SL3eDk/r7zySp09\ne7aqqk6cODH3fRaF3UEYkeB/8yKxi6E4pRal/g6iNIlJNg7QtGlTUlNTc1+/+uqrpKSkkJKSwooV\nK1i+fPkR+1StWpXzzjsPgHbt2uX+F5/XpZdeesQ2n3/+Of379wcgOTmZVq1aBd33448/pn379iQn\nJ/Pf//6XZcuWsXPnTrZt28ZFF10EuI5m1apV46OPPuLqq6+matWqABx99NEFvu8ePXpQt25dwP1z\nMnr0aJKSkujRowfr1q1j27ZtfPLJJ1x++eW5x8v5OXToUF54wd1MvvDCCwwZMqTA8xkTSuPGhVse\nDWlpsHYtZGe7n4VpvVTcO5BwWILwxPrLUr169dznK1eu5IknnuCTTz5h6dKl9OrVK2g7/aOOOir3\neYUKFUIWr1SuXLnAbYLJzMxkxIgRvPHGGyxdupSrr766SP0FKlasSHZ2NsAR+we+7ylTppCRkcGi\nRYtYvHgx9evXz/d8Z599Nj/++CNz586lUqVKnHLKKYWOzURWSW/Hn59YXGCjKS0NJk2C+HgQcT8n\nTYpsE1lLEB4/vyy7du2iZs2a1KpVi02bNvHBB5FvsNWpUydmzpwJwLfffhv0DmXfvn3ExcVRv359\ndu/ezeuvvw5A3bp1adCgAbNnzwbcRT8zM5Pu3bvz/PPPs2/fPgB27NgBuKHXFy5cCMBrr70WMqaM\njAyOOeYYKlasyJw5c9iwwbVkPuecc5gxY0bu8XJ+Alx55ZWkpaXZ3UMJEMt6u/xiKGqCisUFNtqK\ncwcSDksQHj+/LCkpKSQmJnLKKacwcOBAOnXqFPFz3HjjjWzYsIHExETuueceEhMTqV279mHb1KtX\nj0GDBpGYmMh5551Hhw4dctdNmzaNRx55hKSkJDp37szWrVu58MIL6dWrF6mpqbRt25bHHnsMgNtv\nv50nnniClJQUdu4MPXzWVVddxRdffEGbNm2YPn06zZo1A1wR2KhRozjrrLNo27Ytt99+e+4+aWlp\nZGRkcPnll0fy4zFF4HclbyQSVLQvsKVeqMqJ0vaIRDPXsiwrK0v37dunqqo//vijJiQk+N7UtChe\nffVVHTx4cLGPU1a+G9Fsml0Qvyt5o92wJBx+fv6RQj6V1L5f2CP1sASRv507d2pKSoomJSVpmzZt\n9IMPPvA7pEK77rrr9OSTT9ZVq1YV+1hl4bsRiXb8freiKc75/U5QfvejiBRLEMbkURa+G8W9QBf3\nAuf3/n7fQfh9/kjJL0FYHYQxpVRxm2YXtw6huPV2xT2/362QYt003g+WIIwppYrbNDsSF7jiVPIW\n9/x+t0IqCf0oos0ShDGlVHH/g/b7AheJ8/vZCsnvO5hYsARhTClV3P+g/b7A+X3+4vL7DiYWLEFE\nUdeuXY/o9Pb4448zfPjwfPerUaMGABs3bqRfv35Bt+nSpQvp6en5Hufxxx8nM6CQ9/zzz+e3334L\nJ3RTShTnP2i/L3B+nz8Syno/CksQUTRgwACmT59+2LLp06czYMCAsPY/4YQT8u2JXJC8CeLdd9+l\nTp06RT5erKlq7pAdZZXfQ1X4fYHz+/wmf5Ygoqhfv3688847uZMDrV27lo0bN3LmmWeyZ88ezj33\nXFJSUmjTpg1vvfXWEfuvXbuW1q1bA24YjP79+9OyZUsuueSS3OEtAIYPH547VPi4ceMAePLJJ9m4\ncSNdu3ala9eugBsCY9u2bQA8+uijtG7dmtatW+cOFb527VpatmzJtddeS6tWrejRo8dh58kxe/Zs\nOnTowKmnnkq3bt3YsmULAHv27GHIkCG0adOGpKSk3KE63n//fVJSUkhOTubcc88F4O677+bhhx/O\nPWbr1q1Zu3Yta9eupUWLFgwcOJDWrVuzbt26oO8PYMGCBZxxxhkkJyfTvn17du/ezVlnnXXYMOad\nO3dmyZIlhfq9xUpJGKrCmHyFav9a2h4F9YMYOVL17LMj+xg5soAGxqp6wQUX6Jtvvqmqbsjt2267\nTVVdz+acoa63bt2qTZs21ezsbFVVrV69uqqqrlmzRlu1aqWqqo888ogOGTJEVVWXLFmiFSpUyB0u\nPGc47IMHD+rZZ5+tS5YsUVXV+Ph43bp1a24sOa/T09O1devWumfPHt29e7cmJibqokWLdM2aNVqh\nQoXcobovu+wyffnll494Tzt27MiNdfLkyXrrrbeqquqoUaN0ZMCHsmPHDv3111+1UaNGunr16sNi\nzTuBUqtWrXTNmjW6Zs0aFRH98ssvc9cFe3/79+/XJk2a6Ndff62qqhkZGZqVlaUvvvhibgw//PCD\nBvteqJaMfhBlpR29Kd2wfhD+CSxmCixeUlXuvPNOkpKS6NatGxs2bMj9TzyYefPmceWVVwKQlJRE\nUlJS7rqZM2eSkpLCqaeeyrJly4IOxBfo888/55JLLqF69erUqFGDSy+9lM8++wyAJk2a0LZtWyD0\nkOLr16+nZ8+etGnThoceeohly5YB8NFHH3HDDTfkble3bl3mz5/PWWedRZMmTYDwhgSPj4+nY8eO\n+b6/H374geOPP57TTjsNgFq1alGxYkUuu+wy3n77bbKysnj++ecZPHhwgecrjuIUEZWHdvSmdKvo\ndwCx4pWixFyfPn245ZZbWLRoEZmZmbRr1w5wg99t3bqVhQsXUqlSJRISEoo0tPaaNWt4+OGHWbBg\nAXXr1mXw4MFFOk6OnKHCwQ0XHqyI6cYbb+TWW2+ld+/efPrpp9x9992FPk/gkOBw+LDggUOCF/b9\nVatWje7du/PWW28xc+bM3FFloyGniCinmieniAjCK0tv3NjtE2y5MSWB3UFEWY0aNejatStXX331\nYZXTOUNdV6pUiblz5/JzsCtFgLPOOotXXnkFgO+++46lS5cCbqjw6tWrU7t2bbZs2cJ7772Xu0/N\nmjXZvXv3Ecc688wzefPNN8nMzGTv3r288cYbnHnmmWG/p4yMDBo2bAjASy+9lLu8e/fuTJw4Mff1\nzp076dixI/PmzWPNmjXA4UOCL1q0CIBFixblrs8r1Ptr0aIFmzZtYsGCBQDs3r07d+6LoUOHctNN\nN3HaaaflTk4UDaW9J7AxBbEEEQMDBgxgyZIlhyWItLQ00tPTadOmDVOmTClw8pvhw4ezZ88eWrZs\nyV133ZV7J5KcnMypp57KKaecwhVXXHHYUOHDhg2jV69euZXUOVJSUhg8eDDt27enQ4cODB06lFNP\nPTXs93P33Xdz2WWX0a5dO+rXr5+7fOzYsezcuZPWrVuTnJzM3LlzadCgAZMmTeLSSy8lOTk5d5ju\nvn37smPHDlq1asXTTz9N8+bNg54r1Ps76qijmDFjBjfeeCPJycl07949986iXbt21KpVK+pzRpT2\nnsDGFERcHUXpl5qaqnn7BaxYsYKWLVv6FJHxy8aNG+nSpQvff/89cXHB/weKxHcjISF4EVF8vGuy\naUxpICILVTU12Dq7gzBlypQpU+jQoQMTJkwImRwCFaeS2YqITFlXbiqpTfkwcOBABg4cGNa2xa1k\nztlmzBhXrNS4sUsOVkRkyooyfwdRVorQTOTkfCciMWWm9QQ2ZVmZThBVqlRh+/btliRMLlVl+/bt\nVKlSxfohGFOAMl3E1KhRI9avX8/WrVv9DsWUIFWqVKFRo0bWD8GYApTpBFGpUqXcHrzG5DVhwuF1\nEGCVzMYEKtNFTMbkx/ohGJO/Mn0HYUxB0tIsIRgTit1BGGOMCcoShDHGmKAsQZhSze8Z2Ywpy6Ka\nIESkl4j8ICKrRGR0kPWDRWSriCz2HkMD1h0KWD4rmnGa0slmZDMmuqI2WJ+IVAB+BLoD64EFwABV\nXR6wzWAgVVVHBNl/j6rWCPd8wQbrM2WbDZZnTPH5NVhfe2CVqq5W1QPAdKBPFM9nyhnrCW1MdEUz\nQTQE1gW8Xu8ty6uviCwVkddE5MSA5VVEJF1E5ovIxcFOICLDvG3Srbd0+ROqx7P1hDYmMvyupJ4N\nJKhqEjAHeClgXbx323MF8LiINM27s6pOUtVUVU1t0KBBbCI2JYYNt21MdEUzQWwAAu8IGnnLcqnq\ndlXd7718DmgXsG6D93M18CkQ/pRnptQoTisk6wltTHRFsyf1AqCZiDTBJYb+uLuBXCJyvKpu8l72\nBlZ4y+sCmaq6X0TqA52AB6MYq/FBcedjyNnOEoIx0RG1OwhVPQiMAD7AXfhnquoyERkvIr29zW4S\nkWUisgS4CRjsLW8JpHvL5wL/CGz9ZMqGSMzHYIyJnjI9J7Up2eLiXP+FvETcBDzGmOizOalNiWSt\nkIwp2SxBGN9YKyRjSjZLEKZYrBWSMWWXzQdhisxaIRlTttkdhCkya4VkyoL9+yEry+8oSiZLEKbI\nbCwkU9qtWAEtW8Ipp8A33/gdTdGsXAnz5kXn2JYgTJFZKyRTms2bB506wd697i7i9NNdHVhpaPm/\nYwc8+yyccQY0bw7Dh0fnPJYgTJFZKyRTWk2fDt27w7HHwvz57u6hSxf4859h4ECXNEqaAwfgrbeg\nb184/niXFHbtggcegA8/jNJJVbVMPNq1a6cm9qZOVY2PVxVxP6dO9TsiY0LLzlZ94AFVUD3zTNXt\n2/9Yd+iQ6vjx7rucmKi6fLl/cebIzlb9+mvVESNU69VzcR9zjOrNN6suWuTWFxeQriGuq9aT2hhT\nLhw8CDfdBP/8J/TvDy+8AFWqHLndxx/DFVe4u4hJk9zzWPvlF9dKcMoU+P57qFwZ+vRxdzc9ekCl\nSpE7l/WkNiHZnM6mPNi7Fy65xCWHO+5w3/NgyQHg3HNdkVNKimuCPXw4/P579GPcvRtefBHOOcf9\nLd55JzRoAJMnw+bNMGMGXHBBZJNDgULdWpS2hxUxFd7UqarVqrnb1pxHtWpWTGTKlk2bVFNTVePi\nVJ95Jvz9srJUR41yfxcpKao//RT52A4cUH3vPdW0NNWqVd25mjZVveee6JwvGKyIyQRjczqbsu77\n7+G88+DXX91/4BdeWPhjzJoFgwa5f6FeeskV9RTHoUOuBdX06fD667B9O9Sp44q9Bg6Ejh3dyAKx\nkl8Rk/WkLsesH4Mpy+bNg4svdkUy//0vpAa9BBasd29YtAguu8wd7y9/gfvuK1xRT3a2ay01fTr8\n+9+uyKh6dXfs/v2hZ09Xz1DSWIIoxxo3Dn4HYf0YTGk3fbr7r79JE3jvPfezOJo0gf/9D269FR5+\n+I+LfcOGofdRdYll+nR397JunUsCF1zgksIFFxzZTLyksUrqcsz6MZiyRhUefBAGDIAOHeCLL4qf\nHHJUrgwTJ8Krr7pK7LZtYc6cI7f77jsYOxaaNXN3LU88AUlJ8PLLrqjr9dfd3UhJTw6AVVKXd9aP\nwZQVWVmqw4e7it7LL1fdty9651qxQrVVK/d3M26cez1+vOs/Aa5CvFs31eeeO7yvRUmEVVIbY6JF\nNbaVqsHs3euKbd5+G0aNgvvvd023o33O6693fRXAfQZnngmXX+56Ox97bHTPHylWSW2MCSk7G377\nDTIy3M+cR+Dr/J5nZLjOW6+84lrjxNqWLa510qJFrgjo+utjc97q1V2/hYsugo0b4dJLoVGj2Jw7\nVixBGFOO/fab65hV0EimNWu6i3+dOlC7tqucbdXKPY+LcwPHnX46vPMOnHRSbGIHlxQuuQS2bYM3\n33QX61gSgX79YnvOWLIEYUw5dfCgKw757jvXMKFhQ3fBD0wEdepArVpQoUL+x7r0Uvfo0MFdqDt1\nin78U6a4wfUaNHBNWtu1i/45yxtLEMaUU7fc4kYBfe45uOaa4h2rSxfX9POCC9wdyQsvRG8Mo6ws\n19z06aeha1fXhLRBg+icq7yzZq7GlEPPPOMusLfdVvzkkKN5c5ckOnZ0Yxjdc0/k51bYvNmNlZQT\n+4cfWnKIJksQxpQzc+a4UU0vvNDNJRBJ9eq5i/agQXD33XDVVZEb6G7+fFeMlJ7uKsQffhgqWhlI\nVFmCMMZnO3e6yWBi4fvvXSetxER3kS2obqEoKld2RUwTJrhRU7t1g61bi3fMSZPg7LPdsb/80nWE\nM9FnCcIYH6i6/4jT0lx7+Q4doj8G1vbt7q6hcmWYPdu1TIoWETdc9YwZsHChK3b6/vvCH2f/fhg2\nzFVGd+3q7h6SkyMfrwnOEoQxMbR/vxtyoUMH1yz07bddcczq1XDaaW5oiGg4cMB13lq3zrUyio+P\nznny+tOf4NNPYc8elyQ+/jj8fTdscHcNkye7ZPPOO3D00VEL1QRhCcKYGNi4Ee66yw2EOHCgmxxm\n4kRYv95dAOfPd81Ju3Z1Q0pHkirccIMb0fT5511iiqUOHeCrr1wnsl693PstyGefufqGZcvc2EUT\nJkSnOMwUINQYHKXtYWMxmZImO1v1iy9U+/dXrVjRjdtz4YWqH34YfC7h7dtVzz3XjeVz222qBw9G\nJo5HHnHHHDMmMscrqt9+U+3Z08Vy++1uDui8srNVn3zSfV7NmqkuWxb7OMsb8hmLyfcLe6Qe5TVB\n2GB7Jc/vv6u+9JJqu3buL6x2bdVbblFdtargfQ8ccBPUg+r557uLanHMnu2+G337Br8gx1rggHoX\nX6y6Z88f6zIzVQcOdOsuuqj4792ExxJEGWVThpYs69erjh2r2qCB+120bOmmuNy9u/DHevZZ9190\ny5aqK1cWLZ6lS1Vr1HDTZQZeiP2Wna36+OMucaWkqG7YoLp2rXsObrrNkpDMyov8EoSN5lqK2ZSh\nJcOCBfDII66s/NAh11Lopptch67ijHL66aeuYlkVXnvN9VAO16+/Qvv2rnJ6wYL8J7bxy+zZrrlq\nnTqur0RWlmsWW5RpQU3R5Teaa1QrqUWkl4j8ICKrRGR0kPWDRWSriCz2HkMD1g0SkZXeY1A04yyt\nbMpQfx08COPGuUrY9993SXDPB+QAABrVSURBVGHVKjeHcbduxR8Cu0sXd3E/4QQ3Wuozz4S33++/\nuwHsfv3VxVISkwO4gfU+/9x9Tsce696rJYcSJtStRXEfQAXgJ+Ak4ChgCZCYZ5vBwNNB9j0aWO39\nrOs9r5vf+cpjEVN8/OHFSzmP+Hi/Iyv71q1TPess93kPGqS6a1f0zpWR4Sq3wZXfHzgQetvsbNUr\nr3TbzpwZvZgiad8+Vzdh/EE+RUzRvINoD6xS1dWqegCYDvQJc9+ewBxV3aGqO4E5QK8oxVlq2ZSh\n/nj7bTfd5MKFbkTRF1+MbqezWrVc34U77oB//tNNcL99e/Bt//EPmDoVxo93PaZLgypVbMiMkqrA\nBCEiN4pI3SIcuyGwLuD1em9ZXn1FZKmIvCYiJxZmXxEZJiLpIpK+tbh9+UuhtDQ3BEF8vLtNj493\nr9PS/I6sbDpwwI2AetFFrk3/woVurKFYqFDBXfynTIH//c/VLyxffvg2//mP61B2xRVuTmRjiiuc\nO4hjgQUiMtOrU4jk5IKzgQRVTcLdJRSqi5CqTlLVVFVNbVBOh3RMS3MV0tnZ7qclh+hYtQrOOAMe\nfxxGjHAd21q0iH0cV13lOrzt3et6Jr/zjlu+aJFb16ED/N//+T8FqCkbCkwQqjoWaAb8H67OYKWI\n3CciTQvYdQNwYsDrRt6ywGNvV9X93svngHbh7mtMrLz6KqSkwE8/uf/Sn3rKFYv4pWNHV6HbrJm7\nm7nrLujd242k+uab/sZmypaw6iC8iozN3uMgruL4NRF5MJ/dFgDNRKSJiBwF9AdmBW4gIscHvOwN\nrPCefwD0EJG6XvFWD2+ZMYC7Y/rsMzeERKNGrsjl0Ufd0BWRkpkJQ4e6Ips2bWDxYtc6qCQ48UT3\n/vv1g7//3U0dOns2HHec35GZMiVU7bX+0aJoJLAQd4G+DKjkLY8Dfipg3/OBH3GtmcZ4y8YDvb3n\n9wPLcC2c5gKnBOx7NbDKewwpKM7y2IqpvMnOVv3qK9cruWFD11KnalXVSy75o5MVqHburPr006qb\nNxf9XN9+q5qY6Dpz/fWv+bcc8lN2tuq//qU6b57fkZjSiuJ0lBORe4DnVfWILlki0lJVVwTZLebK\nY0e58kAVlixxw0bPmAFr1sBRR7lB3/r3d0UsNWq4bVeudNtMn+4GeYuLc4Pf9e/v5ksOZyRQVTeY\n3MiRbk7ml1+G7t2j+x6N8VN+HeXCSRAdgWWqutt7XQtoqapfRTzSYrAEUbasWPHHxf6HH1wrnu7d\n4fLL4eKLXe/b/Hz33R/7r1rlmlH26OGSRZ8+ruloXhkZbu6BmTPduaZMsSIbU/YVN0F8A6R4tyKI\nSBzuliQl4pEWgyWI0m/16j8u6kuXupY4Xbq4pNC3L9SvX/hjqsI337hjzpjheplXrgznn++Oe+GF\nUL06fP21Sx6//AL33gujRrk7EGPKuvwSRDjdU0QDsoiqZouIdWsxEfP88/Dss65lDrjmpE8+6Spg\njz8+/30LIuJaIKWkuH4EX33lksW//w1vvOE6Fnbp4uZRPuEEmDfPnd8YE14rptUicpOIVPIeI3FD\nXxhTbJMnwzXXuIHaHnrIDT74v//BjTcWPznkFRfnJst54gk3s9rcuW7yniVLXOukb76x5GBMoHCK\nmI4BngTOART4GLhZVX+NfnjhsyKm0mfePDfiabdubvgKmzHMmNgrVhGTlwj6RzwqU679/LOrV2ja\n1HVEs+RgTMlTYIIQkSrANUArILePpqpeHcW4TBm2Z4/r+ZuV5YajLqhFkjHGH+HUQbwMHIcbYfW/\nuGEvdkczKFN2ZWfDoEGuGerMmdC8ud8RGWNCCSdBnKyqfwP2qupLwAVAh+iGZcqq8ePdeEaPPOL6\nJRhjSq5wEkSW9/M3EWkN1AaOiV5Ipqx67TW45x4YMsT1VDbGlGzh9GeY5A2YNxY32F4N4G9RjcqU\nOYsXu6Kl0093k97YcNTGlHz5Jgiv1/QudbO6zcNNH2pMoWzZ4iqljz7aFS9Vrux3RMaYcORbxKSq\n2cCoGMViyqADB1xz1m3b4K23bGwjY0qTcOogPhKRv4jIiSJydM4j6pGZUk8Vrr/e9Yx+8UU33IUx\npvQIpw7icu/nDQHLFCtuMgV46ik3/eXYsfCnP/kdjTGmsMLpSd0kFoGYsmXOHLjlFjc09z33+B2N\nMaYowulJPTDYclWdEvlwTFmwcqUbSrtVKzfhjg2bbUzpFE4R02kBz6sA5wKLAEsQ5ggZGW5Cnrg4\nVymdM9ubMab0CaeI6cbA1yJSB5getYhMqXXoEFxxhbuD+OgjaGKFk8aUakWZ+GcvYH/65gh33gnv\nvusm/zn7bL+jMcYUVzh1ELNxrZbANYtNBGZGMyhT+kydCg8+6Jq1/vnPfkdjjImEcO4gHg54fhD4\nWVXXRykeUwp99RUMHeqm7nz8cb+jMcZESjgJ4hdgk6r+DiAiVUUkQVXXRjUyUyps3uym6zzhBDfP\nc6VKfkdkjImUcBog/hvIDnh9yFtmyrmsLNcBLiPDtViqX9/viIwxkRROgqioqgdyXnjPj4peSOXL\ntGmQkOCahSYkuNelxR13wGefwXPPQZs2fkdjjIm0cBLEVhHpnfNCRPoA26IXUvkxbRoMG+bmZ1Z1\nP4cNKx1JYsYMeOwxuOkmGDDA72iMMdEgqpr/BiJNgWnACd6i9cBAVV0V5dgKJTU1VdPT0/0Oo1AS\nElxSyCs+HtaujXU04Vu2DDp0gLZtYe5cq3cwpjQTkYWqmhpsXTgd5X4COopIDe/1ngjHV2798kvh\nlpcEGRlw6aVQs6ZVShtT1hVYxCQi94lIHVXdo6p7RKSuiNwbi+DKusaNC7fcb6oweDD89BPMnAnH\nH+93RMaYaAqnDuI8Vf0t54U3u9z50Qup/JgwAapVO3xZtWpueUn04IPw5pvw8MNw5pl+R2OMibZw\nEkQFEcmdJFJEqgI2aWQEpKXBpEmuzkHE/Zw0yS0vaT7+2A2lcfnlMHKk39EYY2IhnI5y04CPReQF\nQIDBwEvRDKo8SUsrmQkh0C+/QP/+0LKla9Iq4ndExphYKPAOQlUfAO4FWgItgA+A+HAOLiK9ROQH\nEVklIqPz2a6viKiIpHqvE0Rkn4gs9h7PhvVuTMTt3w/9+rmf//mPDd9tTHkS7miuW3AD9l0GrAFe\nL2gHEakATAS645rGLhCRWaq6PM92NYGRwFd5DvGTqrYNMz4TJSNHwoIFLjk0b+53NMaYWAp5ByEi\nzUVknIh8DzyFG5NJVLWrqj4dxrHbA6tUdbXX+3o60CfIdn8HHgB+L3z4xXfwoBtL6JVX3NAR5g8v\nvAD/+heMHu0+I2NM+ZJfEdP3wDnAharaWVWfwo3DFK6GwLqA1+u9ZblEJAU4UVXfCbJ/ExH5RkT+\nKyJB28yIyDARSReR9K1btxYitD+sWwcrVrh6gCZN4IEHYOfOIh2qTFm0CIYPh3PPhb//3e9ojDF+\nyC9BXApsAuaKyGQRORdXSR0RIhIHPArcFmT1JqCxqp4K3Aq8IiK18m6kqpNUNVVVUxs0aFCkOJo0\ngeXL4Z134JRT3H/LjRrBiBFuZrTyaPt26NsXjjkGXn0VKhZlWiljTKkXMkGo6puq2h84BZgL3Awc\nIyL/FJEeYRx7A3BiwOtG3rIcNYHWwKcishboCMwSkVRV3a+q2704FgI/AVErAY+Lg/PPd9NkLlni\nmnJOngwtWkDv3vDpp66TWHlw6JC7m9q4EV57DYqYd40xZUA4rZj2quorqnoR7iL/DXBHGMdeADQT\nkSYichTQH5gVcNwMVa2vqgmqmgDMB3qrarqINPAquRGRk4BmwOrCvrmiSEqC5593YyT97W/w5ZfQ\ntSukpMDLL8OBAwUfozQbPx4++ACeegrat/c7GmOMn8LpKJdLVXd6xTrnhrHtQWAErlnsCmCmqi4T\nkfGBo8OGcBawVEQWA68B16nqjsLEWlzHHQf33OP6AEye7BLDwIFugL0JE2BbGRzP9u23XYIYMgSu\nvdbvaIwxfitwNNfSItqjuarChx+6Ia4/+ACqVIFBg+Dmm13dRWn300+QmgonnQSffw5Vq/odkTEm\nFvIbzbVQdxDlmQj07Anvv++Gu77qKnjxRde7+Pzz3QW2tMrMdCO0xsXB669bcjDGOJYgiiAx0Y2Z\ntG6dK5KZP98ljyK2tPXdddfBt9/+MbudMcaAJYhiadDAVWS/9x5s2AB9+sC+fX5HVTivveYq38eN\ng169/I7GGFOSWIKIgA4dYOpUdycxaBBkZ/sdUXi2bYPrr4d27WDMGL+jMcaUNJYgIqRvX3joITfL\n2l//6nc04Rk5En77zQ2pYZ3hjDF52WUhgm69FVavdhPrNG0Kw4b5HVFos2a58afuuQfatPE7GmNM\nSWQJIoJE4IknYO1aV3TTuHHJLNffudNVTCcluaFFjDEmGCtiirCKFWHGDPdf+WWXuaE7Sppbb4Vf\nf3VFS0cd5Xc0xpiSyhJEFNSo4Xol16kDF1zgWjiVFO+/7/pv3HGHGz7EGGNCsQQRJQ0buhFid+1y\nSWL3br8jcrFce63r3HfXXX5HY4wp6SxBRFFSkmvV9N13boTYgwf9jWfUKDdK6wsvQOXK/sZijCn5\nLEFEWc+e8MwzrjPdjTf6N2z4J5+42eFuvdX12zDGmIJYK6YYGDbMNX994AHX/PUvf4nt+ffsgaFD\noVkzNzSIMcaEwxJEjNx3H6xZA7ff7sY76tcvdue+807X9HbePBuIzxgTPksQMRIXBy+9BOvXu5Fg\nGzWCjh2jf97PPnOT/9x4I3TuHP3zGWPKDquDiKEqVeCtt1wLp969oz9EeGYmXHONm3f7/vujey5j\nTNljCSLG6teHd991cz9fcAHsiOI8eePGwcqV8NxzUL169M5jjCmbLEH4oHlzdyexZg1ccgns3x/5\nc3z1FTz6KPz5z3DOOZE/vjGm7LME4ZPOnV2P5nnzXDFQJJu/7t/v5pVu2NANHGiMMUVhldQ+GjDA\n3UWMGeNaGY0c6e4oijv09vjxsGKFG1ajVq2IhGqMKYfsDsJnf/0rPP206+H8pz+5CuX77iv69KWL\nFrn+FkOGuE56xhhTVJYgfCYCN9zgKpPfegtOOcXdUZx4orvIf/NN+Mc6cMDtc8wx8Mgj0YvZGFM+\nWIIoISpUcE1f58yBZcvg6qth5kw34mrnzu55Vlb+x/jHP2DpUnj2WahbNzZxG2PKLksQJVBiohu/\nacMG1xJp0yY32F+TJjBhQvDip2+/hXvvhSuucInGGGOKyxJECVanDtxyC/z4I8ye7RLH2LGuF/bg\nwbBwodvu4EFXtFS3Ljz5pK8hG2PKEGvFVApUqAAXXugeK1a4Su2XXnKPM86Ak092yeLf/4Z69fyO\n1hhTVtgdRCnTsiVMnOiKnx57DLZsgSlToG/f2A4AaIwp+yxBlFK1a8PNN7vip88/d3cTxhgTSVbE\nVMrFxUGnTn5HYYwpi+wOopimTXPzO8TFuZ/TpvkdkTHGRIbdQRTDtGlutrjMTPf655/da4C0NP/i\nMsaYSLA7iGIYM+aP5JAjM9MtN8aY0i6qCUJEeonIDyKySkRG57NdXxFREUkNWPZXb78fRKREjir0\nyy+FW26MMaVJ1BKEiFQAJgLnAYnAABFJDLJdTWAk8FXAskSgP9AK6AU84x2vRGncuHDLjTGmNInm\nHUR7YJWqrlbVA8B0oE+Q7f4OPAD8HrCsDzBdVfer6hpglXe8EmXCBKhW7fBl1aq55cYYU9pFM0E0\nBNYFvF7vLcslIinAiar6TmH39fYfJiLpIpK+tajjYxdDWhpMmgTx8W5U1vh499oqqI0xZYFvrZhE\nJA54FBhc1GOo6iRgEkBqamoE52QLX1qaJQRjTNkUzQSxATgx4HUjb1mOmkBr4FMRATgOmCUivcPY\n1xhjTJRFs4hpAdBMRJqIyFG4SudZOStVNUNV66tqgqomAPOB3qqa7m3XX0Qqi0gToBnwdRRjNcYY\nk0fU7iBU9aCIjAA+ACoAz6vqMhEZD6Sr6qx89l0mIjOB5cBB4AZVPRStWI0xxhxJVH0puo+41NRU\nTU9P9zsMY4wpVURkoaqmBltnPamNMcYEZQnCGGNMUJYgjDHGBGUJwhhjTFCWIIwxxgRlCcIYY0xQ\nliCMMcYEZQnCGGNMUJYgjDHGBGUJwhhjTFCWIIwxxgRlCcIYY0xQliCMMcYEZQnCGGNMUJYgjDHG\nBGUJwhhjTFCWIIwxxgRlCcIYY0xQliCMMcYEZQnCGGNMUJYgjDHGBGUJwhhjTFCWIIwxxgRlCcIY\nY0xQliCMMcYEZQnCGGNMUJYgjDHGBGUJwhhjTFDlPkFMmwYJCRAX535Om+Z3RMYYUzJU9DsAP02b\nBsOGQWame/3zz+41QFqaf3EZY0xJUK7vIMaM+SM55MjMdMuNMaa8K9cJ4pdfCrfcGGPKk6gmCBHp\nJSI/iMgqERkdZP11IvKtiCwWkc9FJNFbniAi+7zli0Xk2WjE17hx4ZYbY0x5ErUEISIVgInAeUAi\nMCAnAQR4RVXbqGpb4EHg0YB1P6lqW+9xXTRinDABqlU7fFm1am65McaUd9G8g2gPrFLV1ap6AJgO\n9AncQFV3BbysDmgU4zlCWhpMmgTx8SDifk6aZBXUxhgD0W3F1BBYF/B6PdAh70YicgNwK3AUcE7A\nqiYi8g2wCxirqp9FI8i0NEsIxhgTjO+V1Ko6UVWbAncAY73Fm4DGqnoqLnm8IiK18u4rIsNEJF1E\n0rdu3Rq7oI0xphyIZoLYAJwY8LqRtyyU6cDFAKq6X1W3e88XAj8BzfPuoKqTVDVVVVMbNGgQscCN\nMcZEN0EsAJqJSBMROQroD8wK3EBEmgW8vABY6S1v4FVyIyInAc2A1VGM1RhjTB5Rq4NQ1YMiMgL4\nAKgAPK+qy0RkPJCuqrOAESLSDcgCdgKDvN3PAsaLSBaQDVynqjuiFasxxpgjiWpMGw5FTWpqqqan\np/sdhjHGlCoislBVU4OuKysJQkS2Aj/7HUc+6gPb/A4iHxZf8Vh8xWPxFU9x4otX1aCVuGUmQZR0\nIpIeKkuXBBZf8Vh8xWPxFU+04vO9masxxpiSyRKEMcaYoCxBxM4kvwMogMVXPBZf8Vh8xROV+KwO\nwhhjTFB2B2GMMSYoSxDGGGOCsgQRISJyoojMFZHlIrJMREYG2aaLiGQETIR0lw9xrg2YpOmInoXi\nPOlN8rRURFJiGFuLgM9msYjsEpGb82wT089QRJ4XkV9F5LuAZUeLyBwRWen9rBti30HeNitFZFCw\nbaIU30Mi8r33+3tDROqE2Dff70IU47tbRDYE/A7PD7FvvhOORTG+GQGxrRWRxSH2jcXnF/S6ErPv\noKraIwIP4HggxXteE/gRSMyzTRfgbZ/jXAvUz2f9+cB7gAAdga98irMCsBnXice3zxA37EsK8F3A\nsgeB0d7z0cADQfY7Gjd+2NFAXe953RjF1wOo6D1/IFh84XwXohjf3cBfwvj9/wSchJsKYEnev6do\nxZdn/SPAXT5+fkGvK7H6DtodRISo6iZVXeQ93w2swM2JUdr0AaaoMx+oIyLH+xDHubhZBX3tHa+q\n84C844D1AV7ynr+ENwpxHj2BOaq6Q1V3AnOAXrGIT1U/VNWD3sv5uJGUfRHi8wtHgROORUJ+8YmI\nAH8CXo30ecOVz3UlJt9BSxBRICIJwKnAV0FWny4iS0TkPRFpFdPAHAU+FJGFIjIsyPpgEz35kej6\nE/oP0+/P8FhV3eQ93wwcG2SbkvI5Xo27IwymoO9CNI3wisCeD1E8UhI+vzOBLaq6MsT6mH5+ea4r\nMfkOWoKIMBGpAbwO3KyHT6kKsAhXZJIMPAW8Gev4gM6qmoKbK/wGETnLhxjyJW54+N7Av4OsLgmf\nYS519/Ilsq24iIwBDgLTQmzi13fhn0BToC1ucrBHYnTewhpA/ncPMfv88ruuRPM7aAkigkSkEu6X\nOE1V/5N3varuUtU93vN3gUoiUj+WMarqBu/nr8AbuFv5QIWd6CkazgMWqeqWvCtKwmcIbMkpdvN+\n/hpkG18/RxEZDFwIpHkXkCOE8V2IClXdoqqHVDUbmBzivH5/fhWBS4EZobaJ1ecX4roSk++gJYgI\n8cor/w9YoaqPhtjmOG87RKQ97vPfHsMYq4tIzZznuMrM7/JsNgsY6LVm6ghkBNzKxkrI/9z8/gw9\ns/hj7pJBwFtBtvkA6CEidb0ilB7esqgTkV7AKKC3qmaG2Cac70K04gus07okxHkLnHAsyroB36vq\n+mArY/X55XNdic13MJo18OXpAXTG3eYtBRZ7j/OB63ATHgGMAJbhWmTMB86IcYwneede4sUxxlse\nGKMAE3EtSL4FUmMcY3XcBb92wDLfPkNcotqEm9RqPXANUA/4GDcD4kfA0d62qcBzAfteDazyHkNi\nGN8qXNlzzvfwWW/bE4B38/suxCi+l73v1lLche74vPF5r8/Htdr5KZbxectfzPnOBWzrx+cX6roS\nk++gDbVhjDEmKCtiMsYYE5QlCGOMMUFZgjDGGBOUJQhjjDFBWYIwxhgTlCUIYwogIofk8FFmIzay\nqIgkBI4kakxJUtHvAIwpBfapalu/gzAm1uwOwpgi8uYDeNCbE+BrETnZW54gIp94g9F9LCKNveXH\nipufYYn3OMM7VAURmeyN9/+hiFT1tr/JmwdgqYhM9+ltmnLMEoQxBauap4jp8oB1GaraBngaeNxb\n9hTwkqom4QbKe9Jb/iTwX3UDDabgeuACNAMmqmor4Degr7d8NHCqd5zrovXmjAnFelIbUwAR2aOq\nNYIsXwuco6qrvQHVNqtqPRHZhhs+IstbvklV64vIVqCRqu4POEYCbsz+Zt7rO4BKqnqviLwP7MGN\nWPumeoMUGhMrdgdhTPFoiOeFsT/g+SH+qBu8ADcuVgqwwBth1JiYsQRhTPFcHvDzS+/5F7jRRwHS\ngM+85x8DwwFEpIKI1A51UBGJA05U1bnAHUBt4Ii7GGOiyf4jMaZgVeXwievfV9Wcpq51RWQp7i5g\ngLfsRuAFEbkd2AoM8ZaPBCaJyDW4O4XhuJFEg6kATPWSiABPqupvEXtHxoTB6iCMKSKvDiJVVbf5\nHYsx0WBFTMYYY4KyOwhjjDFB2R2EMcaYoCxBGGOMCcoShDHGmKAsQRhjjAnKEoQxxpig/h/+OCDd\noelNEAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "84ef99b4-19b0-4c14-e768-dd650007af66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.8773 - acc: 0.3893\n",
            "Epoch 2/20\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.4253 - acc: 0.4427\n",
            "Epoch 3/20\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.2379 - acc: 0.4580\n",
            "Epoch 4/20\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.1625 - acc: 0.4656\n",
            "Epoch 5/20\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1127 - acc: 0.5267\n",
            "Epoch 6/20\n",
            "131/131 [==============================] - 0s 171us/step - loss: 1.0713 - acc: 0.5649\n",
            "Epoch 7/20\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0358 - acc: 0.5191\n",
            "Epoch 8/20\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0253 - acc: 0.5038\n",
            "Epoch 9/20\n",
            "131/131 [==============================] - 0s 138us/step - loss: 1.0091 - acc: 0.5725\n",
            "Epoch 10/20\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9874 - acc: 0.5954\n",
            "Epoch 11/20\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9690 - acc: 0.5649\n",
            "Epoch 12/20\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9538 - acc: 0.5954\n",
            "Epoch 13/20\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9525 - acc: 0.6031\n",
            "Epoch 14/20\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9582 - acc: 0.5344\n",
            "Epoch 15/20\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9379 - acc: 0.5954\n",
            "Epoch 16/20\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9345 - acc: 0.5649\n",
            "Epoch 17/20\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9266 - acc: 0.5802\n",
            "Epoch 18/20\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9194 - acc: 0.5802\n",
            "Epoch 19/20\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9175 - acc: 0.5878\n",
            "Epoch 20/20\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9167 - acc: 0.5878\n",
            "34/34 [==============================] - 0s 4ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "031ae466-808f-49d2-dc81-8287f19abda3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "3b2dcf74-5e22-46fe-c1b3-c548a8bfe7a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14705882352941177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merge_dataset_Network_classification_hystology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Merge_dataset_Network_classification_hystology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfLmMhRwN_ua",
        "colab_type": "text"
      },
      "source": [
        "Prove mettendo insieme i due dataset. Partendo da un unico dataset e dividendolo usando train_test_split.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYn4tlNNkCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "6a97d0ae-819a-44ad-b516-7f41377a50c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21MJpB7HPm8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tot_data = pd.concat([train_data, test_data], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IYan5zRsyi",
        "colab_type": "code",
        "outputId": "495135d1-3e5b-4810-f3d2-7e24133c76d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H421egATgdD",
        "colab_type": "code",
        "outputId": "10149185-5078-49d7-d362-280ab6978b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLhL0BxTi96",
        "colab_type": "code",
        "outputId": "c602e687-3e0e-4923-9666-85bd6979916a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tot_data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cisCrQrHVZfj",
        "colab_type": "code",
        "outputId": "ca7d4bba-717a-4f4f-ad6c-0d0de3050519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tot_data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51905.377962</td>\n",
              "      <td>66.288317</td>\n",
              "      <td>51847.748274</td>\n",
              "      <td>50.574214</td>\n",
              "      <td>0.649258</td>\n",
              "      <td>37.884620</td>\n",
              "      <td>0.821088</td>\n",
              "      <td>0.199752</td>\n",
              "      <td>63.135672</td>\n",
              "      <td>0.749090</td>\n",
              "      <td>10356.675894</td>\n",
              "      <td>41.525899</td>\n",
              "      <td>65.067279</td>\n",
              "      <td>55.325619</td>\n",
              "      <td>12.481889</td>\n",
              "      <td>1084.854684</td>\n",
              "      <td>5.949646</td>\n",
              "      <td>918.046673</td>\n",
              "      <td>5312.127441</td>\n",
              "      <td>0.122582</td>\n",
              "      <td>97.652454</td>\n",
              "      <td>0.050648</td>\n",
              "      <td>141.807349</td>\n",
              "      <td>0.124010</td>\n",
              "      <td>42.380287</td>\n",
              "      <td>163828.301666</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>33.190463</td>\n",
              "      <td>66.380925</td>\n",
              "      <td>4.331076</td>\n",
              "      <td>-475.448161</td>\n",
              "      <td>0.308299</td>\n",
              "      <td>0.994676</td>\n",
              "      <td>0.133643</td>\n",
              "      <td>8.029403</td>\n",
              "      <td>2.169232</td>\n",
              "      <td>0.424312</td>\n",
              "      <td>6.065116</td>\n",
              "      <td>0.967009</td>\n",
              "      <td>...</td>\n",
              "      <td>-75.0</td>\n",
              "      <td>17.777521</td>\n",
              "      <td>5.650502</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>17.478141</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.192540</td>\n",
              "      <td>1.399893</td>\n",
              "      <td>2288.112500</td>\n",
              "      <td>3.749302</td>\n",
              "      <td>793.234742</td>\n",
              "      <td>6675.859117</td>\n",
              "      <td>0.776723</td>\n",
              "      <td>4220.221337</td>\n",
              "      <td>0.654950</td>\n",
              "      <td>0.003806</td>\n",
              "      <td>4.209293</td>\n",
              "      <td>1049.544424</td>\n",
              "      <td>0.560736</td>\n",
              "      <td>34.869500</td>\n",
              "      <td>42116.076135</td>\n",
              "      <td>0.060025</td>\n",
              "      <td>0.517739</td>\n",
              "      <td>1145.238698</td>\n",
              "      <td>132.775769</td>\n",
              "      <td>42183.224231</td>\n",
              "      <td>540.316964</td>\n",
              "      <td>0.122035</td>\n",
              "      <td>36.510775</td>\n",
              "      <td>4.877236e+07</td>\n",
              "      <td>751.520796</td>\n",
              "      <td>0.747563</td>\n",
              "      <td>0.002453</td>\n",
              "      <td>5.741322</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>928.016789</td>\n",
              "      <td>1.153806</td>\n",
              "      <td>0.020920</td>\n",
              "      <td>1.306338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13432.502747</td>\n",
              "      <td>58.057539</td>\n",
              "      <td>13312.697411</td>\n",
              "      <td>48.354803</td>\n",
              "      <td>0.572236</td>\n",
              "      <td>18.135097</td>\n",
              "      <td>0.546338</td>\n",
              "      <td>0.356577</td>\n",
              "      <td>40.146103</td>\n",
              "      <td>0.375042</td>\n",
              "      <td>4747.006589</td>\n",
              "      <td>26.418066</td>\n",
              "      <td>32.760898</td>\n",
              "      <td>56.652510</td>\n",
              "      <td>60.615944</td>\n",
              "      <td>1076.589137</td>\n",
              "      <td>7.130906</td>\n",
              "      <td>595.667519</td>\n",
              "      <td>351.846858</td>\n",
              "      <td>0.307871</td>\n",
              "      <td>239.202712</td>\n",
              "      <td>0.126873</td>\n",
              "      <td>32.011715</td>\n",
              "      <td>0.031426</td>\n",
              "      <td>12.612334</td>\n",
              "      <td>41890.348882</td>\n",
              "      <td>0.001812</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>33.522040</td>\n",
              "      <td>67.044080</td>\n",
              "      <td>7.487967</td>\n",
              "      <td>-2829.110940</td>\n",
              "      <td>0.055759</td>\n",
              "      <td>0.985695</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>38.337756</td>\n",
              "      <td>3.408960</td>\n",
              "      <td>0.322015</td>\n",
              "      <td>23.246287</td>\n",
              "      <td>0.935189</td>\n",
              "      <td>...</td>\n",
              "      <td>-397.0</td>\n",
              "      <td>5.224099</td>\n",
              "      <td>-91.310969</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>65.432452</td>\n",
              "      <td>0.003524</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.230922</td>\n",
              "      <td>262.139314</td>\n",
              "      <td>1.567042</td>\n",
              "      <td>926.829706</td>\n",
              "      <td>3237.676584</td>\n",
              "      <td>0.908183</td>\n",
              "      <td>1777.017297</td>\n",
              "      <td>0.869059</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>5.072184</td>\n",
              "      <td>1044.275778</td>\n",
              "      <td>0.790377</td>\n",
              "      <td>84.615342</td>\n",
              "      <td>598.216508</td>\n",
              "      <td>0.033349</td>\n",
              "      <td>0.543736</td>\n",
              "      <td>843.878866</td>\n",
              "      <td>51.757732</td>\n",
              "      <td>607.367912</td>\n",
              "      <td>543.633876</td>\n",
              "      <td>0.330564</td>\n",
              "      <td>0.465530</td>\n",
              "      <td>8.137725e+05</td>\n",
              "      <td>763.567010</td>\n",
              "      <td>0.764879</td>\n",
              "      <td>0.006307</td>\n",
              "      <td>6.451087</td>\n",
              "      <td>0.004959</td>\n",
              "      <td>0.001680</td>\n",
              "      <td>2944.805484</td>\n",
              "      <td>2.266070</td>\n",
              "      <td>0.146173</td>\n",
              "      <td>0.253533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25843.872675</td>\n",
              "      <td>52.918217</td>\n",
              "      <td>25724.437234</td>\n",
              "      <td>39.406474</td>\n",
              "      <td>0.675497</td>\n",
              "      <td>28.487740</td>\n",
              "      <td>0.891907</td>\n",
              "      <td>0.242519</td>\n",
              "      <td>46.415213</td>\n",
              "      <td>0.722920</td>\n",
              "      <td>6238.658603</td>\n",
              "      <td>35.146929</td>\n",
              "      <td>47.180420</td>\n",
              "      <td>46.322906</td>\n",
              "      <td>55.064124</td>\n",
              "      <td>1131.900166</td>\n",
              "      <td>6.932158</td>\n",
              "      <td>844.783490</td>\n",
              "      <td>1023.136953</td>\n",
              "      <td>0.253452</td>\n",
              "      <td>192.059998</td>\n",
              "      <td>0.093605</td>\n",
              "      <td>57.855402</td>\n",
              "      <td>0.045979</td>\n",
              "      <td>22.286238</td>\n",
              "      <td>79266.271357</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>34.046184</td>\n",
              "      <td>68.092368</td>\n",
              "      <td>6.796695</td>\n",
              "      <td>-3133.512010</td>\n",
              "      <td>0.106695</td>\n",
              "      <td>0.984515</td>\n",
              "      <td>0.033042</td>\n",
              "      <td>32.375097</td>\n",
              "      <td>3.182144</td>\n",
              "      <td>0.362633</td>\n",
              "      <td>21.028555</td>\n",
              "      <td>0.936889</td>\n",
              "      <td>...</td>\n",
              "      <td>-363.0</td>\n",
              "      <td>5.387644</td>\n",
              "      <td>-67.724986</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>61.733697</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>0.085594</td>\n",
              "      <td>0.426431</td>\n",
              "      <td>626.125860</td>\n",
              "      <td>1.968083</td>\n",
              "      <td>906.575851</td>\n",
              "      <td>5266.417163</td>\n",
              "      <td>0.870954</td>\n",
              "      <td>2393.005796</td>\n",
              "      <td>0.809077</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>4.968901</td>\n",
              "      <td>1079.567623</td>\n",
              "      <td>0.718481</td>\n",
              "      <td>68.928494</td>\n",
              "      <td>3157.415098</td>\n",
              "      <td>0.036760</td>\n",
              "      <td>0.531612</td>\n",
              "      <td>1319.991542</td>\n",
              "      <td>91.276279</td>\n",
              "      <td>3170.626259</td>\n",
              "      <td>525.709823</td>\n",
              "      <td>0.275125</td>\n",
              "      <td>2.306913</td>\n",
              "      <td>4.377395e+06</td>\n",
              "      <td>732.877970</td>\n",
              "      <td>0.757392</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>6.294554</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>2146.780339</td>\n",
              "      <td>1.238883</td>\n",
              "      <td>0.152919</td>\n",
              "      <td>0.611772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22152.709032</td>\n",
              "      <td>46.635312</td>\n",
              "      <td>22099.016776</td>\n",
              "      <td>38.640414</td>\n",
              "      <td>0.733411</td>\n",
              "      <td>28.129382</td>\n",
              "      <td>0.754141</td>\n",
              "      <td>0.234970</td>\n",
              "      <td>39.274914</td>\n",
              "      <td>0.727978</td>\n",
              "      <td>5192.604222</td>\n",
              "      <td>29.140329</td>\n",
              "      <td>41.329017</td>\n",
              "      <td>46.543116</td>\n",
              "      <td>103.453079</td>\n",
              "      <td>1089.937565</td>\n",
              "      <td>7.205471</td>\n",
              "      <td>1087.724664</td>\n",
              "      <td>511.504912</td>\n",
              "      <td>0.336024</td>\n",
              "      <td>249.889273</td>\n",
              "      <td>0.140606</td>\n",
              "      <td>29.391158</td>\n",
              "      <td>0.026374</td>\n",
              "      <td>11.821883</td>\n",
              "      <td>42717.634566</td>\n",
              "      <td>0.001570</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>33.073644</td>\n",
              "      <td>66.147288</td>\n",
              "      <td>8.211500</td>\n",
              "      <td>-5741.187919</td>\n",
              "      <td>0.049759</td>\n",
              "      <td>0.975646</td>\n",
              "      <td>0.012491</td>\n",
              "      <td>58.811721</td>\n",
              "      <td>3.729477</td>\n",
              "      <td>0.290187</td>\n",
              "      <td>33.232290</td>\n",
              "      <td>0.913791</td>\n",
              "      <td>...</td>\n",
              "      <td>-528.5</td>\n",
              "      <td>2.714807</td>\n",
              "      <td>-102.779860</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>107.887272</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.054549</td>\n",
              "      <td>0.203221</td>\n",
              "      <td>370.362301</td>\n",
              "      <td>1.505960</td>\n",
              "      <td>924.243802</td>\n",
              "      <td>5447.401203</td>\n",
              "      <td>0.914390</td>\n",
              "      <td>1765.537152</td>\n",
              "      <td>0.877247</td>\n",
              "      <td>0.003439</td>\n",
              "      <td>5.297660</td>\n",
              "      <td>1044.150138</td>\n",
              "      <td>0.801694</td>\n",
              "      <td>100.015880</td>\n",
              "      <td>989.809275</td>\n",
              "      <td>0.028357</td>\n",
              "      <td>0.545916</td>\n",
              "      <td>1549.308668</td>\n",
              "      <td>80.477097</td>\n",
              "      <td>997.239605</td>\n",
              "      <td>527.497142</td>\n",
              "      <td>0.366856</td>\n",
              "      <td>0.671664</td>\n",
              "      <td>1.506307e+06</td>\n",
              "      <td>713.093023</td>\n",
              "      <td>0.767331</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>6.545703</td>\n",
              "      <td>0.003888</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>3136.738970</td>\n",
              "      <td>1.157976</td>\n",
              "      <td>0.351327</td>\n",
              "      <td>0.564313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>119385.805617</td>\n",
              "      <td>92.436320</td>\n",
              "      <td>119150.752851</td>\n",
              "      <td>75.629327</td>\n",
              "      <td>0.617296</td>\n",
              "      <td>47.029423</td>\n",
              "      <td>0.722141</td>\n",
              "      <td>0.159206</td>\n",
              "      <td>76.960070</td>\n",
              "      <td>0.621841</td>\n",
              "      <td>18969.476942</td>\n",
              "      <td>54.615019</td>\n",
              "      <td>83.612205</td>\n",
              "      <td>70.719598</td>\n",
              "      <td>67.305748</td>\n",
              "      <td>1445.561800</td>\n",
              "      <td>7.263383</td>\n",
              "      <td>4562.726560</td>\n",
              "      <td>3232.154350</td>\n",
              "      <td>0.241385</td>\n",
              "      <td>264.764085</td>\n",
              "      <td>0.109442</td>\n",
              "      <td>37.885083</td>\n",
              "      <td>0.028865</td>\n",
              "      <td>14.368248</td>\n",
              "      <td>66795.832602</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>37.463123</td>\n",
              "      <td>74.926246</td>\n",
              "      <td>7.979356</td>\n",
              "      <td>-5754.693763</td>\n",
              "      <td>0.063232</td>\n",
              "      <td>0.987519</td>\n",
              "      <td>0.014054</td>\n",
              "      <td>41.318562</td>\n",
              "      <td>3.487282</td>\n",
              "      <td>0.313702</td>\n",
              "      <td>24.584485</td>\n",
              "      <td>0.938581</td>\n",
              "      <td>...</td>\n",
              "      <td>-420.0</td>\n",
              "      <td>5.711559</td>\n",
              "      <td>-109.918927</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>71.446292</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>0.280825</td>\n",
              "      <td>2318.917381</td>\n",
              "      <td>1.665599</td>\n",
              "      <td>1227.793946</td>\n",
              "      <td>27327.614966</td>\n",
              "      <td>0.897233</td>\n",
              "      <td>2548.311647</td>\n",
              "      <td>0.851946</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>5.115966</td>\n",
              "      <td>1400.205550</td>\n",
              "      <td>0.767543</td>\n",
              "      <td>82.273203</td>\n",
              "      <td>7062.537600</td>\n",
              "      <td>0.034543</td>\n",
              "      <td>0.445967</td>\n",
              "      <td>4857.474477</td>\n",
              "      <td>376.237055</td>\n",
              "      <td>7077.188671</td>\n",
              "      <td>703.114744</td>\n",
              "      <td>0.261255</td>\n",
              "      <td>3.905480</td>\n",
              "      <td>1.289007e+07</td>\n",
              "      <td>1048.831160</td>\n",
              "      <td>0.694399</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>6.849399</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>3368.646263</td>\n",
              "      <td>0.390430</td>\n",
              "      <td>0.132602</td>\n",
              "      <td>1.804351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6592.266962</td>\n",
              "      <td>33.622119</td>\n",
              "      <td>6525.211805</td>\n",
              "      <td>26.483717</td>\n",
              "      <td>0.737649</td>\n",
              "      <td>16.326688</td>\n",
              "      <td>0.854215</td>\n",
              "      <td>0.350835</td>\n",
              "      <td>27.010370</td>\n",
              "      <td>0.616480</td>\n",
              "      <td>2289.271748</td>\n",
              "      <td>22.622792</td>\n",
              "      <td>32.335274</td>\n",
              "      <td>29.352647</td>\n",
              "      <td>21.910855</td>\n",
              "      <td>602.049910</td>\n",
              "      <td>6.617736</td>\n",
              "      <td>193.040506</td>\n",
              "      <td>462.455335</td>\n",
              "      <td>0.198303</td>\n",
              "      <td>78.741854</td>\n",
              "      <td>0.069816</td>\n",
              "      <td>94.932731</td>\n",
              "      <td>0.140683</td>\n",
              "      <td>35.081663</td>\n",
              "      <td>67119.461845</td>\n",
              "      <td>0.001239</td>\n",
              "      <td>0.003445</td>\n",
              "      <td>24.859564</td>\n",
              "      <td>49.719128</td>\n",
              "      <td>5.549785</td>\n",
              "      <td>-810.250238</td>\n",
              "      <td>0.148434</td>\n",
              "      <td>0.986138</td>\n",
              "      <td>0.062119</td>\n",
              "      <td>13.257631</td>\n",
              "      <td>2.653226</td>\n",
              "      <td>0.384951</td>\n",
              "      <td>8.884155</td>\n",
              "      <td>0.941150</td>\n",
              "      <td>...</td>\n",
              "      <td>-224.0</td>\n",
              "      <td>7.373263</td>\n",
              "      <td>-38.508861</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>26.164917</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.122205</td>\n",
              "      <td>0.816527</td>\n",
              "      <td>252.082403</td>\n",
              "      <td>2.689565</td>\n",
              "      <td>450.205653</td>\n",
              "      <td>1334.853146</td>\n",
              "      <td>0.829559</td>\n",
              "      <td>1752.475726</td>\n",
              "      <td>0.740910</td>\n",
              "      <td>0.006731</td>\n",
              "      <td>4.573485</td>\n",
              "      <td>567.808195</td>\n",
              "      <td>0.646468</td>\n",
              "      <td>30.944145</td>\n",
              "      <td>1838.790622</td>\n",
              "      <td>0.057837</td>\n",
              "      <td>0.487234</td>\n",
              "      <td>295.264026</td>\n",
              "      <td>35.049505</td>\n",
              "      <td>1859.608911</td>\n",
              "      <td>263.012843</td>\n",
              "      <td>0.219168</td>\n",
              "      <td>2.665039</td>\n",
              "      <td>1.307177e+06</td>\n",
              "      <td>385.825083</td>\n",
              "      <td>0.724813</td>\n",
              "      <td>0.007286</td>\n",
              "      <td>5.736757</td>\n",
              "      <td>0.004595</td>\n",
              "      <td>0.002543</td>\n",
              "      <td>645.520499</td>\n",
              "      <td>2.322894</td>\n",
              "      <td>0.085026</td>\n",
              "      <td>0.359230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3010.031479</td>\n",
              "      <td>29.286452</td>\n",
              "      <td>2976.851594</td>\n",
              "      <td>24.737747</td>\n",
              "      <td>0.653423</td>\n",
              "      <td>12.466119</td>\n",
              "      <td>0.668128</td>\n",
              "      <td>0.514483</td>\n",
              "      <td>23.150905</td>\n",
              "      <td>0.503931</td>\n",
              "      <td>1531.539258</td>\n",
              "      <td>16.527972</td>\n",
              "      <td>27.610305</td>\n",
              "      <td>26.839660</td>\n",
              "      <td>62.881425</td>\n",
              "      <td>869.396436</td>\n",
              "      <td>6.867420</td>\n",
              "      <td>345.579802</td>\n",
              "      <td>201.369901</td>\n",
              "      <td>0.339958</td>\n",
              "      <td>195.491748</td>\n",
              "      <td>0.136863</td>\n",
              "      <td>53.888713</td>\n",
              "      <td>0.047515</td>\n",
              "      <td>26.381284</td>\n",
              "      <td>65473.407525</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>29.963303</td>\n",
              "      <td>59.926606</td>\n",
              "      <td>7.422073</td>\n",
              "      <td>-2502.214292</td>\n",
              "      <td>0.108583</td>\n",
              "      <td>0.975065</td>\n",
              "      <td>0.024723</td>\n",
              "      <td>38.641415</td>\n",
              "      <td>3.546610</td>\n",
              "      <td>0.281848</td>\n",
              "      <td>21.558304</td>\n",
              "      <td>0.910929</td>\n",
              "      <td>...</td>\n",
              "      <td>-471.6</td>\n",
              "      <td>2.851107</td>\n",
              "      <td>-153.137426</td>\n",
              "      <td>0.002704</td>\n",
              "      <td>64.774609</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.056870</td>\n",
              "      <td>0.377849</td>\n",
              "      <td>120.266506</td>\n",
              "      <td>1.809069</td>\n",
              "      <td>689.951676</td>\n",
              "      <td>1625.228381</td>\n",
              "      <td>0.897161</td>\n",
              "      <td>1774.442365</td>\n",
              "      <td>0.836740</td>\n",
              "      <td>0.003555</td>\n",
              "      <td>5.118256</td>\n",
              "      <td>808.826968</td>\n",
              "      <td>0.768649</td>\n",
              "      <td>54.608361</td>\n",
              "      <td>364.145936</td>\n",
              "      <td>0.038857</td>\n",
              "      <td>0.528923</td>\n",
              "      <td>510.939959</td>\n",
              "      <td>37.536232</td>\n",
              "      <td>370.978261</td>\n",
              "      <td>407.183237</td>\n",
              "      <td>0.382574</td>\n",
              "      <td>0.307757</td>\n",
              "      <td>4.555961e+05</td>\n",
              "      <td>570.265010</td>\n",
              "      <td>0.755454</td>\n",
              "      <td>0.004471</td>\n",
              "      <td>6.087835</td>\n",
              "      <td>0.003856</td>\n",
              "      <td>0.002895</td>\n",
              "      <td>1472.717438</td>\n",
              "      <td>2.379395</td>\n",
              "      <td>0.295357</td>\n",
              "      <td>0.263438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>8683.195759</td>\n",
              "      <td>49.195074</td>\n",
              "      <td>8611.074213</td>\n",
              "      <td>46.624467</td>\n",
              "      <td>0.641262</td>\n",
              "      <td>14.348828</td>\n",
              "      <td>0.438028</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>31.523446</td>\n",
              "      <td>0.307753</td>\n",
              "      <td>3168.257409</td>\n",
              "      <td>20.422841</td>\n",
              "      <td>34.268292</td>\n",
              "      <td>37.102042</td>\n",
              "      <td>153.099954</td>\n",
              "      <td>738.203185</td>\n",
              "      <td>7.378490</td>\n",
              "      <td>847.045579</td>\n",
              "      <td>105.449204</td>\n",
              "      <td>0.443610</td>\n",
              "      <td>264.767425</td>\n",
              "      <td>0.232577</td>\n",
              "      <td>16.744097</td>\n",
              "      <td>0.124622</td>\n",
              "      <td>8.324220</td>\n",
              "      <td>22337.763317</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>25.523890</td>\n",
              "      <td>51.047781</td>\n",
              "      <td>9.676885</td>\n",
              "      <td>-2020.290555</td>\n",
              "      <td>0.026769</td>\n",
              "      <td>0.963103</td>\n",
              "      <td>0.002806</td>\n",
              "      <td>96.669264</td>\n",
              "      <td>4.273414</td>\n",
              "      <td>0.174092</td>\n",
              "      <td>40.625092</td>\n",
              "      <td>0.880158</td>\n",
              "      <td>...</td>\n",
              "      <td>-829.0</td>\n",
              "      <td>1.626201</td>\n",
              "      <td>-408.424767</td>\n",
              "      <td>0.008992</td>\n",
              "      <td>146.665900</td>\n",
              "      <td>0.009519</td>\n",
              "      <td>0.027008</td>\n",
              "      <td>0.124834</td>\n",
              "      <td>91.216703</td>\n",
              "      <td>1.292427</td>\n",
              "      <td>654.187277</td>\n",
              "      <td>2977.335607</td>\n",
              "      <td>0.950812</td>\n",
              "      <td>1057.363734</td>\n",
              "      <td>0.926858</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>5.643852</td>\n",
              "      <td>706.485685</td>\n",
              "      <td>0.880887</td>\n",
              "      <td>108.879134</td>\n",
              "      <td>36.053791</td>\n",
              "      <td>0.026627</td>\n",
              "      <td>0.500224</td>\n",
              "      <td>931.916801</td>\n",
              "      <td>49.605475</td>\n",
              "      <td>39.875470</td>\n",
              "      <td>425.432967</td>\n",
              "      <td>0.511532</td>\n",
              "      <td>0.172971</td>\n",
              "      <td>5.926173e+04</td>\n",
              "      <td>592.454106</td>\n",
              "      <td>0.735138</td>\n",
              "      <td>0.007341</td>\n",
              "      <td>6.687140</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>4040.182819</td>\n",
              "      <td>2.076410</td>\n",
              "      <td>0.594629</td>\n",
              "      <td>0.392208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>25939.914844</td>\n",
              "      <td>57.067095</td>\n",
              "      <td>25839.679802</td>\n",
              "      <td>51.745408</td>\n",
              "      <td>0.692948</td>\n",
              "      <td>26.633569</td>\n",
              "      <td>0.617498</td>\n",
              "      <td>0.236059</td>\n",
              "      <td>40.700499</td>\n",
              "      <td>0.514704</td>\n",
              "      <td>6099.697734</td>\n",
              "      <td>31.952700</td>\n",
              "      <td>41.839289</td>\n",
              "      <td>56.857818</td>\n",
              "      <td>32.995425</td>\n",
              "      <td>1449.528401</td>\n",
              "      <td>6.634924</td>\n",
              "      <td>576.773713</td>\n",
              "      <td>2560.111765</td>\n",
              "      <td>0.161550</td>\n",
              "      <td>157.459073</td>\n",
              "      <td>0.053012</td>\n",
              "      <td>163.638419</td>\n",
              "      <td>0.102321</td>\n",
              "      <td>53.465611</td>\n",
              "      <td>264022.398805</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>38.286575</td>\n",
              "      <td>76.573150</td>\n",
              "      <td>5.054850</td>\n",
              "      <td>-2238.871210</td>\n",
              "      <td>0.270459</td>\n",
              "      <td>0.991175</td>\n",
              "      <td>0.109092</td>\n",
              "      <td>17.532514</td>\n",
              "      <td>2.535238</td>\n",
              "      <td>0.343561</td>\n",
              "      <td>13.345420</td>\n",
              "      <td>0.959707</td>\n",
              "      <td>...</td>\n",
              "      <td>-218.0</td>\n",
              "      <td>11.861095</td>\n",
              "      <td>-47.050460</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>45.643158</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.141440</td>\n",
              "      <td>1.736298</td>\n",
              "      <td>985.371574</td>\n",
              "      <td>4.294691</td>\n",
              "      <td>989.771419</td>\n",
              "      <td>3814.874403</td>\n",
              "      <td>0.768468</td>\n",
              "      <td>6660.476094</td>\n",
              "      <td>0.634757</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>4.836890</td>\n",
              "      <td>1358.562247</td>\n",
              "      <td>0.549125</td>\n",
              "      <td>64.574608</td>\n",
              "      <td>13539.270699</td>\n",
              "      <td>0.045526</td>\n",
              "      <td>0.520480</td>\n",
              "      <td>994.636839</td>\n",
              "      <td>87.001047</td>\n",
              "      <td>13571.684982</td>\n",
              "      <td>678.036010</td>\n",
              "      <td>0.175643</td>\n",
              "      <td>8.433580</td>\n",
              "      <td>2.185777e+07</td>\n",
              "      <td>955.398744</td>\n",
              "      <td>0.749551</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>6.114671</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>1541.581036</td>\n",
              "      <td>2.194039</td>\n",
              "      <td>0.059377</td>\n",
              "      <td>0.658985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>73401.852784</td>\n",
              "      <td>73.105484</td>\n",
              "      <td>73287.411984</td>\n",
              "      <td>64.560114</td>\n",
              "      <td>0.791968</td>\n",
              "      <td>36.748716</td>\n",
              "      <td>0.681059</td>\n",
              "      <td>0.145915</td>\n",
              "      <td>52.860788</td>\n",
              "      <td>0.569217</td>\n",
              "      <td>10693.735802</td>\n",
              "      <td>43.969247</td>\n",
              "      <td>65.591520</td>\n",
              "      <td>72.942237</td>\n",
              "      <td>81.163824</td>\n",
              "      <td>991.869458</td>\n",
              "      <td>7.531933</td>\n",
              "      <td>2211.526781</td>\n",
              "      <td>3370.775879</td>\n",
              "      <td>0.190389</td>\n",
              "      <td>124.723669</td>\n",
              "      <td>0.071833</td>\n",
              "      <td>98.566310</td>\n",
              "      <td>0.094550</td>\n",
              "      <td>39.657307</td>\n",
              "      <td>123234.303667</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>30.846682</td>\n",
              "      <td>61.693363</td>\n",
              "      <td>7.183740</td>\n",
              "      <td>-5768.083577</td>\n",
              "      <td>0.140669</td>\n",
              "      <td>0.994400</td>\n",
              "      <td>0.037231</td>\n",
              "      <td>27.783111</td>\n",
              "      <td>3.108671</td>\n",
              "      <td>0.338089</td>\n",
              "      <td>18.042103</td>\n",
              "      <td>0.960845</td>\n",
              "      <td>...</td>\n",
              "      <td>-517.0</td>\n",
              "      <td>4.023106</td>\n",
              "      <td>-108.609186</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>93.372993</td>\n",
              "      <td>0.003554</td>\n",
              "      <td>0.069731</td>\n",
              "      <td>0.936258</td>\n",
              "      <td>1601.793815</td>\n",
              "      <td>2.772571</td>\n",
              "      <td>729.132148</td>\n",
              "      <td>15360.888892</td>\n",
              "      <td>0.842243</td>\n",
              "      <td>3104.316398</td>\n",
              "      <td>0.743261</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>5.455417</td>\n",
              "      <td>914.855419</td>\n",
              "      <td>0.668099</td>\n",
              "      <td>100.361763</td>\n",
              "      <td>13140.774613</td>\n",
              "      <td>0.029868</td>\n",
              "      <td>0.422933</td>\n",
              "      <td>2776.555217</td>\n",
              "      <td>196.083016</td>\n",
              "      <td>13162.766641</td>\n",
              "      <td>412.848389</td>\n",
              "      <td>0.213239</td>\n",
              "      <td>10.418845</td>\n",
              "      <td>1.670978e+07</td>\n",
              "      <td>618.251790</td>\n",
              "      <td>0.675352</td>\n",
              "      <td>0.005772</td>\n",
              "      <td>7.026210</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>4240.096417</td>\n",
              "      <td>1.189118</td>\n",
              "      <td>0.064501</td>\n",
              "      <td>0.895047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>165 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "0    51905.377962          66.288317  ...    0.020920  1.306338\n",
              "1    13432.502747          58.057539  ...    0.146173  0.253533\n",
              "2    25843.872675          52.918217  ...    0.152919  0.611772\n",
              "3    22152.709032          46.635312  ...    0.351327  0.564313\n",
              "4   119385.805617          92.436320  ...    0.132602  1.804351\n",
              "..            ...                ...  ...         ...       ...\n",
              "29    6592.266962          33.622119  ...    0.085026  0.359230\n",
              "30    3010.031479          29.286452  ...    0.295357  0.263438\n",
              "31    8683.195759          49.195074  ...    0.594629  0.392208\n",
              "32   25939.914844          57.067095  ...    0.059377  0.658985\n",
              "33   73401.852784          73.105484  ...    0.064501  0.895047\n",
              "\n",
              "[165 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNh5eL2PVIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tot_label = pd.concat([train_labels, test_labels], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcydh4qRVV6k",
        "colab_type": "code",
        "outputId": "4853be6d-ecaf-43fc-b3f3-6500b3cb6af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tot_label.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cno5GmYCUynr",
        "colab_type": "text"
      },
      "source": [
        "##Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFVj6DbKUj9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFCCTjE6JrQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(tot_data, tot_label,\n",
        "                                                  stratify=tot_label, test_size=0.25, random_state=1) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "a9ceda76-c250-4bef-d3cb-30f638e76f7b"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZF8nUPWbrsR",
        "colab_type": "code",
        "outputId": "5a7c740a-7240-45d9-b567-8fdd06fa84e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_labels_dec.count(0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "0b41b280-c47f-4835-a036-664c97e2f319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "a4a0dbf8-679d-40ff-e4aa-22356ec81ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(5, activation='relu', input_shape=(9,)))\n",
        "\n",
        "#  model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.4, nesterov=True)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "ab6f2a80-9da3-4c3a-be8c-c5fd174f00f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "bb14d80a-fc55-40d1-aa0c-5669d8a49f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  36  38\n",
            "  39  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
            "  59  61  62  63  64  65  66  67  69  70  71  72  73  74  76  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 120\n",
            " 121 122] TEST: [ 19  35  37  41  56  60  68  75  77  78  94 110 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38\n",
            "  39  41  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  77  78\n",
            "  79  80  81  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120] TEST: [ 11  13  33  40  42  51  70  82  83  84 105 121 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37\n",
            "  39  40  41  42  43  45  46  47  48  50  51  52  54  55  56  57  58  59\n",
            "  60  61  62  64  65  67  68  69  70  71  72  73  75  76  77  78  79  80\n",
            "  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  99 100\n",
            " 101 102 103 104 105 106 107 109 110 112 113 114 115 116 117 118 119 120\n",
            " 121 122] TEST: [  9  36  38  44  49  53  63  66  74  86  98 108 111]\n",
            "TRAIN: [  0   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
            "  60  61  62  63  66  67  68  69  70  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 121 122] TEST: [  1   7  21  23  39  55  64  65  71  90 104 120]\n",
            "TRAIN: [  0   1   2   4   6   7   8   9  11  12  13  14  15  17  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  48  49  50  51  52  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
            "  99 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119\n",
            " 120 121 122] TEST: [  3   5  10  16  46  47  59  69  88 100 107 114]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  19  20\n",
            "  21  22  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  86  87  88  89  90  91  92  94  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 114 115 118 119\n",
            " 120 121 122] TEST: [  0  17  18  29  34  62  85  93  95 113 116 117]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  26  28  29  30  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  81  82  83  84  85  86  87  88  90  92  93  94  95  96  97  98  99\n",
            " 100 101 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [  6  25  27  31  32  48  54  80  89  91 102 112]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60\n",
            "  61  62  63  64  65  66  68  69  70  71  72  73  74  75  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 102 103 104 105 106 107 108 110 111 112 113 114 116 117 118 119\n",
            " 120 121 122] TEST: [  2  15  20  24  30  43  57  67  76 101 109 115]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  13  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  80  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 119\n",
            " 120 121 122] TEST: [  4   8  14  28  50  61  72  79  81  92 106 118]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
            "  19  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  46  47  48  49  50  51  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  98 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [ 12  22  26  45  52  58  73  87  96  97  99 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "1a40be7c-845a-4338-eb70-4f551e3d7046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 20\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "110/110 [==============================] - 0s 2ms/step - loss: 1.5017 - acc: 0.4000 - val_loss: 1.4330 - val_acc: 0.3077\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 178us/step - loss: 1.2060 - acc: 0.4182 - val_loss: 1.3452 - val_acc: 0.3846\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 177us/step - loss: 1.0951 - acc: 0.4182 - val_loss: 1.3219 - val_acc: 0.3077\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 222us/step - loss: 1.0418 - acc: 0.4182 - val_loss: 1.2875 - val_acc: 0.2308\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 148us/step - loss: 1.0055 - acc: 0.4000 - val_loss: 1.2787 - val_acc: 0.3077\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 165us/step - loss: 0.9854 - acc: 0.4182 - val_loss: 1.2906 - val_acc: 0.3077\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 165us/step - loss: 0.9700 - acc: 0.4273 - val_loss: 1.3012 - val_acc: 0.3846\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 158us/step - loss: 0.9665 - acc: 0.4273 - val_loss: 1.3079 - val_acc: 0.3846\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 175us/step - loss: 0.9564 - acc: 0.4273 - val_loss: 1.3222 - val_acc: 0.3846\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 188us/step - loss: 0.9394 - acc: 0.4636 - val_loss: 1.3256 - val_acc: 0.4615\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 175us/step - loss: 0.9368 - acc: 0.4545 - val_loss: 1.3215 - val_acc: 0.4615\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 159us/step - loss: 0.9295 - acc: 0.4818 - val_loss: 1.3112 - val_acc: 0.3846\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 176us/step - loss: 0.9262 - acc: 0.4909 - val_loss: 1.3081 - val_acc: 0.3846\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 196us/step - loss: 0.9166 - acc: 0.5000 - val_loss: 1.3194 - val_acc: 0.3846\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 158us/step - loss: 0.9142 - acc: 0.4818 - val_loss: 1.3193 - val_acc: 0.4615\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 159us/step - loss: 0.9155 - acc: 0.4909 - val_loss: 1.3274 - val_acc: 0.4615\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 165us/step - loss: 0.9078 - acc: 0.5273 - val_loss: 1.3210 - val_acc: 0.4615\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 0s 167us/step - loss: 0.8990 - acc: 0.5182 - val_loss: 1.3297 - val_acc: 0.4615\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 0s 198us/step - loss: 0.8896 - acc: 0.5364 - val_loss: 1.3407 - val_acc: 0.4615\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 0s 168us/step - loss: 0.8997 - acc: 0.5273 - val_loss: 1.3254 - val_acc: 0.4615\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 0s 1ms/step - loss: 1.8970 - acc: 0.3091 - val_loss: 1.7591 - val_acc: 0.3846\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 168us/step - loss: 1.3526 - acc: 0.3727 - val_loss: 1.3447 - val_acc: 0.3846\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 168us/step - loss: 1.2212 - acc: 0.4000 - val_loss: 1.1788 - val_acc: 0.4615\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 164us/step - loss: 1.1533 - acc: 0.4273 - val_loss: 1.0757 - val_acc: 0.3846\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 163us/step - loss: 1.1026 - acc: 0.4273 - val_loss: 1.0144 - val_acc: 0.4615\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 154us/step - loss: 1.0821 - acc: 0.4273 - val_loss: 0.9409 - val_acc: 0.4615\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 167us/step - loss: 1.0484 - acc: 0.4455 - val_loss: 0.9142 - val_acc: 0.5385\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 172us/step - loss: 1.0275 - acc: 0.4455 - val_loss: 0.8896 - val_acc: 0.4615\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 161us/step - loss: 1.0103 - acc: 0.4636 - val_loss: 0.8973 - val_acc: 0.5385\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 192us/step - loss: 1.0047 - acc: 0.4818 - val_loss: 0.8627 - val_acc: 0.5385\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 170us/step - loss: 0.9947 - acc: 0.4545 - val_loss: 0.8593 - val_acc: 0.6154\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 160us/step - loss: 0.9832 - acc: 0.4727 - val_loss: 0.8410 - val_acc: 0.5385\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 164us/step - loss: 0.9774 - acc: 0.4727 - val_loss: 0.8326 - val_acc: 0.5385\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 200us/step - loss: 0.9845 - acc: 0.4909 - val_loss: 0.8242 - val_acc: 0.5385\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 176us/step - loss: 0.9621 - acc: 0.4727 - val_loss: 0.8102 - val_acc: 0.5385\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 157us/step - loss: 0.9472 - acc: 0.4909 - val_loss: 0.8396 - val_acc: 0.5385\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 179us/step - loss: 0.9552 - acc: 0.4727 - val_loss: 0.8174 - val_acc: 0.5385\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 0s 174us/step - loss: 0.9424 - acc: 0.4727 - val_loss: 0.7948 - val_acc: 0.6923\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 0s 178us/step - loss: 0.9401 - acc: 0.4727 - val_loss: 0.7983 - val_acc: 0.6154\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 0s 172us/step - loss: 0.9429 - acc: 0.4818 - val_loss: 0.7924 - val_acc: 0.6154\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 0s 2ms/step - loss: 1.3962 - acc: 0.4000 - val_loss: 1.3858 - val_acc: 0.4615\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 156us/step - loss: 1.2248 - acc: 0.4182 - val_loss: 1.2372 - val_acc: 0.3846\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 164us/step - loss: 1.1181 - acc: 0.4727 - val_loss: 1.1566 - val_acc: 0.3846\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 199us/step - loss: 1.0603 - acc: 0.5000 - val_loss: 1.1087 - val_acc: 0.4615\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 171us/step - loss: 1.0247 - acc: 0.5091 - val_loss: 1.0712 - val_acc: 0.5385\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 157us/step - loss: 1.0003 - acc: 0.5273 - val_loss: 1.0737 - val_acc: 0.3846\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 209us/step - loss: 0.9813 - acc: 0.5273 - val_loss: 1.0515 - val_acc: 0.4615\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 198us/step - loss: 0.9707 - acc: 0.5455 - val_loss: 1.0623 - val_acc: 0.3846\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 176us/step - loss: 0.9614 - acc: 0.5455 - val_loss: 1.0481 - val_acc: 0.5385\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 160us/step - loss: 0.9605 - acc: 0.5455 - val_loss: 1.0444 - val_acc: 0.5385\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 166us/step - loss: 0.9530 - acc: 0.5455 - val_loss: 1.0240 - val_acc: 0.5385\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 163us/step - loss: 0.9419 - acc: 0.5636 - val_loss: 1.0087 - val_acc: 0.4615\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 189us/step - loss: 0.9407 - acc: 0.5636 - val_loss: 1.0047 - val_acc: 0.5385\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 165us/step - loss: 0.9404 - acc: 0.5545 - val_loss: 1.0011 - val_acc: 0.4615\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 177us/step - loss: 0.9324 - acc: 0.5818 - val_loss: 1.0109 - val_acc: 0.4615\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 190us/step - loss: 0.9274 - acc: 0.5818 - val_loss: 1.0076 - val_acc: 0.5385\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 153us/step - loss: 0.9226 - acc: 0.5909 - val_loss: 0.9974 - val_acc: 0.4615\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 0s 203us/step - loss: 0.9238 - acc: 0.5636 - val_loss: 1.0095 - val_acc: 0.5385\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 0s 168us/step - loss: 0.9207 - acc: 0.5727 - val_loss: 1.0038 - val_acc: 0.5385\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 0s 166us/step - loss: 0.9102 - acc: 0.6000 - val_loss: 0.9875 - val_acc: 0.4615\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 2.1234 - acc: 0.2973 - val_loss: 1.3088 - val_acc: 0.1667\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 174us/step - loss: 1.2512 - acc: 0.3243 - val_loss: 1.3402 - val_acc: 0.1667\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 166us/step - loss: 1.1516 - acc: 0.3964 - val_loss: 1.3198 - val_acc: 0.2500\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 171us/step - loss: 1.1032 - acc: 0.3784 - val_loss: 1.3265 - val_acc: 0.2500\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 167us/step - loss: 1.0910 - acc: 0.3964 - val_loss: 1.3308 - val_acc: 0.2500\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 164us/step - loss: 1.0599 - acc: 0.4324 - val_loss: 1.2583 - val_acc: 0.3333\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 1.0357 - acc: 0.4234 - val_loss: 1.2165 - val_acc: 0.2500\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 163us/step - loss: 1.0501 - acc: 0.4685 - val_loss: 1.2125 - val_acc: 0.2500\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 1.0231 - acc: 0.4414 - val_loss: 1.2535 - val_acc: 0.2500\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 168us/step - loss: 1.0176 - acc: 0.5045 - val_loss: 1.2431 - val_acc: 0.2500\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 191us/step - loss: 1.0052 - acc: 0.5135 - val_loss: 1.1986 - val_acc: 0.2500\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 183us/step - loss: 1.0014 - acc: 0.5045 - val_loss: 1.1924 - val_acc: 0.2500\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 166us/step - loss: 0.9950 - acc: 0.5135 - val_loss: 1.2218 - val_acc: 0.2500\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 160us/step - loss: 0.9818 - acc: 0.5225 - val_loss: 1.1856 - val_acc: 0.4167\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 161us/step - loss: 0.9670 - acc: 0.5315 - val_loss: 1.2390 - val_acc: 0.3333\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.9715 - acc: 0.5315 - val_loss: 1.2353 - val_acc: 0.3333\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 161us/step - loss: 0.9559 - acc: 0.5225 - val_loss: 1.1887 - val_acc: 0.3333\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 172us/step - loss: 0.9710 - acc: 0.5135 - val_loss: 1.1838 - val_acc: 0.4167\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 168us/step - loss: 0.9530 - acc: 0.5495 - val_loss: 1.1756 - val_acc: 0.4167\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.9337 - acc: 0.5225 - val_loss: 1.2521 - val_acc: 0.3333\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 2ms/step - loss: 1.7366 - acc: 0.2252 - val_loss: 1.9298 - val_acc: 0.1667\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 178us/step - loss: 1.2531 - acc: 0.3784 - val_loss: 1.4414 - val_acc: 0.2500\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 173us/step - loss: 1.0923 - acc: 0.3964 - val_loss: 1.2146 - val_acc: 0.2500\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 180us/step - loss: 1.0265 - acc: 0.4144 - val_loss: 1.0940 - val_acc: 0.2500\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 180us/step - loss: 1.0006 - acc: 0.4685 - val_loss: 1.0232 - val_acc: 0.2500\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 169us/step - loss: 0.9710 - acc: 0.4685 - val_loss: 0.9687 - val_acc: 0.3333\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 160us/step - loss: 0.9466 - acc: 0.4955 - val_loss: 0.9438 - val_acc: 0.4167\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 228us/step - loss: 0.9381 - acc: 0.5225 - val_loss: 0.9102 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 158us/step - loss: 0.9254 - acc: 0.5586 - val_loss: 0.8921 - val_acc: 0.5833\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 169us/step - loss: 0.9197 - acc: 0.5405 - val_loss: 0.8837 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.9122 - acc: 0.5495 - val_loss: 0.8792 - val_acc: 0.4167\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.9043 - acc: 0.5495 - val_loss: 0.8601 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.9020 - acc: 0.5495 - val_loss: 0.8536 - val_acc: 0.5833\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 196us/step - loss: 0.8958 - acc: 0.5405 - val_loss: 0.8383 - val_acc: 0.6667\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 161us/step - loss: 0.8920 - acc: 0.5766 - val_loss: 0.8417 - val_acc: 0.5833\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 184us/step - loss: 0.8818 - acc: 0.5676 - val_loss: 0.8487 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.8883 - acc: 0.5495 - val_loss: 0.8445 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.8768 - acc: 0.6036 - val_loss: 0.8470 - val_acc: 0.5833\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 170us/step - loss: 0.8763 - acc: 0.5676 - val_loss: 0.8256 - val_acc: 0.5833\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.8621 - acc: 0.5676 - val_loss: 0.8204 - val_acc: 0.5833\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 1.7704 - acc: 0.3784 - val_loss: 1.1015 - val_acc: 0.5833\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 164us/step - loss: 1.3319 - acc: 0.4414 - val_loss: 0.9197 - val_acc: 0.6667\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 166us/step - loss: 1.1901 - acc: 0.4595 - val_loss: 0.8469 - val_acc: 0.7500\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 160us/step - loss: 1.1058 - acc: 0.4595 - val_loss: 0.8043 - val_acc: 0.6667\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 160us/step - loss: 1.0589 - acc: 0.5135 - val_loss: 0.7877 - val_acc: 0.6667\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 154us/step - loss: 1.0401 - acc: 0.5315 - val_loss: 0.7931 - val_acc: 0.7500\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 176us/step - loss: 1.0136 - acc: 0.4955 - val_loss: 0.7737 - val_acc: 0.7500\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 164us/step - loss: 0.9948 - acc: 0.5586 - val_loss: 0.7763 - val_acc: 0.7500\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 157us/step - loss: 0.9879 - acc: 0.5586 - val_loss: 0.7746 - val_acc: 0.7500\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 0.9675 - acc: 0.5946 - val_loss: 0.8131 - val_acc: 0.7500\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 165us/step - loss: 0.9536 - acc: 0.5856 - val_loss: 0.7965 - val_acc: 0.8333\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 161us/step - loss: 0.9546 - acc: 0.5856 - val_loss: 0.7938 - val_acc: 0.8333\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 0.9591 - acc: 0.5586 - val_loss: 0.8116 - val_acc: 0.7500\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 167us/step - loss: 0.9319 - acc: 0.6036 - val_loss: 0.8240 - val_acc: 0.6667\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 161us/step - loss: 0.9319 - acc: 0.5766 - val_loss: 0.8260 - val_acc: 0.6667\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 163us/step - loss: 0.9256 - acc: 0.5856 - val_loss: 0.8402 - val_acc: 0.6667\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 158us/step - loss: 0.9190 - acc: 0.5856 - val_loss: 0.7861 - val_acc: 0.7500\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 194us/step - loss: 0.9088 - acc: 0.5946 - val_loss: 0.8091 - val_acc: 0.5833\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.9077 - acc: 0.5946 - val_loss: 0.8041 - val_acc: 0.6667\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.9071 - acc: 0.6126 - val_loss: 0.7816 - val_acc: 0.7500\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 1.5993 - acc: 0.3874 - val_loss: 0.9036 - val_acc: 0.5833\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 186us/step - loss: 1.2535 - acc: 0.4685 - val_loss: 0.8743 - val_acc: 0.5833\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 164us/step - loss: 1.1719 - acc: 0.4775 - val_loss: 0.8825 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 169us/step - loss: 1.1221 - acc: 0.4505 - val_loss: 0.8973 - val_acc: 0.4167\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 287us/step - loss: 1.0889 - acc: 0.5315 - val_loss: 0.9045 - val_acc: 0.4167\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 165us/step - loss: 1.0735 - acc: 0.5135 - val_loss: 0.9107 - val_acc: 0.4167\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 170us/step - loss: 1.0554 - acc: 0.5225 - val_loss: 0.9117 - val_acc: 0.4167\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 157us/step - loss: 1.0433 - acc: 0.5405 - val_loss: 0.9136 - val_acc: 0.4167\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 187us/step - loss: 1.0272 - acc: 0.5676 - val_loss: 0.9162 - val_acc: 0.3333\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 1.0203 - acc: 0.5495 - val_loss: 0.9169 - val_acc: 0.3333\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 151us/step - loss: 1.0116 - acc: 0.4955 - val_loss: 0.9187 - val_acc: 0.4167\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 168us/step - loss: 0.9995 - acc: 0.5676 - val_loss: 0.9086 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.9890 - acc: 0.5766 - val_loss: 0.9001 - val_acc: 0.5833\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 157us/step - loss: 0.9796 - acc: 0.5766 - val_loss: 0.8948 - val_acc: 0.6667\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 155us/step - loss: 0.9769 - acc: 0.5495 - val_loss: 0.8901 - val_acc: 0.6667\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 165us/step - loss: 0.9695 - acc: 0.5946 - val_loss: 0.8856 - val_acc: 0.5833\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 161us/step - loss: 0.9599 - acc: 0.5946 - val_loss: 0.8825 - val_acc: 0.6667\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.9517 - acc: 0.5856 - val_loss: 0.8839 - val_acc: 0.6667\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.9488 - acc: 0.5856 - val_loss: 0.8748 - val_acc: 0.6667\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 162us/step - loss: 0.9535 - acc: 0.5586 - val_loss: 0.8818 - val_acc: 0.6667\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 1.8454 - acc: 0.2523 - val_loss: 1.2962 - val_acc: 0.3333\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 1.2877 - acc: 0.3874 - val_loss: 1.1702 - val_acc: 0.2500\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 190us/step - loss: 1.1271 - acc: 0.4324 - val_loss: 1.1884 - val_acc: 0.2500\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 1.0468 - acc: 0.4775 - val_loss: 1.1943 - val_acc: 0.2500\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.9819 - acc: 0.5315 - val_loss: 1.2254 - val_acc: 0.2500\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 250us/step - loss: 0.9411 - acc: 0.5856 - val_loss: 1.2619 - val_acc: 0.2500\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 171us/step - loss: 0.9137 - acc: 0.5766 - val_loss: 1.3098 - val_acc: 0.3333\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 165us/step - loss: 0.9029 - acc: 0.6126 - val_loss: 1.3511 - val_acc: 0.3333\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 170us/step - loss: 0.8851 - acc: 0.5946 - val_loss: 1.3695 - val_acc: 0.2500\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.8833 - acc: 0.6306 - val_loss: 1.3891 - val_acc: 0.3333\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 173us/step - loss: 0.8725 - acc: 0.6216 - val_loss: 1.4222 - val_acc: 0.3333\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 180us/step - loss: 0.8605 - acc: 0.6306 - val_loss: 1.4387 - val_acc: 0.3333\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.8592 - acc: 0.6036 - val_loss: 1.4384 - val_acc: 0.3333\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 157us/step - loss: 0.8518 - acc: 0.6396 - val_loss: 1.4499 - val_acc: 0.3333\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 0.8366 - acc: 0.6396 - val_loss: 1.4680 - val_acc: 0.3333\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 156us/step - loss: 0.8480 - acc: 0.6757 - val_loss: 1.4712 - val_acc: 0.2500\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 164us/step - loss: 0.8338 - acc: 0.6486 - val_loss: 1.4863 - val_acc: 0.2500\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 176us/step - loss: 0.8300 - acc: 0.6667 - val_loss: 1.4869 - val_acc: 0.3333\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.8310 - acc: 0.6126 - val_loss: 1.4884 - val_acc: 0.2500\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 165us/step - loss: 0.8243 - acc: 0.6757 - val_loss: 1.4970 - val_acc: 0.2500\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 3ms/step - loss: 1.5225 - acc: 0.3153 - val_loss: 1.5583 - val_acc: 0.1667\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 173us/step - loss: 1.2482 - acc: 0.4054 - val_loss: 1.4185 - val_acc: 0.2500\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 165us/step - loss: 1.1468 - acc: 0.4054 - val_loss: 1.3013 - val_acc: 0.2500\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 1.0670 - acc: 0.4955 - val_loss: 1.3235 - val_acc: 0.3333\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 157us/step - loss: 1.0608 - acc: 0.5045 - val_loss: 1.2065 - val_acc: 0.4167\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 155us/step - loss: 1.0420 - acc: 0.5045 - val_loss: 1.1801 - val_acc: 0.4167\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 156us/step - loss: 1.0165 - acc: 0.5225 - val_loss: 1.3046 - val_acc: 0.3333\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 1.0347 - acc: 0.5495 - val_loss: 1.1764 - val_acc: 0.4167\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 177us/step - loss: 1.0072 - acc: 0.5766 - val_loss: 1.2011 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 162us/step - loss: 0.9910 - acc: 0.5676 - val_loss: 1.1813 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 167us/step - loss: 0.9832 - acc: 0.5946 - val_loss: 1.2103 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.9833 - acc: 0.5856 - val_loss: 1.1916 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 180us/step - loss: 0.9893 - acc: 0.5676 - val_loss: 1.1559 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 179us/step - loss: 0.9767 - acc: 0.6126 - val_loss: 1.1436 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 173us/step - loss: 0.9802 - acc: 0.5766 - val_loss: 1.1409 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.9536 - acc: 0.6126 - val_loss: 1.1520 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 173us/step - loss: 0.9611 - acc: 0.5856 - val_loss: 1.2402 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.9452 - acc: 0.5766 - val_loss: 1.1307 - val_acc: 0.4167\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 164us/step - loss: 0.9480 - acc: 0.6216 - val_loss: 1.1511 - val_acc: 0.3333\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 233us/step - loss: 0.9336 - acc: 0.5766 - val_loss: 1.1442 - val_acc: 0.4167\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 0s 4ms/step - loss: 1.3753 - acc: 0.4234 - val_loss: 1.1580 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 227us/step - loss: 1.0810 - acc: 0.4775 - val_loss: 1.0730 - val_acc: 0.4167\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 175us/step - loss: 1.0352 - acc: 0.4955 - val_loss: 1.0293 - val_acc: 0.4167\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 185us/step - loss: 0.9971 - acc: 0.4955 - val_loss: 1.0022 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 170us/step - loss: 0.9862 - acc: 0.4505 - val_loss: 0.9871 - val_acc: 0.4167\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 184us/step - loss: 0.9656 - acc: 0.5135 - val_loss: 0.9925 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 163us/step - loss: 0.9549 - acc: 0.4865 - val_loss: 0.9899 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.9456 - acc: 0.5135 - val_loss: 0.9897 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 166us/step - loss: 0.9428 - acc: 0.5045 - val_loss: 0.9964 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.9365 - acc: 0.5495 - val_loss: 1.0024 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 156us/step - loss: 0.9297 - acc: 0.5225 - val_loss: 1.0006 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.9199 - acc: 0.5586 - val_loss: 1.0030 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.9197 - acc: 0.5315 - val_loss: 1.0049 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.9154 - acc: 0.5225 - val_loss: 1.0046 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.9122 - acc: 0.5405 - val_loss: 1.0056 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.9182 - acc: 0.5225 - val_loss: 1.0131 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 0.9049 - acc: 0.5405 - val_loss: 1.0067 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 251us/step - loss: 0.9050 - acc: 0.5405 - val_loss: 1.0047 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 177us/step - loss: 0.8991 - acc: 0.5315 - val_loss: 0.9998 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 159us/step - loss: 0.9051 - acc: 0.5856 - val_loss: 1.0114 - val_acc: 0.5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "c4a5090c-adfc-44eb-aab1-9b94a001240a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "a8591201-ec63-4151-a8fd-42789ea99678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "612fd9c5-5fdc-416d-f1eb-7b0d52d34e32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "c50b1514-5da4-45a6-de2b-de7cf4d64d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'bo', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'b', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff9d4bceb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXOyZ0/acGdgAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "2cf37956-b9ba-48b9-ec39-82d03163ee5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff9d46a8e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3iUZdbA4d8h9KYIuColQUCkQ4ig\nUhSUYgGkrSgsArIsKOqKjRX3w1XRtbdlVXTtQUBdBFwbCIJYCQgoKIIQpEuTjiTkfH88b+IQZpJJ\nMiWZOfd1zZV5+5lhmDPvU0VVMcYYY3IrFe0AjDHGFE+WIIwxxvhlCcIYY4xfliCMMcb4ZQnCGGOM\nX5YgjDHG+GUJwgRNRBJE5ICI1A3lvtEkIg1EJORtvUXkYhFJ91leLSIdg9m3ENd6QUTuLOzxxgRS\nOtoBmPARkQM+ixWB34Bj3vJfVDW1IOdT1WNA5VDvGw9UtVEoziMiI4DBqnqhz7lHhOLcxuRmCSKG\nqWrOF7T3C3WEqs4NtL+IlFbVzEjEZkx+7PMYfVbEFMdE5D4RmSYib4jIfmCwiJwnIl+KyK8islVE\nnhKRMt7+pUVERSTJW37d2/6+iOwXkS9EpF5B9/W2XyIiP4rIXhF5WkQ+E5GhAeIOJsa/iMhaEdkj\nIk/5HJsgIo+LyC4RWQf0yOP9GS8iU3OtmyQij3nPR4jI997r+cn7dR/oXJtE5ELveUURec2LbSXQ\nJte+d4nIOu+8K0Wkl7e+OfAvoKNXfLfT57292+f4Ud5r3yUi74jI6cG8NwV5n7PjEZG5IrJbRLaJ\nyO0+1/m7957sE5E0ETnDX3GeiCzK/nf23s+F3nV2A3eJSEMRme9dY6f3vp3kc3yi9xp3eNufFJHy\nXsyNffY7XUQOiUj1QK/X+KGq9oiDB5AOXJxr3X3AUaAn7sdCBeAcoB3u7vJM4EdgjLd/aUCBJG/5\ndWAnkAKUAaYBrxdi31OB/UBvb9tYIAMYGuC1BBPjTOAkIAnYnf3agTHASqA2UB1Y6P4b+L3OmcAB\noJLPuX8BUrzlnt4+AnQBDgMtvG0XA+k+59oEXOg9fwT4BKgGJAKrcu37R+B079/kai+GP3jbRgCf\n5IrzdeBu73k3L8ZWQHng38C8YN6bAr7PJwHbgZuAckBVoK237W/AcqCh9xpaAacADXK/18Ci7H9n\n77VlAqOBBNzn8SzgIqCs9zn5DHjE5/V8572flbz923vbJgMTfa5zCzAj2v8PS9oj6gHYI0L/0IET\nxLx8jrsVeNN77u9L/1mffXsB3xVi3+HApz7bBNhKgAQRZIzn+mz/L3Cr93whrqgte9ulub+0cp37\nS+Bq7/klwOo89n0XuN57nleC+Nn33wK4zndfP+f9DrjMe55fgngFuN9nW1VcvVPt/N6bAr7PfwIW\nB9jvp+x4c60PJkGsyyeG/tnXBToC24AEP/u1B9YD4i0vA/qG+v9VrD+siMls9F0QkbNF5H9ekcE+\n4B6gRh7Hb/N5foi8K6YD7XuGbxzq/kdvCnSSIGMM6lrAhjziBZgCXOU9v9pbzo7jchH5yiv++BX3\n6z2v9yrb6XnFICJDRWS5V0zyK3B2kOcF9/pyzqeq+4A9QC2ffYL6N8vnfa6DSwT+5LUtP7k/j6eJ\nyHQR2ezF8HKuGNLVNYg4jqp+hrsb6SAizYC6wP8KGVPcsgRhcjfxfA73i7WBqlYF/g/3iz6ctuJ+\n4QIgIsLxX2i5FSXGrbgvlmz5NcOdDlwsIrVwRWBTvBgrAG8BD+CKf04GPgoyjm2BYhCRM4FncMUs\n1b3z/uBz3vya5G7BFVtln68KrihrcxBx5ZbX+7wRqB/guEDbDnoxVfRZd1qufXK/vgdxre+aezEM\nzRVDoogkBIjjVWAw7m5nuqr+FmA/E4AlCJNbFWAvcNCr5PtLBK75LpAsIj1FpDSuXLtmmGKcDvxV\nRGp5FZZ35LWzqm7DFYO8jCteWuNtKocrF98BHBORy3Fl5cHGcKeInCyun8gYn22VcV+SO3C58s+4\nO4hs24HavpXFubwBXCsiLUSkHC6BfaqqAe/I8pDX+zwLqCsiY0SknIhUFZG23rYXgPtEpL44rUTk\nFFxi3IZrDJEgIiPxSWZ5xHAQ2CsidXDFXNm+AHYB94ur+K8gIu19tr+GK5K6GpcsTAFZgjC53QJc\ng6s0fg5XmRxWqroduBJ4DPcfvj7wDe6XY6hjfAb4GPgWWIy7C8jPFFydQk7xkqr+CtwMzMBV9PbH\nJbpgTMDdyaQD7+Pz5aWqK4Cnga+9fRoBX/kcOwdYA2wXEd+iouzjP8AVBc3wjq8LDAoyrtwCvs+q\nuhfoCvTDJa0fgQu8zQ8D7+De5324CuPyXtHhn4E7cQ0WGuR6bf5MANriEtUs4G2fGDKBy4HGuLuJ\nn3H/Dtnb03H/zr+p6ucFfO2G3ytwjCk2vCKDLUB/Vf002vGYkktEXsVVfN8d7VhKIusoZ4oFEemB\nazF0GNdMMgP3K9qYQvHqc3oDzaMdS0llRUymuOgArMOVvXcH+liloiksEXkA1xfjflX9OdrxlFRW\nxGSMMcYvu4MwxhjjV8zUQdSoUUOTkpKiHYYxxpQoS5Ys2amqfpuVx0yCSEpKIi0tLdphGGNMiSIi\nAUcTsCImY4wxflmCMMYY45clCGOMMX7FTB2EPxkZGWzatIkjR45EOxRTjJQvX57atWtTpkyg4YyM\nMRDjCWLTpk1UqVKFpKQk3AChJt6pKrt27WLTpk3Uq1cv/wOMiWMxXcR05MgRqlevbsnB5BARqlev\nbneVJiakpkJSEpQq5f6mpob2/DF9BwFYcjAnsM+EiQWpqTByJBw65JY3bHDLAIMKO35vLjF9B2GM\nMbFq/Pjfk0O2Q4fc+lCxBBFGu3btolWrVrRq1YrTTjuNWrVq5SwfPXo0qHMMGzaM1atX57nPpEmT\nSA31vaUxplj7OcAQhIHWF0bMFzEVRGqqy74//wx168LEiUW7VatevTrLli0D4O6776Zy5crceuut\nx+2TMzl4Kf+5+qWXXsr3Otdff33hg4ySzMxMSpe2j58xhVW3ritW8rc+VOwOwpNdnrdhA6j+Xp4X\njh/ma9eupUmTJgwaNIimTZuydetWRo4cSUpKCk2bNuWee+7J2bdDhw4sW7aMzMxMTj75ZMaNG0fL\nli0577zz+OWXXwC46667eOKJJ3L2HzduHG3btqVRo0Z8/rmbSOvgwYP069ePJk2a0L9/f1JSUnKS\nl68JEyZwzjnn0KxZM0aNGkX2aL8//vgjXbp0oWXLliQnJ5Oeng7A/fffT/PmzWnZsiXjvXvb7JgB\ntm3bRoMGDQB44YUXuOKKK+jcuTPdu3dn3759dOnSheTkZFq0aMG77/4+IdtLL71EixYtaNmyJcOG\nDWPv3r2ceeaZZGZmArBnz57jlk18CnclbXE2cSJUrHj8uooV3fqQyf4FW9Ifbdq00dxWrVp1wrpA\nEhNVXWo4/pGYGPQp8jRhwgR9+OGHVVV1zZo1KiK6ePHinO27du1SVdWMjAzt0KGDrly5UlVV27dv\nr998841mZGQooO+9956qqt588836wAMPqKrq+PHj9fHHH8/Z//bbb1dV1ZkzZ2r37t1VVfWBBx7Q\n6667TlVVly1bpqVKldJvvvnmhDiz48jKytKBAwfmXC85OVlnzZqlqqqHDx/WgwcP6qxZs7RDhw56\n6NCh447NjllVdevWrVq/fn1VVX3++ee1bt26unv3blVVPXr0qO7du1dVVbdv364NGjTIia9Ro0Y5\n58v+O3jwYJ09e7aqqk6aNCnndRZGQT4bpnh6/XXVihWP//9asaJbH8kYEhNVRdzfgl472serqgJp\nGuB71e4gPJEoz/NVv359UlJScpbfeOMNkpOTSU5O5vvvv2fVqlUnHFOhQgUuueQSANq0aZPzKz63\nvn37nrDPokWLGDhwIAAtW7akadOmfo/9+OOPadu2LS1btmTBggWsXLmSPXv2sHPnTnr27Am4jmYV\nK1Zk7ty5DB8+nAoVKgBwyimn5Pu6u3XrRrVq1QD342TcuHG0aNGCbt26sXHjRnbu3Mm8efO48sor\nc86X/XfEiBE5RW4vvfQSw4YNy/d6JnZFopI2L0UtdQhFqcWgQZCeDllZ7m+oWi9lswThCVRuF8ry\nPF+VKlXKeb5mzRqefPJJ5s2bx4oVK+jRo4ffdvply5bNeZ6QkBCweKVcuXL57uPPoUOHGDNmDDNm\nzGDFihUMHz68UP0FSpcuTVZWFsAJx/u+7ldffZW9e/eydOlSli1bRo0aNfK83gUXXMCPP/7I/Pnz\nKVOmDGeffXaBYzOxIxQ/6opSRFXUBBXtBBcMSxCeiJTnBbBv3z6qVKlC1apV2bp1Kx9++GHIr9G+\nfXumT58OwLfffuv3DuXw4cOUKlWKGjVqsH//ft5++20AqlWrRs2aNZk9ezbgvvQPHTpE165defHF\nFzl8+DAAu3fvBtzQ60uWLAHgrbfeChjT3r17OfXUUyldujRz5sxh8+bNAHTp0oVp06blnC/7L8Dg\nwYMZNGiQ3T2YIv+oK+ov+KImqEiXWhSGJQjPoEEweTIkJoKI+zt5cuhv2fxJTk6mSZMmnH322QwZ\nMoT27duH/Bo33HADmzdvpkmTJvzjH/+gSZMmnHTSScftU716da655hqaNGnCJZdcQrt27XK2paam\n8uijj9KiRQs6dOjAjh07uPzyy+nRowcpKSm0atWKxx9/HIDbbruNJ598kuTkZPbs2RMwpj/96U98\n/vnnNG/enKlTp9KwYUPAFYHdfvvtdOrUiVatWnHbbbflHDNo0CD27t3LlVdeGcq3x5RARf1RV9Rf\n8EVNUJEutSiUQJUTJe1R1ErqWJeRkaGHDx9WVdUff/xRk5KSNCMjI8pRFdwbb7yhQ4cOLfJ57LMR\nG4pSSSviv2GKSPDXLkoleXGoZFfNu5LaGqLHiQMHDnDRRReRmZmJqvLcc8+VuH4Io0ePZu7cuXzw\nwQfRDsUUE4MGFf4uv6j9CLKvW9i+U0U9PhLEJZCSLyUlRXNPOfr999/TuHHjKEVkijP7bIRGqDuX\nRlLusYzAFVFFqmi5uBCRJaqa4m+b1UEYYwolkp1LwyGa9Y4lhSUIY0yhlIRmmvkJdz+Cks4ShDGm\nUKLdD8GEnyUIY0yhRLsfggm/sCYIEekhIqtFZK2IjPOzfaiI7BCRZd5jhM+2Yz7rZ4UzznDp3Lnz\nCZ3ennjiCUaPHp3ncZUrVwZgy5Yt9O/f3+8+F154Ibkr5XN74oknOORTBnDppZfy66+/BhO6KSGi\n+Qs82v0QTAQEav9a1AeQAPwEnAmUBZYDTXLtMxT4V4DjDxTkesWxH8Rzzz13Qpv9du3a6YIFC/I8\nrlKlSvme+4ILLjhusD9/EhMTdceOHfkHWkxlZWXpsWPHwnLuaH82QqE4tKOPZj8EExpEabC+tsBa\nVV2nqkeBqUDvMF6v2Onfvz//+9//ciYHSk9PZ8uWLXTs2DGnX0JycjLNmzdn5syZJxyfnp5Os2bN\nADcMxsCBA2ncuDF9+vTJGd4CXP+A7KHCJ0yYAMBTTz3Fli1b6Ny5M507dwbcEBg7d+4E4LHHHqNZ\ns2Y0a9YsZ6jw9PR0GjduzJ///GeaNm1Kt27djrtOttmzZ9OuXTtat27NxRdfzPbt2wHX12LYsGE0\nb96cFi1a5AzV8cEHH5CcnEzLli256KKLADc/xiOPPJJzzmbNmpGenk56ejqNGjViyJAhNGvWjI0b\nN/p9fQCLFy/m/PPPp2XLlrRt25b9+/fTqVOn44Yx79ChA8uXLy/Qv1tJURx+gRelkrdE9CSOd4Ey\nR1EfQH/gBZ/lP5HrbgF3B7EVWAG8BdTx2ZYJpAFfAlcEuMZIb5+0unXrnpAZfX8l3nST6gUXhPZx\n0035Z+fLLrtM33nnHVV1Q27fcsstqup6NmcPdb1jxw6tX7++ZmVlqervdxDr16/Xpk2bqqrqo48+\nqsOGDVNV1eXLl2tCQkLOHUT2cNiZmZl6wQUX6PLly1X1xDuI7OW0tDRt1qyZHjhwQPfv369NmjTR\npUuX6vr16zUhISFnqO4BAwboa6+9dsJr2r17d06szz//vI4dO1ZVVW+//Xa9yedN2b17t/7yyy9a\nu3ZtXbdu3XGx+g5/rqratGlTXb9+va5fv15FRL/44oucbf5e32+//ab16tXTr7/+WlVV9+7dqxkZ\nGfryyy/nxLB69Wr1d2epGht3ECX9F3hxuAMy0buDCMZsIElVWwBzgFd8tiWq67xxNfCEiNTPfbCq\nTlbVFFVNqVmzZmQiLqCrrrqKqVOnAjB16lSuuuoqwCXmO++8kxYtWnDxxRezefPmnF/i/ixcuJDB\ngwcD0KJFC1q0aJGzbfr06SQnJ9O6dWtWrlzpdyA+X4sWLaJPnz5UqlSJypUr07dvXz799FMA6tWr\nR6tWrYDAQ4pv2rSJ7t2707x5cx5++GFWrlwJwNy5c4+b3a5atWp8+eWXdOrUiXr16gHBDQmemJjI\nueeem+frW716NaeffjrnnHMOAFWrVqV06dIMGDCAd999l4yMDF588UWGDh2a7/VKqpL+C9z6IRR/\n4RxrYTNQx2e5trcuh6ru8ll8AXjIZ9tm7+86EfkEaI2r0ygUrxQl4nr37s3NN9/M0qVLOXToEG3a\ntAHc4Hc7duxgyZIllClThqSkpEINrb1+/XoeeeQRFi9eTLVq1Rg6dGihzpMte6hwcMOF+ytiuuGG\nGxg7diy9evXik08+4e677y7wdXyHBIfjhwX3HRK8oK+vYsWKdO3alZkzZzJ9+vScUWVj0cSJ/nsC\nR2IE4lApylAZJvzCeQexGGgoIvVEpCwwEDiuNZKInO6z2Av43ltfTUTKec9rAO2BvH8WF1OVK1em\nc+fODB8+POfuAX4f6rpMmTLMnz+fDf4GhfHRqVMnpkyZAsB3333HihUrADdUeKVKlTjppJPYvn07\n77//fs4xVapUYf/+/Secq2PHjrzzzjscOnSIgwcPMmPGDDp27Bj0a9q7dy+1atUC4JVXfr/p69q1\nK5MmTcpZ3rNnD+eeey4LFy5k/fr1wPFDgi9duhSApUuX5mzPLdDra9SoEVu3bmXx4sUA7N+/P2fu\nixEjRnDjjTdyzjnn5ExOFC7RbEUUil/g1g/B5CVsdxCqmikiY4APcS2aXlTVlSJyD67MaxZwo4j0\nwtU37MbVSQA0Bp4TkSxcEvunqpbIBAGumKlPnz45RU3ghq3u2bMnzZs3JyUlJd/Jb0aPHs2wYcNo\n3LgxjRs3zrkTadmyJa1bt+bss8+mTp06xw0VPnLkSHr06MEZZ5zB/Pnzc9YnJyczdOhQ2rZtC7gv\n1NatWwecoS63u+++mwEDBlCtWjW6dOmS8+V+1113cf3119OsWTMSEhKYMGECffv2ZfLkyfTt25es\nrCxOPfVU5syZQ79+/Xj11Vdp2rQp7dq146yzzvJ7rUCvr2zZskybNo0bbriBw4cPU6FCBebOnUvl\nypVp06YNVatWDfucEbnH8sluxw+R+1VclF/gxSF+U7zZYH0m5mzZsoULL7yQH374gVKl/N8kh+Kz\nkZTkfzTQxETXoqe4K+nxm9CwwfpM3Hj11Vdp164dEydODJgcQqUkzAiWl5Ievwk/SxAmpgwZMoSN\nGzcyYMCAsF+rpLciKunxm/CL+QQRK0VoJnRC9ZmI5jzmoVDS4zfhF9MJonz58uzatcuShMmhquza\ntYvy5csX+VwlvR1/SY/fhF9MV1JnZGSwadOmIvULMLGnfPny1K5dmzJlykQ7FGOiLq9K6pI1KXEB\nlSlTJqcHrzHFUUmestPEvphOEMYUZ9YPwRR3MV0HYUxxVhxGYzUmL5YgjIkS64dgijtLEMZEifVD\nMMWdJQhjosT6IZjizhKEMVFi/RBMcWetmIyJIpsPwRRndgdh4prNh2BMYHYHYeKW9UMwJm92B2Hi\nlvVDMCZvliBM3LJ+CMbkzRKEiVvWD8GYvFmCMCVaUSqZrR+CMXmzBGFKrOxK5g0bQPX3SuZgk4T1\nQzCh8MEH8L//RTuK8Ijp+SBMbEtKckkht8RESE+PdDQmHi1YABdfDJmZMGGCe4hEO6qCyWs+CLuD\nMCWWVTKbaEpPh/79oX59GDIE/vEPGDwYYml+MusHYUqsunX930FYJbMJt4MH4YorICMDZs6Es86C\ns8+GO+90n8kZM6BmzWhHWXR2B2FKLKtkNtGgCsOGwYoV8MYb0KiRK1b6299g+nRYsgTOPRd++CHa\nkRadJQhTYlklc/SpwlNPwdNPu1/V8eD+++HNN+HBB+GSS47fNmAAfPIJHDgA550H8+ZFJcSQsUpq\nY0yhHDsG113nkjJAjRrw17/C9dfDySdHN7ZwmTULevd2P0Jeey1whXR6Olx+OaxeDc8+C9deG9Ew\nC8QqqY0xIfXbbzBwoEsO48fDokXQrh3cdZe7k7vzTvjll2hHGVqrVrlK6JQUeP75vFsrJSXBZ59B\nly4wYgSMGwdZWRELNWQsQRhjCmT/frjsMnjrLXjsMbjvPmjfHt59F775Bnr0gH/+0yWKG2+MjVZl\nu3dDr16ujmvGDKhQIf9jTjrJ9Y8YNcoVR/3xjyeO/VXcWYIwxgRt50646CJXzv7KK3Dzzcdvb9UK\npk1zFbRXXw3PPOOagQ4fDj/+GJWQiywz090tbdwI//0v1K4d/LGlS8O//w2PP+6OvfBC2Lo1bKGG\nnCUIY0xQNm6Ejh3h22/dr+ghQwLve9ZZ8J//wE8/uXqKqVNdM9Arr4RlyyIXcyjccQfMmeOS3fnn\nF/x4EVc38847sHKlK4r79tvQxxkOliCMMfn64QdXjLR1K3z0EfTsGdxxdevCk0+6Sttx49ywFK1b\nuyKqzz8Pa8gh8eqrrhjthhvcXVBR9OoFn37qKvfbt4f33w9NjOFkCcIYk6fFi6FDBzh61A0t0bFj\nwc9x6qmueeiGDa6fytdfuy/JCy90Cac4Nqb86is3tleXLvDoo6E5Z3Kye+0NGrhWTpMmhea84WLN\nXI0xAX38sesxXLOmK2apXz805z14EF54AR5+GDZvdncU06ZBpUqhOX9RbdniWiuVL+8SZPXqoT3/\ngQOujmb2bFeR/9hjkJDgtmVmwuHD7nHkiP/nuZdr1ICrripcLHk1c0VVw/YAegCrgbXAOD/bhwI7\ngGXeY4TPtmuANd7jmvyu1aZNGzXGhM6bb6qWLavavLnqli3hucaRI6qPPaZaqpTqeeep7toVnusU\nxOHDqu3aqVaqpLpiRfiuk5mpevPNqqB68smqlSurli7tlgv6OOecwscBpGmA79WwjcUkIgnAJKAr\nsAlYLCKzVHVVrl2nqeqYXMeeAkwAUgAFlnjH7glXvMaY302e7Jpnnn+++5VbrVp4rlOunGsJlZjo\nfgF36gQffgi1aoXnevlRda/7q69cq6PmzcN3rYQEd+eQkuLqJipUcI/y5f0/z29bOIRzsL62wFpV\nXQcgIlOB3kDuBOFPd2COqu72jp2Duxt5I0yxGmNwX5D//Kfr6HbppW5IidzjXYVD376u0rZ3b1ff\n8dFH0LBh+K+b25NPuua7d98NffpE5ppXX+0exVE4K6lrARt9ljd563LrJyIrROQtEalTwGONMSGS\nlQW33OKSw+DBrllmJJJDti5dYP58Vz7foYPrdBdJc+a419+nD/z975G9dnEV7VZMs4EkVW0BzAFe\nKcjBIjJSRNJEJG3Hjh1hCdCYeJCR4UYoffxxV2n6yitQpkzk40hJccN2lCvnWjgtWBCZ665d6/po\nNG3qmraWivY3YzERzrdhM1DHZ7m2ty6Hqu5S1d+8xReANsEe6x0/WVVTVDWlZiwMvm5MFBw+DP36\nuS/Ge++FJ56I7hdko0ZuHKNataB7dzdAXjjt2+eKtkTc3A6VK4f3eiVJOD8Gi4GGIlJPRMoCA4Hj\n/qlF5HSfxV7A997zD4FuIlJNRKoB3bx1xpgQ2rULunVz4yj9+99usL3iMGVmnTqu4rZlS1c/8fLL\nob+GqmvG27WrG3X1zTehXr3QX6ckC1uCUNVMYAzui/17YLqqrhSRe0Skl7fbjSKyUkSWAzfimr3i\nVU7fi0syi4F7siusTfGSmupGrixVyv1NTY3s8abwVq92E9ssXuz6IIweHe2Ijle9uvsC79LFFX+F\nqrNaVpa7Uzj3XDef9MaN8Prr7joml0DtX0vaw/pBRN7rr6tWrHh8e+yKFd36SBxvCm/+fNVq1VRr\n1lT9/PNoR5O3I0dUBwxwn4877lDNyirceTIy3GeraVN3rjPPVH3uOXf+eEYe/SCi/sUeqocliMhL\nTFS/nXYSEyNzvCmcl15SLVNGtXFj1XXroh1NcDIzVUeNcp+Pa691X/bBOnLEJYIzz3THN22qmppa\nsHPEsrwSRDj7QZgYF2ic/2DH/y/q8aZgsrJc883773dFK2++WXJmfktIcHUkNWu6ivTdu2HKFNdZ\nLJADB1yHv0cfdUNnnHOO65jWs6e1UgqWvU2m0OrWLdj6UB9vgnf4sJvT4P773QB0771XcpJDNhG4\n5x7XymrGDNeRb9++E/fbs8ftl5jo+jU0agRz57re0b17W3IoCHurTKFNnHhiR6qKFd36SBxvgrN9\nO3Tu7GaAe+QRN0dyNPo4hMpNN7n5oBcudBXL2V2gtm1zczfUrQsTJrjRYj//HObNc5McFYfWWSVO\noLKnkvawOojoeP11V2cg4v4WtIK5qMebvH33nXtfK1ZUnTEj2tGE1rvvqpYvr3rWWaqjR6uWK+cG\n/bvqKtXly6MdXclBHnUQNty3MVF09Kgb++joUTdncfPmoful+9FHMGCAG0J79mxo0yb/Y0qaRYvc\nvAqHDsE118Dtt0dnDKeSLK/hvq2S2pgo2b3bdQJbsMCVi0+cCI0buyEfrrzSTdFZWM8+C2PGuKEj\n3n3XdTyLRR06uGk8S5WC00/Pf39TMFYHYUwUrFnjOmp98YXrHLh1q2ulc+qp8I9/uETRqhU88ACs\nWxf8eY8dg7FjXae3Hj3cL+xYTQ7ZatWy5BAuliCMibCFC11y2LPHVaBefbVLDKNHwyefwKZNrqVO\nxYpuZNX69aFtW9dcc+PGwF7c+k4AABoCSURBVOc9cMDdkWQPuDdzJlSpErGXZWKQJQhjIujVV10f\nhJo14csvXUub3M44w7XU+fxzSE+Hhx5yfRhuvdW10OnQAZ5+2rXaybZ5s5ts59133bYnn/x9Cktj\nCssqqY2JAFXX9PLee13TzLfeKvgsbWvXujGTpk2Db7915e4XXODmc378cdi712279NLwvAYTm/Kq\npLYEYUyYHTniBpubOhWuvRaeeabo/RC+/94lg6lT3aB7deq4u4cWLUITs4kfliCMiZJffoErrnCV\n0Q8+CLfdFtoOW6quwvu006Bq1dCd18SPIjVzFZEbgNdVdU/IIzMmhq1a5drob93qipT69Qv9NUTg\nrLNCf15jILhK6j8Ai0Vkuoj0ELEO67HE5mMIj7lz4fzzXQeuBQvCkxyMCbd8E4Sq3gU0BP6Dm9Bn\njYjcLyL1wxybCbPUVDdw24YNrqhiwwa3bEmiaJ5/3vVBqFsXvv7aNVE1piQKqpmrN17HNu+RCVQD\n3hKRh8IYmwmz8ePdL1xfhw659abgsrJcHcPIkW4ay0WLbGRaU7IFUwdxEzAE2Am8ANymqhkiUgpY\nA9we3hBNuNh8DKFz8CAMHgzvvAPXXef6IZS2gWxMCRfMR/gUoK+qbvBdqapZInJ5eMIykVC3ritW\n8rfeBG/LFujVC775xiWGG26woaVNbAimiOl9YHf2gohUFZF2AKr6fbgCM+Fn8zEUzZo1bkKaZs3g\nhx/c0BY33mjJwcSOYBLEM8ABn+UD3jpTwg0a5KZkTEx0X2qJiW550KBoR1Z8ZWTA22+74TLOOgue\neso9/+IL16TVmFgSTBGTqE9vOq9oyUpXY8SgQZYQgrFpk2ud9Pzzrl9D3bpw330wfLiNJGpiVzBf\n9OtE5EZ+v2u4DijAAMTGlExZWTBnjhsaY/Zs1xS4Rw947jk33pENhmdiXTAJYhTwFHAXoMDHwMhw\nBmVMNO3cCS++6BLBunVu5NXbb3fNV+vVi3Z0xkROvglCVX8BBkYgFmOiRtUNr/3MM/Dmm24K0E6d\nXDFS375Qrly0IzQm8oLpB1EeuBZoCpTPXq+qw8MYlzERk5rqBtL79ls34N3IkTBqlJuu05h4Fkwr\npteA04DuwAKgNrA/nEEZEynTprkObqVKuQroLVvchDuWHIwJrg6igaoOEJHeqvqKiEwBPg13YMaE\n29Klbp6GDh3g44+hbNloR2RM8RLMHUSG9/dXEWkGnAScGr6QjAm/7dvdPA01arihuC05GHOiYO4g\nJotINVwrpllAZeDvYY3KmDA6ehT693etlRYtgj/8IdoRGVM85ZkgvAH59nmTBS0EzoxIVMaEiaob\nK2nRIjddZ3JytCMypvjKs4hJVbOw0VpNDHn2WTecyN/+BldeGe1ojCnegqmDmCsit4pIHRE5JfsR\n9siMCbEFC9xgepddBvfeG+1ojCn+gkkQVwLX44qYlniPtHAGZYJnU4YGJz3d1Ts0aODeIxsmw5j8\nBTPlaD0/j6DqIrw5rFeLyFoRGZfHfv1EREUkxVtOEpHDIrLMezwb/EuKHzZlaHAOHoTevd1IrDNn\nwkknRTsiY0qGYHpSD/G3XlVfzee4BGAS0BXYBCwWkVmquirXflWAm4Cvcp3iJ1VtlV988SyvKUNt\nhFZHFYYOhe++g/fec0N0G2OCE0wz13N8npcHLgKWAnkmCKAtsFZV1wGIyFSgN7Aq1373Ag8CtwUT\nsPmdTRmav4kTXT+Hhx+G7t2jHY0xJUswRUw3+Dz+DCTj+kLkpxaw0Wd5k7cuh4gkA3VU9X9+jq8n\nIt+IyAIR6ejvAiIyUkTSRCRtx44dQYQUWwJNDWpThjozZ8Lf/+6G0rjllmhHY0zJE0wldW4HgSIP\neuz1sXgM8PdfdytQV1VbA2OBKSJSNfdOqjpZVVNUNaVmzZpFDanEsSlDA1u50iWGlBTXrNWmATWm\n4IKpg5iNmwcCXEJpAkwP4tybgTo+y7W9ddmqAM2AT8T97z0NmCUivVQ1DfgNQFWXiMhPwFlY66nj\nZNczjB/vipXq1nXJId7rH3bvdpXSlSvDO+9AhQrRjsiYkimYOohHfJ5nAhtUdVMQxy0GGopIPVxi\nGAhcnb1RVfcCNbKXReQT4FZVTRORmsBuVT0mImcCDbFZ7PyyKUOPl5npOsBt3AiffAK1auV7iDEm\ngGASxM/AVlU9AiAiFUQkSVXT8zpIVTNFZAzwIZAAvKiqK0XkHiBNVWflcXgn4B4RyQCygFGqujuI\nWE2cu+02mDvXzQh33nnRjsaYkk1UNe8dRNKA81X1qLdcFvhMVc/J88AIS0lJ0bQ0K4GKZy+/7Ibv\nvukmeOKJaEdjTMkgIktUNcXftmAqqUtnJwcA77kNjmyKlS+/hL/8BS66CB55JP/9jTH5CyZB7BCR\nXtkLItIb2Bm+kIwpmC1b3LzRtWu7GeJKB1NwaozJVzD/lUYBqSLyL295E+C3d7UxoaAKBw641kh7\n9rhH9nN/f3/4Afbvh48+gurVox29MbEj3wShqj8B54pIZW/5QNijMnEjIwPGjoW0tOMTQmZm4GNK\nl4ZTToFq1dzfVq3g5puhWbPIxW1MPAimH8T9wEOq+qu3XA24RVXvCndw8SA1Nb77MYwdC//6F1x4\nofuiz/7Sr1bt+Oe+fytVso5vxkRCMEVMl6jqndkLqrpHRC7FTUFqiiB7NNbsAfeyR2OF+EgSL7zg\nksPYsfDoo9GOxhiTWzCV1AkiUi57QUQqAOXy2N8EKa/RWGPdZ5/BdddB167w4IPRjsYY408wdxCp\nwMci8hIgwFDglXAGFS/idTTWjRuhXz9ITLRWR8YUZ8FUUj8oIsuBi3FjMn0IJIY7sHhQt64rVvK3\nPlYdPgx9+rg7pXnzXL2CMaZ4CnY01+245DAA6AJ8H7aI4ki8jcaqCtdeC0uXuvqXJk2iHZExJi8B\n7yBE5CzgKu+xE5iGG5qjc4Rii3nxNhrrww/DG2+419izZ7SjMcbkJ+BYTCKSBXwKXKuqa71164Kd\njzrSbCym4u299+Dyy2HAAJg61ZqpGlNcFHYspr64iXvmi8jzInIRrpLamAJZvRquugpatnSjrFpy\nMKZkCJggVPUdVR0InA3MB/4KnCoiz4hIt0gFaEq2X3+FXr2gXDk3BWilStGOyBgTrGDmpD6oqlNU\ntSduVrhvgDvCHpkp8Y4dg6uvhnXr4O23Y7t1ljGxqEBzUqvqHm8e6IvCFZCJHXfeCe+/73pLd+wY\n7WiMMQVVoARhTLBSU+Ghh2DUKDdPgzGm5LEEYUIuLQ1GjIBOneDJJ6MdjTGmsCxBmJDatg2uuAJO\nPRXeegvK2tyDxpRYNgqOCZnffnNjLO3Z4wbjq1kz2hEZY4rCEoQJCVW4/nr4/HOYPt3N7WCMKdms\niMmExKRJ8J//uGFDBgyIdjTGmFCwBGGKbN48+OtfXYe4e+6JdjTGmFCxBGGKJC0N+veHRo3gtdeg\nlH2ijIkZ9t/ZFNqiRdClC5x8Mrz7LlStGu2IjDGhZAnCFMqcOdCtG5xxBixcCPXqRTsiY0yoWYIo\notRUSEpyRStJSW451s2e7YbubtgQFiyA2rWjHZExJhysmWsRpKbCyJFu+kxw04eOHOmex+qkP9Om\nweDB0Lo1fPABnHJKtCMyxoSL3UEUwfjxvyeHbIcOufWx6KWX3Ois550Hc+dacjAm1lmCKIKffy7Y\n+pJs0iQYPhwuvtjdOViFtDGxzxJEEQSa3yDW5j146CEYMwZ694ZZs6BixWhHZIyJBEsQRTBx4olf\nlhUruvWxQBX+7//gjjvclKFvvulmhjPGxAdLEEUwaBBMngyJiW6e5cREtxwLFdSqcMstcO+9cO21\nrhNcmTLRjsoYE0nWiqmIBg2KjYTgKysLRo92ye7GG+Hxx62HtDHxKKz/7UWkh4isFpG1IjIuj/36\niYiKSIrPur95x60Wke7hjNP8LjMTrrnGJYe//Q2eeMKSgzHxKmx3ECKSAEwCugKbgMUiMktVV+Xa\nrwpwE/CVz7omwECgKXAGMFdEzlLVY+GK18DRo66u4b//dfUod94Z7YiMMdEUzt+GbYG1qrpOVY8C\nU4Hefva7F3gQOOKzrjcwVVV/U9X1wFrvfCZMDh92M8H997+uSMmSgzEmnAmiFrDRZ3mTty6HiCQD\ndVT1fwU91jt+pIikiUjajh07QhN1HNq/Hy691PVvmDzZDd1tjDFRK10WkVLAY8AthT2Hqk5W1RRV\nTalp81sWyuHD0KMHfPopvP46/PnP0Y7IGFNchLMV02agjs9ybW9dtipAM+ATEQE4DZglIr2CONaE\ngCqMGAFffOGmCe3fP9oRGWOKk3DeQSwGGopIPREpi6t0npW9UVX3qmoNVU1S1STgS6CXqqZ5+w0U\nkXIiUg9oCHwdxljj0iOPwJQpcN99lhyMMScK2x2EqmaKyBjgQyABeFFVV4rIPUCaqs7K49iVIjId\nWAVkAtdbC6bQev9910P6yitdc1ZjjMlNVDXaMYRESkqKpqWlRTuMEmH1amjXzk3ys2gRVKoU7YiM\nMdEiIktUNcXfNusCFWd+/RV69YKyZWHmTEsOxpjAbKiNOHLsmJvPYd06mDcv9kadNcaEliWIODJ+\nvKt7ePZZ6Ngx2tEYY4o7K2KKE1OmwIMPukH4/vKXaEdjjCkJLEHEgbQ0N2R3p05u8D1jjAmGJYgY\nt20b9OkDp54Kb73lKqeNMSYYVgcRw377Dfr1g9274bPPwEYjMcYUhCWIGKUK118Pn3/uhtFo1Sra\nERljShorYoqyJUvc6KmTJ8OBA6E776RJ8J//uJZLAwaE7rzGmPhhCSIKjh2Dd96BCy6AlBT4179c\ny6JatWDMGPjuu6Kdf/58l3R69oR77glNzMaY+GMJIoIOHICnn4ZGjVzFcXq6GzBv50435EXPnvD8\n89C8ueunMGWKq0coiPXr3R1Do0Zu+G6bLtQYU1j29REBGzfC7bdDnTpw442usnjaNPjpJ7jlFjj5\nZGjf3n2hb9oEDz0EW7bAoEHumHHjXO/n/Bw4AL17uzuUmTOhatXwvzZjTOyyBBFGixe7OZ7r1YNH\nH4WuXV2l8RdfwB//CKX9NBGoWRNuuw3WrHEzvLVvDw8/DA0awCWXwKxZLgHklpUFQ4bAypUu+TRo\nEP7XZ4yJbZYgQuzYMTevc4cO0LYtvPce3HSTu1uYPh3OOy+485QqBd27w4wZsGED/P3vsHy5u0Oo\nV8/N4bB16+/733uv2/eRR6Bbt/C8NmNMfLHhvkNk/3548UV48klXD5CU5BLD8OGhK+rJyIDZs+GZ\nZ2DuXHcHcsUV0KaNm9NhyBB4+WVwE/QZY0z+8hru2xJEER09ChMmwL//Dfv2uSKhm292X9wJCeG7\n7o8/wnPPwUsvwZ497m5lwQIoXz581zTGxB5LEGGyd6/rqfzxx25mtrFj3Rd1JB0+DB9+6Fo9Va8e\n2WsbY0q+vBKE9aQupM2b4dJLYdUqePVV+NOfohNHhQrubsUYY0LNEkQhrFzpWhT9+qurhO7aNdoR\nGWNM6FkrpgJasMC1UMrIgIULLTkYY2KXJYgCmD7dNSE97TT48ksbAM8YE9ssQQTpiSdg4EBXCf3Z\nZ5CYGO2IjDEmvCxB5CMry7VOuvlm6NsX5syBU06JdlTGGBN+liDycOSIGyrj8cfdGErTplk/A2NM\n/LBWTAHs2eOajy5c6MZCuuUW66FsjIkvliD82LgRevRwA+ZNmeLuIowxJt5YgshlxQrXx+HgQddD\nuXPnaEdkjDHRYXUQPubNc0NWiMCnn1pyMMbEN0sQnilTXLFSnTpuvobmzaMdkTHGRFfcJwhVePBB\nN3tb+/Zu6s86daIdlTHGRF/cJ4jVq+Guu1wnuA8+cNN/GmOMsUpqzj7bFSklJ7tZ3IwxxjhxnyAA\nUvyOhG6MMfEtrL+ZRaSHiKwWkbUiMs7P9lEi8q2ILBORRSLSxFufJCKHvfXLROTZcMZpjDHmRGG7\ngxCRBGAS0BXYBCwWkVmquspntymq+qy3fy/gMaCHt+0nVbXxUo0xJkrCeQfRFlirqutU9SgwFejt\nu4Oq7vNZrATExvynxhgTA8KZIGoBG32WN3nrjiMi14vIT8BDwI0+m+qJyDciskBEOvq7gIiMFJE0\nEUnbsWNHKGM3xpi4F/V2O6o6SVXrA3cAd3mrtwJ1VbU1MBaYIiJV/Rw7WVVTVDWlZs2akQvaGGPi\nQDgTxGbAt8tZbW9dIFOBKwBU9TdV3eU9XwL8BJwVpjiNMcb4Ec4EsRhoKCL1RKQsMBCY5buDiDT0\nWbwMWOOtr+lVciMiZwINgXVhjNUYY0wuYWvFpKqZIjIG+BBIAF5U1ZUicg+QpqqzgDEicjGQAewB\nrvEO7wTcIyIZQBYwSlV3hytWY4wxJxLV2Gg4lJKSomlpadEOwxhjShQRWaKqfrsLR72S2hhjTPEU\n9wkiNRWSktw4TElJbtkYY0ycj8WUmgojR8KhQ255wwa3DG74b2OMiWdxfQcxfvzvySHboUNuvTHG\nxLu4ThA//1yw9cYYE0/iOkHUrVuw9cYYE0/iOkFMnAgVKx6/rmJFt94YY+JdXCeIQYNg8mRITAQR\n93fyZKugNsYYiPNWTOCSgSUEY4w5UVzfQRhjjAnMEoQxxhi/LEEYY4zxyxKEMcYYvyxBGGOM8Stm\nhvsWkR3AhmjHkYcawM5oB5EHi69oLL6isfiKpijxJaqq3zmbYyZBFHcikhZozPXiwOIrGouvaCy+\noglXfFbEZIwxxi9LEMYYY/yyBBE5k6MdQD4svqKx+IrG4iuasMRndRDGGGP8sjsIY4wxflmCMMYY\n45cliBARkToiMl9EVonIShG5yc8+F4rIXhFZ5j3+LwpxpovIt9710/xsFxF5SkTWisgKEUmOYGyN\nfN6bZSKyT0T+mmufiL6HIvKiiPwiIt/5rDtFROaIyBrvb7UAx17j7bNGRK6JYHwPi8gP3r/fDBE5\nOcCxeX4Wwhjf3SKy2eff8NIAx/YQkdXeZ3FcBOOb5hNbuogsC3BsJN4/v98rEfsMqqo9QvAATgeS\nvedVgB+BJrn2uRB4N8pxpgM18th+KfA+IMC5wFdRijMB2IbrxBO19xDoBCQD3/msewgY5z0fBzzo\n57hTgHXe32re82oRiq8bUNp7/qC/+IL5LIQxvruBW4P49/8JOBMoCyzP/f8pXPHl2v4o8H9RfP/8\nfq9E6jNodxAhoqpbVXWp93w/8D1QK7pRFUpv4FV1vgROFpHToxDHRcBPqhrV3vGquhDYnWt1b+AV\n7/krwBV+Du0OzFHV3aq6B5gD9IhEfKr6kapmeotfArVDfd1gBXj/gtEWWKuq61T1KDAV976HVF7x\niYgAfwTeCPV1g5XH90pEPoOWIMJARJKA1sBXfjafJyLLReR9EWka0cAcBT4SkSUiMtLP9lrARp/l\nTUQn0Q0k8H/MaL+Hf1DVrd7zbcAf/OxTXN7H4bg7Qn/y+yyE0xivCOzFAMUjxeH96whsV9U1AbZH\n9P3L9b0Skc+gJYgQE5HKwNvAX1V1X67NS3FFJi2Bp4F3Ih0f0EFVk4FLgOtFpFMUYsiTiJQFegFv\n+tlcHN7DHOru5YtlW3ERGQ9kAqkBdonWZ+EZoD7QCtiKK8Ypjq4i77uHiL1/eX2vhPMzaAkihESk\nDO4fMVVV/5t7u6ruU9UD3vP3gDIiUiOSMarqZu/vL8AM3K28r81AHZ/l2t66SLoEWKqq23NvKA7v\nIbA9u9jN+/uLn32i+j6KyFDgcmCQ9wVygiA+C2GhqttV9ZiqZgHPB7hutN+/0kBfYFqgfSL1/gX4\nXonIZ9ASRIh45ZX/Ab5X1ccC7HOatx8i0hb3/u+KYIyVRKRK9nNcZeZ3uXabBQzxWjOdC+z1uZWN\nlIC/3KL9HnpmAdktQq4BZvrZ50Ogm4hU84pQunnrwk5EegC3A71U9VCAfYL5LIQrPt86rT4BrrsY\naCgi9bw7yoG49z1SLgZ+UNVN/jZG6v3L43slMp/BcNbAx9MD6IC7zVsBLPMelwKjgFHePmOAlbgW\nGV8C50c4xjO9ay/34hjvrfeNUYBJuBYk3wIpEY6xEu4L/ySfdVF7D3GJaiuQgSvDvRaoDnwMrAHm\nAqd4+6YAL/gcOxxY6z2GRTC+tbiy5+zP4bPevmcA7+X1WYhQfK95n60VuC+603PH5y1fimu181Mk\n4/PWv5z9mfPZNxrvX6DvlYh8Bm2oDWOMMX5ZEZMxxhi/LEEYY4zxyxKEMcYYvyxBGGOM8csShDHG\nGL8sQRiTDxE5JsePMhuykUVFJMl3JFFjipPS0Q7AmBLgsKq2inYQxkSa3UEYU0jefAAPeXMCfC0i\nDbz1SSIyzxuM7mMRqeut/4O4+RmWe4/zvVMliMjz3nj/H4lIBW//G715AFaIyNQovUwTxyxBGJO/\nCrmKmK702bZXVZsD/wKe8NY9Dbyiqi1wA+U95a1/CligbqDBZFwPXICGwCRVbQr8CvTz1o8DWnvn\nGRWuF2dMINaT2ph8iMgBVa3sZ3060EVV13kDqm1T1eoishM3fESGt36rqtYQkR1AbVX9zeccSbgx\n+xt6y3cAZVT1PhH5ADiAG7H2HfUGKTQmUuwOwpii0QDPC+I3n+fH+L1u8DLcuFjJwGJvhFFjIsYS\nhDFFc6XP3y+855/jRh8FGAR86j3/GBgNICIJInJSoJOKSCmgjqrOB+4ATgJOuIsxJpzsF4kx+asg\nx09c/4GqZjd1rSYiK3B3AVd5624AXhKR24AdwDBv/U3AZBG5FnenMBo3kqg/CcDrXhIR4ClV/TVk\nr8iYIFgdhDGF5NVBpKjqzmjHYkw4WBGTMcYYv+wOwhhjjF92B2GMMcYvSxDGGGP8sgRhjDHGL0sQ\nxhhj/LIEYYwxxq//B1EnFnvnNmReAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "6bc14d6e-d629-4e90-f882-26821fdd53a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 0s 3ms/step - loss: 2.0964 - acc: 0.2846\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 0s 179us/step - loss: 1.3584 - acc: 0.3659\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 0s 165us/step - loss: 1.1604 - acc: 0.4309\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 0s 166us/step - loss: 1.0775 - acc: 0.4715\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 0s 179us/step - loss: 1.0320 - acc: 0.5285\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 0s 152us/step - loss: 0.9960 - acc: 0.5610\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 0s 185us/step - loss: 0.9668 - acc: 0.5528\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 0s 149us/step - loss: 0.9531 - acc: 0.5610\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 0s 160us/step - loss: 0.9326 - acc: 0.6016\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 0s 161us/step - loss: 0.9265 - acc: 0.5854\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 0s 153us/step - loss: 0.9163 - acc: 0.6016\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 0s 185us/step - loss: 0.9063 - acc: 0.6098\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 0s 172us/step - loss: 0.9034 - acc: 0.5935\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 0s 164us/step - loss: 0.8955 - acc: 0.5854\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 0s 148us/step - loss: 0.8933 - acc: 0.6016\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 0s 163us/step - loss: 0.8848 - acc: 0.6341\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 0s 154us/step - loss: 0.8835 - acc: 0.6098\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 0s 201us/step - loss: 0.8749 - acc: 0.6179\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 0s 167us/step - loss: 0.8756 - acc: 0.6179\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 0s 197us/step - loss: 0.8750 - acc: 0.6016\n",
            "42/42 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "e27b3bf6-84c2-4dda-be41-1d45d3aec3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "283bd101-38b4-4b7d-8994-3c4c1a11ab80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33333333475249155"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "28b8fe48-0da1-4f3a-dc04-ba5d4fd7b917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(5, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dvJzYLTzmiQa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "77799e9f-c1ad-4dfa-ac8e-8894634aae96",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4972bfff-db70-4724-fabb-f0d71322d7a6",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  36  38\n",
            "  39  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
            "  59  61  62  63  64  65  66  67  69  70  71  72  73  74  76  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 120\n",
            " 121 122] TEST: [ 19  35  37  41  56  60  68  75  77  78  94 110 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38\n",
            "  39  41  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  77  78\n",
            "  79  80  81  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120] TEST: [ 11  13  33  40  42  51  70  82  83  84 105 121 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37\n",
            "  39  40  41  42  43  45  46  47  48  50  51  52  54  55  56  57  58  59\n",
            "  60  61  62  64  65  67  68  69  70  71  72  73  75  76  77  78  79  80\n",
            "  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  99 100\n",
            " 101 102 103 104 105 106 107 109 110 112 113 114 115 116 117 118 119 120\n",
            " 121 122] TEST: [  9  36  38  44  49  53  63  66  74  86  98 108 111]\n",
            "TRAIN: [  0   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
            "  60  61  62  63  66  67  68  69  70  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 121 122] TEST: [  1   7  21  23  39  55  64  65  71  90 104 120]\n",
            "TRAIN: [  0   1   2   4   6   7   8   9  11  12  13  14  15  17  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  48  49  50  51  52  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
            "  99 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119\n",
            " 120 121 122] TEST: [  3   5  10  16  46  47  59  69  88 100 107 114]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  19  20\n",
            "  21  22  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  86  87  88  89  90  91  92  94  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 114 115 118 119\n",
            " 120 121 122] TEST: [  0  17  18  29  34  62  85  93  95 113 116 117]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  26  28  29  30  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  81  82  83  84  85  86  87  88  90  92  93  94  95  96  97  98  99\n",
            " 100 101 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [  6  25  27  31  32  48  54  80  89  91 102 112]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60\n",
            "  61  62  63  64  65  66  68  69  70  71  72  73  74  75  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 102 103 104 105 106 107 108 110 111 112 113 114 116 117 118 119\n",
            " 120 121 122] TEST: [  2  15  20  24  30  43  57  67  76 101 109 115]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  13  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  80  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 119\n",
            " 120 121 122] TEST: [  4   8  14  28  50  61  72  79  81  92 106 118]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
            "  19  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  46  47  48  49  50  51  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  98 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [ 12  22  26  45  52  58  73  87  96  97  99 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DjbzRWoekKFN",
        "colab": {}
      },
      "source": [
        "#train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a87a8be2-1209-480e-d927-5cffe1a77bad",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 30\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "110/110 [==============================] - 3s 27ms/step - loss: 1.3050 - acc: 0.2727 - val_loss: 1.0822 - val_acc: 0.3846\n",
            "Epoch 2/30\n",
            "110/110 [==============================] - 0s 253us/step - loss: 1.0760 - acc: 0.2273 - val_loss: 0.9121 - val_acc: 0.5385\n",
            "Epoch 3/30\n",
            "110/110 [==============================] - 0s 217us/step - loss: 0.9287 - acc: 0.5364 - val_loss: 0.8013 - val_acc: 0.6923\n",
            "Epoch 4/30\n",
            "110/110 [==============================] - 0s 210us/step - loss: 0.8271 - acc: 0.6818 - val_loss: 0.7257 - val_acc: 0.6923\n",
            "Epoch 5/30\n",
            "110/110 [==============================] - 0s 204us/step - loss: 0.7565 - acc: 0.7636 - val_loss: 0.6685 - val_acc: 0.7692\n",
            "Epoch 6/30\n",
            "110/110 [==============================] - 0s 219us/step - loss: 0.7041 - acc: 0.7909 - val_loss: 0.6274 - val_acc: 0.7692\n",
            "Epoch 7/30\n",
            "110/110 [==============================] - 0s 261us/step - loss: 0.6651 - acc: 0.8091 - val_loss: 0.5938 - val_acc: 0.8462\n",
            "Epoch 8/30\n",
            "110/110 [==============================] - 0s 199us/step - loss: 0.6346 - acc: 0.8273 - val_loss: 0.5675 - val_acc: 0.8462\n",
            "Epoch 9/30\n",
            "110/110 [==============================] - 0s 210us/step - loss: 0.6092 - acc: 0.8273 - val_loss: 0.5458 - val_acc: 0.8462\n",
            "Epoch 10/30\n",
            "110/110 [==============================] - 0s 197us/step - loss: 0.5886 - acc: 0.8455 - val_loss: 0.5271 - val_acc: 0.8462\n",
            "Epoch 11/30\n",
            "110/110 [==============================] - 0s 219us/step - loss: 0.5708 - acc: 0.8545 - val_loss: 0.5121 - val_acc: 0.8462\n",
            "Epoch 12/30\n",
            "110/110 [==============================] - 0s 209us/step - loss: 0.5558 - acc: 0.8727 - val_loss: 0.4987 - val_acc: 0.8462\n",
            "Epoch 13/30\n",
            "110/110 [==============================] - 0s 199us/step - loss: 0.5427 - acc: 0.8818 - val_loss: 0.4872 - val_acc: 0.8462\n",
            "Epoch 14/30\n",
            "110/110 [==============================] - 0s 218us/step - loss: 0.5310 - acc: 0.8909 - val_loss: 0.4766 - val_acc: 0.8462\n",
            "Epoch 15/30\n",
            "110/110 [==============================] - 0s 225us/step - loss: 0.5207 - acc: 0.8909 - val_loss: 0.4672 - val_acc: 0.9231\n",
            "Epoch 16/30\n",
            "110/110 [==============================] - 0s 247us/step - loss: 0.5114 - acc: 0.9000 - val_loss: 0.4590 - val_acc: 0.9231\n",
            "Epoch 17/30\n",
            "110/110 [==============================] - 0s 226us/step - loss: 0.5030 - acc: 0.9091 - val_loss: 0.4512 - val_acc: 0.9231\n",
            "Epoch 18/30\n",
            "110/110 [==============================] - 0s 230us/step - loss: 0.4952 - acc: 0.9182 - val_loss: 0.4442 - val_acc: 0.9231\n",
            "Epoch 19/30\n",
            "110/110 [==============================] - 0s 220us/step - loss: 0.4879 - acc: 0.9182 - val_loss: 0.4373 - val_acc: 0.9231\n",
            "Epoch 20/30\n",
            "110/110 [==============================] - 0s 218us/step - loss: 0.4811 - acc: 0.9182 - val_loss: 0.4310 - val_acc: 0.9231\n",
            "Epoch 21/30\n",
            "110/110 [==============================] - 0s 245us/step - loss: 0.4748 - acc: 0.9273 - val_loss: 0.4256 - val_acc: 0.9231\n",
            "Epoch 22/30\n",
            "110/110 [==============================] - 0s 265us/step - loss: 0.4690 - acc: 0.9273 - val_loss: 0.4204 - val_acc: 0.9231\n",
            "Epoch 23/30\n",
            "110/110 [==============================] - 0s 220us/step - loss: 0.4633 - acc: 0.9364 - val_loss: 0.4156 - val_acc: 0.9231\n",
            "Epoch 24/30\n",
            "110/110 [==============================] - 0s 224us/step - loss: 0.4579 - acc: 0.9364 - val_loss: 0.4106 - val_acc: 0.9231\n",
            "Epoch 25/30\n",
            "110/110 [==============================] - 0s 206us/step - loss: 0.4528 - acc: 0.9364 - val_loss: 0.4061 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "110/110 [==============================] - 0s 217us/step - loss: 0.4479 - acc: 0.9455 - val_loss: 0.4017 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "110/110 [==============================] - 0s 214us/step - loss: 0.4433 - acc: 0.9455 - val_loss: 0.3975 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "110/110 [==============================] - 0s 196us/step - loss: 0.4387 - acc: 0.9455 - val_loss: 0.3936 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "110/110 [==============================] - 0s 223us/step - loss: 0.4344 - acc: 0.9455 - val_loss: 0.3898 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "110/110 [==============================] - 0s 245us/step - loss: 0.4303 - acc: 0.9455 - val_loss: 0.3862 - val_acc: 1.0000\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "110/110 [==============================] - 3s 26ms/step - loss: 1.5792 - acc: 0.0909 - val_loss: 1.5816 - val_acc: 0.2308\n",
            "Epoch 2/30\n",
            "110/110 [==============================] - 0s 268us/step - loss: 1.3827 - acc: 0.2091 - val_loss: 1.4021 - val_acc: 0.2308\n",
            "Epoch 3/30\n",
            "110/110 [==============================] - 0s 248us/step - loss: 1.2204 - acc: 0.3364 - val_loss: 1.2470 - val_acc: 0.2308\n",
            "Epoch 4/30\n",
            "110/110 [==============================] - 0s 247us/step - loss: 1.0881 - acc: 0.4727 - val_loss: 1.1226 - val_acc: 0.3077\n",
            "Epoch 5/30\n",
            "110/110 [==============================] - 0s 239us/step - loss: 0.9816 - acc: 0.5455 - val_loss: 1.0176 - val_acc: 0.4615\n",
            "Epoch 6/30\n",
            "110/110 [==============================] - 0s 227us/step - loss: 0.8930 - acc: 0.5727 - val_loss: 0.9313 - val_acc: 0.4615\n",
            "Epoch 7/30\n",
            "110/110 [==============================] - 0s 216us/step - loss: 0.8213 - acc: 0.5909 - val_loss: 0.8608 - val_acc: 0.4615\n",
            "Epoch 8/30\n",
            "110/110 [==============================] - 0s 197us/step - loss: 0.7615 - acc: 0.5909 - val_loss: 0.7993 - val_acc: 0.5385\n",
            "Epoch 9/30\n",
            "110/110 [==============================] - 0s 222us/step - loss: 0.7116 - acc: 0.5909 - val_loss: 0.7483 - val_acc: 0.6154\n",
            "Epoch 10/30\n",
            "110/110 [==============================] - 0s 259us/step - loss: 0.6689 - acc: 0.5909 - val_loss: 0.7045 - val_acc: 0.6154\n",
            "Epoch 11/30\n",
            "110/110 [==============================] - 0s 209us/step - loss: 0.6319 - acc: 0.6000 - val_loss: 0.6656 - val_acc: 0.6154\n",
            "Epoch 12/30\n",
            "110/110 [==============================] - 0s 245us/step - loss: 0.5995 - acc: 0.6000 - val_loss: 0.6323 - val_acc: 0.6154\n",
            "Epoch 13/30\n",
            "110/110 [==============================] - 0s 191us/step - loss: 0.5715 - acc: 0.6000 - val_loss: 0.6027 - val_acc: 0.6154\n",
            "Epoch 14/30\n",
            "110/110 [==============================] - 0s 230us/step - loss: 0.5467 - acc: 0.6000 - val_loss: 0.5766 - val_acc: 0.6154\n",
            "Epoch 15/30\n",
            "110/110 [==============================] - 0s 214us/step - loss: 0.5248 - acc: 0.6000 - val_loss: 0.5532 - val_acc: 0.6154\n",
            "Epoch 16/30\n",
            "110/110 [==============================] - 0s 234us/step - loss: 0.5046 - acc: 0.6182 - val_loss: 0.5313 - val_acc: 0.6923\n",
            "Epoch 17/30\n",
            "110/110 [==============================] - 0s 243us/step - loss: 0.4862 - acc: 0.6545 - val_loss: 0.5117 - val_acc: 0.6923\n",
            "Epoch 18/30\n",
            "110/110 [==============================] - 0s 208us/step - loss: 0.4690 - acc: 0.6909 - val_loss: 0.4936 - val_acc: 0.6923\n",
            "Epoch 19/30\n",
            "110/110 [==============================] - 0s 216us/step - loss: 0.4530 - acc: 0.7909 - val_loss: 0.4764 - val_acc: 0.8462\n",
            "Epoch 20/30\n",
            "110/110 [==============================] - 0s 210us/step - loss: 0.4379 - acc: 0.9091 - val_loss: 0.4600 - val_acc: 0.9231\n",
            "Epoch 21/30\n",
            "110/110 [==============================] - 0s 211us/step - loss: 0.4237 - acc: 0.9818 - val_loss: 0.4449 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "110/110 [==============================] - 0s 211us/step - loss: 0.4099 - acc: 1.0000 - val_loss: 0.4300 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "110/110 [==============================] - 0s 255us/step - loss: 0.3967 - acc: 1.0000 - val_loss: 0.4161 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "110/110 [==============================] - 0s 218us/step - loss: 0.3840 - acc: 1.0000 - val_loss: 0.4029 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "110/110 [==============================] - 0s 261us/step - loss: 0.3719 - acc: 1.0000 - val_loss: 0.3901 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "110/110 [==============================] - 0s 241us/step - loss: 0.3601 - acc: 1.0000 - val_loss: 0.3775 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "110/110 [==============================] - 0s 250us/step - loss: 0.3487 - acc: 1.0000 - val_loss: 0.3656 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "110/110 [==============================] - 0s 245us/step - loss: 0.3377 - acc: 1.0000 - val_loss: 0.3541 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "110/110 [==============================] - 0s 224us/step - loss: 0.3270 - acc: 1.0000 - val_loss: 0.3428 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "110/110 [==============================] - 0s 240us/step - loss: 0.3167 - acc: 1.0000 - val_loss: 0.3322 - val_acc: 1.0000\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "110/110 [==============================] - 3s 26ms/step - loss: 1.6798 - acc: 0.6091 - val_loss: 1.9072 - val_acc: 0.6154\n",
            "Epoch 2/30\n",
            "110/110 [==============================] - 0s 253us/step - loss: 1.5542 - acc: 0.6091 - val_loss: 1.7628 - val_acc: 0.6154\n",
            "Epoch 3/30\n",
            "110/110 [==============================] - 0s 222us/step - loss: 1.4367 - acc: 0.6091 - val_loss: 1.6322 - val_acc: 0.6154\n",
            "Epoch 4/30\n",
            "110/110 [==============================] - 0s 224us/step - loss: 1.3295 - acc: 0.6091 - val_loss: 1.5102 - val_acc: 0.6154\n",
            "Epoch 5/30\n",
            "110/110 [==============================] - 0s 225us/step - loss: 1.2307 - acc: 0.6091 - val_loss: 1.3981 - val_acc: 0.6154\n",
            "Epoch 6/30\n",
            "110/110 [==============================] - 0s 219us/step - loss: 1.1405 - acc: 0.6091 - val_loss: 1.2908 - val_acc: 0.6154\n",
            "Epoch 7/30\n",
            "110/110 [==============================] - 0s 217us/step - loss: 1.0559 - acc: 0.6091 - val_loss: 1.1922 - val_acc: 0.6154\n",
            "Epoch 8/30\n",
            "110/110 [==============================] - 0s 231us/step - loss: 0.9787 - acc: 0.6091 - val_loss: 1.1032 - val_acc: 0.6154\n",
            "Epoch 9/30\n",
            "110/110 [==============================] - 0s 227us/step - loss: 0.9080 - acc: 0.6182 - val_loss: 1.0199 - val_acc: 0.6154\n",
            "Epoch 10/30\n",
            "110/110 [==============================] - 0s 238us/step - loss: 0.8432 - acc: 0.6364 - val_loss: 0.9411 - val_acc: 0.6154\n",
            "Epoch 11/30\n",
            "110/110 [==============================] - 0s 220us/step - loss: 0.7826 - acc: 0.6545 - val_loss: 0.8689 - val_acc: 0.6154\n",
            "Epoch 12/30\n",
            "110/110 [==============================] - 0s 238us/step - loss: 0.7269 - acc: 0.6636 - val_loss: 0.8043 - val_acc: 0.6154\n",
            "Epoch 13/30\n",
            "110/110 [==============================] - 0s 254us/step - loss: 0.6760 - acc: 0.6636 - val_loss: 0.7426 - val_acc: 0.6154\n",
            "Epoch 14/30\n",
            "110/110 [==============================] - 0s 232us/step - loss: 0.6288 - acc: 0.7091 - val_loss: 0.6855 - val_acc: 0.6154\n",
            "Epoch 15/30\n",
            "110/110 [==============================] - 0s 235us/step - loss: 0.5857 - acc: 0.7273 - val_loss: 0.6330 - val_acc: 0.6923\n",
            "Epoch 16/30\n",
            "110/110 [==============================] - 0s 250us/step - loss: 0.5465 - acc: 0.7545 - val_loss: 0.5863 - val_acc: 0.6923\n",
            "Epoch 17/30\n",
            "110/110 [==============================] - 0s 312us/step - loss: 0.5107 - acc: 0.7818 - val_loss: 0.5426 - val_acc: 0.6923\n",
            "Epoch 18/30\n",
            "110/110 [==============================] - 0s 208us/step - loss: 0.4777 - acc: 0.8091 - val_loss: 0.5025 - val_acc: 0.7692\n",
            "Epoch 19/30\n",
            "110/110 [==============================] - 0s 225us/step - loss: 0.4476 - acc: 0.8727 - val_loss: 0.4653 - val_acc: 0.8462\n",
            "Epoch 20/30\n",
            "110/110 [==============================] - 0s 223us/step - loss: 0.4200 - acc: 0.9000 - val_loss: 0.4317 - val_acc: 0.9231\n",
            "Epoch 21/30\n",
            "110/110 [==============================] - 0s 260us/step - loss: 0.3950 - acc: 0.9182 - val_loss: 0.4012 - val_acc: 0.9231\n",
            "Epoch 22/30\n",
            "110/110 [==============================] - 0s 208us/step - loss: 0.3722 - acc: 0.9364 - val_loss: 0.3748 - val_acc: 0.9231\n",
            "Epoch 23/30\n",
            "110/110 [==============================] - 0s 210us/step - loss: 0.3511 - acc: 0.9455 - val_loss: 0.3488 - val_acc: 0.9231\n",
            "Epoch 24/30\n",
            "110/110 [==============================] - 0s 209us/step - loss: 0.3323 - acc: 0.9455 - val_loss: 0.3256 - val_acc: 0.9231\n",
            "Epoch 25/30\n",
            "110/110 [==============================] - 0s 207us/step - loss: 0.3147 - acc: 0.9545 - val_loss: 0.3052 - val_acc: 0.9231\n",
            "Epoch 26/30\n",
            "110/110 [==============================] - 0s 235us/step - loss: 0.2988 - acc: 0.9545 - val_loss: 0.2860 - val_acc: 0.9231\n",
            "Epoch 27/30\n",
            "110/110 [==============================] - 0s 258us/step - loss: 0.2842 - acc: 0.9545 - val_loss: 0.2691 - val_acc: 0.9231\n",
            "Epoch 28/30\n",
            "110/110 [==============================] - 0s 258us/step - loss: 0.2705 - acc: 0.9636 - val_loss: 0.2533 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "110/110 [==============================] - 0s 244us/step - loss: 0.2581 - acc: 0.9727 - val_loss: 0.2389 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "110/110 [==============================] - 0s 220us/step - loss: 0.2465 - acc: 0.9727 - val_loss: 0.2257 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 27ms/step - loss: 0.5456 - acc: 0.7117 - val_loss: 0.6788 - val_acc: 0.6667\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.5174 - acc: 0.7297 - val_loss: 0.6659 - val_acc: 0.6667\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 224us/step - loss: 0.4958 - acc: 0.7568 - val_loss: 0.6543 - val_acc: 0.7500\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 217us/step - loss: 0.4776 - acc: 0.7838 - val_loss: 0.6438 - val_acc: 0.7500\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 262us/step - loss: 0.4625 - acc: 0.8288 - val_loss: 0.6342 - val_acc: 0.7500\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 246us/step - loss: 0.4486 - acc: 0.8468 - val_loss: 0.6253 - val_acc: 0.7500\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 243us/step - loss: 0.4366 - acc: 0.8559 - val_loss: 0.6168 - val_acc: 0.7500\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 243us/step - loss: 0.4251 - acc: 0.8829 - val_loss: 0.6091 - val_acc: 0.7500\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 237us/step - loss: 0.4147 - acc: 0.8829 - val_loss: 0.6018 - val_acc: 0.7500\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 257us/step - loss: 0.4054 - acc: 0.8919 - val_loss: 0.5950 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.3964 - acc: 0.9009 - val_loss: 0.5886 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 0.3880 - acc: 0.9009 - val_loss: 0.5826 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 245us/step - loss: 0.3803 - acc: 0.9099 - val_loss: 0.5765 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 268us/step - loss: 0.3729 - acc: 0.9099 - val_loss: 0.5710 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 236us/step - loss: 0.3657 - acc: 0.9099 - val_loss: 0.5657 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 257us/step - loss: 0.3591 - acc: 0.9189 - val_loss: 0.5606 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 236us/step - loss: 0.3525 - acc: 0.9189 - val_loss: 0.5552 - val_acc: 0.7500\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 240us/step - loss: 0.3461 - acc: 0.9189 - val_loss: 0.5506 - val_acc: 0.7500\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 248us/step - loss: 0.3400 - acc: 0.9279 - val_loss: 0.5458 - val_acc: 0.7500\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.3343 - acc: 0.9279 - val_loss: 0.5413 - val_acc: 0.7500\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 229us/step - loss: 0.3288 - acc: 0.9369 - val_loss: 0.5369 - val_acc: 0.7500\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.3236 - acc: 0.9459 - val_loss: 0.5326 - val_acc: 0.7500\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.3186 - acc: 0.9459 - val_loss: 0.5285 - val_acc: 0.7500\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.3134 - acc: 0.9550 - val_loss: 0.5244 - val_acc: 0.7500\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.3085 - acc: 0.9550 - val_loss: 0.5205 - val_acc: 0.7500\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.3037 - acc: 0.9550 - val_loss: 0.5163 - val_acc: 0.7500\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 227us/step - loss: 0.2989 - acc: 0.9550 - val_loss: 0.5125 - val_acc: 0.7500\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 224us/step - loss: 0.2943 - acc: 0.9550 - val_loss: 0.5080 - val_acc: 0.7500\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 228us/step - loss: 0.2896 - acc: 0.9550 - val_loss: 0.5043 - val_acc: 0.7500\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.2851 - acc: 0.9550 - val_loss: 0.5003 - val_acc: 0.7500\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 27ms/step - loss: 0.9983 - acc: 0.4505 - val_loss: 0.9800 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 230us/step - loss: 0.9185 - acc: 0.4955 - val_loss: 0.8898 - val_acc: 0.5000\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 0.8513 - acc: 0.5315 - val_loss: 0.8177 - val_acc: 0.5000\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.7936 - acc: 0.5856 - val_loss: 0.7567 - val_acc: 0.5000\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 0.7445 - acc: 0.6216 - val_loss: 0.7032 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.7007 - acc: 0.6757 - val_loss: 0.6546 - val_acc: 0.8333\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.6610 - acc: 0.7658 - val_loss: 0.6117 - val_acc: 0.9167\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 0.6252 - acc: 0.8378 - val_loss: 0.5724 - val_acc: 0.9167\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 237us/step - loss: 0.5924 - acc: 0.8739 - val_loss: 0.5369 - val_acc: 0.9167\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 227us/step - loss: 0.5618 - acc: 0.9369 - val_loss: 0.5056 - val_acc: 0.9167\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.5342 - acc: 0.9459 - val_loss: 0.4770 - val_acc: 1.0000\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 258us/step - loss: 0.5083 - acc: 0.9550 - val_loss: 0.4507 - val_acc: 1.0000\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 0.4846 - acc: 0.9550 - val_loss: 0.4264 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 0.4626 - acc: 0.9640 - val_loss: 0.4035 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 255us/step - loss: 0.4421 - acc: 0.9640 - val_loss: 0.3828 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.4233 - acc: 0.9730 - val_loss: 0.3637 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 252us/step - loss: 0.4056 - acc: 0.9730 - val_loss: 0.3457 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 259us/step - loss: 0.3893 - acc: 0.9730 - val_loss: 0.3289 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 228us/step - loss: 0.3738 - acc: 0.9730 - val_loss: 0.3132 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 286us/step - loss: 0.3593 - acc: 0.9730 - val_loss: 0.2986 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 0.3454 - acc: 0.9820 - val_loss: 0.2847 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 262us/step - loss: 0.3324 - acc: 0.9820 - val_loss: 0.2717 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.3202 - acc: 0.9820 - val_loss: 0.2596 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.3085 - acc: 0.9820 - val_loss: 0.2481 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 0.2975 - acc: 0.9820 - val_loss: 0.2375 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.2873 - acc: 0.9820 - val_loss: 0.2274 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 230us/step - loss: 0.2775 - acc: 0.9820 - val_loss: 0.2177 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.2681 - acc: 0.9820 - val_loss: 0.2089 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 217us/step - loss: 0.2594 - acc: 0.9820 - val_loss: 0.2003 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.2511 - acc: 0.9820 - val_loss: 0.1924 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 27ms/step - loss: 2.4218 - acc: 0.0270 - val_loss: 2.2759 - val_acc: 0.1667\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 281us/step - loss: 2.0102 - acc: 0.0090 - val_loss: 1.8618 - val_acc: 0.1667\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 262us/step - loss: 1.6630 - acc: 0.0000e+00 - val_loss: 1.5143 - val_acc: 0.0833\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 240us/step - loss: 1.3926 - acc: 0.0000e+00 - val_loss: 1.2715 - val_acc: 0.0833\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 1.2005 - acc: 0.2162 - val_loss: 1.0960 - val_acc: 0.3333\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 221us/step - loss: 1.0642 - acc: 0.3423 - val_loss: 0.9763 - val_acc: 0.3333\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 235us/step - loss: 0.9680 - acc: 0.3604 - val_loss: 0.8901 - val_acc: 0.3333\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.8981 - acc: 0.3874 - val_loss: 0.8246 - val_acc: 0.4167\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 246us/step - loss: 0.8428 - acc: 0.3874 - val_loss: 0.7756 - val_acc: 0.4167\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 267us/step - loss: 0.7998 - acc: 0.3874 - val_loss: 0.7361 - val_acc: 0.4167\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 234us/step - loss: 0.7641 - acc: 0.3874 - val_loss: 0.7040 - val_acc: 0.4167\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.7351 - acc: 0.3874 - val_loss: 0.6755 - val_acc: 0.4167\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 236us/step - loss: 0.7100 - acc: 0.3874 - val_loss: 0.6513 - val_acc: 0.4167\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.6883 - acc: 0.3874 - val_loss: 0.6315 - val_acc: 0.4167\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 253us/step - loss: 0.6695 - acc: 0.5405 - val_loss: 0.6162 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 261us/step - loss: 0.6559 - acc: 0.5676 - val_loss: 0.6050 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 269us/step - loss: 0.6448 - acc: 0.6126 - val_loss: 0.5945 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 229us/step - loss: 0.6346 - acc: 0.8108 - val_loss: 0.5848 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 243us/step - loss: 0.6251 - acc: 0.9369 - val_loss: 0.5758 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.6160 - acc: 0.9369 - val_loss: 0.5673 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 241us/step - loss: 0.6074 - acc: 0.9459 - val_loss: 0.5593 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.5993 - acc: 0.9550 - val_loss: 0.5513 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 254us/step - loss: 0.5915 - acc: 0.9550 - val_loss: 0.5440 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.5839 - acc: 0.9550 - val_loss: 0.5367 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 219us/step - loss: 0.5765 - acc: 0.9550 - val_loss: 0.5295 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.5693 - acc: 0.9820 - val_loss: 0.5226 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 0.5624 - acc: 0.9820 - val_loss: 0.5159 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.5556 - acc: 0.9910 - val_loss: 0.5094 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.5489 - acc: 0.9910 - val_loss: 0.5028 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.5423 - acc: 0.9910 - val_loss: 0.4965 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 0.5231 - acc: 0.6847 - val_loss: 0.5767 - val_acc: 0.6667\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 226us/step - loss: 0.3962 - acc: 0.9099 - val_loss: 0.4511 - val_acc: 0.8333\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 194us/step - loss: 0.3203 - acc: 0.9910 - val_loss: 0.3725 - val_acc: 1.0000\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 196us/step - loss: 0.2719 - acc: 0.9910 - val_loss: 0.3215 - val_acc: 1.0000\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 196us/step - loss: 0.2398 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 1.0000\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.2167 - acc: 1.0000 - val_loss: 0.2572 - val_acc: 1.0000\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 235us/step - loss: 0.1992 - acc: 1.0000 - val_loss: 0.2359 - val_acc: 1.0000\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 0.1855 - acc: 1.0000 - val_loss: 0.2190 - val_acc: 1.0000\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 244us/step - loss: 0.1745 - acc: 1.0000 - val_loss: 0.2052 - val_acc: 1.0000\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 243us/step - loss: 0.1652 - acc: 0.9910 - val_loss: 0.1936 - val_acc: 1.0000\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 0.1574 - acc: 0.9910 - val_loss: 0.1840 - val_acc: 1.0000\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 238us/step - loss: 0.1507 - acc: 0.9910 - val_loss: 0.1754 - val_acc: 1.0000\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 237us/step - loss: 0.1447 - acc: 0.9910 - val_loss: 0.1680 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 230us/step - loss: 0.1394 - acc: 0.9910 - val_loss: 0.1615 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 262us/step - loss: 0.1347 - acc: 0.9910 - val_loss: 0.1555 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 252us/step - loss: 0.1305 - acc: 0.9910 - val_loss: 0.1502 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 244us/step - loss: 0.1265 - acc: 0.9910 - val_loss: 0.1454 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 244us/step - loss: 0.1229 - acc: 0.9910 - val_loss: 0.1409 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 264us/step - loss: 0.1195 - acc: 0.9910 - val_loss: 0.1369 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 249us/step - loss: 0.1164 - acc: 0.9910 - val_loss: 0.1331 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 227us/step - loss: 0.1135 - acc: 0.9910 - val_loss: 0.1295 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 249us/step - loss: 0.1107 - acc: 0.9910 - val_loss: 0.1264 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 280us/step - loss: 0.1081 - acc: 0.9910 - val_loss: 0.1233 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 229us/step - loss: 0.1055 - acc: 0.9910 - val_loss: 0.1203 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.1031 - acc: 0.9910 - val_loss: 0.1176 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.1007 - acc: 0.9910 - val_loss: 0.1151 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 252us/step - loss: 0.0985 - acc: 0.9910 - val_loss: 0.1126 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.0965 - acc: 0.9910 - val_loss: 0.1103 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.0945 - acc: 0.9910 - val_loss: 0.1082 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.0926 - acc: 0.9910 - val_loss: 0.1061 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 3.3038 - acc: 0.0360 - val_loss: 2.6260 - val_acc: 0.0000e+00\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 246us/step - loss: 2.9146 - acc: 0.0360 - val_loss: 2.3349 - val_acc: 0.0833\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 2.5806 - acc: 0.0541 - val_loss: 2.0863 - val_acc: 0.1667\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 2.2976 - acc: 0.0811 - val_loss: 1.8820 - val_acc: 0.2500\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 188us/step - loss: 2.0648 - acc: 0.1532 - val_loss: 1.7067 - val_acc: 0.3333\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 1.8643 - acc: 0.2973 - val_loss: 1.5573 - val_acc: 0.3333\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 1.6950 - acc: 0.3333 - val_loss: 1.4268 - val_acc: 0.4167\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 228us/step - loss: 1.5513 - acc: 0.3694 - val_loss: 1.3153 - val_acc: 0.4167\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 1.4256 - acc: 0.3784 - val_loss: 1.2172 - val_acc: 0.4167\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 285us/step - loss: 1.3169 - acc: 0.3874 - val_loss: 1.1307 - val_acc: 0.4167\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 247us/step - loss: 1.2221 - acc: 0.3874 - val_loss: 1.0528 - val_acc: 0.4167\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 251us/step - loss: 1.1357 - acc: 0.3964 - val_loss: 0.9861 - val_acc: 0.4167\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 271us/step - loss: 1.0615 - acc: 0.3964 - val_loss: 0.9285 - val_acc: 0.4167\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.9952 - acc: 0.3964 - val_loss: 0.8825 - val_acc: 0.4167\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.9380 - acc: 0.3964 - val_loss: 0.8448 - val_acc: 0.4167\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.8947 - acc: 0.3964 - val_loss: 0.8160 - val_acc: 0.4167\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.8606 - acc: 0.3964 - val_loss: 0.7897 - val_acc: 0.4167\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.8306 - acc: 0.3964 - val_loss: 0.7652 - val_acc: 0.4167\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 227us/step - loss: 0.8035 - acc: 0.4144 - val_loss: 0.7419 - val_acc: 0.5833\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.7783 - acc: 0.7117 - val_loss: 0.7202 - val_acc: 0.8333\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 0.7549 - acc: 0.7658 - val_loss: 0.6987 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.7324 - acc: 0.7658 - val_loss: 0.6787 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 211us/step - loss: 0.7114 - acc: 0.7658 - val_loss: 0.6594 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.6915 - acc: 0.7658 - val_loss: 0.6411 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.6725 - acc: 0.7658 - val_loss: 0.6233 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 226us/step - loss: 0.6544 - acc: 0.7658 - val_loss: 0.6060 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 247us/step - loss: 0.6371 - acc: 0.7658 - val_loss: 0.5898 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 289us/step - loss: 0.6206 - acc: 0.7658 - val_loss: 0.5736 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 252us/step - loss: 0.6048 - acc: 0.7658 - val_loss: 0.5588 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 287us/step - loss: 0.5897 - acc: 0.7658 - val_loss: 0.5442 - val_acc: 0.8333\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 28ms/step - loss: 1.8174 - acc: 0.0180 - val_loss: 1.7013 - val_acc: 0.0000e+00\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 263us/step - loss: 1.5813 - acc: 0.0360 - val_loss: 1.4985 - val_acc: 0.0833\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 219us/step - loss: 1.3870 - acc: 0.0991 - val_loss: 1.3326 - val_acc: 0.0833\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 1.2304 - acc: 0.2342 - val_loss: 1.1927 - val_acc: 0.0833\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 1.1017 - acc: 0.3514 - val_loss: 1.0777 - val_acc: 0.5000\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.9946 - acc: 0.4324 - val_loss: 0.9773 - val_acc: 0.5000\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.9023 - acc: 0.5045 - val_loss: 0.8940 - val_acc: 0.5000\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 267us/step - loss: 0.8243 - acc: 0.5405 - val_loss: 0.8184 - val_acc: 0.5833\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 246us/step - loss: 0.7550 - acc: 0.5766 - val_loss: 0.7546 - val_acc: 0.5833\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 245us/step - loss: 0.6953 - acc: 0.6306 - val_loss: 0.6963 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 241us/step - loss: 0.6427 - acc: 0.8198 - val_loss: 0.6449 - val_acc: 1.0000\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 247us/step - loss: 0.5963 - acc: 0.9910 - val_loss: 0.5995 - val_acc: 1.0000\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 272us/step - loss: 0.5553 - acc: 0.9910 - val_loss: 0.5593 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 231us/step - loss: 0.5187 - acc: 0.9910 - val_loss: 0.5228 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 241us/step - loss: 0.4857 - acc: 0.9910 - val_loss: 0.4901 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 275us/step - loss: 0.4562 - acc: 0.9910 - val_loss: 0.4608 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 226us/step - loss: 0.4293 - acc: 0.9910 - val_loss: 0.4344 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 229us/step - loss: 0.4051 - acc: 0.9910 - val_loss: 0.4093 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 224us/step - loss: 0.3829 - acc: 0.9910 - val_loss: 0.3875 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.3630 - acc: 0.9910 - val_loss: 0.3672 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 231us/step - loss: 0.3448 - acc: 1.0000 - val_loss: 0.3483 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 252us/step - loss: 0.3281 - acc: 1.0000 - val_loss: 0.3310 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 276us/step - loss: 0.3128 - acc: 1.0000 - val_loss: 0.3152 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 233us/step - loss: 0.2989 - acc: 1.0000 - val_loss: 0.3003 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 260us/step - loss: 0.2859 - acc: 1.0000 - val_loss: 0.2864 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 236us/step - loss: 0.2739 - acc: 1.0000 - val_loss: 0.2737 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 246us/step - loss: 0.2628 - acc: 1.0000 - val_loss: 0.2619 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.2526 - acc: 1.0000 - val_loss: 0.2508 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.2430 - acc: 1.0000 - val_loss: 0.2405 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.2341 - acc: 1.0000 - val_loss: 0.2310 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 3s 29ms/step - loss: 2.3072 - acc: 0.3874 - val_loss: 1.8305 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 233us/step - loss: 1.9988 - acc: 0.3874 - val_loss: 1.5716 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 239us/step - loss: 1.7395 - acc: 0.3874 - val_loss: 1.3636 - val_acc: 0.4167\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 260us/step - loss: 1.5288 - acc: 0.4595 - val_loss: 1.1889 - val_acc: 0.6667\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 230us/step - loss: 1.3530 - acc: 0.5856 - val_loss: 1.0441 - val_acc: 0.8333\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 1.2048 - acc: 0.6757 - val_loss: 0.9259 - val_acc: 0.8333\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 1.0841 - acc: 0.6937 - val_loss: 0.8233 - val_acc: 0.8333\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.9818 - acc: 0.7027 - val_loss: 0.7399 - val_acc: 0.8333\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.8969 - acc: 0.7117 - val_loss: 0.6700 - val_acc: 0.8333\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 230us/step - loss: 0.8255 - acc: 0.7117 - val_loss: 0.6124 - val_acc: 0.8333\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 202us/step - loss: 0.7654 - acc: 0.7207 - val_loss: 0.5635 - val_acc: 0.8333\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.7146 - acc: 0.7207 - val_loss: 0.5216 - val_acc: 0.8333\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.6707 - acc: 0.7207 - val_loss: 0.4854 - val_acc: 0.8333\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.6320 - acc: 0.7297 - val_loss: 0.4538 - val_acc: 0.8333\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.5976 - acc: 0.7387 - val_loss: 0.4261 - val_acc: 0.8333\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.5680 - acc: 0.7387 - val_loss: 0.4030 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 202us/step - loss: 0.5414 - acc: 0.7568 - val_loss: 0.3829 - val_acc: 0.8333\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.5185 - acc: 0.7568 - val_loss: 0.3646 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.4979 - acc: 0.7568 - val_loss: 0.3484 - val_acc: 0.8333\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 275us/step - loss: 0.4795 - acc: 0.7568 - val_loss: 0.3338 - val_acc: 0.8333\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.4629 - acc: 0.7568 - val_loss: 0.3210 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.4480 - acc: 0.7658 - val_loss: 0.3092 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.4345 - acc: 0.7658 - val_loss: 0.2985 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.4219 - acc: 0.7658 - val_loss: 0.2889 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 231us/step - loss: 0.4104 - acc: 0.7658 - val_loss: 0.2797 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 245us/step - loss: 0.3997 - acc: 0.7748 - val_loss: 0.2715 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.3898 - acc: 0.7748 - val_loss: 0.2637 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.3806 - acc: 0.7928 - val_loss: 0.2565 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.3722 - acc: 0.8198 - val_loss: 0.2499 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.3645 - acc: 0.9189 - val_loss: 0.2437 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "09d36018-8b98-40cd-a175-9218836f4256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.94892679, -3.26204263],\n",
              "       [ 3.6902765 ,  1.91193691],\n",
              "       [ 4.33888855,  3.11923441],\n",
              "       [-0.19375808, -1.99141129],\n",
              "       [ 4.92020805,  2.60198864],\n",
              "       [-0.11763691, -3.08731746],\n",
              "       [ 0.27541973, -3.58936325],\n",
              "       [ 4.10074676,  2.79547975],\n",
              "       [ 0.81236067, -4.18441726],\n",
              "       [ 4.62588055,  1.71410556],\n",
              "       [-8.20314863,  2.62258764],\n",
              "       [-7.82534822,  2.32665836]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5fbb97e8-3af1-4517-8946-0004855df656",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "546d9754-7288-4177-ae34-0a45f45a9dff",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0a21a136-7192-4e42-df82-0714898c9091",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e13c8276-fa77-4714-ebad-fd5f7ae44893",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff9cc71b780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e5b792c1-6c00-47c2-f8fc-0e4794f0afa5",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff9cc6930b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxpgIZ4nAGGMinCUCY4yJcJYIjDEmwv0/o7RQ3stIykwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9a964ee1-88c4-4df3-ca10-53ebc6909a24",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "123/123 [==============================] - 3s 22ms/step - loss: 2.8252 - acc: 0.2195\n",
            "Epoch 2/30\n",
            "123/123 [==============================] - 0s 220us/step - loss: 2.3365 - acc: 0.2195\n",
            "Epoch 3/30\n",
            "123/123 [==============================] - 0s 200us/step - loss: 1.9180 - acc: 0.2195\n",
            "Epoch 4/30\n",
            "123/123 [==============================] - 0s 207us/step - loss: 1.5573 - acc: 0.2195\n",
            "Epoch 5/30\n",
            "123/123 [==============================] - 0s 235us/step - loss: 1.2681 - acc: 0.2195\n",
            "Epoch 6/30\n",
            "123/123 [==============================] - 0s 194us/step - loss: 1.0489 - acc: 0.2439\n",
            "Epoch 7/30\n",
            "123/123 [==============================] - 0s 192us/step - loss: 0.8919 - acc: 0.5854\n",
            "Epoch 8/30\n",
            "123/123 [==============================] - 0s 220us/step - loss: 0.7897 - acc: 0.6098\n",
            "Epoch 9/30\n",
            "123/123 [==============================] - 0s 245us/step - loss: 0.7205 - acc: 0.6098\n",
            "Epoch 10/30\n",
            "123/123 [==============================] - 0s 235us/step - loss: 0.6716 - acc: 0.6098\n",
            "Epoch 11/30\n",
            "123/123 [==============================] - 0s 193us/step - loss: 0.6331 - acc: 0.6098\n",
            "Epoch 12/30\n",
            "123/123 [==============================] - 0s 202us/step - loss: 0.6018 - acc: 0.6260\n",
            "Epoch 13/30\n",
            "123/123 [==============================] - 0s 198us/step - loss: 0.5754 - acc: 0.6423\n",
            "Epoch 14/30\n",
            "123/123 [==============================] - 0s 198us/step - loss: 0.5533 - acc: 0.6667\n",
            "Epoch 15/30\n",
            "123/123 [==============================] - 0s 214us/step - loss: 0.5338 - acc: 0.6992\n",
            "Epoch 16/30\n",
            "123/123 [==============================] - 0s 213us/step - loss: 0.5162 - acc: 0.7317\n",
            "Epoch 17/30\n",
            "123/123 [==============================] - 0s 209us/step - loss: 0.5002 - acc: 0.7561\n",
            "Epoch 18/30\n",
            "123/123 [==============================] - 0s 182us/step - loss: 0.4857 - acc: 0.7805\n",
            "Epoch 19/30\n",
            "123/123 [==============================] - 0s 209us/step - loss: 0.4723 - acc: 0.7967\n",
            "Epoch 20/30\n",
            "123/123 [==============================] - 0s 191us/step - loss: 0.4604 - acc: 0.8130\n",
            "Epoch 21/30\n",
            "123/123 [==============================] - 0s 189us/step - loss: 0.4493 - acc: 0.8293\n",
            "Epoch 22/30\n",
            "123/123 [==============================] - 0s 181us/step - loss: 0.4388 - acc: 0.8537\n",
            "Epoch 23/30\n",
            "123/123 [==============================] - 0s 211us/step - loss: 0.4293 - acc: 0.8780\n",
            "Epoch 24/30\n",
            "123/123 [==============================] - 0s 233us/step - loss: 0.4202 - acc: 0.8862\n",
            "Epoch 25/30\n",
            "123/123 [==============================] - 0s 192us/step - loss: 0.4115 - acc: 0.8943\n",
            "Epoch 26/30\n",
            "123/123 [==============================] - 0s 217us/step - loss: 0.4036 - acc: 0.8943\n",
            "Epoch 27/30\n",
            "123/123 [==============================] - 0s 203us/step - loss: 0.3964 - acc: 0.8943\n",
            "Epoch 28/30\n",
            "123/123 [==============================] - 0s 205us/step - loss: 0.3898 - acc: 0.8943\n",
            "Epoch 29/30\n",
            "123/123 [==============================] - 0s 204us/step - loss: 0.3839 - acc: 0.9024\n",
            "Epoch 30/30\n",
            "123/123 [==============================] - 0s 224us/step - loss: 0.3785 - acc: 0.9024\n",
            "42/42 [==============================] - 1s 27ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1a2456b7-a2b1-4d9b-f35b-3b95a53e4467",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0d1b3b17-b9e1-487d-a7f0-44f00e8d5a26",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.33333333617164973"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQV6AwSwrmDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merge_dataset_Network_classification_hystology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Merge_dataset_Network_classification_hystology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfLmMhRwN_ua",
        "colab_type": "text"
      },
      "source": [
        "Prove mettendo insieme i due dataset. Partendo da un unico dataset e dividendolo usando train_test_split.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYn4tlNNkCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "6c654213-5ba2-46a8-8fbe-216aed76e283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21MJpB7HPm8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tot_data = pd.concat([train_data, test_data], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IYan5zRsyi",
        "colab_type": "code",
        "outputId": "48b1401e-4ef0-4be7-b834-5f30d12aa752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H421egATgdD",
        "colab_type": "code",
        "outputId": "331d0996-3e48-43ac-dc11-7ac0a8bf2435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLhL0BxTi96",
        "colab_type": "code",
        "outputId": "0f479828-60ad-4277-8dac-712d2c9a7d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tot_data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cisCrQrHVZfj",
        "colab_type": "code",
        "outputId": "0a89dcec-4fff-483b-bf5a-3a02c27d2758",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tot_data"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51905.377962</td>\n",
              "      <td>66.288317</td>\n",
              "      <td>51847.748274</td>\n",
              "      <td>50.574214</td>\n",
              "      <td>0.649258</td>\n",
              "      <td>37.884620</td>\n",
              "      <td>0.821088</td>\n",
              "      <td>0.199752</td>\n",
              "      <td>63.135672</td>\n",
              "      <td>0.749090</td>\n",
              "      <td>10356.675894</td>\n",
              "      <td>41.525899</td>\n",
              "      <td>65.067279</td>\n",
              "      <td>55.325619</td>\n",
              "      <td>12.481889</td>\n",
              "      <td>1084.854684</td>\n",
              "      <td>5.949646</td>\n",
              "      <td>918.046673</td>\n",
              "      <td>5312.127441</td>\n",
              "      <td>0.122582</td>\n",
              "      <td>97.652454</td>\n",
              "      <td>0.050648</td>\n",
              "      <td>141.807349</td>\n",
              "      <td>0.124010</td>\n",
              "      <td>42.380287</td>\n",
              "      <td>163828.301666</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>33.190463</td>\n",
              "      <td>66.380925</td>\n",
              "      <td>4.331076</td>\n",
              "      <td>-475.448161</td>\n",
              "      <td>0.308299</td>\n",
              "      <td>0.994676</td>\n",
              "      <td>0.133643</td>\n",
              "      <td>8.029403</td>\n",
              "      <td>2.169232</td>\n",
              "      <td>0.424312</td>\n",
              "      <td>6.065116</td>\n",
              "      <td>0.967009</td>\n",
              "      <td>...</td>\n",
              "      <td>-75.0</td>\n",
              "      <td>17.777521</td>\n",
              "      <td>5.650502</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>17.478141</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.192540</td>\n",
              "      <td>1.399893</td>\n",
              "      <td>2288.112500</td>\n",
              "      <td>3.749302</td>\n",
              "      <td>793.234742</td>\n",
              "      <td>6675.859117</td>\n",
              "      <td>0.776723</td>\n",
              "      <td>4220.221337</td>\n",
              "      <td>0.654950</td>\n",
              "      <td>0.003806</td>\n",
              "      <td>4.209293</td>\n",
              "      <td>1049.544424</td>\n",
              "      <td>0.560736</td>\n",
              "      <td>34.869500</td>\n",
              "      <td>42116.076135</td>\n",
              "      <td>0.060025</td>\n",
              "      <td>0.517739</td>\n",
              "      <td>1145.238698</td>\n",
              "      <td>132.775769</td>\n",
              "      <td>42183.224231</td>\n",
              "      <td>540.316964</td>\n",
              "      <td>0.122035</td>\n",
              "      <td>36.510775</td>\n",
              "      <td>4.877236e+07</td>\n",
              "      <td>751.520796</td>\n",
              "      <td>0.747563</td>\n",
              "      <td>0.002453</td>\n",
              "      <td>5.741322</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>928.016789</td>\n",
              "      <td>1.153806</td>\n",
              "      <td>0.020920</td>\n",
              "      <td>1.306338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13432.502747</td>\n",
              "      <td>58.057539</td>\n",
              "      <td>13312.697411</td>\n",
              "      <td>48.354803</td>\n",
              "      <td>0.572236</td>\n",
              "      <td>18.135097</td>\n",
              "      <td>0.546338</td>\n",
              "      <td>0.356577</td>\n",
              "      <td>40.146103</td>\n",
              "      <td>0.375042</td>\n",
              "      <td>4747.006589</td>\n",
              "      <td>26.418066</td>\n",
              "      <td>32.760898</td>\n",
              "      <td>56.652510</td>\n",
              "      <td>60.615944</td>\n",
              "      <td>1076.589137</td>\n",
              "      <td>7.130906</td>\n",
              "      <td>595.667519</td>\n",
              "      <td>351.846858</td>\n",
              "      <td>0.307871</td>\n",
              "      <td>239.202712</td>\n",
              "      <td>0.126873</td>\n",
              "      <td>32.011715</td>\n",
              "      <td>0.031426</td>\n",
              "      <td>12.612334</td>\n",
              "      <td>41890.348882</td>\n",
              "      <td>0.001812</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>33.522040</td>\n",
              "      <td>67.044080</td>\n",
              "      <td>7.487967</td>\n",
              "      <td>-2829.110940</td>\n",
              "      <td>0.055759</td>\n",
              "      <td>0.985695</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>38.337756</td>\n",
              "      <td>3.408960</td>\n",
              "      <td>0.322015</td>\n",
              "      <td>23.246287</td>\n",
              "      <td>0.935189</td>\n",
              "      <td>...</td>\n",
              "      <td>-397.0</td>\n",
              "      <td>5.224099</td>\n",
              "      <td>-91.310969</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>65.432452</td>\n",
              "      <td>0.003524</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.230922</td>\n",
              "      <td>262.139314</td>\n",
              "      <td>1.567042</td>\n",
              "      <td>926.829706</td>\n",
              "      <td>3237.676584</td>\n",
              "      <td>0.908183</td>\n",
              "      <td>1777.017297</td>\n",
              "      <td>0.869059</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>5.072184</td>\n",
              "      <td>1044.275778</td>\n",
              "      <td>0.790377</td>\n",
              "      <td>84.615342</td>\n",
              "      <td>598.216508</td>\n",
              "      <td>0.033349</td>\n",
              "      <td>0.543736</td>\n",
              "      <td>843.878866</td>\n",
              "      <td>51.757732</td>\n",
              "      <td>607.367912</td>\n",
              "      <td>543.633876</td>\n",
              "      <td>0.330564</td>\n",
              "      <td>0.465530</td>\n",
              "      <td>8.137725e+05</td>\n",
              "      <td>763.567010</td>\n",
              "      <td>0.764879</td>\n",
              "      <td>0.006307</td>\n",
              "      <td>6.451087</td>\n",
              "      <td>0.004959</td>\n",
              "      <td>0.001680</td>\n",
              "      <td>2944.805484</td>\n",
              "      <td>2.266070</td>\n",
              "      <td>0.146173</td>\n",
              "      <td>0.253533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25843.872675</td>\n",
              "      <td>52.918217</td>\n",
              "      <td>25724.437234</td>\n",
              "      <td>39.406474</td>\n",
              "      <td>0.675497</td>\n",
              "      <td>28.487740</td>\n",
              "      <td>0.891907</td>\n",
              "      <td>0.242519</td>\n",
              "      <td>46.415213</td>\n",
              "      <td>0.722920</td>\n",
              "      <td>6238.658603</td>\n",
              "      <td>35.146929</td>\n",
              "      <td>47.180420</td>\n",
              "      <td>46.322906</td>\n",
              "      <td>55.064124</td>\n",
              "      <td>1131.900166</td>\n",
              "      <td>6.932158</td>\n",
              "      <td>844.783490</td>\n",
              "      <td>1023.136953</td>\n",
              "      <td>0.253452</td>\n",
              "      <td>192.059998</td>\n",
              "      <td>0.093605</td>\n",
              "      <td>57.855402</td>\n",
              "      <td>0.045979</td>\n",
              "      <td>22.286238</td>\n",
              "      <td>79266.271357</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>34.046184</td>\n",
              "      <td>68.092368</td>\n",
              "      <td>6.796695</td>\n",
              "      <td>-3133.512010</td>\n",
              "      <td>0.106695</td>\n",
              "      <td>0.984515</td>\n",
              "      <td>0.033042</td>\n",
              "      <td>32.375097</td>\n",
              "      <td>3.182144</td>\n",
              "      <td>0.362633</td>\n",
              "      <td>21.028555</td>\n",
              "      <td>0.936889</td>\n",
              "      <td>...</td>\n",
              "      <td>-363.0</td>\n",
              "      <td>5.387644</td>\n",
              "      <td>-67.724986</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>61.733697</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>0.085594</td>\n",
              "      <td>0.426431</td>\n",
              "      <td>626.125860</td>\n",
              "      <td>1.968083</td>\n",
              "      <td>906.575851</td>\n",
              "      <td>5266.417163</td>\n",
              "      <td>0.870954</td>\n",
              "      <td>2393.005796</td>\n",
              "      <td>0.809077</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>4.968901</td>\n",
              "      <td>1079.567623</td>\n",
              "      <td>0.718481</td>\n",
              "      <td>68.928494</td>\n",
              "      <td>3157.415098</td>\n",
              "      <td>0.036760</td>\n",
              "      <td>0.531612</td>\n",
              "      <td>1319.991542</td>\n",
              "      <td>91.276279</td>\n",
              "      <td>3170.626259</td>\n",
              "      <td>525.709823</td>\n",
              "      <td>0.275125</td>\n",
              "      <td>2.306913</td>\n",
              "      <td>4.377395e+06</td>\n",
              "      <td>732.877970</td>\n",
              "      <td>0.757392</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>6.294554</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>2146.780339</td>\n",
              "      <td>1.238883</td>\n",
              "      <td>0.152919</td>\n",
              "      <td>0.611772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22152.709032</td>\n",
              "      <td>46.635312</td>\n",
              "      <td>22099.016776</td>\n",
              "      <td>38.640414</td>\n",
              "      <td>0.733411</td>\n",
              "      <td>28.129382</td>\n",
              "      <td>0.754141</td>\n",
              "      <td>0.234970</td>\n",
              "      <td>39.274914</td>\n",
              "      <td>0.727978</td>\n",
              "      <td>5192.604222</td>\n",
              "      <td>29.140329</td>\n",
              "      <td>41.329017</td>\n",
              "      <td>46.543116</td>\n",
              "      <td>103.453079</td>\n",
              "      <td>1089.937565</td>\n",
              "      <td>7.205471</td>\n",
              "      <td>1087.724664</td>\n",
              "      <td>511.504912</td>\n",
              "      <td>0.336024</td>\n",
              "      <td>249.889273</td>\n",
              "      <td>0.140606</td>\n",
              "      <td>29.391158</td>\n",
              "      <td>0.026374</td>\n",
              "      <td>11.821883</td>\n",
              "      <td>42717.634566</td>\n",
              "      <td>0.001570</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>33.073644</td>\n",
              "      <td>66.147288</td>\n",
              "      <td>8.211500</td>\n",
              "      <td>-5741.187919</td>\n",
              "      <td>0.049759</td>\n",
              "      <td>0.975646</td>\n",
              "      <td>0.012491</td>\n",
              "      <td>58.811721</td>\n",
              "      <td>3.729477</td>\n",
              "      <td>0.290187</td>\n",
              "      <td>33.232290</td>\n",
              "      <td>0.913791</td>\n",
              "      <td>...</td>\n",
              "      <td>-528.5</td>\n",
              "      <td>2.714807</td>\n",
              "      <td>-102.779860</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>107.887272</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.054549</td>\n",
              "      <td>0.203221</td>\n",
              "      <td>370.362301</td>\n",
              "      <td>1.505960</td>\n",
              "      <td>924.243802</td>\n",
              "      <td>5447.401203</td>\n",
              "      <td>0.914390</td>\n",
              "      <td>1765.537152</td>\n",
              "      <td>0.877247</td>\n",
              "      <td>0.003439</td>\n",
              "      <td>5.297660</td>\n",
              "      <td>1044.150138</td>\n",
              "      <td>0.801694</td>\n",
              "      <td>100.015880</td>\n",
              "      <td>989.809275</td>\n",
              "      <td>0.028357</td>\n",
              "      <td>0.545916</td>\n",
              "      <td>1549.308668</td>\n",
              "      <td>80.477097</td>\n",
              "      <td>997.239605</td>\n",
              "      <td>527.497142</td>\n",
              "      <td>0.366856</td>\n",
              "      <td>0.671664</td>\n",
              "      <td>1.506307e+06</td>\n",
              "      <td>713.093023</td>\n",
              "      <td>0.767331</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>6.545703</td>\n",
              "      <td>0.003888</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>3136.738970</td>\n",
              "      <td>1.157976</td>\n",
              "      <td>0.351327</td>\n",
              "      <td>0.564313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>119385.805617</td>\n",
              "      <td>92.436320</td>\n",
              "      <td>119150.752851</td>\n",
              "      <td>75.629327</td>\n",
              "      <td>0.617296</td>\n",
              "      <td>47.029423</td>\n",
              "      <td>0.722141</td>\n",
              "      <td>0.159206</td>\n",
              "      <td>76.960070</td>\n",
              "      <td>0.621841</td>\n",
              "      <td>18969.476942</td>\n",
              "      <td>54.615019</td>\n",
              "      <td>83.612205</td>\n",
              "      <td>70.719598</td>\n",
              "      <td>67.305748</td>\n",
              "      <td>1445.561800</td>\n",
              "      <td>7.263383</td>\n",
              "      <td>4562.726560</td>\n",
              "      <td>3232.154350</td>\n",
              "      <td>0.241385</td>\n",
              "      <td>264.764085</td>\n",
              "      <td>0.109442</td>\n",
              "      <td>37.885083</td>\n",
              "      <td>0.028865</td>\n",
              "      <td>14.368248</td>\n",
              "      <td>66795.832602</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>37.463123</td>\n",
              "      <td>74.926246</td>\n",
              "      <td>7.979356</td>\n",
              "      <td>-5754.693763</td>\n",
              "      <td>0.063232</td>\n",
              "      <td>0.987519</td>\n",
              "      <td>0.014054</td>\n",
              "      <td>41.318562</td>\n",
              "      <td>3.487282</td>\n",
              "      <td>0.313702</td>\n",
              "      <td>24.584485</td>\n",
              "      <td>0.938581</td>\n",
              "      <td>...</td>\n",
              "      <td>-420.0</td>\n",
              "      <td>5.711559</td>\n",
              "      <td>-109.918927</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>71.446292</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>0.280825</td>\n",
              "      <td>2318.917381</td>\n",
              "      <td>1.665599</td>\n",
              "      <td>1227.793946</td>\n",
              "      <td>27327.614966</td>\n",
              "      <td>0.897233</td>\n",
              "      <td>2548.311647</td>\n",
              "      <td>0.851946</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>5.115966</td>\n",
              "      <td>1400.205550</td>\n",
              "      <td>0.767543</td>\n",
              "      <td>82.273203</td>\n",
              "      <td>7062.537600</td>\n",
              "      <td>0.034543</td>\n",
              "      <td>0.445967</td>\n",
              "      <td>4857.474477</td>\n",
              "      <td>376.237055</td>\n",
              "      <td>7077.188671</td>\n",
              "      <td>703.114744</td>\n",
              "      <td>0.261255</td>\n",
              "      <td>3.905480</td>\n",
              "      <td>1.289007e+07</td>\n",
              "      <td>1048.831160</td>\n",
              "      <td>0.694399</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>6.849399</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>3368.646263</td>\n",
              "      <td>0.390430</td>\n",
              "      <td>0.132602</td>\n",
              "      <td>1.804351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6592.266962</td>\n",
              "      <td>33.622119</td>\n",
              "      <td>6525.211805</td>\n",
              "      <td>26.483717</td>\n",
              "      <td>0.737649</td>\n",
              "      <td>16.326688</td>\n",
              "      <td>0.854215</td>\n",
              "      <td>0.350835</td>\n",
              "      <td>27.010370</td>\n",
              "      <td>0.616480</td>\n",
              "      <td>2289.271748</td>\n",
              "      <td>22.622792</td>\n",
              "      <td>32.335274</td>\n",
              "      <td>29.352647</td>\n",
              "      <td>21.910855</td>\n",
              "      <td>602.049910</td>\n",
              "      <td>6.617736</td>\n",
              "      <td>193.040506</td>\n",
              "      <td>462.455335</td>\n",
              "      <td>0.198303</td>\n",
              "      <td>78.741854</td>\n",
              "      <td>0.069816</td>\n",
              "      <td>94.932731</td>\n",
              "      <td>0.140683</td>\n",
              "      <td>35.081663</td>\n",
              "      <td>67119.461845</td>\n",
              "      <td>0.001239</td>\n",
              "      <td>0.003445</td>\n",
              "      <td>24.859564</td>\n",
              "      <td>49.719128</td>\n",
              "      <td>5.549785</td>\n",
              "      <td>-810.250238</td>\n",
              "      <td>0.148434</td>\n",
              "      <td>0.986138</td>\n",
              "      <td>0.062119</td>\n",
              "      <td>13.257631</td>\n",
              "      <td>2.653226</td>\n",
              "      <td>0.384951</td>\n",
              "      <td>8.884155</td>\n",
              "      <td>0.941150</td>\n",
              "      <td>...</td>\n",
              "      <td>-224.0</td>\n",
              "      <td>7.373263</td>\n",
              "      <td>-38.508861</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>26.164917</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.122205</td>\n",
              "      <td>0.816527</td>\n",
              "      <td>252.082403</td>\n",
              "      <td>2.689565</td>\n",
              "      <td>450.205653</td>\n",
              "      <td>1334.853146</td>\n",
              "      <td>0.829559</td>\n",
              "      <td>1752.475726</td>\n",
              "      <td>0.740910</td>\n",
              "      <td>0.006731</td>\n",
              "      <td>4.573485</td>\n",
              "      <td>567.808195</td>\n",
              "      <td>0.646468</td>\n",
              "      <td>30.944145</td>\n",
              "      <td>1838.790622</td>\n",
              "      <td>0.057837</td>\n",
              "      <td>0.487234</td>\n",
              "      <td>295.264026</td>\n",
              "      <td>35.049505</td>\n",
              "      <td>1859.608911</td>\n",
              "      <td>263.012843</td>\n",
              "      <td>0.219168</td>\n",
              "      <td>2.665039</td>\n",
              "      <td>1.307177e+06</td>\n",
              "      <td>385.825083</td>\n",
              "      <td>0.724813</td>\n",
              "      <td>0.007286</td>\n",
              "      <td>5.736757</td>\n",
              "      <td>0.004595</td>\n",
              "      <td>0.002543</td>\n",
              "      <td>645.520499</td>\n",
              "      <td>2.322894</td>\n",
              "      <td>0.085026</td>\n",
              "      <td>0.359230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3010.031479</td>\n",
              "      <td>29.286452</td>\n",
              "      <td>2976.851594</td>\n",
              "      <td>24.737747</td>\n",
              "      <td>0.653423</td>\n",
              "      <td>12.466119</td>\n",
              "      <td>0.668128</td>\n",
              "      <td>0.514483</td>\n",
              "      <td>23.150905</td>\n",
              "      <td>0.503931</td>\n",
              "      <td>1531.539258</td>\n",
              "      <td>16.527972</td>\n",
              "      <td>27.610305</td>\n",
              "      <td>26.839660</td>\n",
              "      <td>62.881425</td>\n",
              "      <td>869.396436</td>\n",
              "      <td>6.867420</td>\n",
              "      <td>345.579802</td>\n",
              "      <td>201.369901</td>\n",
              "      <td>0.339958</td>\n",
              "      <td>195.491748</td>\n",
              "      <td>0.136863</td>\n",
              "      <td>53.888713</td>\n",
              "      <td>0.047515</td>\n",
              "      <td>26.381284</td>\n",
              "      <td>65473.407525</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>29.963303</td>\n",
              "      <td>59.926606</td>\n",
              "      <td>7.422073</td>\n",
              "      <td>-2502.214292</td>\n",
              "      <td>0.108583</td>\n",
              "      <td>0.975065</td>\n",
              "      <td>0.024723</td>\n",
              "      <td>38.641415</td>\n",
              "      <td>3.546610</td>\n",
              "      <td>0.281848</td>\n",
              "      <td>21.558304</td>\n",
              "      <td>0.910929</td>\n",
              "      <td>...</td>\n",
              "      <td>-471.6</td>\n",
              "      <td>2.851107</td>\n",
              "      <td>-153.137426</td>\n",
              "      <td>0.002704</td>\n",
              "      <td>64.774609</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.056870</td>\n",
              "      <td>0.377849</td>\n",
              "      <td>120.266506</td>\n",
              "      <td>1.809069</td>\n",
              "      <td>689.951676</td>\n",
              "      <td>1625.228381</td>\n",
              "      <td>0.897161</td>\n",
              "      <td>1774.442365</td>\n",
              "      <td>0.836740</td>\n",
              "      <td>0.003555</td>\n",
              "      <td>5.118256</td>\n",
              "      <td>808.826968</td>\n",
              "      <td>0.768649</td>\n",
              "      <td>54.608361</td>\n",
              "      <td>364.145936</td>\n",
              "      <td>0.038857</td>\n",
              "      <td>0.528923</td>\n",
              "      <td>510.939959</td>\n",
              "      <td>37.536232</td>\n",
              "      <td>370.978261</td>\n",
              "      <td>407.183237</td>\n",
              "      <td>0.382574</td>\n",
              "      <td>0.307757</td>\n",
              "      <td>4.555961e+05</td>\n",
              "      <td>570.265010</td>\n",
              "      <td>0.755454</td>\n",
              "      <td>0.004471</td>\n",
              "      <td>6.087835</td>\n",
              "      <td>0.003856</td>\n",
              "      <td>0.002895</td>\n",
              "      <td>1472.717438</td>\n",
              "      <td>2.379395</td>\n",
              "      <td>0.295357</td>\n",
              "      <td>0.263438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>8683.195759</td>\n",
              "      <td>49.195074</td>\n",
              "      <td>8611.074213</td>\n",
              "      <td>46.624467</td>\n",
              "      <td>0.641262</td>\n",
              "      <td>14.348828</td>\n",
              "      <td>0.438028</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>31.523446</td>\n",
              "      <td>0.307753</td>\n",
              "      <td>3168.257409</td>\n",
              "      <td>20.422841</td>\n",
              "      <td>34.268292</td>\n",
              "      <td>37.102042</td>\n",
              "      <td>153.099954</td>\n",
              "      <td>738.203185</td>\n",
              "      <td>7.378490</td>\n",
              "      <td>847.045579</td>\n",
              "      <td>105.449204</td>\n",
              "      <td>0.443610</td>\n",
              "      <td>264.767425</td>\n",
              "      <td>0.232577</td>\n",
              "      <td>16.744097</td>\n",
              "      <td>0.124622</td>\n",
              "      <td>8.324220</td>\n",
              "      <td>22337.763317</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>25.523890</td>\n",
              "      <td>51.047781</td>\n",
              "      <td>9.676885</td>\n",
              "      <td>-2020.290555</td>\n",
              "      <td>0.026769</td>\n",
              "      <td>0.963103</td>\n",
              "      <td>0.002806</td>\n",
              "      <td>96.669264</td>\n",
              "      <td>4.273414</td>\n",
              "      <td>0.174092</td>\n",
              "      <td>40.625092</td>\n",
              "      <td>0.880158</td>\n",
              "      <td>...</td>\n",
              "      <td>-829.0</td>\n",
              "      <td>1.626201</td>\n",
              "      <td>-408.424767</td>\n",
              "      <td>0.008992</td>\n",
              "      <td>146.665900</td>\n",
              "      <td>0.009519</td>\n",
              "      <td>0.027008</td>\n",
              "      <td>0.124834</td>\n",
              "      <td>91.216703</td>\n",
              "      <td>1.292427</td>\n",
              "      <td>654.187277</td>\n",
              "      <td>2977.335607</td>\n",
              "      <td>0.950812</td>\n",
              "      <td>1057.363734</td>\n",
              "      <td>0.926858</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>5.643852</td>\n",
              "      <td>706.485685</td>\n",
              "      <td>0.880887</td>\n",
              "      <td>108.879134</td>\n",
              "      <td>36.053791</td>\n",
              "      <td>0.026627</td>\n",
              "      <td>0.500224</td>\n",
              "      <td>931.916801</td>\n",
              "      <td>49.605475</td>\n",
              "      <td>39.875470</td>\n",
              "      <td>425.432967</td>\n",
              "      <td>0.511532</td>\n",
              "      <td>0.172971</td>\n",
              "      <td>5.926173e+04</td>\n",
              "      <td>592.454106</td>\n",
              "      <td>0.735138</td>\n",
              "      <td>0.007341</td>\n",
              "      <td>6.687140</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>4040.182819</td>\n",
              "      <td>2.076410</td>\n",
              "      <td>0.594629</td>\n",
              "      <td>0.392208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>25939.914844</td>\n",
              "      <td>57.067095</td>\n",
              "      <td>25839.679802</td>\n",
              "      <td>51.745408</td>\n",
              "      <td>0.692948</td>\n",
              "      <td>26.633569</td>\n",
              "      <td>0.617498</td>\n",
              "      <td>0.236059</td>\n",
              "      <td>40.700499</td>\n",
              "      <td>0.514704</td>\n",
              "      <td>6099.697734</td>\n",
              "      <td>31.952700</td>\n",
              "      <td>41.839289</td>\n",
              "      <td>56.857818</td>\n",
              "      <td>32.995425</td>\n",
              "      <td>1449.528401</td>\n",
              "      <td>6.634924</td>\n",
              "      <td>576.773713</td>\n",
              "      <td>2560.111765</td>\n",
              "      <td>0.161550</td>\n",
              "      <td>157.459073</td>\n",
              "      <td>0.053012</td>\n",
              "      <td>163.638419</td>\n",
              "      <td>0.102321</td>\n",
              "      <td>53.465611</td>\n",
              "      <td>264022.398805</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>38.286575</td>\n",
              "      <td>76.573150</td>\n",
              "      <td>5.054850</td>\n",
              "      <td>-2238.871210</td>\n",
              "      <td>0.270459</td>\n",
              "      <td>0.991175</td>\n",
              "      <td>0.109092</td>\n",
              "      <td>17.532514</td>\n",
              "      <td>2.535238</td>\n",
              "      <td>0.343561</td>\n",
              "      <td>13.345420</td>\n",
              "      <td>0.959707</td>\n",
              "      <td>...</td>\n",
              "      <td>-218.0</td>\n",
              "      <td>11.861095</td>\n",
              "      <td>-47.050460</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>45.643158</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.141440</td>\n",
              "      <td>1.736298</td>\n",
              "      <td>985.371574</td>\n",
              "      <td>4.294691</td>\n",
              "      <td>989.771419</td>\n",
              "      <td>3814.874403</td>\n",
              "      <td>0.768468</td>\n",
              "      <td>6660.476094</td>\n",
              "      <td>0.634757</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>4.836890</td>\n",
              "      <td>1358.562247</td>\n",
              "      <td>0.549125</td>\n",
              "      <td>64.574608</td>\n",
              "      <td>13539.270699</td>\n",
              "      <td>0.045526</td>\n",
              "      <td>0.520480</td>\n",
              "      <td>994.636839</td>\n",
              "      <td>87.001047</td>\n",
              "      <td>13571.684982</td>\n",
              "      <td>678.036010</td>\n",
              "      <td>0.175643</td>\n",
              "      <td>8.433580</td>\n",
              "      <td>2.185777e+07</td>\n",
              "      <td>955.398744</td>\n",
              "      <td>0.749551</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>6.114671</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>1541.581036</td>\n",
              "      <td>2.194039</td>\n",
              "      <td>0.059377</td>\n",
              "      <td>0.658985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>73401.852784</td>\n",
              "      <td>73.105484</td>\n",
              "      <td>73287.411984</td>\n",
              "      <td>64.560114</td>\n",
              "      <td>0.791968</td>\n",
              "      <td>36.748716</td>\n",
              "      <td>0.681059</td>\n",
              "      <td>0.145915</td>\n",
              "      <td>52.860788</td>\n",
              "      <td>0.569217</td>\n",
              "      <td>10693.735802</td>\n",
              "      <td>43.969247</td>\n",
              "      <td>65.591520</td>\n",
              "      <td>72.942237</td>\n",
              "      <td>81.163824</td>\n",
              "      <td>991.869458</td>\n",
              "      <td>7.531933</td>\n",
              "      <td>2211.526781</td>\n",
              "      <td>3370.775879</td>\n",
              "      <td>0.190389</td>\n",
              "      <td>124.723669</td>\n",
              "      <td>0.071833</td>\n",
              "      <td>98.566310</td>\n",
              "      <td>0.094550</td>\n",
              "      <td>39.657307</td>\n",
              "      <td>123234.303667</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>30.846682</td>\n",
              "      <td>61.693363</td>\n",
              "      <td>7.183740</td>\n",
              "      <td>-5768.083577</td>\n",
              "      <td>0.140669</td>\n",
              "      <td>0.994400</td>\n",
              "      <td>0.037231</td>\n",
              "      <td>27.783111</td>\n",
              "      <td>3.108671</td>\n",
              "      <td>0.338089</td>\n",
              "      <td>18.042103</td>\n",
              "      <td>0.960845</td>\n",
              "      <td>...</td>\n",
              "      <td>-517.0</td>\n",
              "      <td>4.023106</td>\n",
              "      <td>-108.609186</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>93.372993</td>\n",
              "      <td>0.003554</td>\n",
              "      <td>0.069731</td>\n",
              "      <td>0.936258</td>\n",
              "      <td>1601.793815</td>\n",
              "      <td>2.772571</td>\n",
              "      <td>729.132148</td>\n",
              "      <td>15360.888892</td>\n",
              "      <td>0.842243</td>\n",
              "      <td>3104.316398</td>\n",
              "      <td>0.743261</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>5.455417</td>\n",
              "      <td>914.855419</td>\n",
              "      <td>0.668099</td>\n",
              "      <td>100.361763</td>\n",
              "      <td>13140.774613</td>\n",
              "      <td>0.029868</td>\n",
              "      <td>0.422933</td>\n",
              "      <td>2776.555217</td>\n",
              "      <td>196.083016</td>\n",
              "      <td>13162.766641</td>\n",
              "      <td>412.848389</td>\n",
              "      <td>0.213239</td>\n",
              "      <td>10.418845</td>\n",
              "      <td>1.670978e+07</td>\n",
              "      <td>618.251790</td>\n",
              "      <td>0.675352</td>\n",
              "      <td>0.005772</td>\n",
              "      <td>7.026210</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>4240.096417</td>\n",
              "      <td>1.189118</td>\n",
              "      <td>0.064501</td>\n",
              "      <td>0.895047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>165 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "0    51905.377962          66.288317  ...    0.020920  1.306338\n",
              "1    13432.502747          58.057539  ...    0.146173  0.253533\n",
              "2    25843.872675          52.918217  ...    0.152919  0.611772\n",
              "3    22152.709032          46.635312  ...    0.351327  0.564313\n",
              "4   119385.805617          92.436320  ...    0.132602  1.804351\n",
              "..            ...                ...  ...         ...       ...\n",
              "29    6592.266962          33.622119  ...    0.085026  0.359230\n",
              "30    3010.031479          29.286452  ...    0.295357  0.263438\n",
              "31    8683.195759          49.195074  ...    0.594629  0.392208\n",
              "32   25939.914844          57.067095  ...    0.059377  0.658985\n",
              "33   73401.852784          73.105484  ...    0.064501  0.895047\n",
              "\n",
              "[165 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNh5eL2PVIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tot_label = pd.concat([train_labels, test_labels], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcydh4qRVV6k",
        "colab_type": "code",
        "outputId": "af8a58c1-67b4-4d11-d714-a9d836e48b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tot_label.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cno5GmYCUynr",
        "colab_type": "text"
      },
      "source": [
        "##Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFVj6DbKUj9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFCCTjE6JrQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(tot_data, tot_label,\n",
        "                                                  stratify=tot_label, test_size=0.25, random_state=1) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "c3181c82-8095-4703-b029-7804decbb5ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZF8nUPWbrsR",
        "colab_type": "code",
        "outputId": "5b8fac32-111f-4b93-96fb-546f34a6e2a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_labels_dec.count(0)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "31a035e6-c32c-43d0-936b-3c5ec2722832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "382a3159-5ed1-4e6c-c017-c4f8b1d2601a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(5, activation='relu', input_shape=(9,)))\n",
        "\n",
        "#  model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.4, nesterov=True)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "b4b0915d-517e-4fa9-e038-c661e3925f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "7e1605ee-f986-479d-e1da-46a61418d56d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  36  38\n",
            "  39  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
            "  59  61  62  63  64  65  66  67  69  70  71  72  73  74  76  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 120\n",
            " 121 122] TEST: [ 19  35  37  41  56  60  68  75  77  78  94 110 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38\n",
            "  39  41  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  77  78\n",
            "  79  80  81  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120] TEST: [ 11  13  33  40  42  51  70  82  83  84 105 121 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37\n",
            "  39  40  41  42  43  45  46  47  48  50  51  52  54  55  56  57  58  59\n",
            "  60  61  62  64  65  67  68  69  70  71  72  73  75  76  77  78  79  80\n",
            "  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  99 100\n",
            " 101 102 103 104 105 106 107 109 110 112 113 114 115 116 117 118 119 120\n",
            " 121 122] TEST: [  9  36  38  44  49  53  63  66  74  86  98 108 111]\n",
            "TRAIN: [  0   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
            "  60  61  62  63  66  67  68  69  70  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 121 122] TEST: [  1   7  21  23  39  55  64  65  71  90 104 120]\n",
            "TRAIN: [  0   1   2   4   6   7   8   9  11  12  13  14  15  17  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  48  49  50  51  52  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
            "  99 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119\n",
            " 120 121 122] TEST: [  3   5  10  16  46  47  59  69  88 100 107 114]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  19  20\n",
            "  21  22  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  86  87  88  89  90  91  92  94  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 114 115 118 119\n",
            " 120 121 122] TEST: [  0  17  18  29  34  62  85  93  95 113 116 117]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  26  28  29  30  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  81  82  83  84  85  86  87  88  90  92  93  94  95  96  97  98  99\n",
            " 100 101 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [  6  25  27  31  32  48  54  80  89  91 102 112]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60\n",
            "  61  62  63  64  65  66  68  69  70  71  72  73  74  75  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 102 103 104 105 106 107 108 110 111 112 113 114 116 117 118 119\n",
            " 120 121 122] TEST: [  2  15  20  24  30  43  57  67  76 101 109 115]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  13  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  80  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 119\n",
            " 120 121 122] TEST: [  4   8  14  28  50  61  72  79  81  92 106 118]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
            "  19  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  46  47  48  49  50  51  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  98 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [ 12  22  26  45  52  58  73  87  96  97  99 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "1d172b77-a5c9-40da-ddcb-22ee2a8a5415",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 20\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 1.4902 - acc: 0.5364 - val_loss: 2.1007 - val_acc: 0.2308\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 271us/step - loss: 1.2603 - acc: 0.5455 - val_loss: 1.8448 - val_acc: 0.3077\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 188us/step - loss: 1.1624 - acc: 0.5727 - val_loss: 1.7499 - val_acc: 0.4615\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 181us/step - loss: 1.1119 - acc: 0.5545 - val_loss: 1.7056 - val_acc: 0.4615\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 160us/step - loss: 1.0698 - acc: 0.5727 - val_loss: 1.6842 - val_acc: 0.4615\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 160us/step - loss: 1.0444 - acc: 0.5545 - val_loss: 1.5780 - val_acc: 0.4615\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 161us/step - loss: 1.0225 - acc: 0.5818 - val_loss: 1.5566 - val_acc: 0.4615\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 175us/step - loss: 1.0068 - acc: 0.5818 - val_loss: 1.5169 - val_acc: 0.4615\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 178us/step - loss: 0.9977 - acc: 0.5909 - val_loss: 1.4862 - val_acc: 0.4615\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 191us/step - loss: 0.9831 - acc: 0.5818 - val_loss: 1.4813 - val_acc: 0.4615\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 205us/step - loss: 0.9780 - acc: 0.6000 - val_loss: 1.4913 - val_acc: 0.4615\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 185us/step - loss: 0.9730 - acc: 0.5909 - val_loss: 1.4682 - val_acc: 0.4615\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 187us/step - loss: 0.9652 - acc: 0.5818 - val_loss: 1.4444 - val_acc: 0.4615\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 175us/step - loss: 0.9603 - acc: 0.5818 - val_loss: 1.4394 - val_acc: 0.4615\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 179us/step - loss: 0.9551 - acc: 0.5818 - val_loss: 1.4377 - val_acc: 0.4615\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 180us/step - loss: 0.9470 - acc: 0.5909 - val_loss: 1.4311 - val_acc: 0.4615\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 190us/step - loss: 0.9439 - acc: 0.5727 - val_loss: 1.4411 - val_acc: 0.4615\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 0s 204us/step - loss: 0.9414 - acc: 0.6000 - val_loss: 1.4287 - val_acc: 0.4615\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 0s 197us/step - loss: 0.9391 - acc: 0.5909 - val_loss: 1.4196 - val_acc: 0.4615\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 0s 216us/step - loss: 0.9334 - acc: 0.5818 - val_loss: 1.4105 - val_acc: 0.3846\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 1s 5ms/step - loss: 2.8400 - acc: 0.4364 - val_loss: 2.2623 - val_acc: 0.6154\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 196us/step - loss: 2.2029 - acc: 0.4364 - val_loss: 1.4975 - val_acc: 0.5385\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 190us/step - loss: 1.5768 - acc: 0.4455 - val_loss: 1.1474 - val_acc: 0.6154\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 183us/step - loss: 1.2317 - acc: 0.4364 - val_loss: 1.0273 - val_acc: 0.5385\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 166us/step - loss: 1.0879 - acc: 0.4182 - val_loss: 0.9824 - val_acc: 0.5385\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 195us/step - loss: 1.0544 - acc: 0.4364 - val_loss: 0.9617 - val_acc: 0.5385\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 191us/step - loss: 1.0346 - acc: 0.4182 - val_loss: 0.9517 - val_acc: 0.5385\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 172us/step - loss: 1.0258 - acc: 0.4545 - val_loss: 0.9321 - val_acc: 0.5385\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 216us/step - loss: 1.0120 - acc: 0.4636 - val_loss: 0.9232 - val_acc: 0.6154\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 171us/step - loss: 1.0112 - acc: 0.4545 - val_loss: 0.9047 - val_acc: 0.6154\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 188us/step - loss: 1.0095 - acc: 0.4818 - val_loss: 0.9161 - val_acc: 0.6154\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 211us/step - loss: 1.0054 - acc: 0.4727 - val_loss: 0.9135 - val_acc: 0.5385\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 204us/step - loss: 0.9950 - acc: 0.5182 - val_loss: 0.9120 - val_acc: 0.6154\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 187us/step - loss: 0.9942 - acc: 0.4909 - val_loss: 0.9068 - val_acc: 0.6154\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 202us/step - loss: 0.9953 - acc: 0.4909 - val_loss: 0.9187 - val_acc: 0.6154\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 231us/step - loss: 0.9909 - acc: 0.5364 - val_loss: 0.9011 - val_acc: 0.6154\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 258us/step - loss: 0.9857 - acc: 0.5273 - val_loss: 0.9098 - val_acc: 0.5385\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 0s 218us/step - loss: 0.9852 - acc: 0.5636 - val_loss: 0.9107 - val_acc: 0.6154\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 0s 247us/step - loss: 0.9848 - acc: 0.5273 - val_loss: 0.9270 - val_acc: 0.5385\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 0s 220us/step - loss: 0.9875 - acc: 0.5364 - val_loss: 0.9089 - val_acc: 0.5385\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/20\n",
            "110/110 [==============================] - 1s 6ms/step - loss: 2.0266 - acc: 0.3000 - val_loss: 1.8242 - val_acc: 0.3077\n",
            "Epoch 2/20\n",
            "110/110 [==============================] - 0s 195us/step - loss: 1.2400 - acc: 0.3636 - val_loss: 1.3755 - val_acc: 0.2308\n",
            "Epoch 3/20\n",
            "110/110 [==============================] - 0s 198us/step - loss: 1.1042 - acc: 0.4000 - val_loss: 1.2277 - val_acc: 0.2308\n",
            "Epoch 4/20\n",
            "110/110 [==============================] - 0s 178us/step - loss: 1.0638 - acc: 0.4000 - val_loss: 1.1415 - val_acc: 0.3846\n",
            "Epoch 5/20\n",
            "110/110 [==============================] - 0s 189us/step - loss: 1.0412 - acc: 0.4727 - val_loss: 1.0922 - val_acc: 0.3846\n",
            "Epoch 6/20\n",
            "110/110 [==============================] - 0s 181us/step - loss: 1.0299 - acc: 0.4545 - val_loss: 1.0628 - val_acc: 0.3077\n",
            "Epoch 7/20\n",
            "110/110 [==============================] - 0s 168us/step - loss: 1.0123 - acc: 0.4727 - val_loss: 1.0531 - val_acc: 0.3846\n",
            "Epoch 8/20\n",
            "110/110 [==============================] - 0s 184us/step - loss: 1.0181 - acc: 0.4909 - val_loss: 1.0430 - val_acc: 0.3077\n",
            "Epoch 9/20\n",
            "110/110 [==============================] - 0s 190us/step - loss: 1.0000 - acc: 0.4545 - val_loss: 1.0155 - val_acc: 0.4615\n",
            "Epoch 10/20\n",
            "110/110 [==============================] - 0s 196us/step - loss: 0.9875 - acc: 0.4636 - val_loss: 1.0005 - val_acc: 0.3846\n",
            "Epoch 11/20\n",
            "110/110 [==============================] - 0s 218us/step - loss: 0.9876 - acc: 0.5091 - val_loss: 0.9976 - val_acc: 0.3846\n",
            "Epoch 12/20\n",
            "110/110 [==============================] - 0s 223us/step - loss: 0.9828 - acc: 0.4727 - val_loss: 0.9942 - val_acc: 0.4615\n",
            "Epoch 13/20\n",
            "110/110 [==============================] - 0s 201us/step - loss: 0.9752 - acc: 0.5000 - val_loss: 0.9950 - val_acc: 0.4615\n",
            "Epoch 14/20\n",
            "110/110 [==============================] - 0s 195us/step - loss: 0.9730 - acc: 0.4909 - val_loss: 0.9879 - val_acc: 0.4615\n",
            "Epoch 15/20\n",
            "110/110 [==============================] - 0s 193us/step - loss: 0.9572 - acc: 0.5182 - val_loss: 0.9788 - val_acc: 0.3846\n",
            "Epoch 16/20\n",
            "110/110 [==============================] - 0s 229us/step - loss: 0.9618 - acc: 0.5000 - val_loss: 0.9758 - val_acc: 0.4615\n",
            "Epoch 17/20\n",
            "110/110 [==============================] - 0s 242us/step - loss: 0.9633 - acc: 0.5000 - val_loss: 0.9888 - val_acc: 0.4615\n",
            "Epoch 18/20\n",
            "110/110 [==============================] - 0s 234us/step - loss: 0.9488 - acc: 0.5364 - val_loss: 0.9645 - val_acc: 0.4615\n",
            "Epoch 19/20\n",
            "110/110 [==============================] - 0s 205us/step - loss: 0.9480 - acc: 0.5182 - val_loss: 0.9736 - val_acc: 0.4615\n",
            "Epoch 20/20\n",
            "110/110 [==============================] - 0s 197us/step - loss: 0.9414 - acc: 0.5455 - val_loss: 0.9574 - val_acc: 0.4615\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 6ms/step - loss: 1.3611 - acc: 0.3604 - val_loss: 1.5097 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 186us/step - loss: 1.1606 - acc: 0.4144 - val_loss: 1.4142 - val_acc: 0.4167\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 177us/step - loss: 1.0901 - acc: 0.3514 - val_loss: 1.3329 - val_acc: 0.4167\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 223us/step - loss: 1.0450 - acc: 0.3964 - val_loss: 1.2726 - val_acc: 0.4167\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 175us/step - loss: 1.0166 - acc: 0.4054 - val_loss: 1.2346 - val_acc: 0.4167\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.9895 - acc: 0.4505 - val_loss: 1.2241 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 168us/step - loss: 0.9792 - acc: 0.4414 - val_loss: 1.2203 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 183us/step - loss: 0.9606 - acc: 0.4234 - val_loss: 1.1942 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.9595 - acc: 0.4775 - val_loss: 1.2015 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.9484 - acc: 0.4955 - val_loss: 1.1907 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.9451 - acc: 0.4775 - val_loss: 1.1925 - val_acc: 0.4167\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 172us/step - loss: 0.9322 - acc: 0.4955 - val_loss: 1.1858 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.9265 - acc: 0.5135 - val_loss: 1.1872 - val_acc: 0.4167\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.9310 - acc: 0.4865 - val_loss: 1.1978 - val_acc: 0.4167\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 219us/step - loss: 0.9087 - acc: 0.5045 - val_loss: 1.1954 - val_acc: 0.4167\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.9068 - acc: 0.5225 - val_loss: 1.1895 - val_acc: 0.4167\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.9051 - acc: 0.5225 - val_loss: 1.1915 - val_acc: 0.4167\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 183us/step - loss: 0.9005 - acc: 0.5135 - val_loss: 1.1989 - val_acc: 0.4167\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.8929 - acc: 0.5225 - val_loss: 1.1908 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 190us/step - loss: 0.8957 - acc: 0.5225 - val_loss: 1.2014 - val_acc: 0.4167\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 6ms/step - loss: 1.5643 - acc: 0.3784 - val_loss: 2.0794 - val_acc: 0.2500\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 225us/step - loss: 1.3643 - acc: 0.4595 - val_loss: 1.8861 - val_acc: 0.4167\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 222us/step - loss: 1.2683 - acc: 0.4775 - val_loss: 1.7694 - val_acc: 0.4167\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 229us/step - loss: 1.1968 - acc: 0.4775 - val_loss: 1.6510 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 213us/step - loss: 1.1424 - acc: 0.4505 - val_loss: 1.6264 - val_acc: 0.5833\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 190us/step - loss: 1.1009 - acc: 0.4865 - val_loss: 1.6051 - val_acc: 0.5833\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 199us/step - loss: 1.0665 - acc: 0.4955 - val_loss: 1.6194 - val_acc: 0.5833\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 190us/step - loss: 1.0342 - acc: 0.4775 - val_loss: 1.5791 - val_acc: 0.5833\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 207us/step - loss: 1.0190 - acc: 0.5495 - val_loss: 1.6039 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 294us/step - loss: 1.0023 - acc: 0.5405 - val_loss: 1.6175 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.9817 - acc: 0.5405 - val_loss: 1.6048 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.9771 - acc: 0.5676 - val_loss: 1.5760 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.9695 - acc: 0.5315 - val_loss: 1.5810 - val_acc: 0.4167\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.9603 - acc: 0.5135 - val_loss: 1.5788 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.9542 - acc: 0.5315 - val_loss: 1.5687 - val_acc: 0.5833\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.9478 - acc: 0.5315 - val_loss: 1.5940 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.9414 - acc: 0.5315 - val_loss: 1.5577 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.9450 - acc: 0.5225 - val_loss: 1.5875 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 273us/step - loss: 0.9366 - acc: 0.5495 - val_loss: 1.5679 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.9331 - acc: 0.5586 - val_loss: 1.5647 - val_acc: 0.5000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 7ms/step - loss: 1.6391 - acc: 0.4865 - val_loss: 1.2164 - val_acc: 0.5833\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 182us/step - loss: 1.3327 - acc: 0.5225 - val_loss: 0.9962 - val_acc: 0.5833\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 1.2085 - acc: 0.5315 - val_loss: 0.8959 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 180us/step - loss: 1.1046 - acc: 0.5225 - val_loss: 0.8405 - val_acc: 0.5833\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 187us/step - loss: 1.0529 - acc: 0.5495 - val_loss: 0.8205 - val_acc: 0.5833\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 196us/step - loss: 1.0296 - acc: 0.5315 - val_loss: 0.8066 - val_acc: 0.6667\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 1.0104 - acc: 0.4955 - val_loss: 0.8193 - val_acc: 0.6667\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.9909 - acc: 0.5135 - val_loss: 0.8188 - val_acc: 0.6667\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 174us/step - loss: 0.9855 - acc: 0.5315 - val_loss: 0.8270 - val_acc: 0.5833\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 185us/step - loss: 0.9578 - acc: 0.5225 - val_loss: 0.8185 - val_acc: 0.6667\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.9518 - acc: 0.5315 - val_loss: 0.8283 - val_acc: 0.5833\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.9330 - acc: 0.5225 - val_loss: 0.8354 - val_acc: 0.5833\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.9217 - acc: 0.5225 - val_loss: 0.8200 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.9200 - acc: 0.5405 - val_loss: 0.8252 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 185us/step - loss: 0.9149 - acc: 0.5405 - val_loss: 0.8534 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.9119 - acc: 0.5045 - val_loss: 0.8472 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 187us/step - loss: 0.9072 - acc: 0.5225 - val_loss: 0.8578 - val_acc: 0.5833\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.9038 - acc: 0.5405 - val_loss: 0.8564 - val_acc: 0.5833\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.8937 - acc: 0.5405 - val_loss: 0.8326 - val_acc: 0.5833\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.9199 - acc: 0.4865 - val_loss: 0.8320 - val_acc: 0.5000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 7ms/step - loss: 2.0141 - acc: 0.3423 - val_loss: 1.0156 - val_acc: 0.4167\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 186us/step - loss: 1.3890 - acc: 0.3874 - val_loss: 0.8989 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 1.1755 - acc: 0.4775 - val_loss: 0.8635 - val_acc: 0.5833\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 1.1005 - acc: 0.5315 - val_loss: 0.8603 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 207us/step - loss: 1.0527 - acc: 0.5405 - val_loss: 0.8462 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 210us/step - loss: 1.0298 - acc: 0.5225 - val_loss: 0.8337 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 183us/step - loss: 1.0077 - acc: 0.5676 - val_loss: 0.8220 - val_acc: 0.5833\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 175us/step - loss: 1.0026 - acc: 0.5586 - val_loss: 0.8188 - val_acc: 0.5833\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.9818 - acc: 0.5405 - val_loss: 0.8176 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.9729 - acc: 0.5676 - val_loss: 0.7979 - val_acc: 0.5833\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.9715 - acc: 0.5586 - val_loss: 0.7964 - val_acc: 0.5833\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.9618 - acc: 0.5405 - val_loss: 0.7874 - val_acc: 0.5833\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.9543 - acc: 0.5676 - val_loss: 0.7807 - val_acc: 0.5833\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.9542 - acc: 0.5405 - val_loss: 0.7761 - val_acc: 0.5833\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.9407 - acc: 0.5856 - val_loss: 0.7865 - val_acc: 0.5833\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.9509 - acc: 0.5766 - val_loss: 0.7780 - val_acc: 0.5833\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.9351 - acc: 0.5946 - val_loss: 0.7784 - val_acc: 0.5833\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 224us/step - loss: 0.9285 - acc: 0.5856 - val_loss: 0.7696 - val_acc: 0.6667\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 215us/step - loss: 0.9270 - acc: 0.6216 - val_loss: 0.7771 - val_acc: 0.5833\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.9301 - acc: 0.6216 - val_loss: 0.7757 - val_acc: 0.6667\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 7ms/step - loss: 1.8765 - acc: 0.3063 - val_loss: 1.7262 - val_acc: 0.1667\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 198us/step - loss: 1.3077 - acc: 0.3784 - val_loss: 1.5113 - val_acc: 0.5000\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 1.1447 - acc: 0.3874 - val_loss: 1.4402 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 224us/step - loss: 1.0718 - acc: 0.4234 - val_loss: 1.4039 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 208us/step - loss: 1.0271 - acc: 0.4595 - val_loss: 1.3627 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.9956 - acc: 0.4595 - val_loss: 1.3158 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.9712 - acc: 0.4955 - val_loss: 1.2940 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.9462 - acc: 0.5225 - val_loss: 1.2838 - val_acc: 0.3333\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 179us/step - loss: 0.9317 - acc: 0.5766 - val_loss: 1.2806 - val_acc: 0.2500\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.9171 - acc: 0.5856 - val_loss: 1.2651 - val_acc: 0.2500\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.9027 - acc: 0.6036 - val_loss: 1.2799 - val_acc: 0.2500\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.8828 - acc: 0.6126 - val_loss: 1.3066 - val_acc: 0.2500\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.8757 - acc: 0.6126 - val_loss: 1.3061 - val_acc: 0.2500\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 179us/step - loss: 0.8670 - acc: 0.6216 - val_loss: 1.3510 - val_acc: 0.2500\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 171us/step - loss: 0.8544 - acc: 0.6216 - val_loss: 1.3507 - val_acc: 0.2500\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 185us/step - loss: 0.8449 - acc: 0.6306 - val_loss: 1.3651 - val_acc: 0.2500\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.8405 - acc: 0.6216 - val_loss: 1.3978 - val_acc: 0.2500\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 282us/step - loss: 0.8330 - acc: 0.6306 - val_loss: 1.4317 - val_acc: 0.2500\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.8256 - acc: 0.6216 - val_loss: 1.4122 - val_acc: 0.2500\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 287us/step - loss: 0.8204 - acc: 0.6306 - val_loss: 1.4063 - val_acc: 0.2500\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 8ms/step - loss: 1.8160 - acc: 0.3153 - val_loss: 1.2619 - val_acc: 0.5000\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 1.2232 - acc: 0.3964 - val_loss: 1.0567 - val_acc: 0.4167\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 1.0567 - acc: 0.4234 - val_loss: 1.0002 - val_acc: 0.5000\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 171us/step - loss: 1.0108 - acc: 0.4685 - val_loss: 1.0064 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 174us/step - loss: 0.9779 - acc: 0.4865 - val_loss: 1.0006 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 171us/step - loss: 0.9679 - acc: 0.4955 - val_loss: 0.9996 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.9584 - acc: 0.5135 - val_loss: 1.0033 - val_acc: 0.5000\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.9401 - acc: 0.5225 - val_loss: 1.0054 - val_acc: 0.5000\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 183us/step - loss: 0.9338 - acc: 0.5225 - val_loss: 1.0128 - val_acc: 0.5000\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.9334 - acc: 0.5135 - val_loss: 1.0223 - val_acc: 0.5000\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.9256 - acc: 0.5315 - val_loss: 1.0282 - val_acc: 0.5000\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 185us/step - loss: 0.9223 - acc: 0.5045 - val_loss: 1.0218 - val_acc: 0.5000\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.9210 - acc: 0.5045 - val_loss: 1.0324 - val_acc: 0.5000\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.9252 - acc: 0.5045 - val_loss: 1.0319 - val_acc: 0.5000\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.9114 - acc: 0.5315 - val_loss: 1.0301 - val_acc: 0.5000\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.9162 - acc: 0.5135 - val_loss: 1.0323 - val_acc: 0.5000\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 174us/step - loss: 0.9098 - acc: 0.5495 - val_loss: 1.0338 - val_acc: 0.5000\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.9085 - acc: 0.5225 - val_loss: 1.0376 - val_acc: 0.5000\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.9056 - acc: 0.5405 - val_loss: 1.0306 - val_acc: 0.5000\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 206us/step - loss: 0.9012 - acc: 0.5225 - val_loss: 1.0312 - val_acc: 0.5000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/20\n",
            "111/111 [==============================] - 1s 8ms/step - loss: 2.0394 - acc: 0.2973 - val_loss: 1.2041 - val_acc: 0.2500\n",
            "Epoch 2/20\n",
            "111/111 [==============================] - 0s 267us/step - loss: 1.2482 - acc: 0.3964 - val_loss: 1.0119 - val_acc: 0.4167\n",
            "Epoch 3/20\n",
            "111/111 [==============================] - 0s 211us/step - loss: 1.1374 - acc: 0.3784 - val_loss: 0.9614 - val_acc: 0.4167\n",
            "Epoch 4/20\n",
            "111/111 [==============================] - 0s 196us/step - loss: 1.0647 - acc: 0.3874 - val_loss: 0.9407 - val_acc: 0.5000\n",
            "Epoch 5/20\n",
            "111/111 [==============================] - 0s 226us/step - loss: 1.0274 - acc: 0.4324 - val_loss: 0.9418 - val_acc: 0.5000\n",
            "Epoch 6/20\n",
            "111/111 [==============================] - 0s 208us/step - loss: 1.0068 - acc: 0.5135 - val_loss: 0.9529 - val_acc: 0.5000\n",
            "Epoch 7/20\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.9979 - acc: 0.4595 - val_loss: 0.9417 - val_acc: 0.6667\n",
            "Epoch 8/20\n",
            "111/111 [==============================] - 0s 260us/step - loss: 0.9808 - acc: 0.5135 - val_loss: 0.9556 - val_acc: 0.6667\n",
            "Epoch 9/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.9612 - acc: 0.5405 - val_loss: 0.9509 - val_acc: 0.5833\n",
            "Epoch 10/20\n",
            "111/111 [==============================] - 0s 176us/step - loss: 0.9601 - acc: 0.5405 - val_loss: 0.9476 - val_acc: 0.6667\n",
            "Epoch 11/20\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.9462 - acc: 0.5405 - val_loss: 0.9419 - val_acc: 0.5833\n",
            "Epoch 12/20\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.9413 - acc: 0.5405 - val_loss: 0.9462 - val_acc: 0.6667\n",
            "Epoch 13/20\n",
            "111/111 [==============================] - 0s 196us/step - loss: 0.9525 - acc: 0.5405 - val_loss: 0.9449 - val_acc: 0.6667\n",
            "Epoch 14/20\n",
            "111/111 [==============================] - 0s 228us/step - loss: 0.9432 - acc: 0.5586 - val_loss: 0.9444 - val_acc: 0.5833\n",
            "Epoch 15/20\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.9339 - acc: 0.5495 - val_loss: 0.9590 - val_acc: 0.6667\n",
            "Epoch 16/20\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.9285 - acc: 0.5495 - val_loss: 0.9438 - val_acc: 0.5833\n",
            "Epoch 17/20\n",
            "111/111 [==============================] - 0s 177us/step - loss: 0.9293 - acc: 0.5495 - val_loss: 0.9573 - val_acc: 0.6667\n",
            "Epoch 18/20\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.9170 - acc: 0.5586 - val_loss: 0.9649 - val_acc: 0.6667\n",
            "Epoch 19/20\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.9207 - acc: 0.5495 - val_loss: 0.9515 - val_acc: 0.5833\n",
            "Epoch 20/20\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.9227 - acc: 0.5495 - val_loss: 0.9572 - val_acc: 0.5833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "24ee7c00-e8ab-4219-81d0-5480b01f984c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "709cc187-56a4-422a-af45-18461e5f34dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "fb590efe-a471-41e4-c425-ed4f4d0a28ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "0b67e00f-e257-4da8-e368-a29d3583fc40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa9ca603860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np56pbgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "04e0f6ae-0a15-449b-bdc7-f8a4dfd937a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa9ca573ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAr5B9G4PE+a2u6+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "4c3fb7dd-8a9a-468b-8c10-b432d95464d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 1s 7ms/step - loss: 2.3683 - acc: 0.2358\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 0s 185us/step - loss: 1.5760 - acc: 0.3984\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 0s 168us/step - loss: 1.2608 - acc: 0.4878\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 0s 201us/step - loss: 1.1040 - acc: 0.4959\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 0s 165us/step - loss: 1.0249 - acc: 0.4878\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 0s 210us/step - loss: 0.9893 - acc: 0.5366\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 0s 206us/step - loss: 0.9597 - acc: 0.5610\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 0s 183us/step - loss: 0.9466 - acc: 0.5285\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 0s 200us/step - loss: 0.9709 - acc: 0.5366\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 0s 200us/step - loss: 0.9285 - acc: 0.5366\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 0s 188us/step - loss: 0.9189 - acc: 0.5447\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 0s 215us/step - loss: 0.9152 - acc: 0.5285\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 0s 180us/step - loss: 0.9071 - acc: 0.5447\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 0s 174us/step - loss: 0.9049 - acc: 0.5285\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 0s 177us/step - loss: 0.9043 - acc: 0.5285\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 0s 172us/step - loss: 0.8939 - acc: 0.5772\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 0s 168us/step - loss: 0.8959 - acc: 0.5528\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 0s 159us/step - loss: 0.8870 - acc: 0.5528\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 0s 169us/step - loss: 0.8841 - acc: 0.5528\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 0s 163us/step - loss: 0.8768 - acc: 0.5528\n",
            "42/42 [==============================] - 0s 7ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "7abd0f3b-cd08-4c98-b41b-7963c6adaa36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "97569daf-be53-43b2-b08e-0c2ab4254461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4285714299905868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "a62459c0-554a-4e79-9a2c-3cd87aaf5cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(5, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dvJzYLTzmiQa",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f72fa26a-db60-4751-bcce-41681cbca977",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d6e9e604-9682-430e-bae3-312944a2388b",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  36  38\n",
            "  39  40  42  43  44  45  46  47  48  49  50  51  52  53  54  55  57  58\n",
            "  59  61  62  63  64  65  66  67  69  70  71  72  73  74  76  79  80  81\n",
            "  82  83  84  85  86  87  88  89  90  91  92  93  95  96  97  98  99 100\n",
            " 101 102 103 104 105 106 107 108 109 111 112 113 114 115 116 117 118 120\n",
            " 121 122] TEST: [ 19  35  37  41  56  60  68  75  77  78  94 110 119]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  12  14  15  16  17  18  19\n",
            "  20  21  22  23  24  25  26  27  28  29  30  31  32  34  35  36  37  38\n",
            "  39  41  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59\n",
            "  60  61  62  63  64  65  66  67  68  69  71  72  73  74  75  76  77  78\n",
            "  79  80  81  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 120] TEST: [ 11  13  33  40  42  51  70  82  83  84 105 121 122]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  37\n",
            "  39  40  41  42  43  45  46  47  48  50  51  52  54  55  56  57  58  59\n",
            "  60  61  62  64  65  67  68  69  70  71  72  73  75  76  77  78  79  80\n",
            "  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  99 100\n",
            " 101 102 103 104 105 106 107 109 110 112 113 114 115 116 117 118 119 120\n",
            " 121 122] TEST: [  9  36  38  44  49  53  63  66  74  86  98 108 111]\n",
            "TRAIN: [  0   2   3   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19\n",
            "  20  22  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58  59\n",
            "  60  61  62  63  66  67  68  69  70  72  73  74  75  76  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  91  92  93  94  95  96  97  98  99\n",
            " 100 101 102 103 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
            " 119 121 122] TEST: [  1   7  21  23  39  55  64  65  71  90 104 120]\n",
            "TRAIN: [  0   1   2   4   6   7   8   9  11  12  13  14  15  17  18  19  20  21\n",
            "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  48  49  50  51  52  53  54  55  56  57  58  60\n",
            "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  78  79\n",
            "  80  81  82  83  84  85  86  87  89  90  91  92  93  94  95  96  97  98\n",
            "  99 101 102 103 104 105 106 108 109 110 111 112 113 115 116 117 118 119\n",
            " 120 121 122] TEST: [  3   5  10  16  46  47  59  69  88 100 107 114]\n",
            "TRAIN: [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  19  20\n",
            "  21  22  23  24  25  26  27  28  30  31  32  33  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
            "  59  60  61  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
            "  78  79  80  81  82  83  84  86  87  88  89  90  91  92  94  96  97  98\n",
            "  99 100 101 102 103 104 105 106 107 108 109 110 111 112 114 115 118 119\n",
            " 120 121 122] TEST: [  0  17  18  29  34  62  85  93  95 113 116 117]\n",
            "TRAIN: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  18\n",
            "  19  20  21  22  23  24  26  28  29  30  33  34  35  36  37  38  39  40\n",
            "  41  42  43  44  45  46  47  49  50  51  52  53  55  56  57  58  59  60\n",
            "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
            "  79  81  82  83  84  85  86  87  88  90  92  93  94  95  96  97  98  99\n",
            " 100 101 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [  6  25  27  31  32  48  54  80  89  91 102 112]\n",
            "TRAIN: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19\n",
            "  21  22  23  25  26  27  28  29  31  32  33  34  35  36  37  38  39  40\n",
            "  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56  58  59  60\n",
            "  61  62  63  64  65  66  68  69  70  71  72  73  74  75  77  78  79  80\n",
            "  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98\n",
            "  99 100 102 103 104 105 106 107 108 110 111 112 113 114 116 117 118 119\n",
            " 120 121 122] TEST: [  2  15  20  24  30  43  57  67  76 101 109 115]\n",
            "TRAIN: [  0   1   2   3   5   6   7   9  10  11  12  13  15  16  17  18  19  20\n",
            "  21  22  23  24  25  26  27  29  30  31  32  33  34  35  36  37  38  39\n",
            "  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58\n",
            "  59  60  62  63  64  65  66  67  68  69  70  71  73  74  75  76  77  78\n",
            "  80  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97  98  99\n",
            " 100 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 119\n",
            " 120 121 122] TEST: [  4   8  14  28  50  61  72  79  81  92 106 118]\n",
            "TRAIN: [  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
            "  19  20  21  23  24  25  27  28  29  30  31  32  33  34  35  36  37  38\n",
            "  39  40  41  42  43  44  46  47  48  49  50  51  53  54  55  56  57  59\n",
            "  60  61  62  63  64  65  66  67  68  69  70  71  72  74  75  76  77  78\n",
            "  79  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95  98 100\n",
            " 101 102 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
            " 120 121 122] TEST: [ 12  22  26  45  52  58  73  87  96  97  99 103]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DjbzRWoekKFN",
        "colab": {}
      },
      "source": [
        "#train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bc49e7fa-6506-4793-9c62-4c7849f13e87",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 30\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 1.8661 - acc: 0.0091 - val_loss: 1.7615 - val_acc: 0.0000e+00\n",
            "Epoch 2/30\n",
            "110/110 [==============================] - 0s 220us/step - loss: 1.6323 - acc: 0.1727 - val_loss: 1.5451 - val_acc: 0.3846\n",
            "Epoch 3/30\n",
            "110/110 [==============================] - 0s 225us/step - loss: 1.4405 - acc: 0.4000 - val_loss: 1.3639 - val_acc: 0.3846\n",
            "Epoch 4/30\n",
            "110/110 [==============================] - 0s 166us/step - loss: 1.2760 - acc: 0.4000 - val_loss: 1.2086 - val_acc: 0.3846\n",
            "Epoch 5/30\n",
            "110/110 [==============================] - 0s 203us/step - loss: 1.1419 - acc: 0.4000 - val_loss: 1.0806 - val_acc: 0.3846\n",
            "Epoch 6/30\n",
            "110/110 [==============================] - 0s 190us/step - loss: 1.0333 - acc: 0.4000 - val_loss: 0.9857 - val_acc: 0.4615\n",
            "Epoch 7/30\n",
            "110/110 [==============================] - 0s 197us/step - loss: 0.9489 - acc: 0.4364 - val_loss: 0.9105 - val_acc: 0.4615\n",
            "Epoch 8/30\n",
            "110/110 [==============================] - 0s 176us/step - loss: 0.8809 - acc: 0.4727 - val_loss: 0.8492 - val_acc: 0.5385\n",
            "Epoch 9/30\n",
            "110/110 [==============================] - 0s 166us/step - loss: 0.8254 - acc: 0.5818 - val_loss: 0.7997 - val_acc: 0.6154\n",
            "Epoch 10/30\n",
            "110/110 [==============================] - 0s 190us/step - loss: 0.7799 - acc: 0.5909 - val_loss: 0.7580 - val_acc: 0.6154\n",
            "Epoch 11/30\n",
            "110/110 [==============================] - 0s 180us/step - loss: 0.7417 - acc: 0.6000 - val_loss: 0.7232 - val_acc: 0.6154\n",
            "Epoch 12/30\n",
            "110/110 [==============================] - 0s 231us/step - loss: 0.7096 - acc: 0.6091 - val_loss: 0.6927 - val_acc: 0.6154\n",
            "Epoch 13/30\n",
            "110/110 [==============================] - 0s 189us/step - loss: 0.6817 - acc: 0.6273 - val_loss: 0.6673 - val_acc: 0.6154\n",
            "Epoch 14/30\n",
            "110/110 [==============================] - 0s 221us/step - loss: 0.6578 - acc: 0.6364 - val_loss: 0.6449 - val_acc: 0.6154\n",
            "Epoch 15/30\n",
            "110/110 [==============================] - 0s 204us/step - loss: 0.6369 - acc: 0.6364 - val_loss: 0.6243 - val_acc: 0.6154\n",
            "Epoch 16/30\n",
            "110/110 [==============================] - 0s 208us/step - loss: 0.6179 - acc: 0.6364 - val_loss: 0.6069 - val_acc: 0.6154\n",
            "Epoch 17/30\n",
            "110/110 [==============================] - 0s 193us/step - loss: 0.6012 - acc: 0.6364 - val_loss: 0.5907 - val_acc: 0.6154\n",
            "Epoch 18/30\n",
            "110/110 [==============================] - 0s 177us/step - loss: 0.5862 - acc: 0.6455 - val_loss: 0.5758 - val_acc: 0.6154\n",
            "Epoch 19/30\n",
            "110/110 [==============================] - 0s 197us/step - loss: 0.5722 - acc: 0.6455 - val_loss: 0.5628 - val_acc: 0.6923\n",
            "Epoch 20/30\n",
            "110/110 [==============================] - 0s 177us/step - loss: 0.5597 - acc: 0.6455 - val_loss: 0.5505 - val_acc: 0.6923\n",
            "Epoch 21/30\n",
            "110/110 [==============================] - 0s 230us/step - loss: 0.5482 - acc: 0.6455 - val_loss: 0.5392 - val_acc: 0.6923\n",
            "Epoch 22/30\n",
            "110/110 [==============================] - 0s 193us/step - loss: 0.5376 - acc: 0.6545 - val_loss: 0.5288 - val_acc: 0.6923\n",
            "Epoch 23/30\n",
            "110/110 [==============================] - 0s 209us/step - loss: 0.5277 - acc: 0.6545 - val_loss: 0.5190 - val_acc: 0.6923\n",
            "Epoch 24/30\n",
            "110/110 [==============================] - 0s 168us/step - loss: 0.5186 - acc: 0.6727 - val_loss: 0.5101 - val_acc: 0.6923\n",
            "Epoch 25/30\n",
            "110/110 [==============================] - 0s 171us/step - loss: 0.5101 - acc: 0.6818 - val_loss: 0.5019 - val_acc: 0.6923\n",
            "Epoch 26/30\n",
            "110/110 [==============================] - 0s 195us/step - loss: 0.5021 - acc: 0.6818 - val_loss: 0.4940 - val_acc: 0.6923\n",
            "Epoch 27/30\n",
            "110/110 [==============================] - 0s 212us/step - loss: 0.4944 - acc: 0.7091 - val_loss: 0.4864 - val_acc: 0.6923\n",
            "Epoch 28/30\n",
            "110/110 [==============================] - 0s 212us/step - loss: 0.4874 - acc: 0.7182 - val_loss: 0.4795 - val_acc: 0.6923\n",
            "Epoch 29/30\n",
            "110/110 [==============================] - 0s 204us/step - loss: 0.4808 - acc: 0.7182 - val_loss: 0.4732 - val_acc: 0.6923\n",
            "Epoch 30/30\n",
            "110/110 [==============================] - 0s 205us/step - loss: 0.4744 - acc: 0.7273 - val_loss: 0.4667 - val_acc: 0.6923\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "110/110 [==============================] - 1s 9ms/step - loss: 2.8019 - acc: 0.3455 - val_loss: 2.5791 - val_acc: 0.4615\n",
            "Epoch 2/30\n",
            "110/110 [==============================] - 0s 201us/step - loss: 2.3844 - acc: 0.4000 - val_loss: 2.1842 - val_acc: 0.4615\n",
            "Epoch 3/30\n",
            "110/110 [==============================] - 0s 213us/step - loss: 2.0337 - acc: 0.5636 - val_loss: 1.8744 - val_acc: 0.7692\n",
            "Epoch 4/30\n",
            "110/110 [==============================] - 0s 206us/step - loss: 1.7313 - acc: 0.7545 - val_loss: 1.6018 - val_acc: 0.7692\n",
            "Epoch 5/30\n",
            "110/110 [==============================] - 0s 224us/step - loss: 1.4697 - acc: 0.7636 - val_loss: 1.3572 - val_acc: 0.7692\n",
            "Epoch 6/30\n",
            "110/110 [==============================] - 0s 189us/step - loss: 1.2429 - acc: 0.7636 - val_loss: 1.1336 - val_acc: 0.7692\n",
            "Epoch 7/30\n",
            "110/110 [==============================] - 0s 192us/step - loss: 1.0565 - acc: 0.7727 - val_loss: 0.9560 - val_acc: 0.7692\n",
            "Epoch 8/30\n",
            "110/110 [==============================] - 0s 190us/step - loss: 0.8991 - acc: 0.7727 - val_loss: 0.8044 - val_acc: 0.7692\n",
            "Epoch 9/30\n",
            "110/110 [==============================] - 0s 175us/step - loss: 0.7725 - acc: 0.7818 - val_loss: 0.6990 - val_acc: 0.7692\n",
            "Epoch 10/30\n",
            "110/110 [==============================] - 0s 202us/step - loss: 0.6961 - acc: 0.7818 - val_loss: 0.6417 - val_acc: 0.7692\n",
            "Epoch 11/30\n",
            "110/110 [==============================] - 0s 205us/step - loss: 0.6425 - acc: 0.7818 - val_loss: 0.5925 - val_acc: 0.7692\n",
            "Epoch 12/30\n",
            "110/110 [==============================] - 0s 179us/step - loss: 0.5992 - acc: 0.7909 - val_loss: 0.5540 - val_acc: 0.7692\n",
            "Epoch 13/30\n",
            "110/110 [==============================] - 0s 202us/step - loss: 0.5619 - acc: 0.8000 - val_loss: 0.5217 - val_acc: 0.7692\n",
            "Epoch 14/30\n",
            "110/110 [==============================] - 0s 176us/step - loss: 0.5317 - acc: 0.8091 - val_loss: 0.4951 - val_acc: 0.8462\n",
            "Epoch 15/30\n",
            "110/110 [==============================] - 0s 192us/step - loss: 0.5059 - acc: 0.8182 - val_loss: 0.4726 - val_acc: 0.8462\n",
            "Epoch 16/30\n",
            "110/110 [==============================] - 0s 201us/step - loss: 0.4837 - acc: 0.8182 - val_loss: 0.4549 - val_acc: 0.8462\n",
            "Epoch 17/30\n",
            "110/110 [==============================] - 0s 214us/step - loss: 0.4643 - acc: 0.8455 - val_loss: 0.4398 - val_acc: 0.8462\n",
            "Epoch 18/30\n",
            "110/110 [==============================] - 0s 213us/step - loss: 0.4476 - acc: 0.8455 - val_loss: 0.4261 - val_acc: 0.8462\n",
            "Epoch 19/30\n",
            "110/110 [==============================] - 0s 196us/step - loss: 0.4334 - acc: 0.8636 - val_loss: 0.4137 - val_acc: 0.9231\n",
            "Epoch 20/30\n",
            "110/110 [==============================] - 0s 189us/step - loss: 0.4207 - acc: 0.8727 - val_loss: 0.4030 - val_acc: 0.9231\n",
            "Epoch 21/30\n",
            "110/110 [==============================] - 0s 181us/step - loss: 0.4096 - acc: 0.8818 - val_loss: 0.3934 - val_acc: 0.9231\n",
            "Epoch 22/30\n",
            "110/110 [==============================] - 0s 214us/step - loss: 0.3998 - acc: 0.9000 - val_loss: 0.3850 - val_acc: 0.9231\n",
            "Epoch 23/30\n",
            "110/110 [==============================] - 0s 206us/step - loss: 0.3908 - acc: 0.9182 - val_loss: 0.3781 - val_acc: 0.9231\n",
            "Epoch 24/30\n",
            "110/110 [==============================] - 0s 183us/step - loss: 0.3827 - acc: 0.9545 - val_loss: 0.3713 - val_acc: 0.9231\n",
            "Epoch 25/30\n",
            "110/110 [==============================] - 0s 199us/step - loss: 0.3753 - acc: 0.9727 - val_loss: 0.3651 - val_acc: 0.9231\n",
            "Epoch 26/30\n",
            "110/110 [==============================] - 0s 203us/step - loss: 0.3686 - acc: 0.9727 - val_loss: 0.3595 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "110/110 [==============================] - 0s 184us/step - loss: 0.3626 - acc: 0.9727 - val_loss: 0.3542 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "110/110 [==============================] - 0s 187us/step - loss: 0.3572 - acc: 0.9727 - val_loss: 0.3494 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "110/110 [==============================] - 0s 181us/step - loss: 0.3524 - acc: 0.9727 - val_loss: 0.3448 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "110/110 [==============================] - 0s 189us/step - loss: 0.3478 - acc: 0.9727 - val_loss: 0.3405 - val_acc: 1.0000\n",
            "Train on 110 samples, validate on 13 samples\n",
            "Epoch 1/30\n",
            "110/110 [==============================] - 1s 10ms/step - loss: 0.8522 - acc: 0.6000 - val_loss: 0.7122 - val_acc: 1.0000\n",
            "Epoch 2/30\n",
            "110/110 [==============================] - 0s 210us/step - loss: 0.6531 - acc: 0.9636 - val_loss: 0.5937 - val_acc: 1.0000\n",
            "Epoch 3/30\n",
            "110/110 [==============================] - 0s 215us/step - loss: 0.5615 - acc: 0.9909 - val_loss: 0.5347 - val_acc: 1.0000\n",
            "Epoch 4/30\n",
            "110/110 [==============================] - 0s 192us/step - loss: 0.5120 - acc: 0.9909 - val_loss: 0.4979 - val_acc: 1.0000\n",
            "Epoch 5/30\n",
            "110/110 [==============================] - 0s 215us/step - loss: 0.4787 - acc: 0.9909 - val_loss: 0.4704 - val_acc: 1.0000\n",
            "Epoch 6/30\n",
            "110/110 [==============================] - 0s 201us/step - loss: 0.4537 - acc: 0.9909 - val_loss: 0.4483 - val_acc: 1.0000\n",
            "Epoch 7/30\n",
            "110/110 [==============================] - 0s 187us/step - loss: 0.4330 - acc: 0.9909 - val_loss: 0.4290 - val_acc: 1.0000\n",
            "Epoch 8/30\n",
            "110/110 [==============================] - 0s 183us/step - loss: 0.4152 - acc: 0.9909 - val_loss: 0.4121 - val_acc: 1.0000\n",
            "Epoch 9/30\n",
            "110/110 [==============================] - 0s 182us/step - loss: 0.3994 - acc: 0.9909 - val_loss: 0.3967 - val_acc: 1.0000\n",
            "Epoch 10/30\n",
            "110/110 [==============================] - 0s 203us/step - loss: 0.3850 - acc: 0.9909 - val_loss: 0.3825 - val_acc: 1.0000\n",
            "Epoch 11/30\n",
            "110/110 [==============================] - 0s 208us/step - loss: 0.3716 - acc: 0.9909 - val_loss: 0.3692 - val_acc: 1.0000\n",
            "Epoch 12/30\n",
            "110/110 [==============================] - 0s 202us/step - loss: 0.3590 - acc: 0.9909 - val_loss: 0.3568 - val_acc: 1.0000\n",
            "Epoch 13/30\n",
            "110/110 [==============================] - 0s 217us/step - loss: 0.3473 - acc: 0.9909 - val_loss: 0.3450 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "110/110 [==============================] - 0s 186us/step - loss: 0.3361 - acc: 0.9909 - val_loss: 0.3337 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "110/110 [==============================] - 0s 233us/step - loss: 0.3255 - acc: 0.9909 - val_loss: 0.3228 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "110/110 [==============================] - 0s 212us/step - loss: 0.3154 - acc: 0.9909 - val_loss: 0.3123 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "110/110 [==============================] - 0s 186us/step - loss: 0.3057 - acc: 0.9909 - val_loss: 0.3024 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "110/110 [==============================] - 0s 192us/step - loss: 0.2966 - acc: 0.9909 - val_loss: 0.2928 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "110/110 [==============================] - 0s 200us/step - loss: 0.2877 - acc: 0.9909 - val_loss: 0.2838 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "110/110 [==============================] - 0s 192us/step - loss: 0.2793 - acc: 1.0000 - val_loss: 0.2751 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "110/110 [==============================] - 0s 176us/step - loss: 0.2713 - acc: 1.0000 - val_loss: 0.2665 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "110/110 [==============================] - 0s 184us/step - loss: 0.2636 - acc: 1.0000 - val_loss: 0.2584 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "110/110 [==============================] - 0s 191us/step - loss: 0.2562 - acc: 1.0000 - val_loss: 0.2505 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "110/110 [==============================] - 0s 232us/step - loss: 0.2491 - acc: 1.0000 - val_loss: 0.2431 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "110/110 [==============================] - 0s 210us/step - loss: 0.2423 - acc: 1.0000 - val_loss: 0.2358 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "110/110 [==============================] - 0s 186us/step - loss: 0.2357 - acc: 1.0000 - val_loss: 0.2287 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "110/110 [==============================] - 0s 187us/step - loss: 0.2294 - acc: 1.0000 - val_loss: 0.2221 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "110/110 [==============================] - 0s 204us/step - loss: 0.2233 - acc: 1.0000 - val_loss: 0.2156 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "110/110 [==============================] - 0s 213us/step - loss: 0.2175 - acc: 1.0000 - val_loss: 0.2092 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "110/110 [==============================] - 0s 185us/step - loss: 0.2118 - acc: 1.0000 - val_loss: 0.2032 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 10ms/step - loss: 1.3360 - acc: 0.0360 - val_loss: 1.1830 - val_acc: 0.0000e+00\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 221us/step - loss: 1.1665 - acc: 0.0721 - val_loss: 1.0555 - val_acc: 0.0833\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 202us/step - loss: 1.0535 - acc: 0.2432 - val_loss: 0.9809 - val_acc: 0.2500\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.9766 - acc: 0.2523 - val_loss: 0.9251 - val_acc: 0.4167\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 187us/step - loss: 0.9218 - acc: 0.3423 - val_loss: 0.8827 - val_acc: 0.5833\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.8775 - acc: 0.5495 - val_loss: 0.8472 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.8401 - acc: 0.6757 - val_loss: 0.8186 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 0.8084 - acc: 0.6847 - val_loss: 0.7929 - val_acc: 0.7500\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.7810 - acc: 0.7117 - val_loss: 0.7706 - val_acc: 0.8333\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 194us/step - loss: 0.7566 - acc: 0.7207 - val_loss: 0.7509 - val_acc: 0.8333\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.7344 - acc: 0.7568 - val_loss: 0.7329 - val_acc: 0.8333\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 215us/step - loss: 0.7142 - acc: 0.7568 - val_loss: 0.7168 - val_acc: 0.8333\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 219us/step - loss: 0.6953 - acc: 0.7658 - val_loss: 0.7018 - val_acc: 1.0000\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.6781 - acc: 0.7838 - val_loss: 0.6879 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 227us/step - loss: 0.6620 - acc: 0.8108 - val_loss: 0.6754 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.6469 - acc: 0.8378 - val_loss: 0.6636 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.6328 - acc: 0.8559 - val_loss: 0.6524 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 180us/step - loss: 0.6196 - acc: 0.8829 - val_loss: 0.6418 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.6072 - acc: 0.9009 - val_loss: 0.6320 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.5956 - acc: 0.9009 - val_loss: 0.6226 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.5845 - acc: 0.9009 - val_loss: 0.6138 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 227us/step - loss: 0.5739 - acc: 0.9009 - val_loss: 0.6054 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 332us/step - loss: 0.5637 - acc: 0.9279 - val_loss: 0.5974 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 0.5542 - acc: 0.9279 - val_loss: 0.5898 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 251us/step - loss: 0.5451 - acc: 0.9459 - val_loss: 0.5826 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 264us/step - loss: 0.5365 - acc: 0.9550 - val_loss: 0.5756 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 224us/step - loss: 0.5285 - acc: 0.9550 - val_loss: 0.5691 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 224us/step - loss: 0.5208 - acc: 0.9730 - val_loss: 0.5627 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 174us/step - loss: 0.5135 - acc: 0.9730 - val_loss: 0.5566 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.5064 - acc: 0.9730 - val_loss: 0.5508 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 10ms/step - loss: 1.6201 - acc: 0.4685 - val_loss: 1.3881 - val_acc: 0.5000\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 251us/step - loss: 1.4542 - acc: 0.5045 - val_loss: 1.2456 - val_acc: 0.5833\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 225us/step - loss: 1.3178 - acc: 0.5135 - val_loss: 1.1189 - val_acc: 0.5833\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 1.2025 - acc: 0.5405 - val_loss: 1.0194 - val_acc: 0.6667\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 1.1044 - acc: 0.5495 - val_loss: 0.9306 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 184us/step - loss: 1.0198 - acc: 0.5766 - val_loss: 0.8533 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.9476 - acc: 0.5856 - val_loss: 0.7900 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.8857 - acc: 0.5856 - val_loss: 0.7344 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 179us/step - loss: 0.8322 - acc: 0.5856 - val_loss: 0.6877 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.7853 - acc: 0.5856 - val_loss: 0.6461 - val_acc: 0.6667\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 190us/step - loss: 0.7440 - acc: 0.5856 - val_loss: 0.6101 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 190us/step - loss: 0.7074 - acc: 0.5856 - val_loss: 0.5783 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 177us/step - loss: 0.6750 - acc: 0.5856 - val_loss: 0.5498 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.6458 - acc: 0.5946 - val_loss: 0.5251 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.6200 - acc: 0.5946 - val_loss: 0.5021 - val_acc: 0.6667\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 183us/step - loss: 0.5961 - acc: 0.6036 - val_loss: 0.4814 - val_acc: 0.6667\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.5745 - acc: 0.6126 - val_loss: 0.4632 - val_acc: 0.6667\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 217us/step - loss: 0.5544 - acc: 0.6216 - val_loss: 0.4462 - val_acc: 0.6667\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.5359 - acc: 0.6486 - val_loss: 0.4306 - val_acc: 0.6667\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.5189 - acc: 0.6577 - val_loss: 0.4162 - val_acc: 0.6667\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 236us/step - loss: 0.5032 - acc: 0.6757 - val_loss: 0.4026 - val_acc: 0.6667\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 234us/step - loss: 0.4886 - acc: 0.7387 - val_loss: 0.3902 - val_acc: 0.6667\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 230us/step - loss: 0.4750 - acc: 0.8108 - val_loss: 0.3783 - val_acc: 0.9167\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 202us/step - loss: 0.4620 - acc: 0.9099 - val_loss: 0.3669 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.4496 - acc: 0.9640 - val_loss: 0.3561 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.4378 - acc: 0.9730 - val_loss: 0.3459 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 221us/step - loss: 0.4264 - acc: 0.9730 - val_loss: 0.3360 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 0.4156 - acc: 0.9820 - val_loss: 0.3260 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.4050 - acc: 0.9820 - val_loss: 0.3169 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 242us/step - loss: 0.3946 - acc: 0.9820 - val_loss: 0.3079 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 11ms/step - loss: 1.4494 - acc: 0.4144 - val_loss: 1.3050 - val_acc: 0.5833\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 290us/step - loss: 1.2402 - acc: 0.5586 - val_loss: 1.1551 - val_acc: 0.6667\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 207us/step - loss: 1.0827 - acc: 0.5946 - val_loss: 1.0387 - val_acc: 0.6667\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 257us/step - loss: 0.9588 - acc: 0.6036 - val_loss: 0.9463 - val_acc: 0.6667\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 244us/step - loss: 0.8585 - acc: 0.6036 - val_loss: 0.8719 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 201us/step - loss: 0.7750 - acc: 0.6036 - val_loss: 0.8072 - val_acc: 0.6667\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.7037 - acc: 0.6036 - val_loss: 0.7534 - val_acc: 0.6667\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.6421 - acc: 0.6216 - val_loss: 0.7042 - val_acc: 0.6667\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.5877 - acc: 0.7117 - val_loss: 0.6629 - val_acc: 0.6667\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 196us/step - loss: 0.5398 - acc: 0.7568 - val_loss: 0.6241 - val_acc: 0.7500\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 0.4974 - acc: 0.8288 - val_loss: 0.5898 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.4601 - acc: 0.9009 - val_loss: 0.5591 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 211us/step - loss: 0.4270 - acc: 0.9279 - val_loss: 0.5316 - val_acc: 0.7500\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 0.3976 - acc: 0.9640 - val_loss: 0.5065 - val_acc: 0.7500\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 221us/step - loss: 0.3713 - acc: 0.9640 - val_loss: 0.4839 - val_acc: 0.8333\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 209us/step - loss: 0.3475 - acc: 0.9640 - val_loss: 0.4629 - val_acc: 0.8333\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 223us/step - loss: 0.3260 - acc: 0.9640 - val_loss: 0.4435 - val_acc: 0.8333\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.3068 - acc: 0.9730 - val_loss: 0.4256 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 214us/step - loss: 0.2894 - acc: 0.9820 - val_loss: 0.4085 - val_acc: 0.8333\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 202us/step - loss: 0.2735 - acc: 0.9910 - val_loss: 0.3927 - val_acc: 0.8333\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 232us/step - loss: 0.2592 - acc: 0.9910 - val_loss: 0.3784 - val_acc: 0.8333\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 0.2460 - acc: 1.0000 - val_loss: 0.3647 - val_acc: 0.8333\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.2341 - acc: 1.0000 - val_loss: 0.3521 - val_acc: 0.8333\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.2231 - acc: 1.0000 - val_loss: 0.3408 - val_acc: 0.8333\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.2131 - acc: 1.0000 - val_loss: 0.3296 - val_acc: 0.8333\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 183us/step - loss: 0.2038 - acc: 1.0000 - val_loss: 0.3194 - val_acc: 0.8333\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.1952 - acc: 1.0000 - val_loss: 0.3101 - val_acc: 0.8333\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.1873 - acc: 1.0000 - val_loss: 0.3014 - val_acc: 0.8333\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.1800 - acc: 1.0000 - val_loss: 0.2928 - val_acc: 0.8333\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.1731 - acc: 1.0000 - val_loss: 0.2850 - val_acc: 0.8333\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 11ms/step - loss: 1.7363 - acc: 0.0270 - val_loss: 1.6500 - val_acc: 0.0000e+00\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 1.4955 - acc: 0.1171 - val_loss: 1.4153 - val_acc: 0.0000e+00\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 1.3363 - acc: 0.2072 - val_loss: 1.2602 - val_acc: 0.1667\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 192us/step - loss: 1.2214 - acc: 0.2432 - val_loss: 1.1485 - val_acc: 0.2500\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 181us/step - loss: 1.1297 - acc: 0.2523 - val_loss: 1.0618 - val_acc: 0.2500\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 1.0528 - acc: 0.2523 - val_loss: 0.9896 - val_acc: 0.2500\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 159us/step - loss: 0.9869 - acc: 0.2703 - val_loss: 0.9307 - val_acc: 0.3333\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 169us/step - loss: 0.9292 - acc: 0.2973 - val_loss: 0.8786 - val_acc: 0.3333\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 166us/step - loss: 0.8788 - acc: 0.3333 - val_loss: 0.8344 - val_acc: 0.3333\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 187us/step - loss: 0.8346 - acc: 0.3874 - val_loss: 0.7955 - val_acc: 0.3333\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.7959 - acc: 0.4865 - val_loss: 0.7612 - val_acc: 0.5000\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 183us/step - loss: 0.7616 - acc: 0.5495 - val_loss: 0.7318 - val_acc: 0.5833\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 176us/step - loss: 0.7313 - acc: 0.5676 - val_loss: 0.7041 - val_acc: 0.5833\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 173us/step - loss: 0.7038 - acc: 0.5856 - val_loss: 0.6798 - val_acc: 0.5833\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 180us/step - loss: 0.6792 - acc: 0.5946 - val_loss: 0.6590 - val_acc: 0.5833\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 174us/step - loss: 0.6572 - acc: 0.6036 - val_loss: 0.6388 - val_acc: 0.5833\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 177us/step - loss: 0.6372 - acc: 0.6036 - val_loss: 0.6205 - val_acc: 0.5833\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 196us/step - loss: 0.6185 - acc: 0.6757 - val_loss: 0.6040 - val_acc: 0.8333\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.6014 - acc: 0.9730 - val_loss: 0.5884 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 237us/step - loss: 0.5855 - acc: 0.9910 - val_loss: 0.5740 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 226us/step - loss: 0.5709 - acc: 0.9910 - val_loss: 0.5604 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.5568 - acc: 0.9910 - val_loss: 0.5474 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.5436 - acc: 0.9910 - val_loss: 0.5350 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 187us/step - loss: 0.5310 - acc: 0.9910 - val_loss: 0.5233 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.5189 - acc: 0.9910 - val_loss: 0.5117 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.5072 - acc: 0.9910 - val_loss: 0.5007 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 177us/step - loss: 0.4960 - acc: 0.9910 - val_loss: 0.4901 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 220us/step - loss: 0.4851 - acc: 0.9910 - val_loss: 0.4795 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.4744 - acc: 0.9910 - val_loss: 0.4691 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.4639 - acc: 0.9910 - val_loss: 0.4590 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 12ms/step - loss: 2.7425 - acc: 0.2252 - val_loss: 2.5327 - val_acc: 0.1667\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 2.4149 - acc: 0.2252 - val_loss: 2.2149 - val_acc: 0.1667\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 241us/step - loss: 2.1125 - acc: 0.2252 - val_loss: 1.9342 - val_acc: 0.1667\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 194us/step - loss: 1.8452 - acc: 0.2252 - val_loss: 1.6875 - val_acc: 0.1667\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 1.6162 - acc: 0.2252 - val_loss: 1.4708 - val_acc: 0.1667\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 187us/step - loss: 1.4140 - acc: 0.2252 - val_loss: 1.2861 - val_acc: 0.1667\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 175us/step - loss: 1.2401 - acc: 0.2432 - val_loss: 1.1295 - val_acc: 0.1667\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 176us/step - loss: 1.0909 - acc: 0.2703 - val_loss: 0.9973 - val_acc: 0.2500\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 160us/step - loss: 0.9662 - acc: 0.3063 - val_loss: 0.8874 - val_acc: 0.3333\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.8635 - acc: 0.4685 - val_loss: 0.7943 - val_acc: 0.4167\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.7789 - acc: 0.6126 - val_loss: 0.7190 - val_acc: 0.7500\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 187us/step - loss: 0.7095 - acc: 0.6847 - val_loss: 0.6532 - val_acc: 0.7500\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.6494 - acc: 0.7838 - val_loss: 0.6001 - val_acc: 0.8333\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.5973 - acc: 0.8378 - val_loss: 0.5542 - val_acc: 0.9167\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.5508 - acc: 0.8739 - val_loss: 0.5126 - val_acc: 0.9167\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.5107 - acc: 0.9009 - val_loss: 0.4763 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 200us/step - loss: 0.4749 - acc: 0.9189 - val_loss: 0.4441 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.4435 - acc: 0.9279 - val_loss: 0.4154 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 177us/step - loss: 0.4150 - acc: 0.9369 - val_loss: 0.3897 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 182us/step - loss: 0.3892 - acc: 0.9459 - val_loss: 0.3668 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.3657 - acc: 0.9640 - val_loss: 0.3455 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.3445 - acc: 0.9640 - val_loss: 0.3267 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 184us/step - loss: 0.3253 - acc: 0.9730 - val_loss: 0.3095 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.3081 - acc: 0.9820 - val_loss: 0.2944 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 189us/step - loss: 0.2923 - acc: 0.9820 - val_loss: 0.2804 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 190us/step - loss: 0.2779 - acc: 0.9820 - val_loss: 0.2680 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 198us/step - loss: 0.2647 - acc: 0.9820 - val_loss: 0.2566 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 175us/step - loss: 0.2524 - acc: 0.9820 - val_loss: 0.2459 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 271us/step - loss: 0.2412 - acc: 0.9820 - val_loss: 0.2360 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.2307 - acc: 0.9820 - val_loss: 0.2268 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 12ms/step - loss: 2.0659 - acc: 0.0000e+00 - val_loss: 1.7958 - val_acc: 0.0000e+00\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 200us/step - loss: 1.8326 - acc: 0.0000e+00 - val_loss: 1.6137 - val_acc: 0.0000e+00\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 1.6393 - acc: 0.0000e+00 - val_loss: 1.4616 - val_acc: 0.0000e+00\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 191us/step - loss: 1.4782 - acc: 0.0000e+00 - val_loss: 1.3328 - val_acc: 0.0000e+00\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 191us/step - loss: 1.3455 - acc: 0.0270 - val_loss: 1.2262 - val_acc: 0.0833\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 183us/step - loss: 1.2331 - acc: 0.1441 - val_loss: 1.1335 - val_acc: 0.3333\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 188us/step - loss: 1.1392 - acc: 0.3514 - val_loss: 1.0549 - val_acc: 0.4167\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 1.0602 - acc: 0.4144 - val_loss: 0.9880 - val_acc: 0.4167\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.9943 - acc: 0.4324 - val_loss: 0.9302 - val_acc: 0.4167\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.9361 - acc: 0.4955 - val_loss: 0.8785 - val_acc: 0.5000\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 188us/step - loss: 0.8853 - acc: 0.5586 - val_loss: 0.8304 - val_acc: 0.6667\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.8396 - acc: 0.6396 - val_loss: 0.7874 - val_acc: 0.6667\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 194us/step - loss: 0.7983 - acc: 0.6486 - val_loss: 0.7490 - val_acc: 0.6667\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 181us/step - loss: 0.7602 - acc: 0.6577 - val_loss: 0.7136 - val_acc: 0.6667\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 195us/step - loss: 0.7260 - acc: 0.6667 - val_loss: 0.6815 - val_acc: 0.7500\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 191us/step - loss: 0.6943 - acc: 0.6757 - val_loss: 0.6522 - val_acc: 0.7500\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 176us/step - loss: 0.6653 - acc: 0.7748 - val_loss: 0.6253 - val_acc: 0.8333\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 184us/step - loss: 0.6406 - acc: 0.8739 - val_loss: 0.6019 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.6216 - acc: 0.8829 - val_loss: 0.5821 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.6055 - acc: 0.8829 - val_loss: 0.5641 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.5908 - acc: 0.8919 - val_loss: 0.5484 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 178us/step - loss: 0.5774 - acc: 0.9189 - val_loss: 0.5346 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 212us/step - loss: 0.5646 - acc: 0.9009 - val_loss: 0.5218 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.5529 - acc: 0.9099 - val_loss: 0.5097 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.5422 - acc: 0.8919 - val_loss: 0.4984 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.5317 - acc: 0.9189 - val_loss: 0.4877 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 219us/step - loss: 0.5217 - acc: 0.9279 - val_loss: 0.4773 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 203us/step - loss: 0.5121 - acc: 0.9459 - val_loss: 0.4675 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 207us/step - loss: 0.5027 - acc: 0.9459 - val_loss: 0.4580 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.4935 - acc: 0.9459 - val_loss: 0.4489 - val_acc: 1.0000\n",
            "Train on 111 samples, validate on 12 samples\n",
            "Epoch 1/30\n",
            "111/111 [==============================] - 1s 12ms/step - loss: 1.7258 - acc: 0.3784 - val_loss: 1.4105 - val_acc: 0.4167\n",
            "Epoch 2/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 1.3781 - acc: 0.4054 - val_loss: 1.1445 - val_acc: 0.4167\n",
            "Epoch 3/30\n",
            "111/111 [==============================] - 0s 229us/step - loss: 1.1235 - acc: 0.5405 - val_loss: 0.9873 - val_acc: 0.5833\n",
            "Epoch 4/30\n",
            "111/111 [==============================] - 0s 180us/step - loss: 0.9787 - acc: 0.6667 - val_loss: 0.9092 - val_acc: 0.6667\n",
            "Epoch 5/30\n",
            "111/111 [==============================] - 0s 165us/step - loss: 0.9001 - acc: 0.7387 - val_loss: 0.8603 - val_acc: 0.6667\n",
            "Epoch 6/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.8534 - acc: 0.8198 - val_loss: 0.8276 - val_acc: 0.7500\n",
            "Epoch 7/30\n",
            "111/111 [==============================] - 0s 193us/step - loss: 0.8228 - acc: 0.8829 - val_loss: 0.8040 - val_acc: 0.8333\n",
            "Epoch 8/30\n",
            "111/111 [==============================] - 0s 213us/step - loss: 0.8000 - acc: 0.8919 - val_loss: 0.7845 - val_acc: 0.8333\n",
            "Epoch 9/30\n",
            "111/111 [==============================] - 0s 192us/step - loss: 0.7818 - acc: 0.9009 - val_loss: 0.7675 - val_acc: 0.8333\n",
            "Epoch 10/30\n",
            "111/111 [==============================] - 0s 222us/step - loss: 0.7666 - acc: 0.9189 - val_loss: 0.7526 - val_acc: 0.8333\n",
            "Epoch 11/30\n",
            "111/111 [==============================] - 0s 236us/step - loss: 0.7533 - acc: 0.9279 - val_loss: 0.7391 - val_acc: 0.9167\n",
            "Epoch 12/30\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.7411 - acc: 0.9369 - val_loss: 0.7280 - val_acc: 0.9167\n",
            "Epoch 13/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.7302 - acc: 0.9369 - val_loss: 0.7180 - val_acc: 0.9167\n",
            "Epoch 14/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.7199 - acc: 0.9369 - val_loss: 0.7077 - val_acc: 1.0000\n",
            "Epoch 15/30\n",
            "111/111 [==============================] - 0s 197us/step - loss: 0.7102 - acc: 0.9459 - val_loss: 0.6994 - val_acc: 1.0000\n",
            "Epoch 16/30\n",
            "111/111 [==============================] - 0s 210us/step - loss: 0.7008 - acc: 0.9459 - val_loss: 0.6914 - val_acc: 1.0000\n",
            "Epoch 17/30\n",
            "111/111 [==============================] - 0s 205us/step - loss: 0.6916 - acc: 0.9459 - val_loss: 0.6837 - val_acc: 1.0000\n",
            "Epoch 18/30\n",
            "111/111 [==============================] - 0s 204us/step - loss: 0.6828 - acc: 0.9459 - val_loss: 0.6761 - val_acc: 1.0000\n",
            "Epoch 19/30\n",
            "111/111 [==============================] - 0s 216us/step - loss: 0.6743 - acc: 0.9459 - val_loss: 0.6685 - val_acc: 1.0000\n",
            "Epoch 20/30\n",
            "111/111 [==============================] - 0s 219us/step - loss: 0.6660 - acc: 0.9459 - val_loss: 0.6612 - val_acc: 1.0000\n",
            "Epoch 21/30\n",
            "111/111 [==============================] - 0s 250us/step - loss: 0.6580 - acc: 0.9459 - val_loss: 0.6540 - val_acc: 1.0000\n",
            "Epoch 22/30\n",
            "111/111 [==============================] - 0s 199us/step - loss: 0.6500 - acc: 0.9459 - val_loss: 0.6467 - val_acc: 1.0000\n",
            "Epoch 23/30\n",
            "111/111 [==============================] - 0s 218us/step - loss: 0.6424 - acc: 0.9459 - val_loss: 0.6396 - val_acc: 1.0000\n",
            "Epoch 24/30\n",
            "111/111 [==============================] - 0s 235us/step - loss: 0.6349 - acc: 0.9459 - val_loss: 0.6327 - val_acc: 1.0000\n",
            "Epoch 25/30\n",
            "111/111 [==============================] - 0s 208us/step - loss: 0.6276 - acc: 0.9459 - val_loss: 0.6257 - val_acc: 1.0000\n",
            "Epoch 26/30\n",
            "111/111 [==============================] - 0s 194us/step - loss: 0.6205 - acc: 0.9550 - val_loss: 0.6189 - val_acc: 1.0000\n",
            "Epoch 27/30\n",
            "111/111 [==============================] - 0s 186us/step - loss: 0.6135 - acc: 0.9550 - val_loss: 0.6124 - val_acc: 1.0000\n",
            "Epoch 28/30\n",
            "111/111 [==============================] - 0s 281us/step - loss: 0.6066 - acc: 0.9550 - val_loss: 0.6058 - val_acc: 1.0000\n",
            "Epoch 29/30\n",
            "111/111 [==============================] - 0s 206us/step - loss: 0.6000 - acc: 0.9550 - val_loss: 0.5992 - val_acc: 1.0000\n",
            "Epoch 30/30\n",
            "111/111 [==============================] - 0s 229us/step - loss: 0.5936 - acc: 0.9550 - val_loss: 0.5928 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "03dfe5b0-7e26-494c-db14-93ae1264d38c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.94892679, -3.26204263],\n",
              "       [ 3.6902765 ,  1.91193691],\n",
              "       [ 4.33888855,  3.11923441],\n",
              "       [-0.19375808, -1.99141129],\n",
              "       [ 4.92020805,  2.60198864],\n",
              "       [-0.11763691, -3.08731746],\n",
              "       [ 0.27541973, -3.58936325],\n",
              "       [ 4.10074676,  2.79547975],\n",
              "       [ 0.81236067, -4.18441726],\n",
              "       [ 4.62588055,  1.71410556],\n",
              "       [-8.20314863,  2.62258764],\n",
              "       [-7.82534822,  2.32665836]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "73712537-8a14-48dd-91ce-df86266fccd8",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "86283397-87f7-4116-89b9-51aa0c397325",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "57b24971-e3cd-4a41-8340-e7e1cd570fb5",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dea44c5e-a466-44ef-f2fd-4beaee7033ff",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa9c9d89da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eecf2906-e217-4a05-ee8c-e94a417eccd4",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa9c9cfbb38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "cf2a8361-abcf-4226-d972-399266ce23b3",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "123/123 [==============================] - 1s 10ms/step - loss: 2.8781 - acc: 0.0976\n",
            "Epoch 2/30\n",
            "123/123 [==============================] - 0s 204us/step - loss: 2.4768 - acc: 0.1220\n",
            "Epoch 3/30\n",
            "123/123 [==============================] - 0s 179us/step - loss: 2.1509 - acc: 0.4146\n",
            "Epoch 4/30\n",
            "123/123 [==============================] - 0s 188us/step - loss: 1.8451 - acc: 0.6260\n",
            "Epoch 5/30\n",
            "123/123 [==============================] - 0s 174us/step - loss: 1.5793 - acc: 0.6829\n",
            "Epoch 6/30\n",
            "123/123 [==============================] - 0s 178us/step - loss: 1.3439 - acc: 0.7317\n",
            "Epoch 7/30\n",
            "123/123 [==============================] - 0s 188us/step - loss: 1.1264 - acc: 0.7480\n",
            "Epoch 8/30\n",
            "123/123 [==============================] - 0s 218us/step - loss: 0.9525 - acc: 0.7480\n",
            "Epoch 9/30\n",
            "123/123 [==============================] - 0s 252us/step - loss: 0.8298 - acc: 0.8049\n",
            "Epoch 10/30\n",
            "123/123 [==============================] - 0s 180us/step - loss: 0.7405 - acc: 0.9675\n",
            "Epoch 11/30\n",
            "123/123 [==============================] - 0s 188us/step - loss: 0.6783 - acc: 0.9919\n",
            "Epoch 12/30\n",
            "123/123 [==============================] - 0s 168us/step - loss: 0.6332 - acc: 0.9919\n",
            "Epoch 13/30\n",
            "123/123 [==============================] - 0s 192us/step - loss: 0.5981 - acc: 0.9919\n",
            "Epoch 14/30\n",
            "123/123 [==============================] - 0s 205us/step - loss: 0.5680 - acc: 1.0000\n",
            "Epoch 15/30\n",
            "123/123 [==============================] - 0s 177us/step - loss: 0.5420 - acc: 1.0000\n",
            "Epoch 16/30\n",
            "123/123 [==============================] - 0s 247us/step - loss: 0.5181 - acc: 1.0000\n",
            "Epoch 17/30\n",
            "123/123 [==============================] - 0s 208us/step - loss: 0.4965 - acc: 1.0000\n",
            "Epoch 18/30\n",
            "123/123 [==============================] - 0s 189us/step - loss: 0.4764 - acc: 1.0000\n",
            "Epoch 19/30\n",
            "123/123 [==============================] - 0s 169us/step - loss: 0.4575 - acc: 1.0000\n",
            "Epoch 20/30\n",
            "123/123 [==============================] - 0s 165us/step - loss: 0.4394 - acc: 1.0000\n",
            "Epoch 21/30\n",
            "123/123 [==============================] - 0s 222us/step - loss: 0.4224 - acc: 1.0000\n",
            "Epoch 22/30\n",
            "123/123 [==============================] - 0s 193us/step - loss: 0.4062 - acc: 1.0000\n",
            "Epoch 23/30\n",
            "123/123 [==============================] - 0s 158us/step - loss: 0.3909 - acc: 1.0000\n",
            "Epoch 24/30\n",
            "123/123 [==============================] - 0s 165us/step - loss: 0.3762 - acc: 1.0000\n",
            "Epoch 25/30\n",
            "123/123 [==============================] - 0s 174us/step - loss: 0.3621 - acc: 1.0000\n",
            "Epoch 26/30\n",
            "123/123 [==============================] - 0s 167us/step - loss: 0.3488 - acc: 1.0000\n",
            "Epoch 27/30\n",
            "123/123 [==============================] - 0s 168us/step - loss: 0.3359 - acc: 1.0000\n",
            "Epoch 28/30\n",
            "123/123 [==============================] - 0s 191us/step - loss: 0.3237 - acc: 1.0000\n",
            "Epoch 29/30\n",
            "123/123 [==============================] - 0s 193us/step - loss: 0.3120 - acc: 1.0000\n",
            "Epoch 30/30\n",
            "123/123 [==============================] - 0s 180us/step - loss: 0.3008 - acc: 1.0000\n",
            "42/42 [==============================] - 0s 11ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "59d82e7c-1e9e-4a92-b9af-065b2bacdbc5",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4b2dafe-f962-4667-a30f-f50f44a347a7",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38095238237153917"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQV6AwSwrmDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
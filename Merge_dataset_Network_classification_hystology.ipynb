{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Merge_dataset_Network_classification_hystology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Merge_dataset_Network_classification_hystology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfLmMhRwN_ua",
        "colab_type": "text"
      },
      "source": [
        "Prove mettendo insieme i due dataset. Partendo da un unico dataset e dividendolo usando train_test_split.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDYn4tlNNkCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "0a7a616b-a876-4e9c-ce92-0f74c25779b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21MJpB7HPm8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tot_data = pd.concat([train_data, test_data], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01IYan5zRsyi",
        "colab_type": "code",
        "outputId": "7f6147d3-0e96-463e-a843-2c1fce3ff437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 595
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1H421egATgdD",
        "colab_type": "code",
        "outputId": "b5a62952-4ea3-4324-d095-5b85a1456e75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 596
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yLhL0BxTi96",
        "colab_type": "code",
        "outputId": "ed988c4f-fb70-4dc8-9b90-bb9a9dbae00c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tot_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165, 107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 597
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cisCrQrHVZfj",
        "colab_type": "code",
        "outputId": "8fdd5466-6d82-44e8-dad3-4bfa6e10674e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tot_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>51905.377962</td>\n",
              "      <td>66.288317</td>\n",
              "      <td>51847.748274</td>\n",
              "      <td>50.574214</td>\n",
              "      <td>0.649258</td>\n",
              "      <td>37.884620</td>\n",
              "      <td>0.821088</td>\n",
              "      <td>0.199752</td>\n",
              "      <td>63.135672</td>\n",
              "      <td>0.749090</td>\n",
              "      <td>10356.675894</td>\n",
              "      <td>41.525899</td>\n",
              "      <td>65.067279</td>\n",
              "      <td>55.325619</td>\n",
              "      <td>12.481889</td>\n",
              "      <td>1084.854684</td>\n",
              "      <td>5.949646</td>\n",
              "      <td>918.046673</td>\n",
              "      <td>5312.127441</td>\n",
              "      <td>0.122582</td>\n",
              "      <td>97.652454</td>\n",
              "      <td>0.050648</td>\n",
              "      <td>141.807349</td>\n",
              "      <td>0.124010</td>\n",
              "      <td>42.380287</td>\n",
              "      <td>163828.301666</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>33.190463</td>\n",
              "      <td>66.380925</td>\n",
              "      <td>4.331076</td>\n",
              "      <td>-475.448161</td>\n",
              "      <td>0.308299</td>\n",
              "      <td>0.994676</td>\n",
              "      <td>0.133643</td>\n",
              "      <td>8.029403</td>\n",
              "      <td>2.169232</td>\n",
              "      <td>0.424312</td>\n",
              "      <td>6.065116</td>\n",
              "      <td>0.967009</td>\n",
              "      <td>...</td>\n",
              "      <td>-75.0</td>\n",
              "      <td>17.777521</td>\n",
              "      <td>5.650502</td>\n",
              "      <td>0.001166</td>\n",
              "      <td>17.478141</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.192540</td>\n",
              "      <td>1.399893</td>\n",
              "      <td>2288.112500</td>\n",
              "      <td>3.749302</td>\n",
              "      <td>793.234742</td>\n",
              "      <td>6675.859117</td>\n",
              "      <td>0.776723</td>\n",
              "      <td>4220.221337</td>\n",
              "      <td>0.654950</td>\n",
              "      <td>0.003806</td>\n",
              "      <td>4.209293</td>\n",
              "      <td>1049.544424</td>\n",
              "      <td>0.560736</td>\n",
              "      <td>34.869500</td>\n",
              "      <td>42116.076135</td>\n",
              "      <td>0.060025</td>\n",
              "      <td>0.517739</td>\n",
              "      <td>1145.238698</td>\n",
              "      <td>132.775769</td>\n",
              "      <td>42183.224231</td>\n",
              "      <td>540.316964</td>\n",
              "      <td>0.122035</td>\n",
              "      <td>36.510775</td>\n",
              "      <td>4.877236e+07</td>\n",
              "      <td>751.520796</td>\n",
              "      <td>0.747563</td>\n",
              "      <td>0.002453</td>\n",
              "      <td>5.741322</td>\n",
              "      <td>0.001663</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>928.016789</td>\n",
              "      <td>1.153806</td>\n",
              "      <td>0.020920</td>\n",
              "      <td>1.306338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13432.502747</td>\n",
              "      <td>58.057539</td>\n",
              "      <td>13312.697411</td>\n",
              "      <td>48.354803</td>\n",
              "      <td>0.572236</td>\n",
              "      <td>18.135097</td>\n",
              "      <td>0.546338</td>\n",
              "      <td>0.356577</td>\n",
              "      <td>40.146103</td>\n",
              "      <td>0.375042</td>\n",
              "      <td>4747.006589</td>\n",
              "      <td>26.418066</td>\n",
              "      <td>32.760898</td>\n",
              "      <td>56.652510</td>\n",
              "      <td>60.615944</td>\n",
              "      <td>1076.589137</td>\n",
              "      <td>7.130906</td>\n",
              "      <td>595.667519</td>\n",
              "      <td>351.846858</td>\n",
              "      <td>0.307871</td>\n",
              "      <td>239.202712</td>\n",
              "      <td>0.126873</td>\n",
              "      <td>32.011715</td>\n",
              "      <td>0.031426</td>\n",
              "      <td>12.612334</td>\n",
              "      <td>41890.348882</td>\n",
              "      <td>0.001812</td>\n",
              "      <td>0.003214</td>\n",
              "      <td>33.522040</td>\n",
              "      <td>67.044080</td>\n",
              "      <td>7.487967</td>\n",
              "      <td>-2829.110940</td>\n",
              "      <td>0.055759</td>\n",
              "      <td>0.985695</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>38.337756</td>\n",
              "      <td>3.408960</td>\n",
              "      <td>0.322015</td>\n",
              "      <td>23.246287</td>\n",
              "      <td>0.935189</td>\n",
              "      <td>...</td>\n",
              "      <td>-397.0</td>\n",
              "      <td>5.224099</td>\n",
              "      <td>-91.310969</td>\n",
              "      <td>0.003404</td>\n",
              "      <td>65.432452</td>\n",
              "      <td>0.003524</td>\n",
              "      <td>0.064158</td>\n",
              "      <td>0.230922</td>\n",
              "      <td>262.139314</td>\n",
              "      <td>1.567042</td>\n",
              "      <td>926.829706</td>\n",
              "      <td>3237.676584</td>\n",
              "      <td>0.908183</td>\n",
              "      <td>1777.017297</td>\n",
              "      <td>0.869059</td>\n",
              "      <td>0.004164</td>\n",
              "      <td>5.072184</td>\n",
              "      <td>1044.275778</td>\n",
              "      <td>0.790377</td>\n",
              "      <td>84.615342</td>\n",
              "      <td>598.216508</td>\n",
              "      <td>0.033349</td>\n",
              "      <td>0.543736</td>\n",
              "      <td>843.878866</td>\n",
              "      <td>51.757732</td>\n",
              "      <td>607.367912</td>\n",
              "      <td>543.633876</td>\n",
              "      <td>0.330564</td>\n",
              "      <td>0.465530</td>\n",
              "      <td>8.137725e+05</td>\n",
              "      <td>763.567010</td>\n",
              "      <td>0.764879</td>\n",
              "      <td>0.006307</td>\n",
              "      <td>6.451087</td>\n",
              "      <td>0.004959</td>\n",
              "      <td>0.001680</td>\n",
              "      <td>2944.805484</td>\n",
              "      <td>2.266070</td>\n",
              "      <td>0.146173</td>\n",
              "      <td>0.253533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>25843.872675</td>\n",
              "      <td>52.918217</td>\n",
              "      <td>25724.437234</td>\n",
              "      <td>39.406474</td>\n",
              "      <td>0.675497</td>\n",
              "      <td>28.487740</td>\n",
              "      <td>0.891907</td>\n",
              "      <td>0.242519</td>\n",
              "      <td>46.415213</td>\n",
              "      <td>0.722920</td>\n",
              "      <td>6238.658603</td>\n",
              "      <td>35.146929</td>\n",
              "      <td>47.180420</td>\n",
              "      <td>46.322906</td>\n",
              "      <td>55.064124</td>\n",
              "      <td>1131.900166</td>\n",
              "      <td>6.932158</td>\n",
              "      <td>844.783490</td>\n",
              "      <td>1023.136953</td>\n",
              "      <td>0.253452</td>\n",
              "      <td>192.059998</td>\n",
              "      <td>0.093605</td>\n",
              "      <td>57.855402</td>\n",
              "      <td>0.045979</td>\n",
              "      <td>22.286238</td>\n",
              "      <td>79266.271357</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>34.046184</td>\n",
              "      <td>68.092368</td>\n",
              "      <td>6.796695</td>\n",
              "      <td>-3133.512010</td>\n",
              "      <td>0.106695</td>\n",
              "      <td>0.984515</td>\n",
              "      <td>0.033042</td>\n",
              "      <td>32.375097</td>\n",
              "      <td>3.182144</td>\n",
              "      <td>0.362633</td>\n",
              "      <td>21.028555</td>\n",
              "      <td>0.936889</td>\n",
              "      <td>...</td>\n",
              "      <td>-363.0</td>\n",
              "      <td>5.387644</td>\n",
              "      <td>-67.724986</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>61.733697</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>0.085594</td>\n",
              "      <td>0.426431</td>\n",
              "      <td>626.125860</td>\n",
              "      <td>1.968083</td>\n",
              "      <td>906.575851</td>\n",
              "      <td>5266.417163</td>\n",
              "      <td>0.870954</td>\n",
              "      <td>2393.005796</td>\n",
              "      <td>0.809077</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>4.968901</td>\n",
              "      <td>1079.567623</td>\n",
              "      <td>0.718481</td>\n",
              "      <td>68.928494</td>\n",
              "      <td>3157.415098</td>\n",
              "      <td>0.036760</td>\n",
              "      <td>0.531612</td>\n",
              "      <td>1319.991542</td>\n",
              "      <td>91.276279</td>\n",
              "      <td>3170.626259</td>\n",
              "      <td>525.709823</td>\n",
              "      <td>0.275125</td>\n",
              "      <td>2.306913</td>\n",
              "      <td>4.377395e+06</td>\n",
              "      <td>732.877970</td>\n",
              "      <td>0.757392</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>6.294554</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>0.000745</td>\n",
              "      <td>2146.780339</td>\n",
              "      <td>1.238883</td>\n",
              "      <td>0.152919</td>\n",
              "      <td>0.611772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22152.709032</td>\n",
              "      <td>46.635312</td>\n",
              "      <td>22099.016776</td>\n",
              "      <td>38.640414</td>\n",
              "      <td>0.733411</td>\n",
              "      <td>28.129382</td>\n",
              "      <td>0.754141</td>\n",
              "      <td>0.234970</td>\n",
              "      <td>39.274914</td>\n",
              "      <td>0.727978</td>\n",
              "      <td>5192.604222</td>\n",
              "      <td>29.140329</td>\n",
              "      <td>41.329017</td>\n",
              "      <td>46.543116</td>\n",
              "      <td>103.453079</td>\n",
              "      <td>1089.937565</td>\n",
              "      <td>7.205471</td>\n",
              "      <td>1087.724664</td>\n",
              "      <td>511.504912</td>\n",
              "      <td>0.336024</td>\n",
              "      <td>249.889273</td>\n",
              "      <td>0.140606</td>\n",
              "      <td>29.391158</td>\n",
              "      <td>0.026374</td>\n",
              "      <td>11.821883</td>\n",
              "      <td>42717.634566</td>\n",
              "      <td>0.001570</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>33.073644</td>\n",
              "      <td>66.147288</td>\n",
              "      <td>8.211500</td>\n",
              "      <td>-5741.187919</td>\n",
              "      <td>0.049759</td>\n",
              "      <td>0.975646</td>\n",
              "      <td>0.012491</td>\n",
              "      <td>58.811721</td>\n",
              "      <td>3.729477</td>\n",
              "      <td>0.290187</td>\n",
              "      <td>33.232290</td>\n",
              "      <td>0.913791</td>\n",
              "      <td>...</td>\n",
              "      <td>-528.5</td>\n",
              "      <td>2.714807</td>\n",
              "      <td>-102.779860</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>107.887272</td>\n",
              "      <td>0.002923</td>\n",
              "      <td>0.054549</td>\n",
              "      <td>0.203221</td>\n",
              "      <td>370.362301</td>\n",
              "      <td>1.505960</td>\n",
              "      <td>924.243802</td>\n",
              "      <td>5447.401203</td>\n",
              "      <td>0.914390</td>\n",
              "      <td>1765.537152</td>\n",
              "      <td>0.877247</td>\n",
              "      <td>0.003439</td>\n",
              "      <td>5.297660</td>\n",
              "      <td>1044.150138</td>\n",
              "      <td>0.801694</td>\n",
              "      <td>100.015880</td>\n",
              "      <td>989.809275</td>\n",
              "      <td>0.028357</td>\n",
              "      <td>0.545916</td>\n",
              "      <td>1549.308668</td>\n",
              "      <td>80.477097</td>\n",
              "      <td>997.239605</td>\n",
              "      <td>527.497142</td>\n",
              "      <td>0.366856</td>\n",
              "      <td>0.671664</td>\n",
              "      <td>1.506307e+06</td>\n",
              "      <td>713.093023</td>\n",
              "      <td>0.767331</td>\n",
              "      <td>0.004730</td>\n",
              "      <td>6.545703</td>\n",
              "      <td>0.003888</td>\n",
              "      <td>0.000896</td>\n",
              "      <td>3136.738970</td>\n",
              "      <td>1.157976</td>\n",
              "      <td>0.351327</td>\n",
              "      <td>0.564313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>119385.805617</td>\n",
              "      <td>92.436320</td>\n",
              "      <td>119150.752851</td>\n",
              "      <td>75.629327</td>\n",
              "      <td>0.617296</td>\n",
              "      <td>47.029423</td>\n",
              "      <td>0.722141</td>\n",
              "      <td>0.159206</td>\n",
              "      <td>76.960070</td>\n",
              "      <td>0.621841</td>\n",
              "      <td>18969.476942</td>\n",
              "      <td>54.615019</td>\n",
              "      <td>83.612205</td>\n",
              "      <td>70.719598</td>\n",
              "      <td>67.305748</td>\n",
              "      <td>1445.561800</td>\n",
              "      <td>7.263383</td>\n",
              "      <td>4562.726560</td>\n",
              "      <td>3232.154350</td>\n",
              "      <td>0.241385</td>\n",
              "      <td>264.764085</td>\n",
              "      <td>0.109442</td>\n",
              "      <td>37.885083</td>\n",
              "      <td>0.028865</td>\n",
              "      <td>14.368248</td>\n",
              "      <td>66795.832602</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>37.463123</td>\n",
              "      <td>74.926246</td>\n",
              "      <td>7.979356</td>\n",
              "      <td>-5754.693763</td>\n",
              "      <td>0.063232</td>\n",
              "      <td>0.987519</td>\n",
              "      <td>0.014054</td>\n",
              "      <td>41.318562</td>\n",
              "      <td>3.487282</td>\n",
              "      <td>0.313702</td>\n",
              "      <td>24.584485</td>\n",
              "      <td>0.938581</td>\n",
              "      <td>...</td>\n",
              "      <td>-420.0</td>\n",
              "      <td>5.711559</td>\n",
              "      <td>-109.918927</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>71.446292</td>\n",
              "      <td>0.001577</td>\n",
              "      <td>0.065239</td>\n",
              "      <td>0.280825</td>\n",
              "      <td>2318.917381</td>\n",
              "      <td>1.665599</td>\n",
              "      <td>1227.793946</td>\n",
              "      <td>27327.614966</td>\n",
              "      <td>0.897233</td>\n",
              "      <td>2548.311647</td>\n",
              "      <td>0.851946</td>\n",
              "      <td>0.002165</td>\n",
              "      <td>5.115966</td>\n",
              "      <td>1400.205550</td>\n",
              "      <td>0.767543</td>\n",
              "      <td>82.273203</td>\n",
              "      <td>7062.537600</td>\n",
              "      <td>0.034543</td>\n",
              "      <td>0.445967</td>\n",
              "      <td>4857.474477</td>\n",
              "      <td>376.237055</td>\n",
              "      <td>7077.188671</td>\n",
              "      <td>703.114744</td>\n",
              "      <td>0.261255</td>\n",
              "      <td>3.905480</td>\n",
              "      <td>1.289007e+07</td>\n",
              "      <td>1048.831160</td>\n",
              "      <td>0.694399</td>\n",
              "      <td>0.002179</td>\n",
              "      <td>6.849399</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>3368.646263</td>\n",
              "      <td>0.390430</td>\n",
              "      <td>0.132602</td>\n",
              "      <td>1.804351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>6592.266962</td>\n",
              "      <td>33.622119</td>\n",
              "      <td>6525.211805</td>\n",
              "      <td>26.483717</td>\n",
              "      <td>0.737649</td>\n",
              "      <td>16.326688</td>\n",
              "      <td>0.854215</td>\n",
              "      <td>0.350835</td>\n",
              "      <td>27.010370</td>\n",
              "      <td>0.616480</td>\n",
              "      <td>2289.271748</td>\n",
              "      <td>22.622792</td>\n",
              "      <td>32.335274</td>\n",
              "      <td>29.352647</td>\n",
              "      <td>21.910855</td>\n",
              "      <td>602.049910</td>\n",
              "      <td>6.617736</td>\n",
              "      <td>193.040506</td>\n",
              "      <td>462.455335</td>\n",
              "      <td>0.198303</td>\n",
              "      <td>78.741854</td>\n",
              "      <td>0.069816</td>\n",
              "      <td>94.932731</td>\n",
              "      <td>0.140683</td>\n",
              "      <td>35.081663</td>\n",
              "      <td>67119.461845</td>\n",
              "      <td>0.001239</td>\n",
              "      <td>0.003445</td>\n",
              "      <td>24.859564</td>\n",
              "      <td>49.719128</td>\n",
              "      <td>5.549785</td>\n",
              "      <td>-810.250238</td>\n",
              "      <td>0.148434</td>\n",
              "      <td>0.986138</td>\n",
              "      <td>0.062119</td>\n",
              "      <td>13.257631</td>\n",
              "      <td>2.653226</td>\n",
              "      <td>0.384951</td>\n",
              "      <td>8.884155</td>\n",
              "      <td>0.941150</td>\n",
              "      <td>...</td>\n",
              "      <td>-224.0</td>\n",
              "      <td>7.373263</td>\n",
              "      <td>-38.508861</td>\n",
              "      <td>0.003801</td>\n",
              "      <td>26.164917</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.122205</td>\n",
              "      <td>0.816527</td>\n",
              "      <td>252.082403</td>\n",
              "      <td>2.689565</td>\n",
              "      <td>450.205653</td>\n",
              "      <td>1334.853146</td>\n",
              "      <td>0.829559</td>\n",
              "      <td>1752.475726</td>\n",
              "      <td>0.740910</td>\n",
              "      <td>0.006731</td>\n",
              "      <td>4.573485</td>\n",
              "      <td>567.808195</td>\n",
              "      <td>0.646468</td>\n",
              "      <td>30.944145</td>\n",
              "      <td>1838.790622</td>\n",
              "      <td>0.057837</td>\n",
              "      <td>0.487234</td>\n",
              "      <td>295.264026</td>\n",
              "      <td>35.049505</td>\n",
              "      <td>1859.608911</td>\n",
              "      <td>263.012843</td>\n",
              "      <td>0.219168</td>\n",
              "      <td>2.665039</td>\n",
              "      <td>1.307177e+06</td>\n",
              "      <td>385.825083</td>\n",
              "      <td>0.724813</td>\n",
              "      <td>0.007286</td>\n",
              "      <td>5.736757</td>\n",
              "      <td>0.004595</td>\n",
              "      <td>0.002543</td>\n",
              "      <td>645.520499</td>\n",
              "      <td>2.322894</td>\n",
              "      <td>0.085026</td>\n",
              "      <td>0.359230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>3010.031479</td>\n",
              "      <td>29.286452</td>\n",
              "      <td>2976.851594</td>\n",
              "      <td>24.737747</td>\n",
              "      <td>0.653423</td>\n",
              "      <td>12.466119</td>\n",
              "      <td>0.668128</td>\n",
              "      <td>0.514483</td>\n",
              "      <td>23.150905</td>\n",
              "      <td>0.503931</td>\n",
              "      <td>1531.539258</td>\n",
              "      <td>16.527972</td>\n",
              "      <td>27.610305</td>\n",
              "      <td>26.839660</td>\n",
              "      <td>62.881425</td>\n",
              "      <td>869.396436</td>\n",
              "      <td>6.867420</td>\n",
              "      <td>345.579802</td>\n",
              "      <td>201.369901</td>\n",
              "      <td>0.339958</td>\n",
              "      <td>195.491748</td>\n",
              "      <td>0.136863</td>\n",
              "      <td>53.888713</td>\n",
              "      <td>0.047515</td>\n",
              "      <td>26.381284</td>\n",
              "      <td>65473.407525</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.002506</td>\n",
              "      <td>29.963303</td>\n",
              "      <td>59.926606</td>\n",
              "      <td>7.422073</td>\n",
              "      <td>-2502.214292</td>\n",
              "      <td>0.108583</td>\n",
              "      <td>0.975065</td>\n",
              "      <td>0.024723</td>\n",
              "      <td>38.641415</td>\n",
              "      <td>3.546610</td>\n",
              "      <td>0.281848</td>\n",
              "      <td>21.558304</td>\n",
              "      <td>0.910929</td>\n",
              "      <td>...</td>\n",
              "      <td>-471.6</td>\n",
              "      <td>2.851107</td>\n",
              "      <td>-153.137426</td>\n",
              "      <td>0.002704</td>\n",
              "      <td>64.774609</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.056870</td>\n",
              "      <td>0.377849</td>\n",
              "      <td>120.266506</td>\n",
              "      <td>1.809069</td>\n",
              "      <td>689.951676</td>\n",
              "      <td>1625.228381</td>\n",
              "      <td>0.897161</td>\n",
              "      <td>1774.442365</td>\n",
              "      <td>0.836740</td>\n",
              "      <td>0.003555</td>\n",
              "      <td>5.118256</td>\n",
              "      <td>808.826968</td>\n",
              "      <td>0.768649</td>\n",
              "      <td>54.608361</td>\n",
              "      <td>364.145936</td>\n",
              "      <td>0.038857</td>\n",
              "      <td>0.528923</td>\n",
              "      <td>510.939959</td>\n",
              "      <td>37.536232</td>\n",
              "      <td>370.978261</td>\n",
              "      <td>407.183237</td>\n",
              "      <td>0.382574</td>\n",
              "      <td>0.307757</td>\n",
              "      <td>4.555961e+05</td>\n",
              "      <td>570.265010</td>\n",
              "      <td>0.755454</td>\n",
              "      <td>0.004471</td>\n",
              "      <td>6.087835</td>\n",
              "      <td>0.003856</td>\n",
              "      <td>0.002895</td>\n",
              "      <td>1472.717438</td>\n",
              "      <td>2.379395</td>\n",
              "      <td>0.295357</td>\n",
              "      <td>0.263438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>8683.195759</td>\n",
              "      <td>49.195074</td>\n",
              "      <td>8611.074213</td>\n",
              "      <td>46.624467</td>\n",
              "      <td>0.641262</td>\n",
              "      <td>14.348828</td>\n",
              "      <td>0.438028</td>\n",
              "      <td>0.367928</td>\n",
              "      <td>31.523446</td>\n",
              "      <td>0.307753</td>\n",
              "      <td>3168.257409</td>\n",
              "      <td>20.422841</td>\n",
              "      <td>34.268292</td>\n",
              "      <td>37.102042</td>\n",
              "      <td>153.099954</td>\n",
              "      <td>738.203185</td>\n",
              "      <td>7.378490</td>\n",
              "      <td>847.045579</td>\n",
              "      <td>105.449204</td>\n",
              "      <td>0.443610</td>\n",
              "      <td>264.767425</td>\n",
              "      <td>0.232577</td>\n",
              "      <td>16.744097</td>\n",
              "      <td>0.124622</td>\n",
              "      <td>8.324220</td>\n",
              "      <td>22337.763317</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>25.523890</td>\n",
              "      <td>51.047781</td>\n",
              "      <td>9.676885</td>\n",
              "      <td>-2020.290555</td>\n",
              "      <td>0.026769</td>\n",
              "      <td>0.963103</td>\n",
              "      <td>0.002806</td>\n",
              "      <td>96.669264</td>\n",
              "      <td>4.273414</td>\n",
              "      <td>0.174092</td>\n",
              "      <td>40.625092</td>\n",
              "      <td>0.880158</td>\n",
              "      <td>...</td>\n",
              "      <td>-829.0</td>\n",
              "      <td>1.626201</td>\n",
              "      <td>-408.424767</td>\n",
              "      <td>0.008992</td>\n",
              "      <td>146.665900</td>\n",
              "      <td>0.009519</td>\n",
              "      <td>0.027008</td>\n",
              "      <td>0.124834</td>\n",
              "      <td>91.216703</td>\n",
              "      <td>1.292427</td>\n",
              "      <td>654.187277</td>\n",
              "      <td>2977.335607</td>\n",
              "      <td>0.950812</td>\n",
              "      <td>1057.363734</td>\n",
              "      <td>0.926858</td>\n",
              "      <td>0.012501</td>\n",
              "      <td>5.643852</td>\n",
              "      <td>706.485685</td>\n",
              "      <td>0.880887</td>\n",
              "      <td>108.879134</td>\n",
              "      <td>36.053791</td>\n",
              "      <td>0.026627</td>\n",
              "      <td>0.500224</td>\n",
              "      <td>931.916801</td>\n",
              "      <td>49.605475</td>\n",
              "      <td>39.875470</td>\n",
              "      <td>425.432967</td>\n",
              "      <td>0.511532</td>\n",
              "      <td>0.172971</td>\n",
              "      <td>5.926173e+04</td>\n",
              "      <td>592.454106</td>\n",
              "      <td>0.735138</td>\n",
              "      <td>0.007341</td>\n",
              "      <td>6.687140</td>\n",
              "      <td>0.004549</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>4040.182819</td>\n",
              "      <td>2.076410</td>\n",
              "      <td>0.594629</td>\n",
              "      <td>0.392208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>25939.914844</td>\n",
              "      <td>57.067095</td>\n",
              "      <td>25839.679802</td>\n",
              "      <td>51.745408</td>\n",
              "      <td>0.692948</td>\n",
              "      <td>26.633569</td>\n",
              "      <td>0.617498</td>\n",
              "      <td>0.236059</td>\n",
              "      <td>40.700499</td>\n",
              "      <td>0.514704</td>\n",
              "      <td>6099.697734</td>\n",
              "      <td>31.952700</td>\n",
              "      <td>41.839289</td>\n",
              "      <td>56.857818</td>\n",
              "      <td>32.995425</td>\n",
              "      <td>1449.528401</td>\n",
              "      <td>6.634924</td>\n",
              "      <td>576.773713</td>\n",
              "      <td>2560.111765</td>\n",
              "      <td>0.161550</td>\n",
              "      <td>157.459073</td>\n",
              "      <td>0.053012</td>\n",
              "      <td>163.638419</td>\n",
              "      <td>0.102321</td>\n",
              "      <td>53.465611</td>\n",
              "      <td>264022.398805</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.001059</td>\n",
              "      <td>38.286575</td>\n",
              "      <td>76.573150</td>\n",
              "      <td>5.054850</td>\n",
              "      <td>-2238.871210</td>\n",
              "      <td>0.270459</td>\n",
              "      <td>0.991175</td>\n",
              "      <td>0.109092</td>\n",
              "      <td>17.532514</td>\n",
              "      <td>2.535238</td>\n",
              "      <td>0.343561</td>\n",
              "      <td>13.345420</td>\n",
              "      <td>0.959707</td>\n",
              "      <td>...</td>\n",
              "      <td>-218.0</td>\n",
              "      <td>11.861095</td>\n",
              "      <td>-47.050460</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>45.643158</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.141440</td>\n",
              "      <td>1.736298</td>\n",
              "      <td>985.371574</td>\n",
              "      <td>4.294691</td>\n",
              "      <td>989.771419</td>\n",
              "      <td>3814.874403</td>\n",
              "      <td>0.768468</td>\n",
              "      <td>6660.476094</td>\n",
              "      <td>0.634757</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>4.836890</td>\n",
              "      <td>1358.562247</td>\n",
              "      <td>0.549125</td>\n",
              "      <td>64.574608</td>\n",
              "      <td>13539.270699</td>\n",
              "      <td>0.045526</td>\n",
              "      <td>0.520480</td>\n",
              "      <td>994.636839</td>\n",
              "      <td>87.001047</td>\n",
              "      <td>13571.684982</td>\n",
              "      <td>678.036010</td>\n",
              "      <td>0.175643</td>\n",
              "      <td>8.433580</td>\n",
              "      <td>2.185777e+07</td>\n",
              "      <td>955.398744</td>\n",
              "      <td>0.749551</td>\n",
              "      <td>0.002669</td>\n",
              "      <td>6.114671</td>\n",
              "      <td>0.002276</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>1541.581036</td>\n",
              "      <td>2.194039</td>\n",
              "      <td>0.059377</td>\n",
              "      <td>0.658985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>73401.852784</td>\n",
              "      <td>73.105484</td>\n",
              "      <td>73287.411984</td>\n",
              "      <td>64.560114</td>\n",
              "      <td>0.791968</td>\n",
              "      <td>36.748716</td>\n",
              "      <td>0.681059</td>\n",
              "      <td>0.145915</td>\n",
              "      <td>52.860788</td>\n",
              "      <td>0.569217</td>\n",
              "      <td>10693.735802</td>\n",
              "      <td>43.969247</td>\n",
              "      <td>65.591520</td>\n",
              "      <td>72.942237</td>\n",
              "      <td>81.163824</td>\n",
              "      <td>991.869458</td>\n",
              "      <td>7.531933</td>\n",
              "      <td>2211.526781</td>\n",
              "      <td>3370.775879</td>\n",
              "      <td>0.190389</td>\n",
              "      <td>124.723669</td>\n",
              "      <td>0.071833</td>\n",
              "      <td>98.566310</td>\n",
              "      <td>0.094550</td>\n",
              "      <td>39.657307</td>\n",
              "      <td>123234.303667</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>30.846682</td>\n",
              "      <td>61.693363</td>\n",
              "      <td>7.183740</td>\n",
              "      <td>-5768.083577</td>\n",
              "      <td>0.140669</td>\n",
              "      <td>0.994400</td>\n",
              "      <td>0.037231</td>\n",
              "      <td>27.783111</td>\n",
              "      <td>3.108671</td>\n",
              "      <td>0.338089</td>\n",
              "      <td>18.042103</td>\n",
              "      <td>0.960845</td>\n",
              "      <td>...</td>\n",
              "      <td>-517.0</td>\n",
              "      <td>4.023106</td>\n",
              "      <td>-108.609186</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>93.372993</td>\n",
              "      <td>0.003554</td>\n",
              "      <td>0.069731</td>\n",
              "      <td>0.936258</td>\n",
              "      <td>1601.793815</td>\n",
              "      <td>2.772571</td>\n",
              "      <td>729.132148</td>\n",
              "      <td>15360.888892</td>\n",
              "      <td>0.842243</td>\n",
              "      <td>3104.316398</td>\n",
              "      <td>0.743261</td>\n",
              "      <td>0.005497</td>\n",
              "      <td>5.455417</td>\n",
              "      <td>914.855419</td>\n",
              "      <td>0.668099</td>\n",
              "      <td>100.361763</td>\n",
              "      <td>13140.774613</td>\n",
              "      <td>0.029868</td>\n",
              "      <td>0.422933</td>\n",
              "      <td>2776.555217</td>\n",
              "      <td>196.083016</td>\n",
              "      <td>13162.766641</td>\n",
              "      <td>412.848389</td>\n",
              "      <td>0.213239</td>\n",
              "      <td>10.418845</td>\n",
              "      <td>1.670978e+07</td>\n",
              "      <td>618.251790</td>\n",
              "      <td>0.675352</td>\n",
              "      <td>0.005772</td>\n",
              "      <td>7.026210</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>0.000326</td>\n",
              "      <td>4240.096417</td>\n",
              "      <td>1.189118</td>\n",
              "      <td>0.064501</td>\n",
              "      <td>0.895047</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>165 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "0    51905.377962          66.288317  ...    0.020920  1.306338\n",
              "1    13432.502747          58.057539  ...    0.146173  0.253533\n",
              "2    25843.872675          52.918217  ...    0.152919  0.611772\n",
              "3    22152.709032          46.635312  ...    0.351327  0.564313\n",
              "4   119385.805617          92.436320  ...    0.132602  1.804351\n",
              "..            ...                ...  ...         ...       ...\n",
              "29    6592.266962          33.622119  ...    0.085026  0.359230\n",
              "30    3010.031479          29.286452  ...    0.295357  0.263438\n",
              "31    8683.195759          49.195074  ...    0.594629  0.392208\n",
              "32   25939.914844          57.067095  ...    0.059377  0.658985\n",
              "33   73401.852784          73.105484  ...    0.064501  0.895047\n",
              "\n",
              "[165 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 598
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNh5eL2PVIhD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tot_label = pd.concat([train_labels, test_labels], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcydh4qRVV6k",
        "colab_type": "code",
        "outputId": "81acb8aa-54ec-41da-e9df-07cf8e4b9899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tot_label.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(165,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 600
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cno5GmYCUynr",
        "colab_type": "text"
      },
      "source": [
        "##Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFVj6DbKUj9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFCCTjE6JrQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data, test_data, train_labels, test_labels = train_test_split(tot_data, tot_label,\n",
        "                                                  stratify=tot_label, test_size=0.25, random_state=1) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZF8nUPWbrsR",
        "colab_type": "code",
        "outputId": "4391b788-2f85-44e7-959a-de702a5bef0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_labels_dec.count(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 610
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "740dc4d5-be0b-401c-8e8a-c8c12a4400ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 613
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "573f41e6-53c0-408e-efe8-072057f74513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(123, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 615
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(5, activation='relu', input_shape=(9,)))\n",
        "\n",
        "#  model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.4, nesterov=True)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "38e90d93-7e20-410c-dcc2-6e7847885950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 622
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "2c7c6369-a1d7-4a7e-8f18-f978e0f70393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   2   3   4   5   6   9  10  11  15  17  18  19  20  21  23  24\n",
            "  25  26  27  29  30  31  32  33  34  35  38  39  40  41  45  47  49  50\n",
            "  52  53  54  56  57  58  59  61  62  63  68  69  70  71  72  75  76  78\n",
            "  80  83  85  86  87  88  91  92  95  97  98 100 101 102 104 105 106 107\n",
            " 109 110 112 114 115 116 117 119 120 122] TEST: [  7   8  12  13  14  16  22  28  36  37  42  43  44  46  48  51  55  60\n",
            "  64  65  66  67  73  74  77  79  81  82  84  89  90  93  94  96  99 103\n",
            " 108 111 113 118 121]\n",
            "TRAIN: [  0   1   2   7   8   9  11  12  13  14  16  17  19  21  22  26  28  29\n",
            "  31  32  33  34  35  36  37  41  42  43  44  46  48  49  51  52  54  55\n",
            "  57  58  60  61  64  65  66  67  69  70  71  72  73  74  75  76  77  79\n",
            "  81  82  84  87  88  89  90  92  93  94  96  97  99 100 101 102 103 105\n",
            " 106 108 111 113 115 117 118 120 121 122] TEST: [  3   4   5   6  10  15  18  20  23  24  25  27  30  38  39  40  45  47\n",
            "  50  53  56  59  62  63  68  78  80  83  85  86  91  95  98 104 107 109\n",
            " 110 112 114 116 119]\n",
            "TRAIN: [  3   4   5   6   7   8  10  12  13  14  15  16  18  20  22  23  24  25\n",
            "  27  28  30  36  37  38  39  40  42  43  44  45  46  47  48  50  51  53\n",
            "  55  56  59  60  62  63  64  65  66  67  68  73  74  77  78  79  80  81\n",
            "  82  83  84  85  86  89  90  91  93  94  95  96  98  99 103 104 107 108\n",
            " 109 110 111 112 113 114 116 118 119 121] TEST: [  0   1   2   9  11  17  19  21  26  29  31  32  33  34  35  41  49  52\n",
            "  54  57  58  61  69  70  71  72  75  76  87  88  92  97 100 101 102 105\n",
            " 106 115 117 120 122]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "b2d0c77f-6a1b-401f-92e2-cdd573310a9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 20\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 82 samples, validate on 41 samples\n",
            "Epoch 1/20\n",
            "82/82 [==============================] - 1s 16ms/step - loss: 1.8785 - acc: 0.3049 - val_loss: 2.0326 - val_acc: 0.2927\n",
            "Epoch 2/20\n",
            "82/82 [==============================] - 0s 227us/step - loss: 1.4185 - acc: 0.3780 - val_loss: 1.9050 - val_acc: 0.2927\n",
            "Epoch 3/20\n",
            "82/82 [==============================] - 0s 216us/step - loss: 1.3060 - acc: 0.3659 - val_loss: 1.7471 - val_acc: 0.2683\n",
            "Epoch 4/20\n",
            "82/82 [==============================] - 0s 217us/step - loss: 1.2100 - acc: 0.3780 - val_loss: 1.7048 - val_acc: 0.2927\n",
            "Epoch 5/20\n",
            "82/82 [==============================] - 0s 215us/step - loss: 1.1537 - acc: 0.3780 - val_loss: 1.6552 - val_acc: 0.2683\n",
            "Epoch 6/20\n",
            "82/82 [==============================] - 0s 229us/step - loss: 1.1029 - acc: 0.4268 - val_loss: 1.5657 - val_acc: 0.2927\n",
            "Epoch 7/20\n",
            "82/82 [==============================] - 0s 229us/step - loss: 1.0889 - acc: 0.3902 - val_loss: 1.5161 - val_acc: 0.3659\n",
            "Epoch 8/20\n",
            "82/82 [==============================] - 0s 226us/step - loss: 1.0545 - acc: 0.3780 - val_loss: 1.4573 - val_acc: 0.3902\n",
            "Epoch 9/20\n",
            "82/82 [==============================] - 0s 224us/step - loss: 1.0494 - acc: 0.4512 - val_loss: 1.4804 - val_acc: 0.3659\n",
            "Epoch 10/20\n",
            "82/82 [==============================] - 0s 208us/step - loss: 1.0347 - acc: 0.4634 - val_loss: 1.4181 - val_acc: 0.3902\n",
            "Epoch 11/20\n",
            "82/82 [==============================] - 0s 228us/step - loss: 1.0048 - acc: 0.4634 - val_loss: 1.3772 - val_acc: 0.4634\n",
            "Epoch 12/20\n",
            "82/82 [==============================] - 0s 247us/step - loss: 0.9872 - acc: 0.4634 - val_loss: 1.4292 - val_acc: 0.3902\n",
            "Epoch 13/20\n",
            "82/82 [==============================] - 0s 234us/step - loss: 1.0038 - acc: 0.4878 - val_loss: 1.3683 - val_acc: 0.3415\n",
            "Epoch 14/20\n",
            "82/82 [==============================] - 0s 219us/step - loss: 0.9803 - acc: 0.4512 - val_loss: 1.3438 - val_acc: 0.4878\n",
            "Epoch 15/20\n",
            "82/82 [==============================] - 0s 241us/step - loss: 0.9676 - acc: 0.4390 - val_loss: 1.3515 - val_acc: 0.4390\n",
            "Epoch 16/20\n",
            "82/82 [==============================] - 0s 211us/step - loss: 0.9494 - acc: 0.5000 - val_loss: 1.3937 - val_acc: 0.4146\n",
            "Epoch 17/20\n",
            "82/82 [==============================] - 0s 235us/step - loss: 0.9533 - acc: 0.4634 - val_loss: 1.3698 - val_acc: 0.4634\n",
            "Epoch 18/20\n",
            "82/82 [==============================] - 0s 222us/step - loss: 0.9502 - acc: 0.4756 - val_loss: 1.3537 - val_acc: 0.4634\n",
            "Epoch 19/20\n",
            "82/82 [==============================] - 0s 224us/step - loss: 0.9483 - acc: 0.4268 - val_loss: 1.3463 - val_acc: 0.5122\n",
            "Epoch 20/20\n",
            "82/82 [==============================] - 0s 214us/step - loss: 0.9285 - acc: 0.4512 - val_loss: 1.3499 - val_acc: 0.3902\n",
            "Train on 82 samples, validate on 41 samples\n",
            "Epoch 1/20\n",
            "82/82 [==============================] - 1s 16ms/step - loss: 1.9285 - acc: 0.3171 - val_loss: 1.8919 - val_acc: 0.2683\n",
            "Epoch 2/20\n",
            "82/82 [==============================] - 0s 310us/step - loss: 1.4239 - acc: 0.3659 - val_loss: 1.5353 - val_acc: 0.3171\n",
            "Epoch 3/20\n",
            "82/82 [==============================] - 0s 264us/step - loss: 1.2593 - acc: 0.4146 - val_loss: 1.3720 - val_acc: 0.3171\n",
            "Epoch 4/20\n",
            "82/82 [==============================] - 0s 269us/step - loss: 1.1633 - acc: 0.4146 - val_loss: 1.2818 - val_acc: 0.2927\n",
            "Epoch 5/20\n",
            "82/82 [==============================] - 0s 211us/step - loss: 1.0857 - acc: 0.4268 - val_loss: 1.1962 - val_acc: 0.3659\n",
            "Epoch 6/20\n",
            "82/82 [==============================] - 0s 218us/step - loss: 1.0378 - acc: 0.4634 - val_loss: 1.1773 - val_acc: 0.3415\n",
            "Epoch 7/20\n",
            "82/82 [==============================] - 0s 271us/step - loss: 1.0012 - acc: 0.4878 - val_loss: 1.1117 - val_acc: 0.4390\n",
            "Epoch 8/20\n",
            "82/82 [==============================] - 0s 236us/step - loss: 1.0010 - acc: 0.4878 - val_loss: 1.1076 - val_acc: 0.3902\n",
            "Epoch 9/20\n",
            "82/82 [==============================] - 0s 198us/step - loss: 0.9615 - acc: 0.4878 - val_loss: 1.0916 - val_acc: 0.3902\n",
            "Epoch 10/20\n",
            "82/82 [==============================] - 0s 245us/step - loss: 0.9484 - acc: 0.5000 - val_loss: 1.1190 - val_acc: 0.3659\n",
            "Epoch 11/20\n",
            "82/82 [==============================] - 0s 214us/step - loss: 0.9475 - acc: 0.4634 - val_loss: 1.1074 - val_acc: 0.3415\n",
            "Epoch 12/20\n",
            "82/82 [==============================] - 0s 222us/step - loss: 0.9418 - acc: 0.5122 - val_loss: 1.0919 - val_acc: 0.3902\n",
            "Epoch 13/20\n",
            "82/82 [==============================] - 0s 232us/step - loss: 0.9219 - acc: 0.5000 - val_loss: 1.2352 - val_acc: 0.4146\n",
            "Epoch 14/20\n",
            "82/82 [==============================] - 0s 206us/step - loss: 0.9469 - acc: 0.4878 - val_loss: 1.1073 - val_acc: 0.4390\n",
            "Epoch 15/20\n",
            "82/82 [==============================] - 0s 209us/step - loss: 0.9045 - acc: 0.5488 - val_loss: 1.0538 - val_acc: 0.4634\n",
            "Epoch 16/20\n",
            "82/82 [==============================] - 0s 205us/step - loss: 0.9063 - acc: 0.5366 - val_loss: 1.0991 - val_acc: 0.4390\n",
            "Epoch 17/20\n",
            "82/82 [==============================] - 0s 277us/step - loss: 0.9035 - acc: 0.5488 - val_loss: 1.0875 - val_acc: 0.3659\n",
            "Epoch 18/20\n",
            "82/82 [==============================] - 0s 250us/step - loss: 0.8961 - acc: 0.5244 - val_loss: 1.1059 - val_acc: 0.3902\n",
            "Epoch 19/20\n",
            "82/82 [==============================] - 0s 221us/step - loss: 0.8970 - acc: 0.5732 - val_loss: 1.0909 - val_acc: 0.4390\n",
            "Epoch 20/20\n",
            "82/82 [==============================] - 0s 226us/step - loss: 0.8874 - acc: 0.5488 - val_loss: 1.0606 - val_acc: 0.4878\n",
            "Train on 82 samples, validate on 41 samples\n",
            "Epoch 1/20\n",
            "82/82 [==============================] - 1s 16ms/step - loss: 1.7989 - acc: 0.4390 - val_loss: 1.2949 - val_acc: 0.4390\n",
            "Epoch 2/20\n",
            "82/82 [==============================] - 0s 227us/step - loss: 1.2018 - acc: 0.4390 - val_loss: 1.1743 - val_acc: 0.3171\n",
            "Epoch 3/20\n",
            "82/82 [==============================] - 0s 222us/step - loss: 1.0436 - acc: 0.4146 - val_loss: 1.1490 - val_acc: 0.2927\n",
            "Epoch 4/20\n",
            "82/82 [==============================] - 0s 222us/step - loss: 0.9887 - acc: 0.4756 - val_loss: 1.1318 - val_acc: 0.2683\n",
            "Epoch 5/20\n",
            "82/82 [==============================] - 0s 229us/step - loss: 0.9664 - acc: 0.4756 - val_loss: 1.1135 - val_acc: 0.2927\n",
            "Epoch 6/20\n",
            "82/82 [==============================] - 0s 239us/step - loss: 0.9493 - acc: 0.4878 - val_loss: 1.1029 - val_acc: 0.3171\n",
            "Epoch 7/20\n",
            "82/82 [==============================] - 0s 269us/step - loss: 0.9453 - acc: 0.5488 - val_loss: 1.0918 - val_acc: 0.2927\n",
            "Epoch 8/20\n",
            "82/82 [==============================] - 0s 241us/step - loss: 0.9328 - acc: 0.5366 - val_loss: 1.1179 - val_acc: 0.3171\n",
            "Epoch 9/20\n",
            "82/82 [==============================] - 0s 239us/step - loss: 0.9427 - acc: 0.5366 - val_loss: 1.1033 - val_acc: 0.3171\n",
            "Epoch 10/20\n",
            "82/82 [==============================] - 0s 448us/step - loss: 0.9221 - acc: 0.5244 - val_loss: 1.0891 - val_acc: 0.4146\n",
            "Epoch 11/20\n",
            "82/82 [==============================] - 0s 250us/step - loss: 0.9212 - acc: 0.5122 - val_loss: 1.0905 - val_acc: 0.4146\n",
            "Epoch 12/20\n",
            "82/82 [==============================] - 0s 242us/step - loss: 0.9150 - acc: 0.5732 - val_loss: 1.0725 - val_acc: 0.4146\n",
            "Epoch 13/20\n",
            "82/82 [==============================] - 0s 238us/step - loss: 0.9035 - acc: 0.5366 - val_loss: 1.0610 - val_acc: 0.4146\n",
            "Epoch 14/20\n",
            "82/82 [==============================] - 0s 268us/step - loss: 0.9025 - acc: 0.5610 - val_loss: 1.0711 - val_acc: 0.4390\n",
            "Epoch 15/20\n",
            "82/82 [==============================] - 0s 224us/step - loss: 0.8984 - acc: 0.5488 - val_loss: 1.0722 - val_acc: 0.4390\n",
            "Epoch 16/20\n",
            "82/82 [==============================] - 0s 246us/step - loss: 0.8889 - acc: 0.5976 - val_loss: 1.0599 - val_acc: 0.4390\n",
            "Epoch 17/20\n",
            "82/82 [==============================] - 0s 253us/step - loss: 0.8865 - acc: 0.5488 - val_loss: 1.0662 - val_acc: 0.4390\n",
            "Epoch 18/20\n",
            "82/82 [==============================] - 0s 295us/step - loss: 0.8789 - acc: 0.5732 - val_loss: 1.0764 - val_acc: 0.4146\n",
            "Epoch 19/20\n",
            "82/82 [==============================] - 0s 292us/step - loss: 0.8779 - acc: 0.6098 - val_loss: 1.0668 - val_acc: 0.4146\n",
            "Epoch 20/20\n",
            "82/82 [==============================] - 0s 283us/step - loss: 0.8760 - acc: 0.6098 - val_loss: 1.0834 - val_acc: 0.4146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "16efe915-08ab-486c-a46a-9b766fc7cc3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 629
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "6c28d393-c2b2-4ae5-d6f8-0803d61aa92a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 630
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "1ee50fe8-b82b-4ffd-b05f-6e9f7c5a3265",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 632
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "c670d366-77b7-4064-9204-6d92cd0161a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'bo', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'b', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f09186e9fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 635
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIhARyXH/HzPBpwPt+l04AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "7930bc4e-0159-48fb-e548-c3a2b8894cea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f091864d6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 636
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "d78b4ded-ac0e-4e08-c6d0-c92ce3f8ec3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "123/123 [==============================] - 1s 10ms/step - loss: 1.6491 - acc: 0.3415\n",
            "Epoch 2/20\n",
            "123/123 [==============================] - 0s 150us/step - loss: 1.2397 - acc: 0.3496\n",
            "Epoch 3/20\n",
            "123/123 [==============================] - 0s 151us/step - loss: 1.1087 - acc: 0.4228\n",
            "Epoch 4/20\n",
            "123/123 [==============================] - 0s 141us/step - loss: 1.0550 - acc: 0.4797\n",
            "Epoch 5/20\n",
            "123/123 [==============================] - 0s 145us/step - loss: 1.0193 - acc: 0.4797\n",
            "Epoch 6/20\n",
            "123/123 [==============================] - 0s 132us/step - loss: 1.0013 - acc: 0.5447\n",
            "Epoch 7/20\n",
            "123/123 [==============================] - 0s 148us/step - loss: 0.9880 - acc: 0.5285\n",
            "Epoch 8/20\n",
            "123/123 [==============================] - 0s 140us/step - loss: 0.9757 - acc: 0.5366\n",
            "Epoch 9/20\n",
            "123/123 [==============================] - 0s 155us/step - loss: 0.9667 - acc: 0.5528\n",
            "Epoch 10/20\n",
            "123/123 [==============================] - 0s 147us/step - loss: 0.9572 - acc: 0.5691\n",
            "Epoch 11/20\n",
            "123/123 [==============================] - 0s 141us/step - loss: 0.9505 - acc: 0.5528\n",
            "Epoch 12/20\n",
            "123/123 [==============================] - 0s 141us/step - loss: 0.9612 - acc: 0.5447\n",
            "Epoch 13/20\n",
            "123/123 [==============================] - 0s 145us/step - loss: 0.9479 - acc: 0.5447\n",
            "Epoch 14/20\n",
            "123/123 [==============================] - 0s 155us/step - loss: 0.9469 - acc: 0.5528\n",
            "Epoch 15/20\n",
            "123/123 [==============================] - 0s 153us/step - loss: 0.9411 - acc: 0.5528\n",
            "Epoch 16/20\n",
            "123/123 [==============================] - 0s 147us/step - loss: 0.9416 - acc: 0.5772\n",
            "Epoch 17/20\n",
            "123/123 [==============================] - 0s 143us/step - loss: 0.9432 - acc: 0.5528\n",
            "Epoch 18/20\n",
            "123/123 [==============================] - 0s 143us/step - loss: 0.9299 - acc: 0.5691\n",
            "Epoch 19/20\n",
            "123/123 [==============================] - 0s 186us/step - loss: 0.9332 - acc: 0.5691\n",
            "Epoch 20/20\n",
            "123/123 [==============================] - 0s 166us/step - loss: 0.9391 - acc: 0.5691\n",
            "42/42 [==============================] - 0s 11ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "1555c800-6ccb-4cc1-e8b7-683981e525af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 638
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "eb574915-972e-4851-d708-a27670aae02e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.40476190476190477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 639
        }
      ]
    }
  ]
}
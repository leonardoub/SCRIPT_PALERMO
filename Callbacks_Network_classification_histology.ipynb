{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Callbacks_Network_classification_histology.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/Callbacks_Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo87xY_fffSC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "780edf35-43ee-4d6e-d85f-d64e0d98598c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.9, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "974c6e63-4ddb-4872-8af6-029838a99d3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.9, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "45264e26-cfb9-4481-9fd6-a4f8d04015f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VVfbsnKS5CD1"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sDBPQTG05CEA",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s8BmvSBz5CER",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hc7nlRP95CEb",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r4G5Xx4c5CEk",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3naYmgFO5CEt",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(6, activation='relu', input_shape=(9,)))\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "  sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "6b217e7d-2c07-42fd-b96b-1612f669f484",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "20190403-d3de-4bd4-f4b1-034fe878a599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  0   1   4   5   8   9  11  12  14  15  16  17  19  20  22  23  24  25\n",
            "  27  29  30  33  34  36  37  38  39  40  41  42  44  45  46  48  51  52\n",
            "  53  56  57  58  59  60  62  63  65  66  67  69  72  76  77  78  79  80\n",
            "  81  83  84  85  87  88  89  90  92  96  97  98 100 101 102 103 104 105\n",
            " 107 109 110 111 113 115 117 120 121 122 124 125 127 128] TEST: [  2   3   6   7  10  13  18  21  26  28  31  32  35  43  47  49  50  54\n",
            "  55  61  64  68  70  71  73  74  75  82  86  91  93  94  95  99 106 108\n",
            " 112 114 116 118 119 123 126 129 130]\n",
            "TRAIN: [  2   3   5   6   7   8   9  10  11  12  13  18  20  21  25  26  27  28\n",
            "  29  30  31  32  34  35  36  38  39  43  44  45  46  47  48  49  50  53\n",
            "  54  55  57  58  61  63  64  65  66  68  70  71  73  74  75  76  78  82\n",
            "  84  85  86  87  90  91  92  93  94  95  96  99 100 101 102 105 106 108\n",
            " 109 111 112 114 115 116 118 119 122 123 124 125 126 127 129 130] TEST: [  0   1   4  14  15  16  17  19  22  23  24  33  37  40  41  42  51  52\n",
            "  56  59  60  62  67  69  72  77  79  80  81  83  88  89  97  98 103 104\n",
            " 107 110 113 117 120 121 128]\n",
            "TRAIN: [  0   1   2   3   4   6   7  10  13  14  15  16  17  18  19  21  22  23\n",
            "  24  26  28  31  32  33  35  37  40  41  42  43  47  49  50  51  52  54\n",
            "  55  56  59  60  61  62  64  67  68  69  70  71  72  73  74  75  77  79\n",
            "  80  81  82  83  86  88  89  91  93  94  95  97  98  99 103 104 106 107\n",
            " 108 110 112 113 114 116 117 118 119 120 121 123 126 128 129 130] TEST: [  5   8   9  11  12  20  25  27  29  30  34  36  38  39  44  45  46  48\n",
            "  53  57  58  63  65  66  76  78  84  85  87  90  92  96 100 101 102 105\n",
            " 109 111 115 122 124 125 127]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "48655906-da1e-4831-a2d6-31e5de41437f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clhXS8kJB591",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "a3f8f508-5b55-4e91-ba38-3f9b184700c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "  model = build_model()\n",
        "\n",
        "  callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.1, patience=10)]\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=10, callbacks=callbacks_list)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 86 samples, validate on 45 samples\n",
            "Epoch 1/500\n",
            "86/86 [==============================] - 0s 4ms/step - loss: 1.5914 - acc: 0.3953 - val_loss: 1.5316 - val_acc: 0.3778\n",
            "Epoch 2/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 1.5450 - acc: 0.3837 - val_loss: 1.4983 - val_acc: 0.3778\n",
            "Epoch 3/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 1.5056 - acc: 0.3953 - val_loss: 1.4711 - val_acc: 0.3778\n",
            "Epoch 4/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 1.4747 - acc: 0.4070 - val_loss: 1.4444 - val_acc: 0.3778\n",
            "Epoch 5/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 1.4420 - acc: 0.3953 - val_loss: 1.4220 - val_acc: 0.3556\n",
            "Epoch 6/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 1.4099 - acc: 0.4070 - val_loss: 1.4034 - val_acc: 0.3556\n",
            "Epoch 7/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 1.3854 - acc: 0.4070 - val_loss: 1.3859 - val_acc: 0.3556\n",
            "Epoch 8/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 1.3656 - acc: 0.4186 - val_loss: 1.3690 - val_acc: 0.3556\n",
            "Epoch 9/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 1.3432 - acc: 0.4186 - val_loss: 1.3540 - val_acc: 0.3556\n",
            "Epoch 10/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 1.3183 - acc: 0.4186 - val_loss: 1.3408 - val_acc: 0.3556\n",
            "Epoch 11/500\n",
            "86/86 [==============================] - 0s 191us/step - loss: 1.2999 - acc: 0.4302 - val_loss: 1.3262 - val_acc: 0.3556\n",
            "Epoch 12/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 1.2825 - acc: 0.4302 - val_loss: 1.3115 - val_acc: 0.3556\n",
            "Epoch 13/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 1.2637 - acc: 0.4419 - val_loss: 1.2987 - val_acc: 0.3556\n",
            "Epoch 14/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 1.2495 - acc: 0.4419 - val_loss: 1.2875 - val_acc: 0.3556\n",
            "Epoch 15/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 1.2342 - acc: 0.4419 - val_loss: 1.2763 - val_acc: 0.3556\n",
            "Epoch 16/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 1.2200 - acc: 0.4419 - val_loss: 1.2665 - val_acc: 0.3556\n",
            "Epoch 17/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 1.2072 - acc: 0.4419 - val_loss: 1.2563 - val_acc: 0.3556\n",
            "Epoch 18/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 1.1970 - acc: 0.4419 - val_loss: 1.2460 - val_acc: 0.3556\n",
            "Epoch 19/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 1.1848 - acc: 0.4302 - val_loss: 1.2373 - val_acc: 0.3556\n",
            "Epoch 20/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 1.1738 - acc: 0.4419 - val_loss: 1.2288 - val_acc: 0.3556\n",
            "Epoch 21/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 1.1618 - acc: 0.4302 - val_loss: 1.2209 - val_acc: 0.3556\n",
            "Epoch 22/500\n",
            "86/86 [==============================] - 0s 272us/step - loss: 1.1545 - acc: 0.4302 - val_loss: 1.2143 - val_acc: 0.3778\n",
            "Epoch 23/500\n",
            "86/86 [==============================] - 0s 353us/step - loss: 1.1459 - acc: 0.4302 - val_loss: 1.2084 - val_acc: 0.3778\n",
            "Epoch 24/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 1.1412 - acc: 0.4302 - val_loss: 1.2024 - val_acc: 0.4000\n",
            "Epoch 25/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 1.1316 - acc: 0.4302 - val_loss: 1.1966 - val_acc: 0.4000\n",
            "Epoch 26/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 1.1267 - acc: 0.4302 - val_loss: 1.1920 - val_acc: 0.4000\n",
            "Epoch 27/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 1.1209 - acc: 0.4419 - val_loss: 1.1878 - val_acc: 0.4000\n",
            "Epoch 28/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 1.1160 - acc: 0.4302 - val_loss: 1.1849 - val_acc: 0.4000\n",
            "Epoch 29/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 1.1092 - acc: 0.4535 - val_loss: 1.1810 - val_acc: 0.4000\n",
            "Epoch 30/500\n",
            "86/86 [==============================] - 0s 190us/step - loss: 1.1071 - acc: 0.4535 - val_loss: 1.1770 - val_acc: 0.4000\n",
            "Epoch 31/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 1.1013 - acc: 0.4419 - val_loss: 1.1730 - val_acc: 0.4000\n",
            "Epoch 32/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 1.0975 - acc: 0.4535 - val_loss: 1.1693 - val_acc: 0.4000\n",
            "Epoch 33/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 1.0925 - acc: 0.4535 - val_loss: 1.1655 - val_acc: 0.4000\n",
            "Epoch 34/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 1.0889 - acc: 0.4535 - val_loss: 1.1621 - val_acc: 0.4000\n",
            "Epoch 35/500\n",
            "86/86 [==============================] - 0s 267us/step - loss: 1.0854 - acc: 0.4419 - val_loss: 1.1591 - val_acc: 0.4222\n",
            "Epoch 36/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 1.0815 - acc: 0.4535 - val_loss: 1.1559 - val_acc: 0.4222\n",
            "Epoch 37/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 1.0795 - acc: 0.4651 - val_loss: 1.1527 - val_acc: 0.4222\n",
            "Epoch 38/500\n",
            "86/86 [==============================] - 0s 251us/step - loss: 1.0748 - acc: 0.4651 - val_loss: 1.1503 - val_acc: 0.4222\n",
            "Epoch 39/500\n",
            "86/86 [==============================] - 0s 199us/step - loss: 1.0749 - acc: 0.4767 - val_loss: 1.1480 - val_acc: 0.4222\n",
            "Epoch 40/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 1.0712 - acc: 0.5000 - val_loss: 1.1454 - val_acc: 0.4222\n",
            "Epoch 41/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 1.0648 - acc: 0.4884 - val_loss: 1.1432 - val_acc: 0.4444\n",
            "Epoch 42/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 1.0624 - acc: 0.5000 - val_loss: 1.1412 - val_acc: 0.4222\n",
            "Epoch 43/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 1.0624 - acc: 0.4884 - val_loss: 1.1390 - val_acc: 0.4444\n",
            "Epoch 44/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 1.0564 - acc: 0.4884 - val_loss: 1.1366 - val_acc: 0.4444\n",
            "Epoch 45/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 1.0555 - acc: 0.5000 - val_loss: 1.1342 - val_acc: 0.4444\n",
            "Epoch 46/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 1.0528 - acc: 0.5000 - val_loss: 1.1321 - val_acc: 0.4444\n",
            "Epoch 47/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 1.0496 - acc: 0.5000 - val_loss: 1.1303 - val_acc: 0.4222\n",
            "Epoch 48/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 1.0456 - acc: 0.4651 - val_loss: 1.1284 - val_acc: 0.4222\n",
            "Epoch 49/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 1.0436 - acc: 0.4767 - val_loss: 1.1260 - val_acc: 0.4222\n",
            "Epoch 50/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 1.0405 - acc: 0.4884 - val_loss: 1.1233 - val_acc: 0.4444\n",
            "Epoch 51/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 1.0378 - acc: 0.5116 - val_loss: 1.1202 - val_acc: 0.4667\n",
            "Epoch 52/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 1.0336 - acc: 0.5116 - val_loss: 1.1175 - val_acc: 0.4667\n",
            "Epoch 53/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 1.0332 - acc: 0.5116 - val_loss: 1.1148 - val_acc: 0.4667\n",
            "Epoch 54/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 1.0306 - acc: 0.5000 - val_loss: 1.1123 - val_acc: 0.4667\n",
            "Epoch 55/500\n",
            "86/86 [==============================] - 0s 193us/step - loss: 1.0281 - acc: 0.5116 - val_loss: 1.1106 - val_acc: 0.4667\n",
            "Epoch 56/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 1.0239 - acc: 0.4884 - val_loss: 1.1086 - val_acc: 0.4667\n",
            "Epoch 57/500\n",
            "86/86 [==============================] - 0s 251us/step - loss: 1.0245 - acc: 0.5116 - val_loss: 1.1060 - val_acc: 0.4667\n",
            "Epoch 58/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 1.0196 - acc: 0.5116 - val_loss: 1.1040 - val_acc: 0.4667\n",
            "Epoch 59/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 1.0174 - acc: 0.5116 - val_loss: 1.1020 - val_acc: 0.4889\n",
            "Epoch 60/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 1.0161 - acc: 0.5116 - val_loss: 1.0998 - val_acc: 0.4667\n",
            "Epoch 61/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 1.0138 - acc: 0.5116 - val_loss: 1.0976 - val_acc: 0.4889\n",
            "Epoch 62/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 1.0121 - acc: 0.5000 - val_loss: 1.0957 - val_acc: 0.4667\n",
            "Epoch 63/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 1.0085 - acc: 0.5000 - val_loss: 1.0940 - val_acc: 0.4667\n",
            "Epoch 64/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 1.0070 - acc: 0.5000 - val_loss: 1.0923 - val_acc: 0.4667\n",
            "Epoch 65/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 1.0061 - acc: 0.5000 - val_loss: 1.0907 - val_acc: 0.4667\n",
            "Epoch 66/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 1.0058 - acc: 0.5000 - val_loss: 1.0890 - val_acc: 0.4889\n",
            "Epoch 67/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.9995 - acc: 0.5000 - val_loss: 1.0875 - val_acc: 0.4667\n",
            "Epoch 68/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.9997 - acc: 0.4884 - val_loss: 1.0860 - val_acc: 0.4667\n",
            "Epoch 69/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.9966 - acc: 0.5116 - val_loss: 1.0844 - val_acc: 0.4667\n",
            "Epoch 70/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.9948 - acc: 0.5116 - val_loss: 1.0830 - val_acc: 0.4667\n",
            "Epoch 71/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.9932 - acc: 0.5000 - val_loss: 1.0813 - val_acc: 0.4667\n",
            "Epoch 72/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.9920 - acc: 0.5116 - val_loss: 1.0798 - val_acc: 0.4667\n",
            "Epoch 73/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.9880 - acc: 0.5233 - val_loss: 1.0784 - val_acc: 0.4667\n",
            "Epoch 74/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.9851 - acc: 0.5116 - val_loss: 1.0774 - val_acc: 0.4667\n",
            "Epoch 75/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.9833 - acc: 0.5116 - val_loss: 1.0760 - val_acc: 0.4667\n",
            "Epoch 76/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.9816 - acc: 0.5233 - val_loss: 1.0746 - val_acc: 0.4667\n",
            "Epoch 77/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.9789 - acc: 0.5233 - val_loss: 1.0729 - val_acc: 0.4667\n",
            "Epoch 78/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.9785 - acc: 0.5233 - val_loss: 1.0714 - val_acc: 0.4667\n",
            "Epoch 79/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.9760 - acc: 0.5233 - val_loss: 1.0701 - val_acc: 0.4667\n",
            "Epoch 80/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 0.9765 - acc: 0.5349 - val_loss: 1.0691 - val_acc: 0.4667\n",
            "Epoch 81/500\n",
            "86/86 [==============================] - 0s 256us/step - loss: 0.9733 - acc: 0.5233 - val_loss: 1.0681 - val_acc: 0.4667\n",
            "Epoch 82/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.9719 - acc: 0.5349 - val_loss: 1.0670 - val_acc: 0.4667\n",
            "Epoch 83/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.9691 - acc: 0.5349 - val_loss: 1.0660 - val_acc: 0.4889\n",
            "Epoch 84/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.9676 - acc: 0.5465 - val_loss: 1.0651 - val_acc: 0.4889\n",
            "Epoch 85/500\n",
            "86/86 [==============================] - 0s 266us/step - loss: 0.9656 - acc: 0.5349 - val_loss: 1.0643 - val_acc: 0.4889\n",
            "Epoch 86/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.9667 - acc: 0.5465 - val_loss: 1.0633 - val_acc: 0.4889\n",
            "Epoch 87/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.9656 - acc: 0.5465 - val_loss: 1.0623 - val_acc: 0.4889\n",
            "Epoch 88/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.9628 - acc: 0.5349 - val_loss: 1.0614 - val_acc: 0.4889\n",
            "Epoch 89/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.9624 - acc: 0.5465 - val_loss: 1.0601 - val_acc: 0.4889\n",
            "Epoch 90/500\n",
            "86/86 [==============================] - 0s 255us/step - loss: 0.9627 - acc: 0.5349 - val_loss: 1.0591 - val_acc: 0.4889\n",
            "Epoch 91/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.9592 - acc: 0.5465 - val_loss: 1.0585 - val_acc: 0.4889\n",
            "Epoch 92/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.9607 - acc: 0.5349 - val_loss: 1.0574 - val_acc: 0.4889\n",
            "Epoch 93/500\n",
            "86/86 [==============================] - 0s 251us/step - loss: 0.9575 - acc: 0.5465 - val_loss: 1.0563 - val_acc: 0.4889\n",
            "Epoch 94/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.9543 - acc: 0.5465 - val_loss: 1.0554 - val_acc: 0.4889\n",
            "Epoch 95/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.9535 - acc: 0.5465 - val_loss: 1.0545 - val_acc: 0.4889\n",
            "Epoch 96/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.9528 - acc: 0.5465 - val_loss: 1.0537 - val_acc: 0.4889\n",
            "Epoch 97/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.9513 - acc: 0.5465 - val_loss: 1.0526 - val_acc: 0.4889\n",
            "Epoch 98/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.9509 - acc: 0.5465 - val_loss: 1.0515 - val_acc: 0.5111\n",
            "Epoch 99/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.9491 - acc: 0.5465 - val_loss: 1.0510 - val_acc: 0.5111\n",
            "Epoch 100/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.9471 - acc: 0.5465 - val_loss: 1.0502 - val_acc: 0.4889\n",
            "Epoch 101/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.9499 - acc: 0.5233 - val_loss: 1.0494 - val_acc: 0.5111\n",
            "Epoch 102/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.9465 - acc: 0.5465 - val_loss: 1.0486 - val_acc: 0.5111\n",
            "Epoch 103/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.9439 - acc: 0.5465 - val_loss: 1.0481 - val_acc: 0.5111\n",
            "Epoch 104/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.9443 - acc: 0.5465 - val_loss: 1.0474 - val_acc: 0.5111\n",
            "Epoch 105/500\n",
            "86/86 [==============================] - 0s 194us/step - loss: 0.9434 - acc: 0.5465 - val_loss: 1.0467 - val_acc: 0.5111\n",
            "Epoch 106/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.9426 - acc: 0.5465 - val_loss: 1.0459 - val_acc: 0.5111\n",
            "Epoch 107/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.9442 - acc: 0.5349 - val_loss: 1.0456 - val_acc: 0.5333\n",
            "Epoch 108/500\n",
            "86/86 [==============================] - 0s 307us/step - loss: 0.9395 - acc: 0.5465 - val_loss: 1.0450 - val_acc: 0.5333\n",
            "Epoch 109/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.9391 - acc: 0.5349 - val_loss: 1.0445 - val_acc: 0.5333\n",
            "Epoch 110/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.9401 - acc: 0.5465 - val_loss: 1.0443 - val_acc: 0.5333\n",
            "Epoch 111/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.9366 - acc: 0.5465 - val_loss: 1.0439 - val_acc: 0.5333\n",
            "Epoch 112/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.9357 - acc: 0.5581 - val_loss: 1.0435 - val_acc: 0.5111\n",
            "Epoch 113/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.9355 - acc: 0.5581 - val_loss: 1.0431 - val_acc: 0.5111\n",
            "Epoch 114/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.9351 - acc: 0.5465 - val_loss: 1.0426 - val_acc: 0.5111\n",
            "Epoch 115/500\n",
            "86/86 [==============================] - 0s 254us/step - loss: 0.9333 - acc: 0.5581 - val_loss: 1.0421 - val_acc: 0.5111\n",
            "Epoch 116/500\n",
            "86/86 [==============================] - 0s 276us/step - loss: 0.9323 - acc: 0.5581 - val_loss: 1.0418 - val_acc: 0.5333\n",
            "Epoch 117/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.9323 - acc: 0.5465 - val_loss: 1.0413 - val_acc: 0.5333\n",
            "Epoch 118/500\n",
            "86/86 [==============================] - 0s 265us/step - loss: 0.9318 - acc: 0.5581 - val_loss: 1.0408 - val_acc: 0.5333\n",
            "Epoch 119/500\n",
            "86/86 [==============================] - 0s 261us/step - loss: 0.9304 - acc: 0.5698 - val_loss: 1.0404 - val_acc: 0.5333\n",
            "Epoch 120/500\n",
            "86/86 [==============================] - 0s 265us/step - loss: 0.9299 - acc: 0.5814 - val_loss: 1.0401 - val_acc: 0.5333\n",
            "Epoch 121/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.9298 - acc: 0.5581 - val_loss: 1.0397 - val_acc: 0.5333\n",
            "Epoch 122/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.9285 - acc: 0.5814 - val_loss: 1.0390 - val_acc: 0.5333\n",
            "Epoch 123/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.9279 - acc: 0.5814 - val_loss: 1.0386 - val_acc: 0.5333\n",
            "Epoch 124/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.9269 - acc: 0.5814 - val_loss: 1.0386 - val_acc: 0.5333\n",
            "Epoch 125/500\n",
            "86/86 [==============================] - 0s 271us/step - loss: 0.9258 - acc: 0.5814 - val_loss: 1.0382 - val_acc: 0.5333\n",
            "Epoch 126/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.9256 - acc: 0.5698 - val_loss: 1.0379 - val_acc: 0.5333\n",
            "Epoch 127/500\n",
            "86/86 [==============================] - 0s 321us/step - loss: 0.9245 - acc: 0.5814 - val_loss: 1.0374 - val_acc: 0.5333\n",
            "Epoch 128/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.9235 - acc: 0.5814 - val_loss: 1.0368 - val_acc: 0.5333\n",
            "Epoch 129/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.9249 - acc: 0.5698 - val_loss: 1.0369 - val_acc: 0.5333\n",
            "Epoch 130/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.9221 - acc: 0.5814 - val_loss: 1.0369 - val_acc: 0.5333\n",
            "Epoch 131/500\n",
            "86/86 [==============================] - 0s 269us/step - loss: 0.9219 - acc: 0.5814 - val_loss: 1.0364 - val_acc: 0.5333\n",
            "Epoch 132/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.9209 - acc: 0.5814 - val_loss: 1.0359 - val_acc: 0.5333\n",
            "Epoch 133/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.9201 - acc: 0.5930 - val_loss: 1.0355 - val_acc: 0.5333\n",
            "Epoch 134/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.9189 - acc: 0.5930 - val_loss: 1.0353 - val_acc: 0.5333\n",
            "Epoch 135/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.9188 - acc: 0.5930 - val_loss: 1.0351 - val_acc: 0.5333\n",
            "Epoch 136/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.9178 - acc: 0.5930 - val_loss: 1.0347 - val_acc: 0.5333\n",
            "Epoch 137/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.9185 - acc: 0.5930 - val_loss: 1.0345 - val_acc: 0.5333\n",
            "Epoch 138/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.9167 - acc: 0.5930 - val_loss: 1.0345 - val_acc: 0.5333\n",
            "Epoch 139/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.9160 - acc: 0.5930 - val_loss: 1.0345 - val_acc: 0.5333\n",
            "Epoch 140/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.9160 - acc: 0.5930 - val_loss: 1.0344 - val_acc: 0.5333\n",
            "Epoch 141/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.9153 - acc: 0.5814 - val_loss: 1.0341 - val_acc: 0.5333\n",
            "Epoch 142/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.9139 - acc: 0.6047 - val_loss: 1.0336 - val_acc: 0.5333\n",
            "Epoch 143/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.9134 - acc: 0.6047 - val_loss: 1.0335 - val_acc: 0.5333\n",
            "Epoch 144/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.9128 - acc: 0.5930 - val_loss: 1.0333 - val_acc: 0.5333\n",
            "Epoch 145/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.9126 - acc: 0.5814 - val_loss: 1.0331 - val_acc: 0.5333\n",
            "Epoch 146/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.9133 - acc: 0.5930 - val_loss: 1.0330 - val_acc: 0.5333\n",
            "Epoch 147/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.9122 - acc: 0.5930 - val_loss: 1.0329 - val_acc: 0.5333\n",
            "Epoch 148/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.9101 - acc: 0.5930 - val_loss: 1.0330 - val_acc: 0.5111\n",
            "Epoch 149/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.9096 - acc: 0.5930 - val_loss: 1.0330 - val_acc: 0.5111\n",
            "Epoch 150/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.9085 - acc: 0.5814 - val_loss: 1.0328 - val_acc: 0.5111\n",
            "Epoch 151/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.9114 - acc: 0.5698 - val_loss: 1.0328 - val_acc: 0.5111\n",
            "Epoch 152/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.9078 - acc: 0.5930 - val_loss: 1.0328 - val_acc: 0.5111\n",
            "Epoch 153/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.9069 - acc: 0.5814 - val_loss: 1.0328 - val_acc: 0.5111\n",
            "Epoch 154/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.9068 - acc: 0.5814 - val_loss: 1.0327 - val_acc: 0.5111\n",
            "Epoch 155/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.9067 - acc: 0.5814 - val_loss: 1.0327 - val_acc: 0.5111\n",
            "Epoch 156/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.9055 - acc: 0.5814 - val_loss: 1.0330 - val_acc: 0.5111\n",
            "Epoch 157/500\n",
            "86/86 [==============================] - 0s 190us/step - loss: 0.9051 - acc: 0.6163 - val_loss: 1.0329 - val_acc: 0.5111\n",
            "Epoch 158/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 0.9061 - acc: 0.6047 - val_loss: 1.0325 - val_acc: 0.5111\n",
            "Epoch 159/500\n",
            "86/86 [==============================] - 0s 191us/step - loss: 0.9040 - acc: 0.6047 - val_loss: 1.0321 - val_acc: 0.5111\n",
            "Epoch 160/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.9022 - acc: 0.5930 - val_loss: 1.0317 - val_acc: 0.5111\n",
            "Epoch 161/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.9034 - acc: 0.6163 - val_loss: 1.0313 - val_acc: 0.5111\n",
            "Epoch 162/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.9011 - acc: 0.6047 - val_loss: 1.0311 - val_acc: 0.5111\n",
            "Epoch 163/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.9017 - acc: 0.6047 - val_loss: 1.0312 - val_acc: 0.5111\n",
            "Epoch 164/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.9016 - acc: 0.6047 - val_loss: 1.0314 - val_acc: 0.5111\n",
            "Epoch 165/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.9002 - acc: 0.6047 - val_loss: 1.0314 - val_acc: 0.5111\n",
            "Epoch 166/500\n",
            "86/86 [==============================] - 0s 261us/step - loss: 0.8994 - acc: 0.6047 - val_loss: 1.0314 - val_acc: 0.5111\n",
            "Epoch 167/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8989 - acc: 0.6047 - val_loss: 1.0313 - val_acc: 0.5111\n",
            "Epoch 168/500\n",
            "86/86 [==============================] - 0s 264us/step - loss: 0.8983 - acc: 0.6047 - val_loss: 1.0312 - val_acc: 0.5111\n",
            "Epoch 169/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8979 - acc: 0.6047 - val_loss: 1.0315 - val_acc: 0.5111\n",
            "Epoch 170/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8980 - acc: 0.6047 - val_loss: 1.0316 - val_acc: 0.5111\n",
            "Epoch 171/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.8994 - acc: 0.6047 - val_loss: 1.0312 - val_acc: 0.5111\n",
            "Epoch 172/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8972 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 173/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8963 - acc: 0.6047 - val_loss: 1.0310 - val_acc: 0.5111\n",
            "Epoch 174/500\n",
            "86/86 [==============================] - 0s 199us/step - loss: 0.8951 - acc: 0.6047 - val_loss: 1.0313 - val_acc: 0.5111\n",
            "Epoch 175/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8962 - acc: 0.6047 - val_loss: 1.0314 - val_acc: 0.5111\n",
            "Epoch 176/500\n",
            "86/86 [==============================] - 0s 190us/step - loss: 0.8952 - acc: 0.6047 - val_loss: 1.0313 - val_acc: 0.5111\n",
            "Epoch 177/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8950 - acc: 0.6047 - val_loss: 1.0312 - val_acc: 0.5111\n",
            "Epoch 178/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8933 - acc: 0.6047 - val_loss: 1.0313 - val_acc: 0.5111\n",
            "Epoch 179/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8938 - acc: 0.6047 - val_loss: 1.0313 - val_acc: 0.5111\n",
            "Epoch 180/500\n",
            "86/86 [==============================] - 0s 275us/step - loss: 0.8927 - acc: 0.6047 - val_loss: 1.0312 - val_acc: 0.5111\n",
            "Epoch 181/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8916 - acc: 0.6047 - val_loss: 1.0311 - val_acc: 0.5111\n",
            "Epoch 182/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8932 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 183/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.8889 - acc: 0.6047 - val_loss: 1.0307 - val_acc: 0.5111\n",
            "Epoch 184/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8888 - acc: 0.6047 - val_loss: 1.0307 - val_acc: 0.5111\n",
            "Epoch 185/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8887 - acc: 0.6047 - val_loss: 1.0307 - val_acc: 0.5111\n",
            "Epoch 186/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8887 - acc: 0.6047 - val_loss: 1.0307 - val_acc: 0.5111\n",
            "Epoch 187/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8886 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 188/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8888 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 189/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8888 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 190/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8885 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 191/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8885 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 192/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.8884 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 193/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.8883 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 194/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8881 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 195/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8881 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 196/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8881 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 197/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8881 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 198/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8881 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 199/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8881 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 200/500\n",
            "86/86 [==============================] - 0s 238us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 201/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 202/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 203/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 204/500\n",
            "86/86 [==============================] - 0s 186us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 205/500\n",
            "86/86 [==============================] - 0s 251us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 206/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 207/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 208/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 209/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 210/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 211/500\n",
            "86/86 [==============================] - 0s 266us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 212/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 213/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 214/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 215/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 216/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 217/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 218/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 219/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 220/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 221/500\n",
            "86/86 [==============================] - 0s 310us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 222/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 223/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 224/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 225/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 226/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 227/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 228/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 229/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 230/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 231/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 232/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 233/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 234/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 235/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 236/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 237/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 238/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 239/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 240/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 241/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 242/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 243/500\n",
            "86/86 [==============================] - 0s 261us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 244/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 245/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 246/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 247/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 248/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 249/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 250/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 251/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 252/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 253/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 254/500\n",
            "86/86 [==============================] - 0s 187us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 255/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 256/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 257/500\n",
            "86/86 [==============================] - 0s 190us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 258/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 259/500\n",
            "86/86 [==============================] - 0s 188us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 260/500\n",
            "86/86 [==============================] - 0s 197us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 261/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 262/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 263/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 264/500\n",
            "86/86 [==============================] - 0s 190us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 265/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 266/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 267/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 268/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 269/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 270/500\n",
            "86/86 [==============================] - 0s 189us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 271/500\n",
            "86/86 [==============================] - 0s 259us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 272/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 273/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 274/500\n",
            "86/86 [==============================] - 0s 188us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 275/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 276/500\n",
            "86/86 [==============================] - 0s 189us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 277/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 278/500\n",
            "86/86 [==============================] - 0s 194us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 279/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 280/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 281/500\n",
            "86/86 [==============================] - 0s 268us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 282/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 283/500\n",
            "86/86 [==============================] - 0s 244us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 284/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 285/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 286/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 287/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 288/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 289/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 290/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 291/500\n",
            "86/86 [==============================] - 0s 191us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 292/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 293/500\n",
            "86/86 [==============================] - 0s 269us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 294/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 295/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 296/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 297/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 298/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 299/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 300/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 301/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 302/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 303/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 304/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 305/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 306/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 307/500\n",
            "86/86 [==============================] - 0s 186us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 308/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 309/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 310/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 311/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 312/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 313/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 314/500\n",
            "86/86 [==============================] - 0s 188us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 315/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 316/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 317/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 318/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 319/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 320/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 321/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 322/500\n",
            "86/86 [==============================] - 0s 266us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 323/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 324/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 325/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 326/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 327/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 328/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 329/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 330/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 331/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 332/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 333/500\n",
            "86/86 [==============================] - 0s 232us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 334/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 335/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 336/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 337/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 338/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 339/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 340/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 341/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 342/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 343/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 344/500\n",
            "86/86 [==============================] - 0s 250us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 345/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 346/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 347/500\n",
            "86/86 [==============================] - 0s 255us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 348/500\n",
            "86/86 [==============================] - 0s 273us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 349/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 350/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 351/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 352/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 353/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 354/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 355/500\n",
            "86/86 [==============================] - 0s 357us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 356/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 357/500\n",
            "86/86 [==============================] - 0s 247us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 358/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 359/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 360/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 361/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 362/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 363/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 364/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 365/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 366/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 367/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 368/500\n",
            "86/86 [==============================] - 0s 268us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 369/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 370/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 371/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 372/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 373/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 374/500\n",
            "86/86 [==============================] - 0s 189us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 375/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 376/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 377/500\n",
            "86/86 [==============================] - 0s 260us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 378/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 379/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 380/500\n",
            "86/86 [==============================] - 0s 187us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 381/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 382/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 383/500\n",
            "86/86 [==============================] - 0s 193us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 384/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 385/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 386/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 387/500\n",
            "86/86 [==============================] - 0s 246us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 388/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 389/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 390/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 391/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 392/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 393/500\n",
            "86/86 [==============================] - 0s 254us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 394/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 395/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 396/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 397/500\n",
            "86/86 [==============================] - 0s 230us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 398/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 399/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 400/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 401/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 402/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 403/500\n",
            "86/86 [==============================] - 0s 257us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 404/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 405/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 406/500\n",
            "86/86 [==============================] - 0s 194us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 407/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 408/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 409/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 410/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 411/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 412/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 413/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 414/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 415/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 416/500\n",
            "86/86 [==============================] - 0s 201us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 417/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 418/500\n",
            "86/86 [==============================] - 0s 200us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 419/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 420/500\n",
            "86/86 [==============================] - 0s 209us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 421/500\n",
            "86/86 [==============================] - 0s 206us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 422/500\n",
            "86/86 [==============================] - 0s 192us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 423/500\n",
            "86/86 [==============================] - 0s 195us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 424/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 425/500\n",
            "86/86 [==============================] - 0s 215us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 426/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 427/500\n",
            "86/86 [==============================] - 0s 275us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 428/500\n",
            "86/86 [==============================] - 0s 216us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 429/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 430/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 431/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 432/500\n",
            "86/86 [==============================] - 0s 224us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 433/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 434/500\n",
            "86/86 [==============================] - 0s 193us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 435/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 436/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 437/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 438/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 439/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 440/500\n",
            "86/86 [==============================] - 0s 263us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 441/500\n",
            "86/86 [==============================] - 0s 203us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 442/500\n",
            "86/86 [==============================] - 0s 239us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 443/500\n",
            "86/86 [==============================] - 0s 227us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 444/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 445/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 446/500\n",
            "86/86 [==============================] - 0s 242us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 447/500\n",
            "86/86 [==============================] - 0s 205us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 448/500\n",
            "86/86 [==============================] - 0s 256us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 449/500\n",
            "86/86 [==============================] - 0s 236us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 450/500\n",
            "86/86 [==============================] - 0s 241us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 451/500\n",
            "86/86 [==============================] - 0s 197us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 452/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 453/500\n",
            "86/86 [==============================] - 0s 248us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 454/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 455/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 456/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 457/500\n",
            "86/86 [==============================] - 0s 234us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 458/500\n",
            "86/86 [==============================] - 0s 210us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 459/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 460/500\n",
            "86/86 [==============================] - 0s 220us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 461/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 462/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 463/500\n",
            "86/86 [==============================] - 0s 223us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 464/500\n",
            "86/86 [==============================] - 0s 233us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 465/500\n",
            "86/86 [==============================] - 0s 283us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 466/500\n",
            "86/86 [==============================] - 0s 229us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 467/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 468/500\n",
            "86/86 [==============================] - 0s 196us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 469/500\n",
            "86/86 [==============================] - 0s 222us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 470/500\n",
            "86/86 [==============================] - 0s 213us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 471/500\n",
            "86/86 [==============================] - 0s 198us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 472/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 473/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 474/500\n",
            "86/86 [==============================] - 0s 207us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 475/500\n",
            "86/86 [==============================] - 0s 218us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 476/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 477/500\n",
            "86/86 [==============================] - 0s 231us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 478/500\n",
            "86/86 [==============================] - 0s 208us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 479/500\n",
            "86/86 [==============================] - 0s 226us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 480/500\n",
            "86/86 [==============================] - 0s 212us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 481/500\n",
            "86/86 [==============================] - 0s 235us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 482/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 483/500\n",
            "86/86 [==============================] - 0s 243us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 484/500\n",
            "86/86 [==============================] - 0s 201us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 485/500\n",
            "86/86 [==============================] - 0s 202us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 486/500\n",
            "86/86 [==============================] - 0s 225us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 487/500\n",
            "86/86 [==============================] - 0s 211us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 488/500\n",
            "86/86 [==============================] - 0s 240us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 489/500\n",
            "86/86 [==============================] - 0s 253us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 490/500\n",
            "86/86 [==============================] - 0s 317us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 491/500\n",
            "86/86 [==============================] - 0s 214us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 492/500\n",
            "86/86 [==============================] - 0s 221us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 493/500\n",
            "86/86 [==============================] - 0s 219us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 494/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 495/500\n",
            "86/86 [==============================] - 0s 237us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 496/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 497/500\n",
            "86/86 [==============================] - 0s 228us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 498/500\n",
            "86/86 [==============================] - 0s 217us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 499/500\n",
            "86/86 [==============================] - 0s 204us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Epoch 500/500\n",
            "86/86 [==============================] - 0s 245us/step - loss: 0.8880 - acc: 0.6047 - val_loss: 1.0308 - val_acc: 0.5111\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/500\n",
            "88/88 [==============================] - 0s 5ms/step - loss: 2.0457 - acc: 0.2386 - val_loss: 1.5094 - val_acc: 0.3953\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.9448 - acc: 0.2614 - val_loss: 1.4687 - val_acc: 0.3953\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.8642 - acc: 0.3068 - val_loss: 1.4351 - val_acc: 0.3721\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.7988 - acc: 0.3523 - val_loss: 1.4062 - val_acc: 0.3721\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 1.7457 - acc: 0.3750 - val_loss: 1.3826 - val_acc: 0.4186\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.7016 - acc: 0.4205 - val_loss: 1.3622 - val_acc: 0.4186\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.6639 - acc: 0.4318 - val_loss: 1.3442 - val_acc: 0.4651\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.6305 - acc: 0.4432 - val_loss: 1.3288 - val_acc: 0.4419\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.6006 - acc: 0.4773 - val_loss: 1.3135 - val_acc: 0.4186\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 1.5761 - acc: 0.4886 - val_loss: 1.2992 - val_acc: 0.4186\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.5550 - acc: 0.4886 - val_loss: 1.2879 - val_acc: 0.4419\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.5357 - acc: 0.4886 - val_loss: 1.2780 - val_acc: 0.4186\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.5183 - acc: 0.5341 - val_loss: 1.2689 - val_acc: 0.4419\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.5040 - acc: 0.5341 - val_loss: 1.2601 - val_acc: 0.4419\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.4874 - acc: 0.5455 - val_loss: 1.2511 - val_acc: 0.4419\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 1.4720 - acc: 0.5682 - val_loss: 1.2432 - val_acc: 0.4419\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.4602 - acc: 0.5682 - val_loss: 1.2363 - val_acc: 0.4651\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.4489 - acc: 0.5682 - val_loss: 1.2302 - val_acc: 0.4651\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.4364 - acc: 0.5682 - val_loss: 1.2239 - val_acc: 0.4651\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.4275 - acc: 0.5795 - val_loss: 1.2195 - val_acc: 0.4651\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 1.4169 - acc: 0.5795 - val_loss: 1.2142 - val_acc: 0.4651\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.4097 - acc: 0.5682 - val_loss: 1.2106 - val_acc: 0.4884\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 0s 353us/step - loss: 1.4003 - acc: 0.5795 - val_loss: 1.2058 - val_acc: 0.4884\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 0s 303us/step - loss: 1.3931 - acc: 0.5795 - val_loss: 1.2022 - val_acc: 0.4884\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.3873 - acc: 0.5795 - val_loss: 1.1987 - val_acc: 0.4884\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.3813 - acc: 0.5795 - val_loss: 1.1955 - val_acc: 0.4884\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 0s 264us/step - loss: 1.3761 - acc: 0.5795 - val_loss: 1.1917 - val_acc: 0.4884\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.3691 - acc: 0.5682 - val_loss: 1.1879 - val_acc: 0.4884\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.3647 - acc: 0.5682 - val_loss: 1.1846 - val_acc: 0.4884\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.3607 - acc: 0.5682 - val_loss: 1.1822 - val_acc: 0.4884\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.3548 - acc: 0.5682 - val_loss: 1.1794 - val_acc: 0.4884\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.3509 - acc: 0.5568 - val_loss: 1.1765 - val_acc: 0.4884\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 1.3461 - acc: 0.5568 - val_loss: 1.1739 - val_acc: 0.4884\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.3421 - acc: 0.5682 - val_loss: 1.1715 - val_acc: 0.4884\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.3387 - acc: 0.5568 - val_loss: 1.1701 - val_acc: 0.4884\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 1.3353 - acc: 0.5682 - val_loss: 1.1690 - val_acc: 0.4651\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.3309 - acc: 0.5682 - val_loss: 1.1678 - val_acc: 0.4651\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 0s 265us/step - loss: 1.3263 - acc: 0.5682 - val_loss: 1.1675 - val_acc: 0.4419\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.3212 - acc: 0.5682 - val_loss: 1.1655 - val_acc: 0.4419\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.3158 - acc: 0.5795 - val_loss: 1.1646 - val_acc: 0.4419\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.3063 - acc: 0.5682 - val_loss: 1.1632 - val_acc: 0.4419\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.2998 - acc: 0.5682 - val_loss: 1.1618 - val_acc: 0.4419\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.2921 - acc: 0.5682 - val_loss: 1.1613 - val_acc: 0.4419\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 1.2843 - acc: 0.5682 - val_loss: 1.1603 - val_acc: 0.4419\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.2792 - acc: 0.5682 - val_loss: 1.1597 - val_acc: 0.4186\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 1.2723 - acc: 0.5682 - val_loss: 1.1585 - val_acc: 0.4186\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.2659 - acc: 0.5682 - val_loss: 1.1569 - val_acc: 0.4186\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.2604 - acc: 0.5795 - val_loss: 1.1553 - val_acc: 0.4186\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.2553 - acc: 0.5795 - val_loss: 1.1538 - val_acc: 0.4186\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.2488 - acc: 0.5795 - val_loss: 1.1519 - val_acc: 0.4419\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.2449 - acc: 0.5795 - val_loss: 1.1505 - val_acc: 0.4419\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 1.2382 - acc: 0.5795 - val_loss: 1.1492 - val_acc: 0.4419\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.2324 - acc: 0.5795 - val_loss: 1.1487 - val_acc: 0.4419\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.2275 - acc: 0.5795 - val_loss: 1.1475 - val_acc: 0.4419\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.2235 - acc: 0.5909 - val_loss: 1.1459 - val_acc: 0.4419\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.2133 - acc: 0.6023 - val_loss: 1.1473 - val_acc: 0.4419\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 1.1985 - acc: 0.6023 - val_loss: 1.1484 - val_acc: 0.4419\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.1844 - acc: 0.6136 - val_loss: 1.1501 - val_acc: 0.4419\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.1723 - acc: 0.6136 - val_loss: 1.1513 - val_acc: 0.4419\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.1604 - acc: 0.6023 - val_loss: 1.1508 - val_acc: 0.4651\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.1490 - acc: 0.6023 - val_loss: 1.1511 - val_acc: 0.4651\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.1376 - acc: 0.6023 - val_loss: 1.1520 - val_acc: 0.4651\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1291 - acc: 0.6023 - val_loss: 1.1526 - val_acc: 0.4651\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.1164 - acc: 0.5795 - val_loss: 1.1528 - val_acc: 0.4651\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.1054 - acc: 0.5795 - val_loss: 1.1532 - val_acc: 0.4651\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 1.0939 - acc: 0.5682 - val_loss: 1.1535 - val_acc: 0.4651\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0929 - acc: 0.5682 - val_loss: 1.1534 - val_acc: 0.4651\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.0919 - acc: 0.5682 - val_loss: 1.1533 - val_acc: 0.4651\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0911 - acc: 0.5682 - val_loss: 1.1533 - val_acc: 0.4651\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 0s 183us/step - loss: 1.0902 - acc: 0.5682 - val_loss: 1.1533 - val_acc: 0.4651\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0891 - acc: 0.5682 - val_loss: 1.1532 - val_acc: 0.4651\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0883 - acc: 0.5682 - val_loss: 1.1532 - val_acc: 0.4651\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0873 - acc: 0.5682 - val_loss: 1.1532 - val_acc: 0.4651\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0865 - acc: 0.5682 - val_loss: 1.1531 - val_acc: 0.4651\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0856 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0845 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0844 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0843 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0842 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 1.0841 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0840 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0839 - acc: 0.5795 - val_loss: 1.1530 - val_acc: 0.4884\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 1.0838 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0837 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0837 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0835 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 0s 181us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 0s 340us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 0s 297us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 0s 265us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 0s 362us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 0s 290us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 0s 177us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 0s 250us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 0s 279us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 0s 380us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 0s 302us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 0s 266us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 0s 257us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 0s 181us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 0s 298us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 321/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 322/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 323/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 324/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 325/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 326/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 327/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 328/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 329/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 330/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 331/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 332/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 333/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 334/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 335/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 336/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 337/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 338/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 339/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 340/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 341/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 342/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 343/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 344/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 345/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 346/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 347/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 348/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 349/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 350/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 351/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 352/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 353/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 354/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 355/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 356/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 357/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 358/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 359/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 360/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 361/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 362/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 363/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 364/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 365/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 366/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 367/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 368/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 369/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 370/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 371/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 372/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 373/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 374/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 375/500\n",
            "88/88 [==============================] - 0s 331us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 376/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 377/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 378/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 379/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 380/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 381/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 382/500\n",
            "88/88 [==============================] - 0s 296us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 383/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 384/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 385/500\n",
            "88/88 [==============================] - 0s 289us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 386/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 387/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 388/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 389/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 390/500\n",
            "88/88 [==============================] - 0s 289us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 391/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 392/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 393/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 394/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 395/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 396/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 397/500\n",
            "88/88 [==============================] - 0s 267us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 398/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 399/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 400/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 401/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 402/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 403/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 404/500\n",
            "88/88 [==============================] - 0s 283us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 405/500\n",
            "88/88 [==============================] - 0s 281us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 406/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 407/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 408/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 409/500\n",
            "88/88 [==============================] - 0s 303us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 410/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 411/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 412/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 413/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 414/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 415/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 416/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 417/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 418/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 419/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 420/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 421/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 422/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 423/500\n",
            "88/88 [==============================] - 0s 177us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 424/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 425/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 426/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 427/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 428/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 429/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 430/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 431/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 432/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 433/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 434/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 435/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 436/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 437/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 438/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 439/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 440/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 441/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 442/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 443/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 444/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 445/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 446/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 447/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 448/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 449/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 450/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 451/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 452/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 453/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 454/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 455/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 456/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 457/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 458/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 459/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 460/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 461/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 462/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 463/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 464/500\n",
            "88/88 [==============================] - 0s 177us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 465/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 466/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 467/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 468/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 469/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 470/500\n",
            "88/88 [==============================] - 0s 270us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 471/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 472/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 473/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 474/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 475/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 476/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 477/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 478/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 479/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 480/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 481/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 482/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 483/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 484/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 485/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 486/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 487/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 488/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 489/500\n",
            "88/88 [==============================] - 0s 181us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 490/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 491/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 492/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 493/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 494/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 495/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 496/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 497/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 498/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 499/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Epoch 500/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.0834 - acc: 0.5795 - val_loss: 1.1529 - val_acc: 0.4884\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/500\n",
            "88/88 [==============================] - 0s 6ms/step - loss: 2.9009 - acc: 0.2386 - val_loss: 3.5017 - val_acc: 0.2558\n",
            "Epoch 2/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 2.7517 - acc: 0.2614 - val_loss: 3.3871 - val_acc: 0.2326\n",
            "Epoch 3/500\n",
            "88/88 [==============================] - 0s 304us/step - loss: 2.6155 - acc: 0.2614 - val_loss: 3.2855 - val_acc: 0.2326\n",
            "Epoch 4/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 2.4899 - acc: 0.2841 - val_loss: 3.1957 - val_acc: 0.2791\n",
            "Epoch 5/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 2.3771 - acc: 0.2614 - val_loss: 3.1000 - val_acc: 0.3023\n",
            "Epoch 6/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 2.2744 - acc: 0.2727 - val_loss: 3.0087 - val_acc: 0.3488\n",
            "Epoch 7/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 2.1854 - acc: 0.3182 - val_loss: 2.9285 - val_acc: 0.3488\n",
            "Epoch 8/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 2.1067 - acc: 0.3182 - val_loss: 2.8541 - val_acc: 0.3488\n",
            "Epoch 9/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 2.0339 - acc: 0.3182 - val_loss: 2.7837 - val_acc: 0.3488\n",
            "Epoch 10/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 1.9658 - acc: 0.3182 - val_loss: 2.7193 - val_acc: 0.3256\n",
            "Epoch 11/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.9053 - acc: 0.3295 - val_loss: 2.6602 - val_acc: 0.3256\n",
            "Epoch 12/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.8512 - acc: 0.3295 - val_loss: 2.6051 - val_acc: 0.3488\n",
            "Epoch 13/500\n",
            "88/88 [==============================] - 0s 276us/step - loss: 1.8035 - acc: 0.3409 - val_loss: 2.5514 - val_acc: 0.3488\n",
            "Epoch 14/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.7552 - acc: 0.3409 - val_loss: 2.5037 - val_acc: 0.3488\n",
            "Epoch 15/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.7123 - acc: 0.3182 - val_loss: 2.4553 - val_acc: 0.3488\n",
            "Epoch 16/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.6763 - acc: 0.3068 - val_loss: 2.4122 - val_acc: 0.3488\n",
            "Epoch 17/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.6383 - acc: 0.2955 - val_loss: 2.3673 - val_acc: 0.3488\n",
            "Epoch 18/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.6052 - acc: 0.2955 - val_loss: 2.3192 - val_acc: 0.3488\n",
            "Epoch 19/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.5772 - acc: 0.3182 - val_loss: 2.2752 - val_acc: 0.3488\n",
            "Epoch 20/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.5482 - acc: 0.3182 - val_loss: 2.2360 - val_acc: 0.3488\n",
            "Epoch 21/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.5201 - acc: 0.3182 - val_loss: 2.1983 - val_acc: 0.3488\n",
            "Epoch 22/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 1.4978 - acc: 0.3182 - val_loss: 2.1640 - val_acc: 0.3721\n",
            "Epoch 23/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 1.4739 - acc: 0.3182 - val_loss: 2.1305 - val_acc: 0.3488\n",
            "Epoch 24/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.4500 - acc: 0.3182 - val_loss: 2.0991 - val_acc: 0.3488\n",
            "Epoch 25/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.4299 - acc: 0.3182 - val_loss: 2.0691 - val_acc: 0.3488\n",
            "Epoch 26/500\n",
            "88/88 [==============================] - 0s 254us/step - loss: 1.4114 - acc: 0.3295 - val_loss: 2.0412 - val_acc: 0.3488\n",
            "Epoch 27/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.3927 - acc: 0.3523 - val_loss: 2.0149 - val_acc: 0.3721\n",
            "Epoch 28/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.3767 - acc: 0.3523 - val_loss: 1.9901 - val_acc: 0.3721\n",
            "Epoch 29/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.3629 - acc: 0.3523 - val_loss: 1.9662 - val_acc: 0.3953\n",
            "Epoch 30/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.3468 - acc: 0.3750 - val_loss: 1.9432 - val_acc: 0.3953\n",
            "Epoch 31/500\n",
            "88/88 [==============================] - 0s 251us/step - loss: 1.3342 - acc: 0.3750 - val_loss: 1.9211 - val_acc: 0.3953\n",
            "Epoch 32/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 1.3195 - acc: 0.3977 - val_loss: 1.9000 - val_acc: 0.4186\n",
            "Epoch 33/500\n",
            "88/88 [==============================] - 0s 312us/step - loss: 1.3056 - acc: 0.3864 - val_loss: 1.8798 - val_acc: 0.4186\n",
            "Epoch 34/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.2925 - acc: 0.3977 - val_loss: 1.8609 - val_acc: 0.4186\n",
            "Epoch 35/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.2843 - acc: 0.3977 - val_loss: 1.8429 - val_acc: 0.4186\n",
            "Epoch 36/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 1.2706 - acc: 0.3977 - val_loss: 1.8274 - val_acc: 0.4186\n",
            "Epoch 37/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.2603 - acc: 0.3977 - val_loss: 1.8116 - val_acc: 0.4186\n",
            "Epoch 38/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.2506 - acc: 0.3977 - val_loss: 1.7945 - val_acc: 0.4186\n",
            "Epoch 39/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.2410 - acc: 0.3977 - val_loss: 1.7784 - val_acc: 0.4186\n",
            "Epoch 40/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 1.2289 - acc: 0.3977 - val_loss: 1.7648 - val_acc: 0.4186\n",
            "Epoch 41/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 1.2198 - acc: 0.3977 - val_loss: 1.7499 - val_acc: 0.4419\n",
            "Epoch 42/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.2122 - acc: 0.3977 - val_loss: 1.7371 - val_acc: 0.4419\n",
            "Epoch 43/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 1.2061 - acc: 0.3977 - val_loss: 1.7244 - val_acc: 0.4419\n",
            "Epoch 44/500\n",
            "88/88 [==============================] - 0s 324us/step - loss: 1.1951 - acc: 0.4091 - val_loss: 1.7111 - val_acc: 0.4419\n",
            "Epoch 45/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 1.1865 - acc: 0.4091 - val_loss: 1.6992 - val_acc: 0.4419\n",
            "Epoch 46/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.1796 - acc: 0.4091 - val_loss: 1.6860 - val_acc: 0.4419\n",
            "Epoch 47/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.1713 - acc: 0.4205 - val_loss: 1.6730 - val_acc: 0.4419\n",
            "Epoch 48/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 1.1625 - acc: 0.4318 - val_loss: 1.6617 - val_acc: 0.4186\n",
            "Epoch 49/500\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.1555 - acc: 0.4432 - val_loss: 1.6511 - val_acc: 0.4186\n",
            "Epoch 50/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.1494 - acc: 0.4432 - val_loss: 1.6418 - val_acc: 0.4186\n",
            "Epoch 51/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 1.1430 - acc: 0.4318 - val_loss: 1.6314 - val_acc: 0.4186\n",
            "Epoch 52/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.1363 - acc: 0.4432 - val_loss: 1.6213 - val_acc: 0.4186\n",
            "Epoch 53/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1291 - acc: 0.4432 - val_loss: 1.6119 - val_acc: 0.4186\n",
            "Epoch 54/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.1236 - acc: 0.4205 - val_loss: 1.6028 - val_acc: 0.4186\n",
            "Epoch 55/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.1173 - acc: 0.4318 - val_loss: 1.5943 - val_acc: 0.4186\n",
            "Epoch 56/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1122 - acc: 0.4318 - val_loss: 1.5863 - val_acc: 0.4186\n",
            "Epoch 57/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 1.1073 - acc: 0.4318 - val_loss: 1.5771 - val_acc: 0.4186\n",
            "Epoch 58/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.1003 - acc: 0.4318 - val_loss: 1.5703 - val_acc: 0.4419\n",
            "Epoch 59/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 1.0971 - acc: 0.4318 - val_loss: 1.5619 - val_acc: 0.4419\n",
            "Epoch 60/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 1.0914 - acc: 0.4318 - val_loss: 1.5545 - val_acc: 0.4419\n",
            "Epoch 61/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.0858 - acc: 0.4318 - val_loss: 1.5481 - val_acc: 0.4419\n",
            "Epoch 62/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.0816 - acc: 0.4318 - val_loss: 1.5416 - val_acc: 0.4419\n",
            "Epoch 63/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 1.0774 - acc: 0.4318 - val_loss: 1.5353 - val_acc: 0.4419\n",
            "Epoch 64/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0736 - acc: 0.4318 - val_loss: 1.5287 - val_acc: 0.4419\n",
            "Epoch 65/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 1.0687 - acc: 0.4432 - val_loss: 1.5234 - val_acc: 0.4419\n",
            "Epoch 66/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 1.0654 - acc: 0.4432 - val_loss: 1.5169 - val_acc: 0.4419\n",
            "Epoch 67/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 1.0619 - acc: 0.4545 - val_loss: 1.5110 - val_acc: 0.4186\n",
            "Epoch 68/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.0580 - acc: 0.4773 - val_loss: 1.5052 - val_acc: 0.4186\n",
            "Epoch 69/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 1.0546 - acc: 0.4886 - val_loss: 1.5000 - val_acc: 0.4186\n",
            "Epoch 70/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0511 - acc: 0.4886 - val_loss: 1.4946 - val_acc: 0.4186\n",
            "Epoch 71/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0477 - acc: 0.5000 - val_loss: 1.4892 - val_acc: 0.4186\n",
            "Epoch 72/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 1.0468 - acc: 0.5114 - val_loss: 1.4841 - val_acc: 0.4186\n",
            "Epoch 73/500\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.0422 - acc: 0.5227 - val_loss: 1.4795 - val_acc: 0.4186\n",
            "Epoch 74/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 1.0403 - acc: 0.5227 - val_loss: 1.4751 - val_acc: 0.4186\n",
            "Epoch 75/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 1.0367 - acc: 0.5341 - val_loss: 1.4704 - val_acc: 0.4186\n",
            "Epoch 76/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0346 - acc: 0.5227 - val_loss: 1.4662 - val_acc: 0.4186\n",
            "Epoch 77/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.0321 - acc: 0.5455 - val_loss: 1.4623 - val_acc: 0.4186\n",
            "Epoch 78/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 1.0284 - acc: 0.5455 - val_loss: 1.4595 - val_acc: 0.4186\n",
            "Epoch 79/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0261 - acc: 0.5455 - val_loss: 1.4556 - val_acc: 0.4186\n",
            "Epoch 80/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0239 - acc: 0.5455 - val_loss: 1.4519 - val_acc: 0.4186\n",
            "Epoch 81/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0208 - acc: 0.5568 - val_loss: 1.4488 - val_acc: 0.4186\n",
            "Epoch 82/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.0192 - acc: 0.5568 - val_loss: 1.4452 - val_acc: 0.4186\n",
            "Epoch 83/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 1.0174 - acc: 0.5568 - val_loss: 1.4425 - val_acc: 0.4186\n",
            "Epoch 84/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0143 - acc: 0.5682 - val_loss: 1.4389 - val_acc: 0.4186\n",
            "Epoch 85/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0126 - acc: 0.5682 - val_loss: 1.4355 - val_acc: 0.4186\n",
            "Epoch 86/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.0116 - acc: 0.5682 - val_loss: 1.4316 - val_acc: 0.4186\n",
            "Epoch 87/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0083 - acc: 0.5682 - val_loss: 1.4287 - val_acc: 0.4186\n",
            "Epoch 88/500\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.0059 - acc: 0.5682 - val_loss: 1.4257 - val_acc: 0.4186\n",
            "Epoch 89/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0040 - acc: 0.5682 - val_loss: 1.4226 - val_acc: 0.4186\n",
            "Epoch 90/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 1.0019 - acc: 0.5682 - val_loss: 1.4191 - val_acc: 0.4186\n",
            "Epoch 91/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.0001 - acc: 0.5682 - val_loss: 1.4164 - val_acc: 0.4186\n",
            "Epoch 92/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.9980 - acc: 0.5682 - val_loss: 1.4137 - val_acc: 0.4419\n",
            "Epoch 93/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.9965 - acc: 0.5682 - val_loss: 1.4111 - val_acc: 0.4186\n",
            "Epoch 94/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.9957 - acc: 0.5682 - val_loss: 1.4082 - val_acc: 0.4186\n",
            "Epoch 95/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.9934 - acc: 0.5682 - val_loss: 1.4054 - val_acc: 0.4186\n",
            "Epoch 96/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.9921 - acc: 0.5682 - val_loss: 1.4032 - val_acc: 0.4186\n",
            "Epoch 97/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.9897 - acc: 0.5682 - val_loss: 1.4009 - val_acc: 0.4186\n",
            "Epoch 98/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.9890 - acc: 0.5682 - val_loss: 1.3983 - val_acc: 0.3953\n",
            "Epoch 99/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.9890 - acc: 0.5682 - val_loss: 1.3960 - val_acc: 0.4186\n",
            "Epoch 100/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.9870 - acc: 0.5682 - val_loss: 1.3935 - val_acc: 0.4186\n",
            "Epoch 101/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.9853 - acc: 0.5682 - val_loss: 1.3918 - val_acc: 0.4186\n",
            "Epoch 102/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9834 - acc: 0.5682 - val_loss: 1.3897 - val_acc: 0.4186\n",
            "Epoch 103/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.9823 - acc: 0.5682 - val_loss: 1.3877 - val_acc: 0.4186\n",
            "Epoch 104/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9812 - acc: 0.5682 - val_loss: 1.3857 - val_acc: 0.4186\n",
            "Epoch 105/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.9800 - acc: 0.5682 - val_loss: 1.3840 - val_acc: 0.4186\n",
            "Epoch 106/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9796 - acc: 0.5682 - val_loss: 1.3818 - val_acc: 0.4186\n",
            "Epoch 107/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9771 - acc: 0.5682 - val_loss: 1.3796 - val_acc: 0.4186\n",
            "Epoch 108/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9776 - acc: 0.5795 - val_loss: 1.3776 - val_acc: 0.4186\n",
            "Epoch 109/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.9755 - acc: 0.5795 - val_loss: 1.3754 - val_acc: 0.4186\n",
            "Epoch 110/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.9746 - acc: 0.5795 - val_loss: 1.3732 - val_acc: 0.4186\n",
            "Epoch 111/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.9738 - acc: 0.5795 - val_loss: 1.3716 - val_acc: 0.4419\n",
            "Epoch 112/500\n",
            "88/88 [==============================] - 0s 297us/step - loss: 0.9722 - acc: 0.5795 - val_loss: 1.3695 - val_acc: 0.4419\n",
            "Epoch 113/500\n",
            "88/88 [==============================] - 0s 296us/step - loss: 0.9712 - acc: 0.5795 - val_loss: 1.3681 - val_acc: 0.4419\n",
            "Epoch 114/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9697 - acc: 0.5795 - val_loss: 1.3664 - val_acc: 0.4419\n",
            "Epoch 115/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.9694 - acc: 0.5682 - val_loss: 1.3647 - val_acc: 0.4419\n",
            "Epoch 116/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9679 - acc: 0.5682 - val_loss: 1.3629 - val_acc: 0.4419\n",
            "Epoch 117/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.9670 - acc: 0.5909 - val_loss: 1.3617 - val_acc: 0.4419\n",
            "Epoch 118/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.9666 - acc: 0.5682 - val_loss: 1.3604 - val_acc: 0.4419\n",
            "Epoch 119/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9652 - acc: 0.5682 - val_loss: 1.3592 - val_acc: 0.4419\n",
            "Epoch 120/500\n",
            "88/88 [==============================] - 0s 289us/step - loss: 0.9637 - acc: 0.5795 - val_loss: 1.3568 - val_acc: 0.4419\n",
            "Epoch 121/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9629 - acc: 0.5795 - val_loss: 1.3553 - val_acc: 0.4419\n",
            "Epoch 122/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9616 - acc: 0.5795 - val_loss: 1.3539 - val_acc: 0.4419\n",
            "Epoch 123/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9611 - acc: 0.5795 - val_loss: 1.3524 - val_acc: 0.4419\n",
            "Epoch 124/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.9598 - acc: 0.5795 - val_loss: 1.3506 - val_acc: 0.4419\n",
            "Epoch 125/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.9588 - acc: 0.5795 - val_loss: 1.3492 - val_acc: 0.4419\n",
            "Epoch 126/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.9576 - acc: 0.5795 - val_loss: 1.3473 - val_acc: 0.4419\n",
            "Epoch 127/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.9573 - acc: 0.5795 - val_loss: 1.3457 - val_acc: 0.4419\n",
            "Epoch 128/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.9561 - acc: 0.5795 - val_loss: 1.3445 - val_acc: 0.4419\n",
            "Epoch 129/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.9552 - acc: 0.5909 - val_loss: 1.3430 - val_acc: 0.4419\n",
            "Epoch 130/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.9544 - acc: 0.5795 - val_loss: 1.3416 - val_acc: 0.4419\n",
            "Epoch 131/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.9539 - acc: 0.5795 - val_loss: 1.3404 - val_acc: 0.4419\n",
            "Epoch 132/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9527 - acc: 0.5795 - val_loss: 1.3388 - val_acc: 0.4419\n",
            "Epoch 133/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.9522 - acc: 0.5795 - val_loss: 1.3382 - val_acc: 0.4419\n",
            "Epoch 134/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.9509 - acc: 0.6023 - val_loss: 1.3367 - val_acc: 0.4419\n",
            "Epoch 135/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9502 - acc: 0.5795 - val_loss: 1.3355 - val_acc: 0.4419\n",
            "Epoch 136/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.9494 - acc: 0.5795 - val_loss: 1.3337 - val_acc: 0.4186\n",
            "Epoch 137/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9480 - acc: 0.5909 - val_loss: 1.3320 - val_acc: 0.4186\n",
            "Epoch 138/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.9472 - acc: 0.6023 - val_loss: 1.3303 - val_acc: 0.4186\n",
            "Epoch 139/500\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9466 - acc: 0.5795 - val_loss: 1.3297 - val_acc: 0.4186\n",
            "Epoch 140/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.9462 - acc: 0.6023 - val_loss: 1.3280 - val_acc: 0.4186\n",
            "Epoch 141/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.9453 - acc: 0.5795 - val_loss: 1.3268 - val_acc: 0.4186\n",
            "Epoch 142/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.9442 - acc: 0.5909 - val_loss: 1.3254 - val_acc: 0.4186\n",
            "Epoch 143/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.9435 - acc: 0.6136 - val_loss: 1.3238 - val_acc: 0.4186\n",
            "Epoch 144/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.9434 - acc: 0.6023 - val_loss: 1.3225 - val_acc: 0.4186\n",
            "Epoch 145/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.9421 - acc: 0.6136 - val_loss: 1.3208 - val_acc: 0.4186\n",
            "Epoch 146/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9411 - acc: 0.6023 - val_loss: 1.3196 - val_acc: 0.4186\n",
            "Epoch 147/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.9403 - acc: 0.5909 - val_loss: 1.3181 - val_acc: 0.4186\n",
            "Epoch 148/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9394 - acc: 0.6136 - val_loss: 1.3170 - val_acc: 0.4186\n",
            "Epoch 149/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.9382 - acc: 0.6136 - val_loss: 1.3158 - val_acc: 0.4186\n",
            "Epoch 150/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.9369 - acc: 0.6136 - val_loss: 1.3146 - val_acc: 0.4186\n",
            "Epoch 151/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.9365 - acc: 0.6023 - val_loss: 1.3135 - val_acc: 0.4186\n",
            "Epoch 152/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.9351 - acc: 0.6136 - val_loss: 1.3128 - val_acc: 0.4186\n",
            "Epoch 153/500\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.9343 - acc: 0.6136 - val_loss: 1.3119 - val_acc: 0.4186\n",
            "Epoch 154/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.9325 - acc: 0.6136 - val_loss: 1.3111 - val_acc: 0.4186\n",
            "Epoch 155/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.9324 - acc: 0.6136 - val_loss: 1.3101 - val_acc: 0.4186\n",
            "Epoch 156/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.9313 - acc: 0.6136 - val_loss: 1.3094 - val_acc: 0.4186\n",
            "Epoch 157/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.9303 - acc: 0.6136 - val_loss: 1.3087 - val_acc: 0.4419\n",
            "Epoch 158/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.9292 - acc: 0.6136 - val_loss: 1.3079 - val_acc: 0.4419\n",
            "Epoch 159/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9288 - acc: 0.6136 - val_loss: 1.3073 - val_acc: 0.4419\n",
            "Epoch 160/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.9272 - acc: 0.6136 - val_loss: 1.3063 - val_acc: 0.4419\n",
            "Epoch 161/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.9263 - acc: 0.6250 - val_loss: 1.3058 - val_acc: 0.4419\n",
            "Epoch 162/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9253 - acc: 0.6136 - val_loss: 1.3052 - val_acc: 0.4419\n",
            "Epoch 163/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9248 - acc: 0.6136 - val_loss: 1.3048 - val_acc: 0.4419\n",
            "Epoch 164/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.9232 - acc: 0.6136 - val_loss: 1.3040 - val_acc: 0.4419\n",
            "Epoch 165/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.9223 - acc: 0.6250 - val_loss: 1.3036 - val_acc: 0.4419\n",
            "Epoch 166/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.9216 - acc: 0.6136 - val_loss: 1.3031 - val_acc: 0.4419\n",
            "Epoch 167/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.9202 - acc: 0.6250 - val_loss: 1.3023 - val_acc: 0.4419\n",
            "Epoch 168/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.9192 - acc: 0.6136 - val_loss: 1.3021 - val_acc: 0.4419\n",
            "Epoch 169/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9182 - acc: 0.6250 - val_loss: 1.3017 - val_acc: 0.4419\n",
            "Epoch 170/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.9171 - acc: 0.6136 - val_loss: 1.3019 - val_acc: 0.4419\n",
            "Epoch 171/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.9158 - acc: 0.6136 - val_loss: 1.3018 - val_acc: 0.4419\n",
            "Epoch 172/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9154 - acc: 0.6250 - val_loss: 1.3018 - val_acc: 0.4419\n",
            "Epoch 173/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.9137 - acc: 0.6250 - val_loss: 1.3012 - val_acc: 0.4419\n",
            "Epoch 174/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.9124 - acc: 0.6136 - val_loss: 1.3012 - val_acc: 0.4419\n",
            "Epoch 175/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.9117 - acc: 0.6250 - val_loss: 1.3009 - val_acc: 0.4419\n",
            "Epoch 176/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.9102 - acc: 0.6136 - val_loss: 1.3004 - val_acc: 0.4419\n",
            "Epoch 177/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.9090 - acc: 0.6250 - val_loss: 1.3000 - val_acc: 0.4419\n",
            "Epoch 178/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9084 - acc: 0.6250 - val_loss: 1.2996 - val_acc: 0.4419\n",
            "Epoch 179/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.9072 - acc: 0.6250 - val_loss: 1.2990 - val_acc: 0.4419\n",
            "Epoch 180/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9066 - acc: 0.6136 - val_loss: 1.2988 - val_acc: 0.4419\n",
            "Epoch 181/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.9056 - acc: 0.6250 - val_loss: 1.2980 - val_acc: 0.4419\n",
            "Epoch 182/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.9047 - acc: 0.6250 - val_loss: 1.2975 - val_acc: 0.4419\n",
            "Epoch 183/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.9040 - acc: 0.6250 - val_loss: 1.2969 - val_acc: 0.4419\n",
            "Epoch 184/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.9035 - acc: 0.6250 - val_loss: 1.2964 - val_acc: 0.4419\n",
            "Epoch 185/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.9021 - acc: 0.6250 - val_loss: 1.2964 - val_acc: 0.4419\n",
            "Epoch 186/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 0.9023 - acc: 0.6250 - val_loss: 1.2955 - val_acc: 0.4419\n",
            "Epoch 187/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8997 - acc: 0.6250 - val_loss: 1.2953 - val_acc: 0.4419\n",
            "Epoch 188/500\n",
            "88/88 [==============================] - 0s 183us/step - loss: 0.8994 - acc: 0.6250 - val_loss: 1.2947 - val_acc: 0.4651\n",
            "Epoch 189/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8985 - acc: 0.6250 - val_loss: 1.2946 - val_acc: 0.4651\n",
            "Epoch 190/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8974 - acc: 0.6250 - val_loss: 1.2944 - val_acc: 0.4651\n",
            "Epoch 191/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8965 - acc: 0.6250 - val_loss: 1.2941 - val_acc: 0.4651\n",
            "Epoch 192/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8950 - acc: 0.6136 - val_loss: 1.2945 - val_acc: 0.4651\n",
            "Epoch 193/500\n",
            "88/88 [==============================] - 0s 179us/step - loss: 0.8937 - acc: 0.6136 - val_loss: 1.2942 - val_acc: 0.4651\n",
            "Epoch 194/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8927 - acc: 0.6136 - val_loss: 1.2943 - val_acc: 0.4651\n",
            "Epoch 195/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8919 - acc: 0.6136 - val_loss: 1.2943 - val_acc: 0.4651\n",
            "Epoch 196/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8912 - acc: 0.6136 - val_loss: 1.2947 - val_acc: 0.4651\n",
            "Epoch 197/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.8904 - acc: 0.6136 - val_loss: 1.2947 - val_acc: 0.4884\n",
            "Epoch 198/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.8896 - acc: 0.6136 - val_loss: 1.2944 - val_acc: 0.4884\n",
            "Epoch 199/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8890 - acc: 0.6136 - val_loss: 1.2938 - val_acc: 0.4884\n",
            "Epoch 200/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8886 - acc: 0.6250 - val_loss: 1.2935 - val_acc: 0.4884\n",
            "Epoch 201/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.8874 - acc: 0.6136 - val_loss: 1.2931 - val_acc: 0.4884\n",
            "Epoch 202/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8867 - acc: 0.6250 - val_loss: 1.2930 - val_acc: 0.4884\n",
            "Epoch 203/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8862 - acc: 0.6250 - val_loss: 1.2928 - val_acc: 0.4884\n",
            "Epoch 204/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8857 - acc: 0.6250 - val_loss: 1.2925 - val_acc: 0.4884\n",
            "Epoch 205/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8849 - acc: 0.6250 - val_loss: 1.2924 - val_acc: 0.4884\n",
            "Epoch 206/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8840 - acc: 0.6250 - val_loss: 1.2916 - val_acc: 0.4884\n",
            "Epoch 207/500\n",
            "88/88 [==============================] - 0s 183us/step - loss: 0.8846 - acc: 0.6250 - val_loss: 1.2916 - val_acc: 0.4884\n",
            "Epoch 208/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8831 - acc: 0.6250 - val_loss: 1.2917 - val_acc: 0.4884\n",
            "Epoch 209/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8825 - acc: 0.6250 - val_loss: 1.2915 - val_acc: 0.4884\n",
            "Epoch 210/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8814 - acc: 0.6136 - val_loss: 1.2913 - val_acc: 0.4884\n",
            "Epoch 211/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8811 - acc: 0.6136 - val_loss: 1.2910 - val_acc: 0.4884\n",
            "Epoch 212/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8801 - acc: 0.6023 - val_loss: 1.2907 - val_acc: 0.4884\n",
            "Epoch 213/500\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8797 - acc: 0.6023 - val_loss: 1.2903 - val_acc: 0.4884\n",
            "Epoch 214/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8794 - acc: 0.6136 - val_loss: 1.2904 - val_acc: 0.4884\n",
            "Epoch 215/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8785 - acc: 0.6023 - val_loss: 1.2903 - val_acc: 0.4884\n",
            "Epoch 216/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8783 - acc: 0.6023 - val_loss: 1.2898 - val_acc: 0.4884\n",
            "Epoch 217/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8777 - acc: 0.5909 - val_loss: 1.2892 - val_acc: 0.4884\n",
            "Epoch 218/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8767 - acc: 0.5909 - val_loss: 1.2890 - val_acc: 0.4884\n",
            "Epoch 219/500\n",
            "88/88 [==============================] - 0s 177us/step - loss: 0.8758 - acc: 0.6023 - val_loss: 1.2892 - val_acc: 0.4884\n",
            "Epoch 220/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8754 - acc: 0.6136 - val_loss: 1.2891 - val_acc: 0.4884\n",
            "Epoch 221/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8751 - acc: 0.6136 - val_loss: 1.2889 - val_acc: 0.4884\n",
            "Epoch 222/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8745 - acc: 0.6023 - val_loss: 1.2893 - val_acc: 0.4884\n",
            "Epoch 223/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8739 - acc: 0.6136 - val_loss: 1.2891 - val_acc: 0.4884\n",
            "Epoch 224/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8725 - acc: 0.6023 - val_loss: 1.2890 - val_acc: 0.4884\n",
            "Epoch 225/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8723 - acc: 0.6023 - val_loss: 1.2887 - val_acc: 0.4884\n",
            "Epoch 226/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8716 - acc: 0.6023 - val_loss: 1.2888 - val_acc: 0.4884\n",
            "Epoch 227/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 0.8713 - acc: 0.6023 - val_loss: 1.2886 - val_acc: 0.4884\n",
            "Epoch 228/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8711 - acc: 0.6136 - val_loss: 1.2884 - val_acc: 0.4884\n",
            "Epoch 229/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8697 - acc: 0.6023 - val_loss: 1.2889 - val_acc: 0.4884\n",
            "Epoch 230/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8695 - acc: 0.6023 - val_loss: 1.2889 - val_acc: 0.5116\n",
            "Epoch 231/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8679 - acc: 0.6023 - val_loss: 1.2886 - val_acc: 0.5116\n",
            "Epoch 232/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8669 - acc: 0.6023 - val_loss: 1.2883 - val_acc: 0.5116\n",
            "Epoch 233/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8669 - acc: 0.6023 - val_loss: 1.2882 - val_acc: 0.5116\n",
            "Epoch 234/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8666 - acc: 0.6023 - val_loss: 1.2880 - val_acc: 0.5116\n",
            "Epoch 235/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8653 - acc: 0.6023 - val_loss: 1.2881 - val_acc: 0.5116\n",
            "Epoch 236/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8649 - acc: 0.6023 - val_loss: 1.2878 - val_acc: 0.5116\n",
            "Epoch 237/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8647 - acc: 0.6023 - val_loss: 1.2876 - val_acc: 0.5116\n",
            "Epoch 238/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.8642 - acc: 0.6023 - val_loss: 1.2873 - val_acc: 0.5116\n",
            "Epoch 239/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8638 - acc: 0.6023 - val_loss: 1.2870 - val_acc: 0.5116\n",
            "Epoch 240/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8625 - acc: 0.6023 - val_loss: 1.2866 - val_acc: 0.5116\n",
            "Epoch 241/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8617 - acc: 0.6023 - val_loss: 1.2865 - val_acc: 0.5116\n",
            "Epoch 242/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8617 - acc: 0.6023 - val_loss: 1.2860 - val_acc: 0.5116\n",
            "Epoch 243/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8612 - acc: 0.6023 - val_loss: 1.2859 - val_acc: 0.5116\n",
            "Epoch 244/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8602 - acc: 0.6023 - val_loss: 1.2853 - val_acc: 0.4884\n",
            "Epoch 245/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.8605 - acc: 0.6023 - val_loss: 1.2856 - val_acc: 0.5116\n",
            "Epoch 246/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8594 - acc: 0.6023 - val_loss: 1.2854 - val_acc: 0.5116\n",
            "Epoch 247/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8589 - acc: 0.6023 - val_loss: 1.2855 - val_acc: 0.5116\n",
            "Epoch 248/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8584 - acc: 0.6023 - val_loss: 1.2852 - val_acc: 0.5116\n",
            "Epoch 249/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8578 - acc: 0.6023 - val_loss: 1.2850 - val_acc: 0.5116\n",
            "Epoch 250/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8577 - acc: 0.6023 - val_loss: 1.2849 - val_acc: 0.5116\n",
            "Epoch 251/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8574 - acc: 0.6023 - val_loss: 1.2848 - val_acc: 0.5116\n",
            "Epoch 252/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8562 - acc: 0.6023 - val_loss: 1.2845 - val_acc: 0.5116\n",
            "Epoch 253/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8562 - acc: 0.6023 - val_loss: 1.2843 - val_acc: 0.5116\n",
            "Epoch 254/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8550 - acc: 0.6023 - val_loss: 1.2843 - val_acc: 0.5116\n",
            "Epoch 255/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8550 - acc: 0.6023 - val_loss: 1.2841 - val_acc: 0.5116\n",
            "Epoch 256/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8544 - acc: 0.6023 - val_loss: 1.2840 - val_acc: 0.5116\n",
            "Epoch 257/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8535 - acc: 0.6023 - val_loss: 1.2846 - val_acc: 0.5116\n",
            "Epoch 258/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8531 - acc: 0.6023 - val_loss: 1.2848 - val_acc: 0.5116\n",
            "Epoch 259/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8530 - acc: 0.6023 - val_loss: 1.2847 - val_acc: 0.5116\n",
            "Epoch 260/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8520 - acc: 0.6023 - val_loss: 1.2848 - val_acc: 0.5116\n",
            "Epoch 261/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8523 - acc: 0.6023 - val_loss: 1.2846 - val_acc: 0.5116\n",
            "Epoch 262/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8508 - acc: 0.6023 - val_loss: 1.2840 - val_acc: 0.5116\n",
            "Epoch 263/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8515 - acc: 0.6023 - val_loss: 1.2838 - val_acc: 0.5116\n",
            "Epoch 264/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8507 - acc: 0.6023 - val_loss: 1.2834 - val_acc: 0.5116\n",
            "Epoch 265/500\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8503 - acc: 0.6023 - val_loss: 1.2832 - val_acc: 0.5116\n",
            "Epoch 266/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8496 - acc: 0.6023 - val_loss: 1.2833 - val_acc: 0.4884\n",
            "Epoch 267/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8492 - acc: 0.6023 - val_loss: 1.2827 - val_acc: 0.4884\n",
            "Epoch 268/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8487 - acc: 0.6023 - val_loss: 1.2827 - val_acc: 0.4884\n",
            "Epoch 269/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8483 - acc: 0.6023 - val_loss: 1.2823 - val_acc: 0.4884\n",
            "Epoch 270/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8478 - acc: 0.6023 - val_loss: 1.2829 - val_acc: 0.4884\n",
            "Epoch 271/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8470 - acc: 0.6023 - val_loss: 1.2826 - val_acc: 0.4884\n",
            "Epoch 272/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8467 - acc: 0.6023 - val_loss: 1.2821 - val_acc: 0.4884\n",
            "Epoch 273/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.8465 - acc: 0.6023 - val_loss: 1.2818 - val_acc: 0.4884\n",
            "Epoch 274/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8459 - acc: 0.6023 - val_loss: 1.2820 - val_acc: 0.4884\n",
            "Epoch 275/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8455 - acc: 0.6023 - val_loss: 1.2817 - val_acc: 0.4884\n",
            "Epoch 276/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.8455 - acc: 0.6023 - val_loss: 1.2818 - val_acc: 0.4884\n",
            "Epoch 277/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8456 - acc: 0.6023 - val_loss: 1.2815 - val_acc: 0.4884\n",
            "Epoch 278/500\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.8445 - acc: 0.6023 - val_loss: 1.2811 - val_acc: 0.4884\n",
            "Epoch 279/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8436 - acc: 0.6023 - val_loss: 1.2810 - val_acc: 0.4884\n",
            "Epoch 280/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8437 - acc: 0.6023 - val_loss: 1.2805 - val_acc: 0.4884\n",
            "Epoch 281/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8436 - acc: 0.6023 - val_loss: 1.2803 - val_acc: 0.4884\n",
            "Epoch 282/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8422 - acc: 0.6136 - val_loss: 1.2800 - val_acc: 0.4884\n",
            "Epoch 283/500\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8422 - acc: 0.6136 - val_loss: 1.2798 - val_acc: 0.4884\n",
            "Epoch 284/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8424 - acc: 0.6136 - val_loss: 1.2798 - val_acc: 0.4884\n",
            "Epoch 285/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8416 - acc: 0.6136 - val_loss: 1.2794 - val_acc: 0.4884\n",
            "Epoch 286/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8410 - acc: 0.6136 - val_loss: 1.2794 - val_acc: 0.4884\n",
            "Epoch 287/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8412 - acc: 0.6136 - val_loss: 1.2791 - val_acc: 0.4884\n",
            "Epoch 288/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8408 - acc: 0.6136 - val_loss: 1.2793 - val_acc: 0.4884\n",
            "Epoch 289/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8398 - acc: 0.6136 - val_loss: 1.2790 - val_acc: 0.4884\n",
            "Epoch 290/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8401 - acc: 0.6250 - val_loss: 1.2785 - val_acc: 0.4884\n",
            "Epoch 291/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.8389 - acc: 0.6250 - val_loss: 1.2785 - val_acc: 0.4884\n",
            "Epoch 292/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8398 - acc: 0.6250 - val_loss: 1.2782 - val_acc: 0.4884\n",
            "Epoch 293/500\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.8386 - acc: 0.6136 - val_loss: 1.2780 - val_acc: 0.4884\n",
            "Epoch 294/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8384 - acc: 0.6250 - val_loss: 1.2779 - val_acc: 0.4884\n",
            "Epoch 295/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8374 - acc: 0.6136 - val_loss: 1.2779 - val_acc: 0.4884\n",
            "Epoch 296/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8376 - acc: 0.6136 - val_loss: 1.2777 - val_acc: 0.4884\n",
            "Epoch 297/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 0.8369 - acc: 0.6136 - val_loss: 1.2775 - val_acc: 0.4884\n",
            "Epoch 298/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8368 - acc: 0.6250 - val_loss: 1.2770 - val_acc: 0.4884\n",
            "Epoch 299/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8362 - acc: 0.6136 - val_loss: 1.2768 - val_acc: 0.4884\n",
            "Epoch 300/500\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.8356 - acc: 0.6136 - val_loss: 1.2766 - val_acc: 0.4884\n",
            "Epoch 301/500\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.8353 - acc: 0.6136 - val_loss: 1.2763 - val_acc: 0.4884\n",
            "Epoch 302/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8356 - acc: 0.6136 - val_loss: 1.2760 - val_acc: 0.4884\n",
            "Epoch 303/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8345 - acc: 0.6136 - val_loss: 1.2755 - val_acc: 0.4884\n",
            "Epoch 304/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8342 - acc: 0.6136 - val_loss: 1.2757 - val_acc: 0.4884\n",
            "Epoch 305/500\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8340 - acc: 0.6136 - val_loss: 1.2755 - val_acc: 0.4884\n",
            "Epoch 306/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8339 - acc: 0.6136 - val_loss: 1.2750 - val_acc: 0.4884\n",
            "Epoch 307/500\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.8335 - acc: 0.6136 - val_loss: 1.2751 - val_acc: 0.4884\n",
            "Epoch 308/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8330 - acc: 0.6136 - val_loss: 1.2750 - val_acc: 0.4884\n",
            "Epoch 309/500\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8330 - acc: 0.6136 - val_loss: 1.2748 - val_acc: 0.4884\n",
            "Epoch 310/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8322 - acc: 0.6136 - val_loss: 1.2748 - val_acc: 0.4884\n",
            "Epoch 311/500\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8322 - acc: 0.6136 - val_loss: 1.2748 - val_acc: 0.4884\n",
            "Epoch 312/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8316 - acc: 0.6136 - val_loss: 1.2745 - val_acc: 0.4884\n",
            "Epoch 313/500\n",
            "88/88 [==============================] - 0s 180us/step - loss: 0.8313 - acc: 0.6136 - val_loss: 1.2745 - val_acc: 0.4884\n",
            "Epoch 314/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.8307 - acc: 0.6136 - val_loss: 1.2742 - val_acc: 0.4884\n",
            "Epoch 315/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8310 - acc: 0.6136 - val_loss: 1.2739 - val_acc: 0.4884\n",
            "Epoch 316/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8301 - acc: 0.6136 - val_loss: 1.2743 - val_acc: 0.4884\n",
            "Epoch 317/500\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.8301 - acc: 0.6136 - val_loss: 1.2742 - val_acc: 0.4884\n",
            "Epoch 318/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8295 - acc: 0.6136 - val_loss: 1.2738 - val_acc: 0.4884\n",
            "Epoch 319/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8292 - acc: 0.6250 - val_loss: 1.2736 - val_acc: 0.4884\n",
            "Epoch 320/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8287 - acc: 0.6250 - val_loss: 1.2740 - val_acc: 0.4884\n",
            "Epoch 321/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8289 - acc: 0.6136 - val_loss: 1.2740 - val_acc: 0.4884\n",
            "Epoch 322/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8281 - acc: 0.6250 - val_loss: 1.2737 - val_acc: 0.4884\n",
            "Epoch 323/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8285 - acc: 0.6250 - val_loss: 1.2737 - val_acc: 0.4884\n",
            "Epoch 324/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8277 - acc: 0.6250 - val_loss: 1.2736 - val_acc: 0.4884\n",
            "Epoch 325/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8272 - acc: 0.6250 - val_loss: 1.2735 - val_acc: 0.4884\n",
            "Epoch 326/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8271 - acc: 0.6250 - val_loss: 1.2735 - val_acc: 0.4884\n",
            "Epoch 327/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8271 - acc: 0.6250 - val_loss: 1.2740 - val_acc: 0.4884\n",
            "Epoch 328/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8263 - acc: 0.6250 - val_loss: 1.2739 - val_acc: 0.4884\n",
            "Epoch 329/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8258 - acc: 0.6250 - val_loss: 1.2735 - val_acc: 0.4884\n",
            "Epoch 330/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8262 - acc: 0.6250 - val_loss: 1.2733 - val_acc: 0.4884\n",
            "Epoch 331/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8250 - acc: 0.6250 - val_loss: 1.2730 - val_acc: 0.4884\n",
            "Epoch 332/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8253 - acc: 0.6250 - val_loss: 1.2728 - val_acc: 0.4884\n",
            "Epoch 333/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8247 - acc: 0.6250 - val_loss: 1.2726 - val_acc: 0.4884\n",
            "Epoch 334/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8246 - acc: 0.6250 - val_loss: 1.2722 - val_acc: 0.4884\n",
            "Epoch 335/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8243 - acc: 0.6250 - val_loss: 1.2718 - val_acc: 0.4884\n",
            "Epoch 336/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8242 - acc: 0.6250 - val_loss: 1.2719 - val_acc: 0.4884\n",
            "Epoch 337/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8237 - acc: 0.6250 - val_loss: 1.2714 - val_acc: 0.4884\n",
            "Epoch 338/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8232 - acc: 0.6250 - val_loss: 1.2715 - val_acc: 0.4884\n",
            "Epoch 339/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8226 - acc: 0.6250 - val_loss: 1.2713 - val_acc: 0.4884\n",
            "Epoch 340/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8224 - acc: 0.6250 - val_loss: 1.2716 - val_acc: 0.4884\n",
            "Epoch 341/500\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8222 - acc: 0.6250 - val_loss: 1.2715 - val_acc: 0.4884\n",
            "Epoch 342/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8218 - acc: 0.6250 - val_loss: 1.2712 - val_acc: 0.4884\n",
            "Epoch 343/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8216 - acc: 0.6250 - val_loss: 1.2711 - val_acc: 0.4884\n",
            "Epoch 344/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8209 - acc: 0.6250 - val_loss: 1.2707 - val_acc: 0.4884\n",
            "Epoch 345/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8207 - acc: 0.6250 - val_loss: 1.2708 - val_acc: 0.4884\n",
            "Epoch 346/500\n",
            "88/88 [==============================] - 0s 174us/step - loss: 0.8202 - acc: 0.6250 - val_loss: 1.2703 - val_acc: 0.4884\n",
            "Epoch 347/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8201 - acc: 0.6250 - val_loss: 1.2701 - val_acc: 0.4884\n",
            "Epoch 348/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8199 - acc: 0.6250 - val_loss: 1.2698 - val_acc: 0.4884\n",
            "Epoch 349/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8193 - acc: 0.6250 - val_loss: 1.2695 - val_acc: 0.4884\n",
            "Epoch 350/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8193 - acc: 0.6250 - val_loss: 1.2694 - val_acc: 0.4884\n",
            "Epoch 351/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8188 - acc: 0.6250 - val_loss: 1.2698 - val_acc: 0.4884\n",
            "Epoch 352/500\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8184 - acc: 0.6250 - val_loss: 1.2696 - val_acc: 0.4884\n",
            "Epoch 353/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8183 - acc: 0.6250 - val_loss: 1.2696 - val_acc: 0.4884\n",
            "Epoch 354/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8179 - acc: 0.6250 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 355/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8177 - acc: 0.6364 - val_loss: 1.2689 - val_acc: 0.4884\n",
            "Epoch 356/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8176 - acc: 0.6364 - val_loss: 1.2693 - val_acc: 0.4884\n",
            "Epoch 357/500\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8167 - acc: 0.6364 - val_loss: 1.2689 - val_acc: 0.4884\n",
            "Epoch 358/500\n",
            "88/88 [==============================] - 0s 179us/step - loss: 0.8165 - acc: 0.6364 - val_loss: 1.2690 - val_acc: 0.4884\n",
            "Epoch 359/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8165 - acc: 0.6364 - val_loss: 1.2688 - val_acc: 0.4884\n",
            "Epoch 360/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8159 - acc: 0.6364 - val_loss: 1.2686 - val_acc: 0.4884\n",
            "Epoch 361/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8158 - acc: 0.6364 - val_loss: 1.2683 - val_acc: 0.4884\n",
            "Epoch 362/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8152 - acc: 0.6364 - val_loss: 1.2683 - val_acc: 0.4884\n",
            "Epoch 363/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8152 - acc: 0.6364 - val_loss: 1.2681 - val_acc: 0.4884\n",
            "Epoch 364/500\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8152 - acc: 0.6364 - val_loss: 1.2681 - val_acc: 0.4884\n",
            "Epoch 365/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8140 - acc: 0.6364 - val_loss: 1.2683 - val_acc: 0.4884\n",
            "Epoch 366/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8142 - acc: 0.6364 - val_loss: 1.2680 - val_acc: 0.4884\n",
            "Epoch 367/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.8143 - acc: 0.6364 - val_loss: 1.2684 - val_acc: 0.4884\n",
            "Epoch 368/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8138 - acc: 0.6364 - val_loss: 1.2681 - val_acc: 0.4884\n",
            "Epoch 369/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8134 - acc: 0.6364 - val_loss: 1.2675 - val_acc: 0.4884\n",
            "Epoch 370/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8132 - acc: 0.6364 - val_loss: 1.2673 - val_acc: 0.4884\n",
            "Epoch 371/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8131 - acc: 0.6364 - val_loss: 1.2684 - val_acc: 0.4884\n",
            "Epoch 372/500\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.8135 - acc: 0.6364 - val_loss: 1.2677 - val_acc: 0.4884\n",
            "Epoch 373/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8121 - acc: 0.6364 - val_loss: 1.2675 - val_acc: 0.4884\n",
            "Epoch 374/500\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8118 - acc: 0.6364 - val_loss: 1.2675 - val_acc: 0.4884\n",
            "Epoch 375/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8118 - acc: 0.6364 - val_loss: 1.2672 - val_acc: 0.4884\n",
            "Epoch 376/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8118 - acc: 0.6364 - val_loss: 1.2676 - val_acc: 0.4884\n",
            "Epoch 377/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.8112 - acc: 0.6364 - val_loss: 1.2673 - val_acc: 0.4884\n",
            "Epoch 378/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8112 - acc: 0.6364 - val_loss: 1.2675 - val_acc: 0.4884\n",
            "Epoch 379/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.8101 - acc: 0.6364 - val_loss: 1.2669 - val_acc: 0.4884\n",
            "Epoch 380/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8102 - acc: 0.6364 - val_loss: 1.2671 - val_acc: 0.4884\n",
            "Epoch 381/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8101 - acc: 0.6364 - val_loss: 1.2669 - val_acc: 0.4884\n",
            "Epoch 382/500\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.8096 - acc: 0.6364 - val_loss: 1.2673 - val_acc: 0.4884\n",
            "Epoch 383/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8091 - acc: 0.6364 - val_loss: 1.2682 - val_acc: 0.4884\n",
            "Epoch 384/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8096 - acc: 0.6364 - val_loss: 1.2682 - val_acc: 0.4884\n",
            "Epoch 385/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 0.8091 - acc: 0.6364 - val_loss: 1.2683 - val_acc: 0.4884\n",
            "Epoch 386/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8088 - acc: 0.6364 - val_loss: 1.2684 - val_acc: 0.4884\n",
            "Epoch 387/500\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8084 - acc: 0.6364 - val_loss: 1.2684 - val_acc: 0.4884\n",
            "Epoch 388/500\n",
            "88/88 [==============================] - 0s 185us/step - loss: 0.8079 - acc: 0.6364 - val_loss: 1.2693 - val_acc: 0.4884\n",
            "Epoch 389/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8079 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 390/500\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8066 - acc: 0.6364 - val_loss: 1.2690 - val_acc: 0.4884\n",
            "Epoch 391/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8065 - acc: 0.6364 - val_loss: 1.2690 - val_acc: 0.4884\n",
            "Epoch 392/500\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8064 - acc: 0.6364 - val_loss: 1.2690 - val_acc: 0.4884\n",
            "Epoch 393/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8063 - acc: 0.6364 - val_loss: 1.2691 - val_acc: 0.4884\n",
            "Epoch 394/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8063 - acc: 0.6364 - val_loss: 1.2691 - val_acc: 0.4884\n",
            "Epoch 395/500\n",
            "88/88 [==============================] - 0s 182us/step - loss: 0.8063 - acc: 0.6364 - val_loss: 1.2691 - val_acc: 0.4884\n",
            "Epoch 396/500\n",
            "88/88 [==============================] - 0s 190us/step - loss: 0.8062 - acc: 0.6364 - val_loss: 1.2691 - val_acc: 0.4884\n",
            "Epoch 397/500\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8062 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 398/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.8062 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 399/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8062 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 400/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8060 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 401/500\n",
            "88/88 [==============================] - 0s 180us/step - loss: 0.8060 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 402/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8060 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 403/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8060 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 404/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8060 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 405/500\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.8060 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 406/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 407/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 408/500\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 409/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 410/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 411/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 412/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 413/500\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 414/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 415/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 416/500\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 417/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 418/500\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 419/500\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 420/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 421/500\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 422/500\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 423/500\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 424/500\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 425/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 426/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 427/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 428/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 429/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 430/500\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 431/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 432/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 433/500\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 434/500\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 435/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 436/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 437/500\n",
            "88/88 [==============================] - 0s 194us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 438/500\n",
            "88/88 [==============================] - 0s 189us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 439/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 440/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 441/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 442/500\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 443/500\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 444/500\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 445/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 446/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 447/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 448/500\n",
            "88/88 [==============================] - 0s 195us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 449/500\n",
            "88/88 [==============================] - 0s 193us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 450/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 451/500\n",
            "88/88 [==============================] - 0s 186us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 452/500\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 453/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 454/500\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 455/500\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 456/500\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 457/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 458/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 459/500\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 460/500\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 461/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 462/500\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 463/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 464/500\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 465/500\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 466/500\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 467/500\n",
            "88/88 [==============================] - 0s 176us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 468/500\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 469/500\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 470/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 471/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 472/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 473/500\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 474/500\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 475/500\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 476/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 477/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 478/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 479/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 480/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 481/500\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 482/500\n",
            "88/88 [==============================] - 0s 198us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 483/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 484/500\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 485/500\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 486/500\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 487/500\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 488/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 489/500\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 490/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 491/500\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 492/500\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 493/500\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 494/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 495/500\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 496/500\n",
            "88/88 [==============================] - 0s 191us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 497/500\n",
            "88/88 [==============================] - 0s 188us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 498/500\n",
            "88/88 [==============================] - 0s 192us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 499/500\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n",
            "Epoch 500/500\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8059 - acc: 0.6364 - val_loss: 1.2692 - val_acc: 0.4884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "62c7acdd-bc2b-4fa3-b638-5d5f4cf4877e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'lr'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "09930320-b401-4b71-834f-827f7ec19fc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "34833ce3-1b04-4a4b-b63b-a446faf32d4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "d4586d2f-52e6-49ea-b8c8-c0ab940dc97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'bo', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'b', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f92064aa080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFHDksxrsz+sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "c3cd7583-b39a-494b-8f9a-0635168e26c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f9205fe3b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noOQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(6, activation='relu', input_shape=(9,)))\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "  sgd = SGD(lr=0.001, decay=1e-6, momentum=0.5, nesterov=True)\n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_NO_kfold.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOg9D5rMhP0WGlLa/ShO4A+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Classification_NO_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i4O_AtKVzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb00g1tWTqw",
        "colab_type": "code",
        "outputId": "5cd6e17b-2b7e-4cd5-f994-066331e93cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "c7a7a987-afb9-48f4-bfd5-9fc9f3410824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "#LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "0bfb6e36-cf6d-4a4a-d66e-3a5096b05c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, y_train).transform(train_data_stand)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "b62dab1a-42b3-4998-bbd1-a644e069b8d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0UwBbCjf13g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = lda.transform(val_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5fsvoQwBqR",
        "colab_type": "text"
      },
      "source": [
        "##Z-score dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYbsLEB_6nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_lda = scaler_2.fit_transform(train_data_stand_lda)\n",
        "val_data_stand_lda = scaler_2.transform(val_data_stand_lda)\n",
        "test_data_stand_lda = scaler_2.transform(test_data_stand_lda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLYdHX-mYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2EW9Sb0FDCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(8, activation='relu', input_shape=(2,), kernel_regularizer=regularizers.l2(l=0.1)))\n",
        "  model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAd2LU_dEO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_sSb3CnLM2",
        "colab_type": "code",
        "outputId": "a32a178e-157f-4c24-804a-3be94f14befb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_val_labels.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hc4rG203_u",
        "colab_type": "code",
        "outputId": "f083737c-3831-4f21-eee1-c59a8b168764",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        }
      },
      "source": [
        "num_epochs = 1500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=105, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-192-3efd298ce86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n\u001b[0m\u001b[1;32m      5\u001b[0m                       epochs= num_epochs, batch_size=105, callbacks=[red_lr])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_stand_lda' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d67e79ff-2e01-44c3-d8a0-fe25d4f0a7b5",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d8da1dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPpfkOnjukgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "16adbec2-638e-486d-dc50-dc2ed9e7f629",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb28fcf5b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBCIiEff/AcMUIAqfJry1AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5d19de46-56f5-4970-e387-6b3f1b67bf74",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=105, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1500\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.4181 - acc: 0.4667\n",
            "Epoch 2/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.4174 - acc: 0.4667\n",
            "Epoch 3/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.4159 - acc: 0.4667\n",
            "Epoch 4/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.4137 - acc: 0.4667\n",
            "Epoch 5/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.4110 - acc: 0.4667\n",
            "Epoch 6/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.4079 - acc: 0.4667\n",
            "Epoch 7/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.4042 - acc: 0.4667\n",
            "Epoch 8/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.4002 - acc: 0.4667\n",
            "Epoch 9/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.3959 - acc: 0.4667\n",
            "Epoch 10/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.3913 - acc: 0.4667\n",
            "Epoch 11/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.3864 - acc: 0.4667\n",
            "Epoch 12/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.3813 - acc: 0.4667\n",
            "Epoch 13/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.3760 - acc: 0.4667\n",
            "Epoch 14/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.3704 - acc: 0.4667\n",
            "Epoch 15/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.3647 - acc: 0.4667\n",
            "Epoch 16/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.3589 - acc: 0.4667\n",
            "Epoch 17/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.3530 - acc: 0.4667\n",
            "Epoch 18/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.3470 - acc: 0.4667\n",
            "Epoch 19/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.3409 - acc: 0.4667\n",
            "Epoch 20/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.3347 - acc: 0.4667\n",
            "Epoch 21/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.3285 - acc: 0.4667\n",
            "Epoch 22/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.3222 - acc: 0.4667\n",
            "Epoch 23/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.3159 - acc: 0.4667\n",
            "Epoch 24/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.3096 - acc: 0.4667\n",
            "Epoch 25/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.3033 - acc: 0.4667\n",
            "Epoch 26/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.2970 - acc: 0.4667\n",
            "Epoch 27/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2907 - acc: 0.4667\n",
            "Epoch 28/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.2845 - acc: 0.4667\n",
            "Epoch 29/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.2782 - acc: 0.4667\n",
            "Epoch 30/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.2720 - acc: 0.4667\n",
            "Epoch 31/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.2658 - acc: 0.4667\n",
            "Epoch 32/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2597 - acc: 0.4667\n",
            "Epoch 33/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.2536 - acc: 0.4667\n",
            "Epoch 34/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2476 - acc: 0.4762\n",
            "Epoch 35/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.2416 - acc: 0.4762\n",
            "Epoch 36/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2356 - acc: 0.4762\n",
            "Epoch 37/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.2297 - acc: 0.4857\n",
            "Epoch 38/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2239 - acc: 0.5048\n",
            "Epoch 39/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2181 - acc: 0.5048\n",
            "Epoch 40/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2124 - acc: 0.5048\n",
            "Epoch 41/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2068 - acc: 0.5048\n",
            "Epoch 42/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.2012 - acc: 0.5048\n",
            "Epoch 43/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1956 - acc: 0.5048\n",
            "Epoch 44/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1901 - acc: 0.5048\n",
            "Epoch 45/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1846 - acc: 0.5048\n",
            "Epoch 46/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1792 - acc: 0.5143\n",
            "Epoch 47/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1738 - acc: 0.5238\n",
            "Epoch 48/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1685 - acc: 0.5238\n",
            "Epoch 49/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1633 - acc: 0.5333\n",
            "Epoch 50/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 1.1580 - acc: 0.5524\n",
            "Epoch 51/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1529 - acc: 0.5619\n",
            "Epoch 52/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1477 - acc: 0.5619\n",
            "Epoch 53/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1427 - acc: 0.5714\n",
            "Epoch 54/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 1.1376 - acc: 0.5619\n",
            "Epoch 55/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1326 - acc: 0.5810\n",
            "Epoch 56/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 1.1277 - acc: 0.6000\n",
            "Epoch 57/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1228 - acc: 0.6190\n",
            "Epoch 58/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1179 - acc: 0.6286\n",
            "Epoch 59/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1131 - acc: 0.6381\n",
            "Epoch 60/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1084 - acc: 0.6952\n",
            "Epoch 61/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.1036 - acc: 0.7048\n",
            "Epoch 62/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.0989 - acc: 0.7429\n",
            "Epoch 63/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0943 - acc: 0.7524\n",
            "Epoch 64/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0897 - acc: 0.7714\n",
            "Epoch 65/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0851 - acc: 0.7905\n",
            "Epoch 66/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0806 - acc: 0.8095\n",
            "Epoch 67/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0761 - acc: 0.8190\n",
            "Epoch 68/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0716 - acc: 0.8190\n",
            "Epoch 69/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0672 - acc: 0.8286\n",
            "Epoch 70/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0628 - acc: 0.8381\n",
            "Epoch 71/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0584 - acc: 0.8571\n",
            "Epoch 72/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0541 - acc: 0.8571\n",
            "Epoch 73/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0498 - acc: 0.8571\n",
            "Epoch 74/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 1.0456 - acc: 0.8667\n",
            "Epoch 75/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0413 - acc: 0.8667\n",
            "Epoch 76/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0372 - acc: 0.8667\n",
            "Epoch 77/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0330 - acc: 0.8667\n",
            "Epoch 78/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.0289 - acc: 0.8667\n",
            "Epoch 79/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0248 - acc: 0.8667\n",
            "Epoch 80/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0208 - acc: 0.8667\n",
            "Epoch 81/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0167 - acc: 0.8667\n",
            "Epoch 82/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0128 - acc: 0.8762\n",
            "Epoch 83/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.0088 - acc: 0.8762\n",
            "Epoch 84/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0049 - acc: 0.8857\n",
            "Epoch 85/1500\n",
            "105/105 [==============================] - 0s 82us/step - loss: 1.0010 - acc: 0.8857\n",
            "Epoch 86/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9971 - acc: 0.8857\n",
            "Epoch 87/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9933 - acc: 0.8857\n",
            "Epoch 88/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9895 - acc: 0.8857\n",
            "Epoch 89/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.9857 - acc: 0.8857\n",
            "Epoch 90/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.9819 - acc: 0.8857\n",
            "Epoch 91/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9782 - acc: 0.8857\n",
            "Epoch 92/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9745 - acc: 0.8857\n",
            "Epoch 93/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9709 - acc: 0.8857\n",
            "Epoch 94/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9672 - acc: 0.8857\n",
            "Epoch 95/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9636 - acc: 0.8857\n",
            "Epoch 96/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9600 - acc: 0.8857\n",
            "Epoch 97/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9565 - acc: 0.8857\n",
            "Epoch 98/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9529 - acc: 0.8857\n",
            "Epoch 99/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.9494 - acc: 0.8857\n",
            "Epoch 100/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.9459 - acc: 0.8857\n",
            "Epoch 101/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9425 - acc: 0.8857\n",
            "Epoch 102/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9391 - acc: 0.8857\n",
            "Epoch 103/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.9356 - acc: 0.8857\n",
            "Epoch 104/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.9323 - acc: 0.8857\n",
            "Epoch 105/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.9289 - acc: 0.8857\n",
            "Epoch 106/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.9256 - acc: 0.8857\n",
            "Epoch 107/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.9222 - acc: 0.8857\n",
            "Epoch 108/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9190 - acc: 0.8857\n",
            "Epoch 109/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9157 - acc: 0.8857\n",
            "Epoch 110/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.9124 - acc: 0.8857\n",
            "Epoch 111/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.9092 - acc: 0.8857\n",
            "Epoch 112/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.9060 - acc: 0.8857\n",
            "Epoch 113/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.9028 - acc: 0.8857\n",
            "Epoch 114/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8997 - acc: 0.8857\n",
            "Epoch 115/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8965 - acc: 0.8857\n",
            "Epoch 116/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8934 - acc: 0.8857\n",
            "Epoch 117/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8903 - acc: 0.8857\n",
            "Epoch 118/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8873 - acc: 0.8857\n",
            "Epoch 119/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8842 - acc: 0.8857\n",
            "Epoch 120/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8812 - acc: 0.8857\n",
            "Epoch 121/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8782 - acc: 0.8857\n",
            "Epoch 122/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8753 - acc: 0.8857\n",
            "Epoch 123/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8723 - acc: 0.8857\n",
            "Epoch 124/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8694 - acc: 0.8857\n",
            "Epoch 125/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8665 - acc: 0.8857\n",
            "Epoch 126/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8636 - acc: 0.8857\n",
            "Epoch 127/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8607 - acc: 0.8857\n",
            "Epoch 128/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8578 - acc: 0.8857\n",
            "Epoch 129/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8550 - acc: 0.8857\n",
            "Epoch 130/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8522 - acc: 0.8857\n",
            "Epoch 131/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8494 - acc: 0.8857\n",
            "Epoch 132/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8467 - acc: 0.8857\n",
            "Epoch 133/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8440 - acc: 0.8857\n",
            "Epoch 134/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8413 - acc: 0.8857\n",
            "Epoch 135/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8386 - acc: 0.8857\n",
            "Epoch 136/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8359 - acc: 0.8857\n",
            "Epoch 137/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8333 - acc: 0.8857\n",
            "Epoch 138/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8307 - acc: 0.8857\n",
            "Epoch 139/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8281 - acc: 0.8857\n",
            "Epoch 140/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8255 - acc: 0.8857\n",
            "Epoch 141/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8229 - acc: 0.8857\n",
            "Epoch 142/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8204 - acc: 0.8857\n",
            "Epoch 143/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.8179 - acc: 0.8857\n",
            "Epoch 144/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8154 - acc: 0.8857\n",
            "Epoch 145/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8129 - acc: 0.8857\n",
            "Epoch 146/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8105 - acc: 0.8857\n",
            "Epoch 147/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8080 - acc: 0.8857\n",
            "Epoch 148/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8056 - acc: 0.8857\n",
            "Epoch 149/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8032 - acc: 0.8857\n",
            "Epoch 150/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8008 - acc: 0.8857\n",
            "Epoch 151/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7984 - acc: 0.8857\n",
            "Epoch 152/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7960 - acc: 0.8857\n",
            "Epoch 153/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7937 - acc: 0.8857\n",
            "Epoch 154/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7913 - acc: 0.8857\n",
            "Epoch 155/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7890 - acc: 0.8857\n",
            "Epoch 156/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7867 - acc: 0.8857\n",
            "Epoch 157/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7845 - acc: 0.8857\n",
            "Epoch 158/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7822 - acc: 0.8857\n",
            "Epoch 159/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7799 - acc: 0.8857\n",
            "Epoch 160/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7777 - acc: 0.8857\n",
            "Epoch 161/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7755 - acc: 0.8857\n",
            "Epoch 162/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7733 - acc: 0.8857\n",
            "Epoch 163/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7711 - acc: 0.8857\n",
            "Epoch 164/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7689 - acc: 0.8857\n",
            "Epoch 165/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7668 - acc: 0.8857\n",
            "Epoch 166/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7646 - acc: 0.8857\n",
            "Epoch 167/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7625 - acc: 0.8857\n",
            "Epoch 168/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7604 - acc: 0.8857\n",
            "Epoch 169/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7583 - acc: 0.8857\n",
            "Epoch 170/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7562 - acc: 0.8857\n",
            "Epoch 171/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7541 - acc: 0.8857\n",
            "Epoch 172/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7520 - acc: 0.8857\n",
            "Epoch 173/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7500 - acc: 0.8857\n",
            "Epoch 174/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7479 - acc: 0.8857\n",
            "Epoch 175/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7459 - acc: 0.8857\n",
            "Epoch 176/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7438 - acc: 0.8857\n",
            "Epoch 177/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7418 - acc: 0.8857\n",
            "Epoch 178/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7398 - acc: 0.8857\n",
            "Epoch 179/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7378 - acc: 0.8857\n",
            "Epoch 180/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7358 - acc: 0.8857\n",
            "Epoch 181/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7338 - acc: 0.8857\n",
            "Epoch 182/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7319 - acc: 0.8857\n",
            "Epoch 183/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7299 - acc: 0.8857\n",
            "Epoch 184/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7279 - acc: 0.8857\n",
            "Epoch 185/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7260 - acc: 0.8857\n",
            "Epoch 186/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7241 - acc: 0.8857\n",
            "Epoch 187/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7222 - acc: 0.8857\n",
            "Epoch 188/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7203 - acc: 0.8857\n",
            "Epoch 189/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7184 - acc: 0.8857\n",
            "Epoch 190/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7165 - acc: 0.8857\n",
            "Epoch 191/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7146 - acc: 0.8857\n",
            "Epoch 192/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7127 - acc: 0.8857\n",
            "Epoch 193/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7109 - acc: 0.8857\n",
            "Epoch 194/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7090 - acc: 0.8857\n",
            "Epoch 195/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7072 - acc: 0.8857\n",
            "Epoch 196/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7053 - acc: 0.8857\n",
            "Epoch 197/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7035 - acc: 0.8857\n",
            "Epoch 198/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7017 - acc: 0.8857\n",
            "Epoch 199/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6999 - acc: 0.8857\n",
            "Epoch 200/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6981 - acc: 0.8857\n",
            "Epoch 201/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6963 - acc: 0.8857\n",
            "Epoch 202/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6945 - acc: 0.8857\n",
            "Epoch 203/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6928 - acc: 0.8857\n",
            "Epoch 204/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6910 - acc: 0.8857\n",
            "Epoch 205/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6892 - acc: 0.8857\n",
            "Epoch 206/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6875 - acc: 0.8857\n",
            "Epoch 207/1500\n",
            "105/105 [==============================] - 0s 88us/step - loss: 0.6857 - acc: 0.8857\n",
            "Epoch 208/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6840 - acc: 0.8857\n",
            "Epoch 209/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6823 - acc: 0.8857\n",
            "Epoch 210/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6806 - acc: 0.8857\n",
            "Epoch 211/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6789 - acc: 0.8857\n",
            "Epoch 212/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6772 - acc: 0.8857\n",
            "Epoch 213/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6755 - acc: 0.8857\n",
            "Epoch 214/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6738 - acc: 0.8857\n",
            "Epoch 215/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6721 - acc: 0.8857\n",
            "Epoch 216/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6705 - acc: 0.8857\n",
            "Epoch 217/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6688 - acc: 0.8857\n",
            "Epoch 218/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6672 - acc: 0.8857\n",
            "Epoch 219/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6655 - acc: 0.8857\n",
            "Epoch 220/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6639 - acc: 0.8857\n",
            "Epoch 221/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6623 - acc: 0.8857\n",
            "Epoch 222/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6606 - acc: 0.8857\n",
            "Epoch 223/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6590 - acc: 0.8857\n",
            "Epoch 224/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6574 - acc: 0.8857\n",
            "Epoch 225/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6558 - acc: 0.8857\n",
            "Epoch 226/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6542 - acc: 0.8857\n",
            "Epoch 227/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6527 - acc: 0.8857\n",
            "Epoch 228/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6511 - acc: 0.8857\n",
            "Epoch 229/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6495 - acc: 0.8857\n",
            "Epoch 230/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6480 - acc: 0.8857\n",
            "Epoch 231/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6464 - acc: 0.8857\n",
            "Epoch 232/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6449 - acc: 0.8857\n",
            "Epoch 233/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6433 - acc: 0.8857\n",
            "Epoch 234/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6418 - acc: 0.8857\n",
            "Epoch 235/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6403 - acc: 0.8857\n",
            "Epoch 236/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6388 - acc: 0.8857\n",
            "Epoch 237/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6372 - acc: 0.8857\n",
            "Epoch 238/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6357 - acc: 0.8857\n",
            "Epoch 239/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6342 - acc: 0.8857\n",
            "Epoch 240/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6328 - acc: 0.8857\n",
            "Epoch 241/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6313 - acc: 0.8857\n",
            "Epoch 242/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6298 - acc: 0.8857\n",
            "Epoch 243/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6283 - acc: 0.8857\n",
            "Epoch 244/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6269 - acc: 0.8857\n",
            "Epoch 245/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6254 - acc: 0.8857\n",
            "Epoch 246/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6240 - acc: 0.8857\n",
            "Epoch 247/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6226 - acc: 0.8857\n",
            "Epoch 248/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6211 - acc: 0.8857\n",
            "Epoch 249/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6197 - acc: 0.8857\n",
            "Epoch 250/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6183 - acc: 0.8857\n",
            "Epoch 251/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6169 - acc: 0.8857\n",
            "Epoch 252/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6155 - acc: 0.8857\n",
            "Epoch 253/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6141 - acc: 0.8857\n",
            "Epoch 254/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.6127 - acc: 0.8857\n",
            "Epoch 255/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6113 - acc: 0.8857\n",
            "Epoch 256/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6099 - acc: 0.8857\n",
            "Epoch 257/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6085 - acc: 0.8857\n",
            "Epoch 258/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6072 - acc: 0.8857\n",
            "Epoch 259/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6058 - acc: 0.8857\n",
            "Epoch 260/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6045 - acc: 0.8857\n",
            "Epoch 261/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6031 - acc: 0.8857\n",
            "Epoch 262/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6018 - acc: 0.8857\n",
            "Epoch 263/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6004 - acc: 0.8857\n",
            "Epoch 264/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5991 - acc: 0.8857\n",
            "Epoch 265/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5978 - acc: 0.8857\n",
            "Epoch 266/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5965 - acc: 0.8857\n",
            "Epoch 267/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5952 - acc: 0.8857\n",
            "Epoch 268/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5939 - acc: 0.8857\n",
            "Epoch 269/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5926 - acc: 0.8857\n",
            "Epoch 270/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5913 - acc: 0.8857\n",
            "Epoch 271/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5900 - acc: 0.8857\n",
            "Epoch 272/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5887 - acc: 0.8857\n",
            "Epoch 273/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5874 - acc: 0.8857\n",
            "Epoch 274/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5862 - acc: 0.8857\n",
            "Epoch 275/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5849 - acc: 0.8857\n",
            "Epoch 276/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5837 - acc: 0.8857\n",
            "Epoch 277/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5824 - acc: 0.8857\n",
            "Epoch 278/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5812 - acc: 0.8857\n",
            "Epoch 279/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5799 - acc: 0.8857\n",
            "Epoch 280/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5787 - acc: 0.8857\n",
            "Epoch 281/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5775 - acc: 0.8857\n",
            "Epoch 282/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5762 - acc: 0.8857\n",
            "Epoch 283/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5750 - acc: 0.8857\n",
            "Epoch 284/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5738 - acc: 0.8857\n",
            "Epoch 285/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5726 - acc: 0.8857\n",
            "Epoch 286/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5714 - acc: 0.8857\n",
            "Epoch 287/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5702 - acc: 0.8857\n",
            "Epoch 288/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5690 - acc: 0.8857\n",
            "Epoch 289/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5678 - acc: 0.8857\n",
            "Epoch 290/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5667 - acc: 0.8857\n",
            "Epoch 291/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5655 - acc: 0.8857\n",
            "Epoch 292/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5643 - acc: 0.8857\n",
            "Epoch 293/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5632 - acc: 0.8857\n",
            "Epoch 294/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5620 - acc: 0.8857\n",
            "Epoch 295/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5608 - acc: 0.8857\n",
            "Epoch 296/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5597 - acc: 0.8857\n",
            "Epoch 297/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5586 - acc: 0.8857\n",
            "Epoch 298/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5574 - acc: 0.8857\n",
            "Epoch 299/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5563 - acc: 0.8857\n",
            "Epoch 300/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5552 - acc: 0.8857\n",
            "Epoch 301/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5540 - acc: 0.8857\n",
            "Epoch 302/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5529 - acc: 0.8857\n",
            "Epoch 303/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5518 - acc: 0.8857\n",
            "Epoch 304/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5507 - acc: 0.8857\n",
            "Epoch 305/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5496 - acc: 0.8857\n",
            "Epoch 306/1500\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.5485 - acc: 0.8857\n",
            "Epoch 307/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5474 - acc: 0.8857\n",
            "Epoch 308/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5463 - acc: 0.8857\n",
            "Epoch 309/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5453 - acc: 0.8857\n",
            "Epoch 310/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5442 - acc: 0.8857\n",
            "Epoch 311/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5431 - acc: 0.8857\n",
            "Epoch 312/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5420 - acc: 0.8857\n",
            "Epoch 313/1500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5410 - acc: 0.8857\n",
            "Epoch 314/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5399 - acc: 0.8857\n",
            "Epoch 315/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5389 - acc: 0.8857\n",
            "Epoch 316/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5378 - acc: 0.8857\n",
            "Epoch 317/1500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5368 - acc: 0.8857\n",
            "Epoch 318/1500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5358 - acc: 0.8857\n",
            "Epoch 319/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5347 - acc: 0.8857\n",
            "Epoch 320/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5337 - acc: 0.8857\n",
            "Epoch 321/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5327 - acc: 0.8857\n",
            "Epoch 322/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5317 - acc: 0.8857\n",
            "Epoch 323/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5307 - acc: 0.8857\n",
            "Epoch 324/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5296 - acc: 0.8857\n",
            "Epoch 325/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5286 - acc: 0.8857\n",
            "Epoch 326/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5276 - acc: 0.8857\n",
            "Epoch 327/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5267 - acc: 0.8857\n",
            "Epoch 328/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5257 - acc: 0.8857\n",
            "Epoch 329/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5247 - acc: 0.8857\n",
            "Epoch 330/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5237 - acc: 0.8857\n",
            "Epoch 331/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5227 - acc: 0.8857\n",
            "Epoch 332/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5217 - acc: 0.8857\n",
            "Epoch 333/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5208 - acc: 0.8857\n",
            "Epoch 334/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5198 - acc: 0.8857\n",
            "Epoch 335/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5189 - acc: 0.8857\n",
            "Epoch 336/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5179 - acc: 0.8857\n",
            "Epoch 337/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5170 - acc: 0.8857\n",
            "Epoch 338/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5160 - acc: 0.8857\n",
            "Epoch 339/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5151 - acc: 0.8857\n",
            "Epoch 340/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5141 - acc: 0.8857\n",
            "Epoch 341/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5132 - acc: 0.8857\n",
            "Epoch 342/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5123 - acc: 0.8857\n",
            "Epoch 343/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5113 - acc: 0.8857\n",
            "Epoch 344/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5104 - acc: 0.8857\n",
            "Epoch 345/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5095 - acc: 0.8857\n",
            "Epoch 346/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5086 - acc: 0.8857\n",
            "Epoch 347/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5077 - acc: 0.8857\n",
            "Epoch 348/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5068 - acc: 0.8857\n",
            "Epoch 349/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5059 - acc: 0.8857\n",
            "Epoch 350/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5050 - acc: 0.8857\n",
            "Epoch 351/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5041 - acc: 0.8857\n",
            "Epoch 352/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5032 - acc: 0.8857\n",
            "Epoch 353/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5023 - acc: 0.8857\n",
            "Epoch 354/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5014 - acc: 0.8857\n",
            "Epoch 355/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5005 - acc: 0.8857\n",
            "Epoch 356/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4996 - acc: 0.8857\n",
            "Epoch 357/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4988 - acc: 0.8857\n",
            "Epoch 358/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4979 - acc: 0.8857\n",
            "Epoch 359/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4970 - acc: 0.8857\n",
            "Epoch 360/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4962 - acc: 0.8857\n",
            "Epoch 361/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4953 - acc: 0.8857\n",
            "Epoch 362/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4944 - acc: 0.8857\n",
            "Epoch 363/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4936 - acc: 0.8857\n",
            "Epoch 364/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4927 - acc: 0.8857\n",
            "Epoch 365/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4919 - acc: 0.8857\n",
            "Epoch 366/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4911 - acc: 0.8857\n",
            "Epoch 367/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4902 - acc: 0.8857\n",
            "Epoch 368/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4894 - acc: 0.8857\n",
            "Epoch 369/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4886 - acc: 0.8857\n",
            "Epoch 370/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4877 - acc: 0.8857\n",
            "Epoch 371/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4869 - acc: 0.8857\n",
            "Epoch 372/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4861 - acc: 0.8857\n",
            "Epoch 373/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4853 - acc: 0.8857\n",
            "Epoch 374/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4845 - acc: 0.8857\n",
            "Epoch 375/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4837 - acc: 0.8857\n",
            "Epoch 376/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4829 - acc: 0.8857\n",
            "Epoch 377/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4821 - acc: 0.8857\n",
            "Epoch 378/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4813 - acc: 0.8857\n",
            "Epoch 379/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4805 - acc: 0.8857\n",
            "Epoch 380/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4797 - acc: 0.8857\n",
            "Epoch 381/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4789 - acc: 0.8857\n",
            "Epoch 382/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4781 - acc: 0.8857\n",
            "Epoch 383/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4773 - acc: 0.8857\n",
            "Epoch 384/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4765 - acc: 0.8857\n",
            "Epoch 385/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4758 - acc: 0.8857\n",
            "Epoch 386/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4750 - acc: 0.8857\n",
            "Epoch 387/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4742 - acc: 0.8857\n",
            "Epoch 388/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4734 - acc: 0.8857\n",
            "Epoch 389/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4727 - acc: 0.8857\n",
            "Epoch 390/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4719 - acc: 0.8857\n",
            "Epoch 391/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4712 - acc: 0.8857\n",
            "Epoch 392/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4704 - acc: 0.8857\n",
            "Epoch 393/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4696 - acc: 0.8857\n",
            "Epoch 394/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.4689 - acc: 0.8857\n",
            "Epoch 395/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4681 - acc: 0.8857\n",
            "Epoch 396/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4674 - acc: 0.8857\n",
            "Epoch 397/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4666 - acc: 0.8857\n",
            "Epoch 398/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4659 - acc: 0.8857\n",
            "Epoch 399/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4652 - acc: 0.8857\n",
            "Epoch 400/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4644 - acc: 0.8857\n",
            "Epoch 401/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4637 - acc: 0.8857\n",
            "Epoch 402/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4630 - acc: 0.8857\n",
            "Epoch 403/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4622 - acc: 0.8857\n",
            "Epoch 404/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4615 - acc: 0.8857\n",
            "Epoch 405/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4608 - acc: 0.8857\n",
            "Epoch 406/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4600 - acc: 0.8857\n",
            "Epoch 407/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4593 - acc: 0.8857\n",
            "Epoch 408/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4586 - acc: 0.8857\n",
            "Epoch 409/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4579 - acc: 0.8857\n",
            "Epoch 410/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4572 - acc: 0.8857\n",
            "Epoch 411/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4565 - acc: 0.8857\n",
            "Epoch 412/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4557 - acc: 0.8857\n",
            "Epoch 413/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4550 - acc: 0.8857\n",
            "Epoch 414/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4543 - acc: 0.8857\n",
            "Epoch 415/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4536 - acc: 0.8857\n",
            "Epoch 416/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4529 - acc: 0.8857\n",
            "Epoch 417/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4522 - acc: 0.8857\n",
            "Epoch 418/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4516 - acc: 0.8857\n",
            "Epoch 419/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.4509 - acc: 0.8857\n",
            "Epoch 420/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4502 - acc: 0.8857\n",
            "Epoch 421/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4495 - acc: 0.8857\n",
            "Epoch 422/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4488 - acc: 0.8857\n",
            "Epoch 423/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4481 - acc: 0.8857\n",
            "Epoch 424/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4475 - acc: 0.8857\n",
            "Epoch 425/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4468 - acc: 0.8857\n",
            "Epoch 426/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4461 - acc: 0.8857\n",
            "Epoch 427/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4454 - acc: 0.8857\n",
            "Epoch 428/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4448 - acc: 0.8857\n",
            "Epoch 429/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4441 - acc: 0.8857\n",
            "Epoch 430/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4434 - acc: 0.8857\n",
            "Epoch 431/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4428 - acc: 0.8857\n",
            "Epoch 432/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4421 - acc: 0.8857\n",
            "Epoch 433/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4414 - acc: 0.8857\n",
            "Epoch 434/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4408 - acc: 0.8857\n",
            "Epoch 435/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4401 - acc: 0.8857\n",
            "Epoch 436/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4395 - acc: 0.8857\n",
            "Epoch 437/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4388 - acc: 0.8857\n",
            "Epoch 438/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4382 - acc: 0.8857\n",
            "Epoch 439/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4375 - acc: 0.8857\n",
            "Epoch 440/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4369 - acc: 0.8857\n",
            "Epoch 441/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4362 - acc: 0.8857\n",
            "Epoch 442/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4356 - acc: 0.8857\n",
            "Epoch 443/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4350 - acc: 0.8857\n",
            "Epoch 444/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4344 - acc: 0.8857\n",
            "Epoch 445/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4337 - acc: 0.8857\n",
            "Epoch 446/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.4331 - acc: 0.8857\n",
            "Epoch 447/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4325 - acc: 0.8857\n",
            "Epoch 448/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4319 - acc: 0.8857\n",
            "Epoch 449/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4313 - acc: 0.8857\n",
            "Epoch 450/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4306 - acc: 0.8857\n",
            "Epoch 451/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4300 - acc: 0.8857\n",
            "Epoch 452/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4294 - acc: 0.8857\n",
            "Epoch 453/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4288 - acc: 0.8857\n",
            "Epoch 454/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4282 - acc: 0.8857\n",
            "Epoch 455/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4276 - acc: 0.8857\n",
            "Epoch 456/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4270 - acc: 0.8857\n",
            "Epoch 457/1500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4264 - acc: 0.8857\n",
            "Epoch 458/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4259 - acc: 0.8857\n",
            "Epoch 459/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4253 - acc: 0.8857\n",
            "Epoch 460/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4247 - acc: 0.8857\n",
            "Epoch 461/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4241 - acc: 0.8857\n",
            "Epoch 462/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.4235 - acc: 0.8857\n",
            "Epoch 463/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4229 - acc: 0.8857\n",
            "Epoch 464/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4223 - acc: 0.8857\n",
            "Epoch 465/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4217 - acc: 0.8857\n",
            "Epoch 466/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4212 - acc: 0.8857\n",
            "Epoch 467/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4206 - acc: 0.8857\n",
            "Epoch 468/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4200 - acc: 0.8857\n",
            "Epoch 469/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.4194 - acc: 0.8857\n",
            "Epoch 470/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4189 - acc: 0.8857\n",
            "Epoch 471/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4183 - acc: 0.8857\n",
            "Epoch 472/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4177 - acc: 0.8857\n",
            "Epoch 473/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4172 - acc: 0.8857\n",
            "Epoch 474/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4166 - acc: 0.8857\n",
            "Epoch 475/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4161 - acc: 0.8857\n",
            "Epoch 476/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.4155 - acc: 0.8857\n",
            "Epoch 477/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.4150 - acc: 0.8857\n",
            "Epoch 478/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4144 - acc: 0.8857\n",
            "Epoch 479/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.4139 - acc: 0.8857\n",
            "Epoch 480/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4133 - acc: 0.8857\n",
            "Epoch 481/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4128 - acc: 0.8857\n",
            "Epoch 482/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4122 - acc: 0.8857\n",
            "Epoch 483/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4117 - acc: 0.8857\n",
            "Epoch 484/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.4112 - acc: 0.8857\n",
            "Epoch 485/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4106 - acc: 0.8857\n",
            "Epoch 486/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4101 - acc: 0.8857\n",
            "Epoch 487/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4096 - acc: 0.8857\n",
            "Epoch 488/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4090 - acc: 0.8857\n",
            "Epoch 489/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4085 - acc: 0.8857\n",
            "Epoch 490/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4080 - acc: 0.8857\n",
            "Epoch 491/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4075 - acc: 0.8857\n",
            "Epoch 492/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4069 - acc: 0.8857\n",
            "Epoch 493/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4064 - acc: 0.8857\n",
            "Epoch 494/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4059 - acc: 0.8857\n",
            "Epoch 495/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4054 - acc: 0.8857\n",
            "Epoch 496/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4048 - acc: 0.8857\n",
            "Epoch 497/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4043 - acc: 0.8857\n",
            "Epoch 498/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4038 - acc: 0.8857\n",
            "Epoch 499/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4033 - acc: 0.8857\n",
            "Epoch 500/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4028 - acc: 0.8857\n",
            "Epoch 501/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4023 - acc: 0.8857\n",
            "Epoch 502/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.4018 - acc: 0.8857\n",
            "Epoch 503/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.4013 - acc: 0.8857\n",
            "Epoch 504/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4008 - acc: 0.8857\n",
            "Epoch 505/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.4003 - acc: 0.8857\n",
            "Epoch 506/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3998 - acc: 0.8857\n",
            "Epoch 507/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3993 - acc: 0.8857\n",
            "Epoch 508/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3988 - acc: 0.8857\n",
            "Epoch 509/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3983 - acc: 0.8857\n",
            "Epoch 510/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3978 - acc: 0.8857\n",
            "Epoch 511/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3973 - acc: 0.8857\n",
            "Epoch 512/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3968 - acc: 0.8857\n",
            "Epoch 513/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3963 - acc: 0.8857\n",
            "Epoch 514/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3959 - acc: 0.8857\n",
            "Epoch 515/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3954 - acc: 0.8857\n",
            "Epoch 516/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3949 - acc: 0.8857\n",
            "Epoch 517/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3945 - acc: 0.8857\n",
            "Epoch 518/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3940 - acc: 0.8857\n",
            "Epoch 519/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3935 - acc: 0.8857\n",
            "Epoch 520/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3931 - acc: 0.8857\n",
            "Epoch 521/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3926 - acc: 0.8857\n",
            "Epoch 522/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3921 - acc: 0.8857\n",
            "Epoch 523/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3917 - acc: 0.8857\n",
            "Epoch 524/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3912 - acc: 0.8857\n",
            "Epoch 525/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3908 - acc: 0.8857\n",
            "Epoch 526/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3903 - acc: 0.8857\n",
            "Epoch 527/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3899 - acc: 0.8857\n",
            "Epoch 528/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3894 - acc: 0.8857\n",
            "Epoch 529/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3890 - acc: 0.8857\n",
            "Epoch 530/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3885 - acc: 0.8857\n",
            "Epoch 531/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3881 - acc: 0.8857\n",
            "Epoch 532/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3876 - acc: 0.8857\n",
            "Epoch 533/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3872 - acc: 0.8857\n",
            "Epoch 534/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3868 - acc: 0.8857\n",
            "Epoch 535/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3863 - acc: 0.8857\n",
            "Epoch 536/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3859 - acc: 0.8857\n",
            "Epoch 537/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.3855 - acc: 0.8857\n",
            "Epoch 538/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3850 - acc: 0.8857\n",
            "Epoch 539/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3846 - acc: 0.8857\n",
            "Epoch 540/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3842 - acc: 0.8857\n",
            "Epoch 541/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3837 - acc: 0.8857\n",
            "Epoch 542/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3833 - acc: 0.8857\n",
            "Epoch 543/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3829 - acc: 0.8857\n",
            "Epoch 544/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3825 - acc: 0.8857\n",
            "Epoch 545/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3820 - acc: 0.8857\n",
            "Epoch 546/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3816 - acc: 0.8857\n",
            "Epoch 547/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3812 - acc: 0.8857\n",
            "Epoch 548/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3808 - acc: 0.8857\n",
            "Epoch 549/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3804 - acc: 0.8857\n",
            "Epoch 550/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3800 - acc: 0.8857\n",
            "Epoch 551/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3796 - acc: 0.8857\n",
            "Epoch 552/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3791 - acc: 0.8857\n",
            "Epoch 553/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3787 - acc: 0.8857\n",
            "Epoch 554/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3783 - acc: 0.8857\n",
            "Epoch 555/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3779 - acc: 0.8857\n",
            "Epoch 556/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3775 - acc: 0.8857\n",
            "Epoch 557/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3771 - acc: 0.8857\n",
            "Epoch 558/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3767 - acc: 0.8857\n",
            "Epoch 559/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3763 - acc: 0.8857\n",
            "Epoch 560/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3759 - acc: 0.8857\n",
            "Epoch 561/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3755 - acc: 0.8857\n",
            "Epoch 562/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3752 - acc: 0.8857\n",
            "Epoch 563/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3748 - acc: 0.8857\n",
            "Epoch 564/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3744 - acc: 0.8857\n",
            "Epoch 565/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3740 - acc: 0.8857\n",
            "Epoch 566/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3736 - acc: 0.8857\n",
            "Epoch 567/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3732 - acc: 0.8857\n",
            "Epoch 568/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3729 - acc: 0.8857\n",
            "Epoch 569/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3725 - acc: 0.8857\n",
            "Epoch 570/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3721 - acc: 0.8857\n",
            "Epoch 571/1500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3717 - acc: 0.8857\n",
            "Epoch 572/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3714 - acc: 0.8857\n",
            "Epoch 573/1500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3710 - acc: 0.8857\n",
            "Epoch 574/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3706 - acc: 0.8857\n",
            "Epoch 575/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3703 - acc: 0.8857\n",
            "Epoch 576/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3699 - acc: 0.8857\n",
            "Epoch 577/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3695 - acc: 0.8857\n",
            "Epoch 578/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3692 - acc: 0.8857\n",
            "Epoch 579/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3688 - acc: 0.8857\n",
            "Epoch 580/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3685 - acc: 0.8857\n",
            "Epoch 581/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3681 - acc: 0.8857\n",
            "Epoch 582/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3677 - acc: 0.8857\n",
            "Epoch 583/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3674 - acc: 0.8857\n",
            "Epoch 584/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3670 - acc: 0.8857\n",
            "Epoch 585/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3667 - acc: 0.8857\n",
            "Epoch 586/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3663 - acc: 0.8857\n",
            "Epoch 587/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3660 - acc: 0.8857\n",
            "Epoch 588/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3656 - acc: 0.8857\n",
            "Epoch 589/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3653 - acc: 0.8857\n",
            "Epoch 590/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3649 - acc: 0.8857\n",
            "Epoch 591/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3646 - acc: 0.8857\n",
            "Epoch 592/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3642 - acc: 0.8857\n",
            "Epoch 593/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3639 - acc: 0.8857\n",
            "Epoch 594/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3636 - acc: 0.8857\n",
            "Epoch 595/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3632 - acc: 0.8857\n",
            "Epoch 596/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3629 - acc: 0.8857\n",
            "Epoch 597/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3625 - acc: 0.8857\n",
            "Epoch 598/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3622 - acc: 0.8857\n",
            "Epoch 599/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3619 - acc: 0.8857\n",
            "Epoch 600/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3615 - acc: 0.8857\n",
            "Epoch 601/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3612 - acc: 0.8857\n",
            "Epoch 602/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3609 - acc: 0.8857\n",
            "Epoch 603/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3605 - acc: 0.8857\n",
            "Epoch 604/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3602 - acc: 0.8857\n",
            "Epoch 605/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3599 - acc: 0.8857\n",
            "Epoch 606/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3596 - acc: 0.8857\n",
            "Epoch 607/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3592 - acc: 0.8857\n",
            "Epoch 608/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3589 - acc: 0.8857\n",
            "Epoch 609/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3586 - acc: 0.8857\n",
            "Epoch 610/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3583 - acc: 0.8857\n",
            "Epoch 611/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3580 - acc: 0.8857\n",
            "Epoch 612/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3576 - acc: 0.8857\n",
            "Epoch 613/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3573 - acc: 0.8857\n",
            "Epoch 614/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3570 - acc: 0.8857\n",
            "Epoch 615/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3567 - acc: 0.8857\n",
            "Epoch 616/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3564 - acc: 0.8857\n",
            "Epoch 617/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3561 - acc: 0.8857\n",
            "Epoch 618/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3558 - acc: 0.8857\n",
            "Epoch 619/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3555 - acc: 0.8857\n",
            "Epoch 620/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3552 - acc: 0.8857\n",
            "Epoch 621/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3549 - acc: 0.8857\n",
            "Epoch 622/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3545 - acc: 0.8857\n",
            "Epoch 623/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3542 - acc: 0.8857\n",
            "Epoch 624/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3539 - acc: 0.8857\n",
            "Epoch 625/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3536 - acc: 0.8857\n",
            "Epoch 626/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3533 - acc: 0.8857\n",
            "Epoch 627/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3530 - acc: 0.8857\n",
            "Epoch 628/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3527 - acc: 0.8857\n",
            "Epoch 629/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3524 - acc: 0.8857\n",
            "Epoch 630/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3521 - acc: 0.8857\n",
            "Epoch 631/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3519 - acc: 0.8857\n",
            "Epoch 632/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3516 - acc: 0.8857\n",
            "Epoch 633/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3513 - acc: 0.8857\n",
            "Epoch 634/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3510 - acc: 0.8857\n",
            "Epoch 635/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3507 - acc: 0.8857\n",
            "Epoch 636/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3504 - acc: 0.8857\n",
            "Epoch 637/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3501 - acc: 0.8857\n",
            "Epoch 638/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3498 - acc: 0.8857\n",
            "Epoch 639/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3495 - acc: 0.8857\n",
            "Epoch 640/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3492 - acc: 0.8857\n",
            "Epoch 641/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3490 - acc: 0.8857\n",
            "Epoch 642/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3487 - acc: 0.8857\n",
            "Epoch 643/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3484 - acc: 0.8857\n",
            "Epoch 644/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3481 - acc: 0.8857\n",
            "Epoch 645/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3478 - acc: 0.8857\n",
            "Epoch 646/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3476 - acc: 0.8857\n",
            "Epoch 647/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3473 - acc: 0.8857\n",
            "Epoch 648/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3470 - acc: 0.8857\n",
            "Epoch 649/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3467 - acc: 0.8857\n",
            "Epoch 650/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3464 - acc: 0.8857\n",
            "Epoch 651/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3462 - acc: 0.8857\n",
            "Epoch 652/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3459 - acc: 0.8857\n",
            "Epoch 653/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3456 - acc: 0.8857\n",
            "Epoch 654/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3454 - acc: 0.8857\n",
            "Epoch 655/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3451 - acc: 0.8857\n",
            "Epoch 656/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3448 - acc: 0.8857\n",
            "Epoch 657/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3446 - acc: 0.8857\n",
            "Epoch 658/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3443 - acc: 0.8857\n",
            "Epoch 659/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3440 - acc: 0.8857\n",
            "Epoch 660/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3438 - acc: 0.8857\n",
            "Epoch 661/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3435 - acc: 0.8857\n",
            "Epoch 662/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3432 - acc: 0.8857\n",
            "Epoch 663/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3430 - acc: 0.8857\n",
            "Epoch 664/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3427 - acc: 0.8857\n",
            "Epoch 665/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3425 - acc: 0.8857\n",
            "Epoch 666/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3422 - acc: 0.8857\n",
            "Epoch 667/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3419 - acc: 0.8857\n",
            "Epoch 668/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3417 - acc: 0.8857\n",
            "Epoch 669/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3414 - acc: 0.8857\n",
            "Epoch 670/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3412 - acc: 0.8857\n",
            "Epoch 671/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3409 - acc: 0.8857\n",
            "Epoch 672/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3407 - acc: 0.8857\n",
            "Epoch 673/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3404 - acc: 0.8857\n",
            "Epoch 674/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3402 - acc: 0.8857\n",
            "Epoch 675/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3399 - acc: 0.8857\n",
            "Epoch 676/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3397 - acc: 0.8857\n",
            "Epoch 677/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3394 - acc: 0.8857\n",
            "Epoch 678/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3392 - acc: 0.8857\n",
            "Epoch 679/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3389 - acc: 0.8857\n",
            "Epoch 680/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3387 - acc: 0.8857\n",
            "Epoch 681/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3384 - acc: 0.8857\n",
            "Epoch 682/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3382 - acc: 0.8857\n",
            "Epoch 683/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3379 - acc: 0.8857\n",
            "Epoch 684/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3377 - acc: 0.8857\n",
            "Epoch 685/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3374 - acc: 0.8857\n",
            "Epoch 686/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3372 - acc: 0.8857\n",
            "Epoch 687/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3369 - acc: 0.8857\n",
            "Epoch 688/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3367 - acc: 0.8857\n",
            "Epoch 689/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3365 - acc: 0.8857\n",
            "Epoch 690/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3362 - acc: 0.8857\n",
            "Epoch 691/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3360 - acc: 0.8857\n",
            "Epoch 692/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3357 - acc: 0.8857\n",
            "Epoch 693/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3355 - acc: 0.8857\n",
            "Epoch 694/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3353 - acc: 0.8857\n",
            "Epoch 695/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3350 - acc: 0.8857\n",
            "Epoch 696/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3348 - acc: 0.8857\n",
            "Epoch 697/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3345 - acc: 0.8857\n",
            "Epoch 698/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3343 - acc: 0.8857\n",
            "Epoch 699/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3341 - acc: 0.8857\n",
            "Epoch 700/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3338 - acc: 0.8857\n",
            "Epoch 701/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3336 - acc: 0.8857\n",
            "Epoch 702/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3334 - acc: 0.8857\n",
            "Epoch 703/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3331 - acc: 0.8857\n",
            "Epoch 704/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3329 - acc: 0.8857\n",
            "Epoch 705/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3327 - acc: 0.8857\n",
            "Epoch 706/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3325 - acc: 0.8857\n",
            "Epoch 707/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3322 - acc: 0.8857\n",
            "Epoch 708/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3320 - acc: 0.8857\n",
            "Epoch 709/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3318 - acc: 0.8857\n",
            "Epoch 710/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3315 - acc: 0.8857\n",
            "Epoch 711/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3313 - acc: 0.8857\n",
            "Epoch 712/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3311 - acc: 0.8857\n",
            "Epoch 713/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3309 - acc: 0.8857\n",
            "Epoch 714/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3306 - acc: 0.8857\n",
            "Epoch 715/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3304 - acc: 0.8857\n",
            "Epoch 716/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3302 - acc: 0.8857\n",
            "Epoch 717/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3300 - acc: 0.8857\n",
            "Epoch 718/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3298 - acc: 0.8857\n",
            "Epoch 719/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3295 - acc: 0.8857\n",
            "Epoch 720/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3293 - acc: 0.8857\n",
            "Epoch 721/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3291 - acc: 0.8857\n",
            "Epoch 722/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3289 - acc: 0.8857\n",
            "Epoch 723/1500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3287 - acc: 0.8857\n",
            "Epoch 724/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3284 - acc: 0.8857\n",
            "Epoch 725/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3282 - acc: 0.8857\n",
            "Epoch 726/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3280 - acc: 0.8857\n",
            "Epoch 727/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3278 - acc: 0.8857\n",
            "Epoch 728/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3276 - acc: 0.8857\n",
            "Epoch 729/1500\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3274 - acc: 0.8857\n",
            "Epoch 730/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3271 - acc: 0.8857\n",
            "Epoch 731/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3269 - acc: 0.8857\n",
            "Epoch 732/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3267 - acc: 0.8857\n",
            "Epoch 733/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3265 - acc: 0.8857\n",
            "Epoch 734/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3263 - acc: 0.8857\n",
            "Epoch 735/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3261 - acc: 0.8857\n",
            "Epoch 736/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3259 - acc: 0.8857\n",
            "Epoch 737/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3257 - acc: 0.8857\n",
            "Epoch 738/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3255 - acc: 0.8857\n",
            "Epoch 739/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3252 - acc: 0.8857\n",
            "Epoch 740/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3250 - acc: 0.8857\n",
            "Epoch 741/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3248 - acc: 0.8857\n",
            "Epoch 742/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3246 - acc: 0.8857\n",
            "Epoch 743/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3244 - acc: 0.8857\n",
            "Epoch 744/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3242 - acc: 0.8857\n",
            "Epoch 745/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3240 - acc: 0.8857\n",
            "Epoch 746/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3238 - acc: 0.8857\n",
            "Epoch 747/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3236 - acc: 0.8857\n",
            "Epoch 748/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3234 - acc: 0.8857\n",
            "Epoch 749/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3232 - acc: 0.8857\n",
            "Epoch 750/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3230 - acc: 0.8857\n",
            "Epoch 751/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3228 - acc: 0.8857\n",
            "Epoch 752/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3226 - acc: 0.8857\n",
            "Epoch 753/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3224 - acc: 0.8857\n",
            "Epoch 754/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3222 - acc: 0.8857\n",
            "Epoch 755/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3220 - acc: 0.8857\n",
            "Epoch 756/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3218 - acc: 0.8857\n",
            "Epoch 757/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3216 - acc: 0.8857\n",
            "Epoch 758/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3214 - acc: 0.8857\n",
            "Epoch 759/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3212 - acc: 0.8857\n",
            "Epoch 760/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3210 - acc: 0.8857\n",
            "Epoch 761/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3208 - acc: 0.8857\n",
            "Epoch 762/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3206 - acc: 0.8857\n",
            "Epoch 763/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3204 - acc: 0.8857\n",
            "Epoch 764/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3202 - acc: 0.8857\n",
            "Epoch 765/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3200 - acc: 0.8857\n",
            "Epoch 766/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3198 - acc: 0.8857\n",
            "Epoch 767/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3196 - acc: 0.8857\n",
            "Epoch 768/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3194 - acc: 0.8857\n",
            "Epoch 769/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3192 - acc: 0.8857\n",
            "Epoch 770/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3190 - acc: 0.8857\n",
            "Epoch 771/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3188 - acc: 0.8857\n",
            "Epoch 772/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3186 - acc: 0.8857\n",
            "Epoch 773/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3184 - acc: 0.8857\n",
            "Epoch 774/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3182 - acc: 0.8857\n",
            "Epoch 775/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3180 - acc: 0.8857\n",
            "Epoch 776/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3179 - acc: 0.8857\n",
            "Epoch 777/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3177 - acc: 0.8857\n",
            "Epoch 778/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3175 - acc: 0.8857\n",
            "Epoch 779/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3173 - acc: 0.8857\n",
            "Epoch 780/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3171 - acc: 0.8857\n",
            "Epoch 781/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3169 - acc: 0.8857\n",
            "Epoch 782/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3167 - acc: 0.8857\n",
            "Epoch 783/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3165 - acc: 0.8857\n",
            "Epoch 784/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3163 - acc: 0.8857\n",
            "Epoch 785/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3162 - acc: 0.8857\n",
            "Epoch 786/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3160 - acc: 0.8857\n",
            "Epoch 787/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3158 - acc: 0.8857\n",
            "Epoch 788/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3156 - acc: 0.8857\n",
            "Epoch 789/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3154 - acc: 0.8857\n",
            "Epoch 790/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3152 - acc: 0.8857\n",
            "Epoch 791/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3150 - acc: 0.8857\n",
            "Epoch 792/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3149 - acc: 0.8857\n",
            "Epoch 793/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3147 - acc: 0.8857\n",
            "Epoch 794/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3145 - acc: 0.8857\n",
            "Epoch 795/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3143 - acc: 0.8857\n",
            "Epoch 796/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3141 - acc: 0.8857\n",
            "Epoch 797/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3139 - acc: 0.8857\n",
            "Epoch 798/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3138 - acc: 0.8857\n",
            "Epoch 799/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3136 - acc: 0.8857\n",
            "Epoch 800/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3134 - acc: 0.8857\n",
            "Epoch 801/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3132 - acc: 0.8857\n",
            "Epoch 802/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3130 - acc: 0.8857\n",
            "Epoch 803/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3128 - acc: 0.8857\n",
            "Epoch 804/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3127 - acc: 0.8857\n",
            "Epoch 805/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3125 - acc: 0.8857\n",
            "Epoch 806/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3123 - acc: 0.8857\n",
            "Epoch 807/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3121 - acc: 0.8857\n",
            "Epoch 808/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3119 - acc: 0.8857\n",
            "Epoch 809/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3118 - acc: 0.8857\n",
            "Epoch 810/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3116 - acc: 0.8857\n",
            "Epoch 811/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3114 - acc: 0.8857\n",
            "Epoch 812/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3112 - acc: 0.8857\n",
            "Epoch 813/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3111 - acc: 0.8857\n",
            "Epoch 814/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3109 - acc: 0.8857\n",
            "Epoch 815/1500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3107 - acc: 0.8857\n",
            "Epoch 816/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3105 - acc: 0.8857\n",
            "Epoch 817/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3103 - acc: 0.8857\n",
            "Epoch 818/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3102 - acc: 0.8857\n",
            "Epoch 819/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3100 - acc: 0.8857\n",
            "Epoch 820/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3098 - acc: 0.8857\n",
            "Epoch 821/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3096 - acc: 0.8857\n",
            "Epoch 822/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3095 - acc: 0.8857\n",
            "Epoch 823/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3093 - acc: 0.8857\n",
            "Epoch 824/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3091 - acc: 0.8857\n",
            "Epoch 825/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3089 - acc: 0.8857\n",
            "Epoch 826/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3088 - acc: 0.8857\n",
            "Epoch 827/1500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3086 - acc: 0.8857\n",
            "Epoch 828/1500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3084 - acc: 0.8857\n",
            "Epoch 829/1500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3082 - acc: 0.8857\n",
            "Epoch 830/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3081 - acc: 0.8857\n",
            "Epoch 831/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3079 - acc: 0.8857\n",
            "Epoch 832/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3077 - acc: 0.8857\n",
            "Epoch 833/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3076 - acc: 0.8857\n",
            "Epoch 834/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3074 - acc: 0.8857\n",
            "Epoch 835/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3072 - acc: 0.8857\n",
            "Epoch 836/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3070 - acc: 0.8857\n",
            "Epoch 837/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3069 - acc: 0.8857\n",
            "Epoch 838/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3067 - acc: 0.8857\n",
            "Epoch 839/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3065 - acc: 0.8857\n",
            "Epoch 840/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3064 - acc: 0.8857\n",
            "Epoch 841/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3062 - acc: 0.8857\n",
            "Epoch 842/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3060 - acc: 0.8857\n",
            "Epoch 843/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3058 - acc: 0.8857\n",
            "Epoch 844/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3057 - acc: 0.8857\n",
            "Epoch 845/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3055 - acc: 0.8857\n",
            "Epoch 846/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3053 - acc: 0.8857\n",
            "Epoch 847/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.8857\n",
            "Epoch 848/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3050 - acc: 0.8857\n",
            "Epoch 849/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3048 - acc: 0.8857\n",
            "Epoch 850/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3047 - acc: 0.8857\n",
            "Epoch 851/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3045 - acc: 0.8857\n",
            "Epoch 852/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3043 - acc: 0.8857\n",
            "Epoch 853/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3042 - acc: 0.8857\n",
            "Epoch 854/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3040 - acc: 0.8857\n",
            "Epoch 855/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3038 - acc: 0.8857\n",
            "Epoch 856/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3037 - acc: 0.8857\n",
            "Epoch 857/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3035 - acc: 0.8857\n",
            "Epoch 858/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3033 - acc: 0.8857\n",
            "Epoch 859/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3032 - acc: 0.8857\n",
            "Epoch 860/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3030 - acc: 0.8857\n",
            "Epoch 861/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3028 - acc: 0.8857\n",
            "Epoch 862/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3027 - acc: 0.8857\n",
            "Epoch 863/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.8857\n",
            "Epoch 864/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3023 - acc: 0.8857\n",
            "Epoch 865/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3022 - acc: 0.8857\n",
            "Epoch 866/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3020 - acc: 0.8857\n",
            "Epoch 867/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3018 - acc: 0.8857\n",
            "Epoch 868/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3017 - acc: 0.8857\n",
            "Epoch 869/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.3015 - acc: 0.8857\n",
            "Epoch 870/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.8857\n",
            "Epoch 871/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3012 - acc: 0.8857\n",
            "Epoch 872/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.3010 - acc: 0.8857\n",
            "Epoch 873/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3009 - acc: 0.8857\n",
            "Epoch 874/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3007 - acc: 0.8857\n",
            "Epoch 875/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3005 - acc: 0.8857\n",
            "Epoch 876/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3004 - acc: 0.8857\n",
            "Epoch 877/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3002 - acc: 0.8857\n",
            "Epoch 878/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.3001 - acc: 0.8857\n",
            "Epoch 879/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2999 - acc: 0.8857\n",
            "Epoch 880/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2997 - acc: 0.8857\n",
            "Epoch 881/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2996 - acc: 0.8857\n",
            "Epoch 882/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2994 - acc: 0.8857\n",
            "Epoch 883/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2993 - acc: 0.8857\n",
            "Epoch 884/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2991 - acc: 0.8857\n",
            "Epoch 885/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2989 - acc: 0.8857\n",
            "Epoch 886/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2988 - acc: 0.8857\n",
            "Epoch 887/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2986 - acc: 0.8857\n",
            "Epoch 888/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2985 - acc: 0.8857\n",
            "Epoch 889/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2983 - acc: 0.8857\n",
            "Epoch 890/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2982 - acc: 0.8857\n",
            "Epoch 891/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2980 - acc: 0.8857\n",
            "Epoch 892/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2978 - acc: 0.8857\n",
            "Epoch 893/1500\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.2977 - acc: 0.8857\n",
            "Epoch 894/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2975 - acc: 0.8857\n",
            "Epoch 895/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2974 - acc: 0.8857\n",
            "Epoch 896/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2972 - acc: 0.8857\n",
            "Epoch 897/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2971 - acc: 0.8857\n",
            "Epoch 898/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2969 - acc: 0.8857\n",
            "Epoch 899/1500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2967 - acc: 0.8857\n",
            "Epoch 900/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2966 - acc: 0.8857\n",
            "Epoch 901/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2964 - acc: 0.8857\n",
            "Epoch 902/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2963 - acc: 0.8857\n",
            "Epoch 903/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2961 - acc: 0.8857\n",
            "Epoch 904/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2960 - acc: 0.8857\n",
            "Epoch 905/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2958 - acc: 0.8857\n",
            "Epoch 906/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2957 - acc: 0.8857\n",
            "Epoch 907/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2955 - acc: 0.8857\n",
            "Epoch 908/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2953 - acc: 0.8857\n",
            "Epoch 909/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2952 - acc: 0.8857\n",
            "Epoch 910/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2950 - acc: 0.8857\n",
            "Epoch 911/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.8857\n",
            "Epoch 912/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2947 - acc: 0.8857\n",
            "Epoch 913/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2946 - acc: 0.8857\n",
            "Epoch 914/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2944 - acc: 0.8857\n",
            "Epoch 915/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2943 - acc: 0.8857\n",
            "Epoch 916/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2941 - acc: 0.8857\n",
            "Epoch 917/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2940 - acc: 0.8857\n",
            "Epoch 918/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2938 - acc: 0.8857\n",
            "Epoch 919/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2937 - acc: 0.8857\n",
            "Epoch 920/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2935 - acc: 0.8857\n",
            "Epoch 921/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2933 - acc: 0.8857\n",
            "Epoch 922/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2932 - acc: 0.8857\n",
            "Epoch 923/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2930 - acc: 0.8857\n",
            "Epoch 924/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2929 - acc: 0.8857\n",
            "Epoch 925/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2927 - acc: 0.8857\n",
            "Epoch 926/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2926 - acc: 0.8857\n",
            "Epoch 927/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2924 - acc: 0.8857\n",
            "Epoch 928/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2923 - acc: 0.8857\n",
            "Epoch 929/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2921 - acc: 0.8857\n",
            "Epoch 930/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2920 - acc: 0.8857\n",
            "Epoch 931/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2918 - acc: 0.8857\n",
            "Epoch 932/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2917 - acc: 0.8857\n",
            "Epoch 933/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2915 - acc: 0.8857\n",
            "Epoch 934/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2914 - acc: 0.8857\n",
            "Epoch 935/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2912 - acc: 0.8857\n",
            "Epoch 936/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2911 - acc: 0.8857\n",
            "Epoch 937/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2909 - acc: 0.8857\n",
            "Epoch 938/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2908 - acc: 0.8857\n",
            "Epoch 939/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2906 - acc: 0.8857\n",
            "Epoch 940/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2905 - acc: 0.8857\n",
            "Epoch 941/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2903 - acc: 0.8857\n",
            "Epoch 942/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2902 - acc: 0.8857\n",
            "Epoch 943/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2900 - acc: 0.8857\n",
            "Epoch 944/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2899 - acc: 0.8857\n",
            "Epoch 945/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2897 - acc: 0.8857\n",
            "Epoch 946/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2896 - acc: 0.8857\n",
            "Epoch 947/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2894 - acc: 0.8857\n",
            "Epoch 948/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2893 - acc: 0.8857\n",
            "Epoch 949/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2891 - acc: 0.8857\n",
            "Epoch 950/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2890 - acc: 0.8857\n",
            "Epoch 951/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2888 - acc: 0.8857\n",
            "Epoch 952/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2887 - acc: 0.8857\n",
            "Epoch 953/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2885 - acc: 0.8857\n",
            "Epoch 954/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2884 - acc: 0.8857\n",
            "Epoch 955/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2882 - acc: 0.8857\n",
            "Epoch 956/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2881 - acc: 0.8857\n",
            "Epoch 957/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2879 - acc: 0.8857\n",
            "Epoch 958/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2878 - acc: 0.8857\n",
            "Epoch 959/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2876 - acc: 0.8857\n",
            "Epoch 960/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2875 - acc: 0.8857\n",
            "Epoch 961/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2874 - acc: 0.8857\n",
            "Epoch 962/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2872 - acc: 0.8857\n",
            "Epoch 963/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2871 - acc: 0.8857\n",
            "Epoch 964/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2869 - acc: 0.8857\n",
            "Epoch 965/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2868 - acc: 0.8857\n",
            "Epoch 966/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2866 - acc: 0.8857\n",
            "Epoch 967/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2865 - acc: 0.8857\n",
            "Epoch 968/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2863 - acc: 0.8857\n",
            "Epoch 969/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2862 - acc: 0.8857\n",
            "Epoch 970/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2860 - acc: 0.8857\n",
            "Epoch 971/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2859 - acc: 0.8857\n",
            "Epoch 972/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2857 - acc: 0.8857\n",
            "Epoch 973/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2856 - acc: 0.8857\n",
            "Epoch 974/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2854 - acc: 0.8857\n",
            "Epoch 975/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2853 - acc: 0.8857\n",
            "Epoch 976/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2852 - acc: 0.8857\n",
            "Epoch 977/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2850 - acc: 0.8857\n",
            "Epoch 978/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2849 - acc: 0.8857\n",
            "Epoch 979/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2847 - acc: 0.8857\n",
            "Epoch 980/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2846 - acc: 0.8857\n",
            "Epoch 981/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2844 - acc: 0.8857\n",
            "Epoch 982/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2843 - acc: 0.8857\n",
            "Epoch 983/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2841 - acc: 0.8857\n",
            "Epoch 984/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2840 - acc: 0.8857\n",
            "Epoch 985/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2838 - acc: 0.8857\n",
            "Epoch 986/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2837 - acc: 0.8857\n",
            "Epoch 987/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2836 - acc: 0.8857\n",
            "Epoch 988/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2834 - acc: 0.8857\n",
            "Epoch 989/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2833 - acc: 0.8857\n",
            "Epoch 990/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2831 - acc: 0.8857\n",
            "Epoch 991/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2830 - acc: 0.8857\n",
            "Epoch 992/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2828 - acc: 0.8857\n",
            "Epoch 993/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2827 - acc: 0.8857\n",
            "Epoch 994/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2825 - acc: 0.8857\n",
            "Epoch 995/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2824 - acc: 0.8857\n",
            "Epoch 996/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2823 - acc: 0.8857\n",
            "Epoch 997/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2821 - acc: 0.8857\n",
            "Epoch 998/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2820 - acc: 0.8857\n",
            "Epoch 999/1500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2818 - acc: 0.8857\n",
            "Epoch 1000/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2817 - acc: 0.8857\n",
            "Epoch 1001/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2815 - acc: 0.8857\n",
            "Epoch 1002/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2814 - acc: 0.8857\n",
            "Epoch 1003/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2813 - acc: 0.8857\n",
            "Epoch 1004/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2811 - acc: 0.8857\n",
            "Epoch 1005/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2810 - acc: 0.8857\n",
            "Epoch 1006/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2808 - acc: 0.8857\n",
            "Epoch 1007/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2807 - acc: 0.8857\n",
            "Epoch 1008/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2805 - acc: 0.8857\n",
            "Epoch 1009/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2804 - acc: 0.8857\n",
            "Epoch 1010/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2803 - acc: 0.8857\n",
            "Epoch 1011/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2801 - acc: 0.8857\n",
            "Epoch 1012/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2800 - acc: 0.8857\n",
            "Epoch 1013/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2798 - acc: 0.8857\n",
            "Epoch 1014/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2797 - acc: 0.8857\n",
            "Epoch 1015/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2795 - acc: 0.8857\n",
            "Epoch 1016/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2794 - acc: 0.8857\n",
            "Epoch 1017/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2793 - acc: 0.8857\n",
            "Epoch 1018/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2791 - acc: 0.8857\n",
            "Epoch 1019/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2790 - acc: 0.8857\n",
            "Epoch 1020/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2788 - acc: 0.8857\n",
            "Epoch 1021/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2787 - acc: 0.8857\n",
            "Epoch 1022/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2785 - acc: 0.8857\n",
            "Epoch 1023/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2784 - acc: 0.8857\n",
            "Epoch 1024/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2783 - acc: 0.8857\n",
            "Epoch 1025/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2781 - acc: 0.8857\n",
            "Epoch 1026/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2780 - acc: 0.8857\n",
            "Epoch 1027/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2778 - acc: 0.8857\n",
            "Epoch 1028/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2777 - acc: 0.8857\n",
            "Epoch 1029/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2776 - acc: 0.8857\n",
            "Epoch 1030/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2774 - acc: 0.8857\n",
            "Epoch 1031/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2773 - acc: 0.8857\n",
            "Epoch 1032/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2771 - acc: 0.8857\n",
            "Epoch 1033/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2770 - acc: 0.8857\n",
            "Epoch 1034/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2768 - acc: 0.8857\n",
            "Epoch 1035/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2767 - acc: 0.8857\n",
            "Epoch 1036/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2766 - acc: 0.8857\n",
            "Epoch 1037/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2764 - acc: 0.8857\n",
            "Epoch 1038/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2763 - acc: 0.8857\n",
            "Epoch 1039/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2761 - acc: 0.8857\n",
            "Epoch 1040/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2760 - acc: 0.8857\n",
            "Epoch 1041/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2759 - acc: 0.8857\n",
            "Epoch 1042/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2757 - acc: 0.8857\n",
            "Epoch 1043/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2756 - acc: 0.8857\n",
            "Epoch 1044/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2754 - acc: 0.8857\n",
            "Epoch 1045/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2753 - acc: 0.8857\n",
            "Epoch 1046/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2752 - acc: 0.8857\n",
            "Epoch 1047/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2750 - acc: 0.8857\n",
            "Epoch 1048/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2749 - acc: 0.8857\n",
            "Epoch 1049/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2747 - acc: 0.8857\n",
            "Epoch 1050/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2746 - acc: 0.8857\n",
            "Epoch 1051/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2745 - acc: 0.8857\n",
            "Epoch 1052/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2743 - acc: 0.8857\n",
            "Epoch 1053/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2742 - acc: 0.8857\n",
            "Epoch 1054/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2740 - acc: 0.8857\n",
            "Epoch 1055/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2739 - acc: 0.8857\n",
            "Epoch 1056/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2738 - acc: 0.8857\n",
            "Epoch 1057/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2736 - acc: 0.8857\n",
            "Epoch 1058/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2735 - acc: 0.8857\n",
            "Epoch 1059/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2733 - acc: 0.8857\n",
            "Epoch 1060/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2732 - acc: 0.8857\n",
            "Epoch 1061/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2731 - acc: 0.8857\n",
            "Epoch 1062/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2729 - acc: 0.8857\n",
            "Epoch 1063/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2728 - acc: 0.8857\n",
            "Epoch 1064/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2727 - acc: 0.8857\n",
            "Epoch 1065/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2725 - acc: 0.8857\n",
            "Epoch 1066/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2724 - acc: 0.8857\n",
            "Epoch 1067/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2722 - acc: 0.8857\n",
            "Epoch 1068/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2721 - acc: 0.8857\n",
            "Epoch 1069/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2720 - acc: 0.8857\n",
            "Epoch 1070/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2718 - acc: 0.8857\n",
            "Epoch 1071/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2717 - acc: 0.8857\n",
            "Epoch 1072/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2715 - acc: 0.8857\n",
            "Epoch 1073/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2714 - acc: 0.8857\n",
            "Epoch 1074/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2713 - acc: 0.8857\n",
            "Epoch 1075/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2711 - acc: 0.8857\n",
            "Epoch 1076/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2710 - acc: 0.8857\n",
            "Epoch 1077/1500\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.2708 - acc: 0.8857\n",
            "Epoch 1078/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2707 - acc: 0.8857\n",
            "Epoch 1079/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2706 - acc: 0.8857\n",
            "Epoch 1080/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2704 - acc: 0.8857\n",
            "Epoch 1081/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2703 - acc: 0.8857\n",
            "Epoch 1082/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2702 - acc: 0.8857\n",
            "Epoch 1083/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2700 - acc: 0.8857\n",
            "Epoch 1084/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2699 - acc: 0.8857\n",
            "Epoch 1085/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2697 - acc: 0.8857\n",
            "Epoch 1086/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2696 - acc: 0.8857\n",
            "Epoch 1087/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2695 - acc: 0.8857\n",
            "Epoch 1088/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2693 - acc: 0.8857\n",
            "Epoch 1089/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2692 - acc: 0.8857\n",
            "Epoch 1090/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2691 - acc: 0.8857\n",
            "Epoch 1091/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2689 - acc: 0.8857\n",
            "Epoch 1092/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2688 - acc: 0.8857\n",
            "Epoch 1093/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2686 - acc: 0.8857\n",
            "Epoch 1094/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2685 - acc: 0.8857\n",
            "Epoch 1095/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2684 - acc: 0.8857\n",
            "Epoch 1096/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2682 - acc: 0.8857\n",
            "Epoch 1097/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2681 - acc: 0.8857\n",
            "Epoch 1098/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2680 - acc: 0.8857\n",
            "Epoch 1099/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2678 - acc: 0.8857\n",
            "Epoch 1100/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2677 - acc: 0.8857\n",
            "Epoch 1101/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2675 - acc: 0.8857\n",
            "Epoch 1102/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2674 - acc: 0.8857\n",
            "Epoch 1103/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2673 - acc: 0.8857\n",
            "Epoch 1104/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2671 - acc: 0.8857\n",
            "Epoch 1105/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2670 - acc: 0.8857\n",
            "Epoch 1106/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2669 - acc: 0.8857\n",
            "Epoch 1107/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2667 - acc: 0.8857\n",
            "Epoch 1108/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2666 - acc: 0.8857\n",
            "Epoch 1109/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2665 - acc: 0.8857\n",
            "Epoch 1110/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2663 - acc: 0.8857\n",
            "Epoch 1111/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2662 - acc: 0.8857\n",
            "Epoch 1112/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2660 - acc: 0.8857\n",
            "Epoch 1113/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2659 - acc: 0.8857\n",
            "Epoch 1114/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2658 - acc: 0.8857\n",
            "Epoch 1115/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2656 - acc: 0.8857\n",
            "Epoch 1116/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2655 - acc: 0.8857\n",
            "Epoch 1117/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2654 - acc: 0.8857\n",
            "Epoch 1118/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2652 - acc: 0.8857\n",
            "Epoch 1119/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2651 - acc: 0.8857\n",
            "Epoch 1120/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2650 - acc: 0.8857\n",
            "Epoch 1121/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2648 - acc: 0.8857\n",
            "Epoch 1122/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2647 - acc: 0.8857\n",
            "Epoch 1123/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2646 - acc: 0.8857\n",
            "Epoch 1124/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2644 - acc: 0.8857\n",
            "Epoch 1125/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2643 - acc: 0.8857\n",
            "Epoch 1126/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2641 - acc: 0.8857\n",
            "Epoch 1127/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2640 - acc: 0.8857\n",
            "Epoch 1128/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2639 - acc: 0.8857\n",
            "Epoch 1129/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2637 - acc: 0.8857\n",
            "Epoch 1130/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2636 - acc: 0.8857\n",
            "Epoch 1131/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2635 - acc: 0.8857\n",
            "Epoch 1132/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2633 - acc: 0.8857\n",
            "Epoch 1133/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2632 - acc: 0.8857\n",
            "Epoch 1134/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2631 - acc: 0.8857\n",
            "Epoch 1135/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2629 - acc: 0.8857\n",
            "Epoch 1136/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2628 - acc: 0.8857\n",
            "Epoch 1137/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2627 - acc: 0.8857\n",
            "Epoch 1138/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2625 - acc: 0.8857\n",
            "Epoch 1139/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2624 - acc: 0.8857\n",
            "Epoch 1140/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2623 - acc: 0.8857\n",
            "Epoch 1141/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2621 - acc: 0.8857\n",
            "Epoch 1142/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2620 - acc: 0.8857\n",
            "Epoch 1143/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2618 - acc: 0.8857\n",
            "Epoch 1144/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2617 - acc: 0.8857\n",
            "Epoch 1145/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2616 - acc: 0.8857\n",
            "Epoch 1146/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2614 - acc: 0.8857\n",
            "Epoch 1147/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2613 - acc: 0.8857\n",
            "Epoch 1148/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2612 - acc: 0.8857\n",
            "Epoch 1149/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2610 - acc: 0.8857\n",
            "Epoch 1150/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2609 - acc: 0.8857\n",
            "Epoch 1151/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2608 - acc: 0.8857\n",
            "Epoch 1152/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2606 - acc: 0.8857\n",
            "Epoch 1153/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2605 - acc: 0.8857\n",
            "Epoch 1154/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2604 - acc: 0.8857\n",
            "Epoch 1155/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2602 - acc: 0.8857\n",
            "Epoch 1156/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2601 - acc: 0.8857\n",
            "Epoch 1157/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2600 - acc: 0.8857\n",
            "Epoch 1158/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2598 - acc: 0.8857\n",
            "Epoch 1159/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2597 - acc: 0.8857\n",
            "Epoch 1160/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2595 - acc: 0.8857\n",
            "Epoch 1161/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2594 - acc: 0.8857\n",
            "Epoch 1162/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2593 - acc: 0.8857\n",
            "Epoch 1163/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2591 - acc: 0.8857\n",
            "Epoch 1164/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2590 - acc: 0.8857\n",
            "Epoch 1165/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2589 - acc: 0.8857\n",
            "Epoch 1166/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2587 - acc: 0.8857\n",
            "Epoch 1167/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2586 - acc: 0.8857\n",
            "Epoch 1168/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2585 - acc: 0.8857\n",
            "Epoch 1169/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2583 - acc: 0.8857\n",
            "Epoch 1170/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2582 - acc: 0.8857\n",
            "Epoch 1171/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2581 - acc: 0.8857\n",
            "Epoch 1172/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2579 - acc: 0.8857\n",
            "Epoch 1173/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2578 - acc: 0.8857\n",
            "Epoch 1174/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2577 - acc: 0.8857\n",
            "Epoch 1175/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2575 - acc: 0.8857\n",
            "Epoch 1176/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2574 - acc: 0.8857\n",
            "Epoch 1177/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2573 - acc: 0.8857\n",
            "Epoch 1178/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2571 - acc: 0.8857\n",
            "Epoch 1179/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2570 - acc: 0.8857\n",
            "Epoch 1180/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2568 - acc: 0.8857\n",
            "Epoch 1181/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2567 - acc: 0.8857\n",
            "Epoch 1182/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2566 - acc: 0.8857\n",
            "Epoch 1183/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2564 - acc: 0.8857\n",
            "Epoch 1184/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2563 - acc: 0.8857\n",
            "Epoch 1185/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2562 - acc: 0.8857\n",
            "Epoch 1186/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2560 - acc: 0.8857\n",
            "Epoch 1187/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2559 - acc: 0.8857\n",
            "Epoch 1188/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2558 - acc: 0.8857\n",
            "Epoch 1189/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2556 - acc: 0.8857\n",
            "Epoch 1190/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2555 - acc: 0.8857\n",
            "Epoch 1191/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2554 - acc: 0.8857\n",
            "Epoch 1192/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2552 - acc: 0.8857\n",
            "Epoch 1193/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2551 - acc: 0.8857\n",
            "Epoch 1194/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2550 - acc: 0.8857\n",
            "Epoch 1195/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2548 - acc: 0.8857\n",
            "Epoch 1196/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2547 - acc: 0.8857\n",
            "Epoch 1197/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2546 - acc: 0.8857\n",
            "Epoch 1198/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2544 - acc: 0.8857\n",
            "Epoch 1199/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2543 - acc: 0.8857\n",
            "Epoch 1200/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2542 - acc: 0.8857\n",
            "Epoch 1201/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2540 - acc: 0.8857\n",
            "Epoch 1202/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2539 - acc: 0.8857\n",
            "Epoch 1203/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2537 - acc: 0.8857\n",
            "Epoch 1204/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2536 - acc: 0.8857\n",
            "Epoch 1205/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2535 - acc: 0.8857\n",
            "Epoch 1206/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2533 - acc: 0.8857\n",
            "Epoch 1207/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2532 - acc: 0.8857\n",
            "Epoch 1208/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2531 - acc: 0.8857\n",
            "Epoch 1209/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2529 - acc: 0.8857\n",
            "Epoch 1210/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2528 - acc: 0.8857\n",
            "Epoch 1211/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2527 - acc: 0.8857\n",
            "Epoch 1212/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2525 - acc: 0.8857\n",
            "Epoch 1213/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2524 - acc: 0.8857\n",
            "Epoch 1214/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2523 - acc: 0.8857\n",
            "Epoch 1215/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2521 - acc: 0.8857\n",
            "Epoch 1216/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2520 - acc: 0.8857\n",
            "Epoch 1217/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2519 - acc: 0.8857\n",
            "Epoch 1218/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2517 - acc: 0.8857\n",
            "Epoch 1219/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2516 - acc: 0.8857\n",
            "Epoch 1220/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2515 - acc: 0.8857\n",
            "Epoch 1221/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2513 - acc: 0.8857\n",
            "Epoch 1222/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2512 - acc: 0.8857\n",
            "Epoch 1223/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2510 - acc: 0.8857\n",
            "Epoch 1224/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2509 - acc: 0.8857\n",
            "Epoch 1225/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2508 - acc: 0.8857\n",
            "Epoch 1226/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2506 - acc: 0.8857\n",
            "Epoch 1227/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2505 - acc: 0.8857\n",
            "Epoch 1228/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2504 - acc: 0.8857\n",
            "Epoch 1229/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2502 - acc: 0.8857\n",
            "Epoch 1230/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2501 - acc: 0.8857\n",
            "Epoch 1231/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2500 - acc: 0.8857\n",
            "Epoch 1232/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2498 - acc: 0.8857\n",
            "Epoch 1233/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2497 - acc: 0.8857\n",
            "Epoch 1234/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2496 - acc: 0.8857\n",
            "Epoch 1235/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2494 - acc: 0.8857\n",
            "Epoch 1236/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2493 - acc: 0.8857\n",
            "Epoch 1237/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2492 - acc: 0.8857\n",
            "Epoch 1238/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2490 - acc: 0.8857\n",
            "Epoch 1239/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2489 - acc: 0.8857\n",
            "Epoch 1240/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2487 - acc: 0.8857\n",
            "Epoch 1241/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2486 - acc: 0.8857\n",
            "Epoch 1242/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2485 - acc: 0.8857\n",
            "Epoch 1243/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2483 - acc: 0.8857\n",
            "Epoch 1244/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2482 - acc: 0.8857\n",
            "Epoch 1245/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2481 - acc: 0.8857\n",
            "Epoch 1246/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2479 - acc: 0.8857\n",
            "Epoch 1247/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2478 - acc: 0.8857\n",
            "Epoch 1248/1500\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2477 - acc: 0.8857\n",
            "Epoch 1249/1500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2475 - acc: 0.8857\n",
            "Epoch 1250/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2474 - acc: 0.8857\n",
            "Epoch 1251/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2473 - acc: 0.8857\n",
            "Epoch 1252/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2471 - acc: 0.8857\n",
            "Epoch 1253/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2470 - acc: 0.8857\n",
            "Epoch 1254/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2469 - acc: 0.8857\n",
            "Epoch 1255/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2467 - acc: 0.8857\n",
            "Epoch 1256/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2466 - acc: 0.8857\n",
            "Epoch 1257/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2464 - acc: 0.8857\n",
            "Epoch 1258/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2463 - acc: 0.8857\n",
            "Epoch 1259/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2462 - acc: 0.8857\n",
            "Epoch 1260/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2460 - acc: 0.8857\n",
            "Epoch 1261/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2459 - acc: 0.8857\n",
            "Epoch 1262/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2458 - acc: 0.8857\n",
            "Epoch 1263/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2456 - acc: 0.8857\n",
            "Epoch 1264/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2455 - acc: 0.8857\n",
            "Epoch 1265/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2454 - acc: 0.8857\n",
            "Epoch 1266/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2452 - acc: 0.8857\n",
            "Epoch 1267/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2451 - acc: 0.8857\n",
            "Epoch 1268/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2449 - acc: 0.8857\n",
            "Epoch 1269/1500\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.2448 - acc: 0.8857\n",
            "Epoch 1270/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2447 - acc: 0.8857\n",
            "Epoch 1271/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2445 - acc: 0.8857\n",
            "Epoch 1272/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2444 - acc: 0.8857\n",
            "Epoch 1273/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2443 - acc: 0.8857\n",
            "Epoch 1274/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2441 - acc: 0.8857\n",
            "Epoch 1275/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2440 - acc: 0.8857\n",
            "Epoch 1276/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2439 - acc: 0.8857\n",
            "Epoch 1277/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2437 - acc: 0.8857\n",
            "Epoch 1278/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2436 - acc: 0.8857\n",
            "Epoch 1279/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2435 - acc: 0.8857\n",
            "Epoch 1280/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2433 - acc: 0.8857\n",
            "Epoch 1281/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2432 - acc: 0.8857\n",
            "Epoch 1282/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2431 - acc: 0.8857\n",
            "Epoch 1283/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2429 - acc: 0.8857\n",
            "Epoch 1284/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2428 - acc: 0.8857\n",
            "Epoch 1285/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2427 - acc: 0.8857\n",
            "Epoch 1286/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2425 - acc: 0.8857\n",
            "Epoch 1287/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2424 - acc: 0.8857\n",
            "Epoch 1288/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2423 - acc: 0.8857\n",
            "Epoch 1289/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2421 - acc: 0.8857\n",
            "Epoch 1290/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2420 - acc: 0.8857\n",
            "Epoch 1291/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2418 - acc: 0.8857\n",
            "Epoch 1292/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2417 - acc: 0.8857\n",
            "Epoch 1293/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2416 - acc: 0.8857\n",
            "Epoch 1294/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2414 - acc: 0.8857\n",
            "Epoch 1295/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2413 - acc: 0.8857\n",
            "Epoch 1296/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2412 - acc: 0.8857\n",
            "Epoch 1297/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2410 - acc: 0.8857\n",
            "Epoch 1298/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2409 - acc: 0.8857\n",
            "Epoch 1299/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2408 - acc: 0.8857\n",
            "Epoch 1300/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2406 - acc: 0.8857\n",
            "Epoch 1301/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2405 - acc: 0.8952\n",
            "Epoch 1302/1500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2404 - acc: 0.8952\n",
            "Epoch 1303/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2402 - acc: 0.8952\n",
            "Epoch 1304/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2401 - acc: 0.8952\n",
            "Epoch 1305/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2400 - acc: 0.8952\n",
            "Epoch 1306/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2398 - acc: 0.8952\n",
            "Epoch 1307/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2397 - acc: 0.8952\n",
            "Epoch 1308/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2396 - acc: 0.8952\n",
            "Epoch 1309/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2394 - acc: 0.8952\n",
            "Epoch 1310/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2393 - acc: 0.8952\n",
            "Epoch 1311/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2392 - acc: 0.9048\n",
            "Epoch 1312/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2390 - acc: 0.9048\n",
            "Epoch 1313/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2389 - acc: 0.9048\n",
            "Epoch 1314/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2388 - acc: 0.9048\n",
            "Epoch 1315/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2386 - acc: 0.9048\n",
            "Epoch 1316/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2385 - acc: 0.9048\n",
            "Epoch 1317/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2384 - acc: 0.9048\n",
            "Epoch 1318/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2382 - acc: 0.9048\n",
            "Epoch 1319/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2381 - acc: 0.9048\n",
            "Epoch 1320/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2380 - acc: 0.9048\n",
            "Epoch 1321/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2378 - acc: 0.9048\n",
            "Epoch 1322/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2377 - acc: 0.9048\n",
            "Epoch 1323/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2376 - acc: 0.9048\n",
            "Epoch 1324/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2374 - acc: 0.9143\n",
            "Epoch 1325/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2373 - acc: 0.9143\n",
            "Epoch 1326/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2372 - acc: 0.9143\n",
            "Epoch 1327/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2370 - acc: 0.9143\n",
            "Epoch 1328/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2369 - acc: 0.9143\n",
            "Epoch 1329/1500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2368 - acc: 0.9143\n",
            "Epoch 1330/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2366 - acc: 0.9238\n",
            "Epoch 1331/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2365 - acc: 0.9238\n",
            "Epoch 1332/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2364 - acc: 0.9238\n",
            "Epoch 1333/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2362 - acc: 0.9238\n",
            "Epoch 1334/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2361 - acc: 0.9238\n",
            "Epoch 1335/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2360 - acc: 0.9238\n",
            "Epoch 1336/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2358 - acc: 0.9238\n",
            "Epoch 1337/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2357 - acc: 0.9238\n",
            "Epoch 1338/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2356 - acc: 0.9238\n",
            "Epoch 1339/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2354 - acc: 0.9238\n",
            "Epoch 1340/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2353 - acc: 0.9238\n",
            "Epoch 1341/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2352 - acc: 0.9238\n",
            "Epoch 1342/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2350 - acc: 0.9238\n",
            "Epoch 1343/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2349 - acc: 0.9238\n",
            "Epoch 1344/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2348 - acc: 0.9238\n",
            "Epoch 1345/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2346 - acc: 0.9238\n",
            "Epoch 1346/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2345 - acc: 0.9238\n",
            "Epoch 1347/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2344 - acc: 0.9238\n",
            "Epoch 1348/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2342 - acc: 0.9238\n",
            "Epoch 1349/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2341 - acc: 0.9238\n",
            "Epoch 1350/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2340 - acc: 0.9238\n",
            "Epoch 1351/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2338 - acc: 0.9238\n",
            "Epoch 1352/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2337 - acc: 0.9238\n",
            "Epoch 1353/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2336 - acc: 0.9238\n",
            "Epoch 1354/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2334 - acc: 0.9238\n",
            "Epoch 1355/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2333 - acc: 0.9238\n",
            "Epoch 1356/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2332 - acc: 0.9238\n",
            "Epoch 1357/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2330 - acc: 0.9238\n",
            "Epoch 1358/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2329 - acc: 0.9238\n",
            "Epoch 1359/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2328 - acc: 0.9238\n",
            "Epoch 1360/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2326 - acc: 0.9238\n",
            "Epoch 1361/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2325 - acc: 0.9238\n",
            "Epoch 1362/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2324 - acc: 0.9238\n",
            "Epoch 1363/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2322 - acc: 0.9333\n",
            "Epoch 1364/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2321 - acc: 0.9333\n",
            "Epoch 1365/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2320 - acc: 0.9333\n",
            "Epoch 1366/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2318 - acc: 0.9333\n",
            "Epoch 1367/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2317 - acc: 0.9333\n",
            "Epoch 1368/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2316 - acc: 0.9333\n",
            "Epoch 1369/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2314 - acc: 0.9333\n",
            "Epoch 1370/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2313 - acc: 0.9333\n",
            "Epoch 1371/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2312 - acc: 0.9333\n",
            "Epoch 1372/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2310 - acc: 0.9333\n",
            "Epoch 1373/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2309 - acc: 0.9333\n",
            "Epoch 1374/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2308 - acc: 0.9333\n",
            "Epoch 1375/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2306 - acc: 0.9333\n",
            "Epoch 1376/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2305 - acc: 0.9333\n",
            "Epoch 1377/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2303 - acc: 0.9333\n",
            "Epoch 1378/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2302 - acc: 0.9333\n",
            "Epoch 1379/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2301 - acc: 0.9333\n",
            "Epoch 1380/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2300 - acc: 0.9333\n",
            "Epoch 1381/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2298 - acc: 0.9333\n",
            "Epoch 1382/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2297 - acc: 0.9333\n",
            "Epoch 1383/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2296 - acc: 0.9333\n",
            "Epoch 1384/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2294 - acc: 0.9333\n",
            "Epoch 1385/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2293 - acc: 0.9333\n",
            "Epoch 1386/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2292 - acc: 0.9333\n",
            "Epoch 1387/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2290 - acc: 0.9333\n",
            "Epoch 1388/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2289 - acc: 0.9333\n",
            "Epoch 1389/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2288 - acc: 0.9333\n",
            "Epoch 1390/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2286 - acc: 0.9333\n",
            "Epoch 1391/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2285 - acc: 0.9333\n",
            "Epoch 1392/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2284 - acc: 0.9429\n",
            "Epoch 1393/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2282 - acc: 0.9429\n",
            "Epoch 1394/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2281 - acc: 0.9429\n",
            "Epoch 1395/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2280 - acc: 0.9429\n",
            "Epoch 1396/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2278 - acc: 0.9429\n",
            "Epoch 1397/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2277 - acc: 0.9429\n",
            "Epoch 1398/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2276 - acc: 0.9429\n",
            "Epoch 1399/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2274 - acc: 0.9429\n",
            "Epoch 1400/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2273 - acc: 0.9429\n",
            "Epoch 1401/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2272 - acc: 0.9429\n",
            "Epoch 1402/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2270 - acc: 0.9429\n",
            "Epoch 1403/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2269 - acc: 0.9429\n",
            "Epoch 1404/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2268 - acc: 0.9429\n",
            "Epoch 1405/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2266 - acc: 0.9429\n",
            "Epoch 1406/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2265 - acc: 0.9429\n",
            "Epoch 1407/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2264 - acc: 0.9429\n",
            "Epoch 1408/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2262 - acc: 0.9524\n",
            "Epoch 1409/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2261 - acc: 0.9524\n",
            "Epoch 1410/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2260 - acc: 0.9524\n",
            "Epoch 1411/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2258 - acc: 0.9524\n",
            "Epoch 1412/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2257 - acc: 0.9524\n",
            "Epoch 1413/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2256 - acc: 0.9524\n",
            "Epoch 1414/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2254 - acc: 0.9524\n",
            "Epoch 1415/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2253 - acc: 0.9524\n",
            "Epoch 1416/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2252 - acc: 0.9524\n",
            "Epoch 1417/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2250 - acc: 0.9524\n",
            "Epoch 1418/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2249 - acc: 0.9524\n",
            "Epoch 1419/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2248 - acc: 0.9524\n",
            "Epoch 1420/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2246 - acc: 0.9524\n",
            "Epoch 1421/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2245 - acc: 0.9524\n",
            "Epoch 1422/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2244 - acc: 0.9524\n",
            "Epoch 1423/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2243 - acc: 0.9524\n",
            "Epoch 1424/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2241 - acc: 0.9524\n",
            "Epoch 1425/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2240 - acc: 0.9524\n",
            "Epoch 1426/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2239 - acc: 0.9524\n",
            "Epoch 1427/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2237 - acc: 0.9524\n",
            "Epoch 1428/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2236 - acc: 0.9524\n",
            "Epoch 1429/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2235 - acc: 0.9524\n",
            "Epoch 1430/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2233 - acc: 0.9619\n",
            "Epoch 1431/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2232 - acc: 0.9619\n",
            "Epoch 1432/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2231 - acc: 0.9619\n",
            "Epoch 1433/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2229 - acc: 0.9619\n",
            "Epoch 1434/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2228 - acc: 0.9619\n",
            "Epoch 1435/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2227 - acc: 0.9619\n",
            "Epoch 1436/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2225 - acc: 0.9619\n",
            "Epoch 1437/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2224 - acc: 0.9619\n",
            "Epoch 1438/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2223 - acc: 0.9619\n",
            "Epoch 1439/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2222 - acc: 0.9619\n",
            "Epoch 1440/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2220 - acc: 0.9619\n",
            "Epoch 1441/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2219 - acc: 0.9619\n",
            "Epoch 1442/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2218 - acc: 0.9619\n",
            "Epoch 1443/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2216 - acc: 0.9619\n",
            "Epoch 1444/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2215 - acc: 0.9619\n",
            "Epoch 1445/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2214 - acc: 0.9619\n",
            "Epoch 1446/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2212 - acc: 0.9714\n",
            "Epoch 1447/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2211 - acc: 0.9714\n",
            "Epoch 1448/1500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2210 - acc: 0.9714\n",
            "Epoch 1449/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2208 - acc: 0.9714\n",
            "Epoch 1450/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2207 - acc: 0.9714\n",
            "Epoch 1451/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2206 - acc: 0.9714\n",
            "Epoch 1452/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2205 - acc: 0.9714\n",
            "Epoch 1453/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2203 - acc: 0.9714\n",
            "Epoch 1454/1500\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2202 - acc: 0.9714\n",
            "Epoch 1455/1500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2201 - acc: 0.9714\n",
            "Epoch 1456/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2199 - acc: 0.9714\n",
            "Epoch 1457/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2198 - acc: 0.9714\n",
            "Epoch 1458/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2197 - acc: 0.9714\n",
            "Epoch 1459/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2195 - acc: 0.9714\n",
            "Epoch 1460/1500\n",
            "105/105 [==============================] - 0s 94us/step - loss: 0.2194 - acc: 0.9714\n",
            "Epoch 1461/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2193 - acc: 0.9714\n",
            "Epoch 1462/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2191 - acc: 0.9714\n",
            "Epoch 1463/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2190 - acc: 0.9714\n",
            "Epoch 1464/1500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.2189 - acc: 0.9714\n",
            "Epoch 1465/1500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2187 - acc: 0.9714\n",
            "Epoch 1466/1500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2186 - acc: 0.9714\n",
            "Epoch 1467/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2185 - acc: 0.9714\n",
            "Epoch 1468/1500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2184 - acc: 0.9714\n",
            "Epoch 1469/1500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2182 - acc: 0.9714\n",
            "Epoch 1470/1500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2181 - acc: 0.9714\n",
            "Epoch 1471/1500\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.2180 - acc: 0.9714\n",
            "Epoch 1472/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2178 - acc: 0.9714\n",
            "Epoch 1473/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2177 - acc: 0.9714\n",
            "Epoch 1474/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2176 - acc: 0.9714\n",
            "Epoch 1475/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2174 - acc: 0.9714\n",
            "Epoch 1476/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2173 - acc: 0.9714\n",
            "Epoch 1477/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2172 - acc: 0.9714\n",
            "Epoch 1478/1500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2170 - acc: 0.9714\n",
            "Epoch 1479/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2169 - acc: 0.9714\n",
            "Epoch 1480/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2168 - acc: 0.9714\n",
            "Epoch 1481/1500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2166 - acc: 0.9714\n",
            "Epoch 1482/1500\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2165 - acc: 0.9714\n",
            "Epoch 1483/1500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2164 - acc: 0.9714\n",
            "Epoch 1484/1500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2163 - acc: 0.9714\n",
            "Epoch 1485/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2161 - acc: 0.9714\n",
            "Epoch 1486/1500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2160 - acc: 0.9714\n",
            "Epoch 1487/1500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.2159 - acc: 0.9714\n",
            "Epoch 1488/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2157 - acc: 0.9714\n",
            "Epoch 1489/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2156 - acc: 0.9714\n",
            "Epoch 1490/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2155 - acc: 0.9714\n",
            "Epoch 1491/1500\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.2153 - acc: 0.9714\n",
            "Epoch 1492/1500\n",
            "105/105 [==============================] - 0s 12us/step - loss: 0.2152 - acc: 0.9714\n",
            "Epoch 1493/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2151 - acc: 0.9714\n",
            "Epoch 1494/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2150 - acc: 0.9714\n",
            "Epoch 1495/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2148 - acc: 0.9714\n",
            "Epoch 1496/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2147 - acc: 0.9714\n",
            "Epoch 1497/1500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2146 - acc: 0.9714\n",
            "Epoch 1498/1500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2144 - acc: 0.9714\n",
            "Epoch 1499/1500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2143 - acc: 0.9714\n",
            "Epoch 1500/1500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2142 - acc: 0.9714\n",
            "13/13 [==============================] - 0s 16ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff7576f8-bef3-4d03-d19d-4ff558c89c12",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f03e7529-3521-44ff-b37b-db4acd91e107",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad8f5b4f-4590-4cf7-8c44-0a5868e102f7"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(9,), kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "714e6ad1-0a94-4a09-8de6-ae6db00c2339",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4451c311-89b8-49b3-e2ad-4474dda20eef",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=105)\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "105/105 [==============================] - 3s 27ms/step - loss: 1.2288 - acc: 0.3714 - val_loss: 1.1590 - val_acc: 0.5385\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 1.1973 - acc: 0.3524 - val_loss: 1.1482 - val_acc: 0.5385\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.1665 - acc: 0.4190 - val_loss: 1.1378 - val_acc: 0.5385\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.1567 - acc: 0.4476 - val_loss: 1.1277 - val_acc: 0.5385\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.1307 - acc: 0.4667 - val_loss: 1.1186 - val_acc: 0.5385\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.1641 - acc: 0.3524 - val_loss: 1.1103 - val_acc: 0.5385\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1336 - acc: 0.4476 - val_loss: 1.1021 - val_acc: 0.5385\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1309 - acc: 0.4571 - val_loss: 1.0938 - val_acc: 0.5385\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1387 - acc: 0.4000 - val_loss: 1.0860 - val_acc: 0.5385\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.1251 - acc: 0.4381 - val_loss: 1.0785 - val_acc: 0.5385\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.1069 - acc: 0.4476 - val_loss: 1.0713 - val_acc: 0.5385\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.1086 - acc: 0.4667 - val_loss: 1.0645 - val_acc: 0.5385\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0954 - acc: 0.5333 - val_loss: 1.0583 - val_acc: 0.5385\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0564 - acc: 0.5143 - val_loss: 1.0527 - val_acc: 0.5385\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0927 - acc: 0.4667 - val_loss: 1.0479 - val_acc: 0.5385\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0678 - acc: 0.5524 - val_loss: 1.0434 - val_acc: 0.5385\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0493 - acc: 0.5524 - val_loss: 1.0391 - val_acc: 0.5385\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0493 - acc: 0.5048 - val_loss: 1.0353 - val_acc: 0.4615\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0409 - acc: 0.5524 - val_loss: 1.0319 - val_acc: 0.4615\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0224 - acc: 0.5714 - val_loss: 1.0290 - val_acc: 0.4615\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0453 - acc: 0.4857 - val_loss: 1.0262 - val_acc: 0.4615\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0334 - acc: 0.5238 - val_loss: 1.0239 - val_acc: 0.4615\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0136 - acc: 0.5238 - val_loss: 1.0222 - val_acc: 0.4615\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0048 - acc: 0.5524 - val_loss: 1.0208 - val_acc: 0.4615\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0128 - acc: 0.5619 - val_loss: 1.0193 - val_acc: 0.3846\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0043 - acc: 0.5524 - val_loss: 1.0181 - val_acc: 0.3846\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9904 - acc: 0.5333 - val_loss: 1.0169 - val_acc: 0.3846\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9967 - acc: 0.6381 - val_loss: 1.0161 - val_acc: 0.3846\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.9961 - acc: 0.5714 - val_loss: 1.0153 - val_acc: 0.4615\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.9692 - acc: 0.6000 - val_loss: 1.0146 - val_acc: 0.4615\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.9797 - acc: 0.5810 - val_loss: 1.0140 - val_acc: 0.4615\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.9707 - acc: 0.5714 - val_loss: 1.0138 - val_acc: 0.4615\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.9609 - acc: 0.5905 - val_loss: 1.0139 - val_acc: 0.4615\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.9787 - acc: 0.5714 - val_loss: 1.0145 - val_acc: 0.4615\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.9704 - acc: 0.5619 - val_loss: 1.0152 - val_acc: 0.3846\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.9828 - acc: 0.5333 - val_loss: 1.0159 - val_acc: 0.3846\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.9608 - acc: 0.5714 - val_loss: 1.0166 - val_acc: 0.3846\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9595 - acc: 0.5524 - val_loss: 1.0177 - val_acc: 0.3846\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.9549 - acc: 0.6095 - val_loss: 1.0187 - val_acc: 0.3846\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.9455 - acc: 0.6571 - val_loss: 1.0202 - val_acc: 0.3846\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.9487 - acc: 0.6286 - val_loss: 1.0216 - val_acc: 0.3846\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9524 - acc: 0.6095 - val_loss: 1.0232 - val_acc: 0.3846\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.9240 - acc: 0.5905 - val_loss: 1.0250 - val_acc: 0.3846\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9214 - acc: 0.5905 - val_loss: 1.0269 - val_acc: 0.3846\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.9548 - acc: 0.6190 - val_loss: 1.0288 - val_acc: 0.3846\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9302 - acc: 0.6286 - val_loss: 1.0304 - val_acc: 0.3846\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9516 - acc: 0.5714 - val_loss: 1.0323 - val_acc: 0.3846\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.9104 - acc: 0.5619 - val_loss: 1.0345 - val_acc: 0.3846\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9143 - acc: 0.6190 - val_loss: 1.0367 - val_acc: 0.3846\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8959 - acc: 0.6000 - val_loss: 1.0391 - val_acc: 0.3846\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.9187 - acc: 0.5810 - val_loss: 1.0412 - val_acc: 0.3846\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.9434 - acc: 0.5714 - val_loss: 1.0435 - val_acc: 0.3846\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.9152 - acc: 0.6095 - val_loss: 1.0459 - val_acc: 0.3846\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8941 - acc: 0.6190 - val_loss: 1.0487 - val_acc: 0.3846\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9346 - acc: 0.5810 - val_loss: 1.0515 - val_acc: 0.3846\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9295 - acc: 0.5905 - val_loss: 1.0540 - val_acc: 0.3846\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8923 - acc: 0.6286 - val_loss: 1.0560 - val_acc: 0.3846\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.9143 - acc: 0.6095 - val_loss: 1.0580 - val_acc: 0.3846\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8971 - acc: 0.6571 - val_loss: 1.0603 - val_acc: 0.3846\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.9419 - acc: 0.5905 - val_loss: 1.0627 - val_acc: 0.3846\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8862 - acc: 0.6286 - val_loss: 1.0654 - val_acc: 0.3846\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.9013 - acc: 0.6381 - val_loss: 1.0675 - val_acc: 0.3846\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.8907 - acc: 0.6095 - val_loss: 1.0694 - val_acc: 0.4615\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.9046 - acc: 0.6286 - val_loss: 1.0711 - val_acc: 0.4615\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.8802 - acc: 0.6571 - val_loss: 1.0728 - val_acc: 0.4615\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8915 - acc: 0.6000 - val_loss: 1.0745 - val_acc: 0.4615\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9022 - acc: 0.6286 - val_loss: 1.0766 - val_acc: 0.4615\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8939 - acc: 0.6190 - val_loss: 1.0782 - val_acc: 0.4615\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.8977 - acc: 0.6190 - val_loss: 1.0801 - val_acc: 0.4615\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8665 - acc: 0.6095 - val_loss: 1.0820 - val_acc: 0.4615\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.8987 - acc: 0.6095 - val_loss: 1.0843 - val_acc: 0.4615\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.8490 - acc: 0.6476 - val_loss: 1.0872 - val_acc: 0.4615\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8689 - acc: 0.6381 - val_loss: 1.0896 - val_acc: 0.4615\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.8841 - acc: 0.6190 - val_loss: 1.0921 - val_acc: 0.4615\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.8870 - acc: 0.6190 - val_loss: 1.0941 - val_acc: 0.4615\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.8678 - acc: 0.6286 - val_loss: 1.0962 - val_acc: 0.4615\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8671 - acc: 0.6190 - val_loss: 1.0978 - val_acc: 0.4615\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8729 - acc: 0.6095 - val_loss: 1.0987 - val_acc: 0.4615\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8900 - acc: 0.6286 - val_loss: 1.0997 - val_acc: 0.4615\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.8612 - acc: 0.6286 - val_loss: 1.1011 - val_acc: 0.4615\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.8695 - acc: 0.6667 - val_loss: 1.1025 - val_acc: 0.4615\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8809 - acc: 0.6286 - val_loss: 1.1033 - val_acc: 0.4615\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8835 - acc: 0.6571 - val_loss: 1.1040 - val_acc: 0.4615\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.8298 - acc: 0.6476 - val_loss: 1.1044 - val_acc: 0.4615\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8437 - acc: 0.6381 - val_loss: 1.1053 - val_acc: 0.4615\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.8473 - acc: 0.6571 - val_loss: 1.1054 - val_acc: 0.4615\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8418 - acc: 0.6762 - val_loss: 1.1060 - val_acc: 0.4615\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8585 - acc: 0.6381 - val_loss: 1.1064 - val_acc: 0.4615\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.8431 - acc: 0.6571 - val_loss: 1.1070 - val_acc: 0.4615\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.8468 - acc: 0.6476 - val_loss: 1.1080 - val_acc: 0.4615\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8219 - acc: 0.6762 - val_loss: 1.1091 - val_acc: 0.4615\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 85us/step - loss: 0.8674 - acc: 0.6190 - val_loss: 1.1100 - val_acc: 0.4615\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.8173 - acc: 0.6571 - val_loss: 1.1106 - val_acc: 0.4615\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8496 - acc: 0.6476 - val_loss: 1.1107 - val_acc: 0.4615\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.8376 - acc: 0.6476 - val_loss: 1.1115 - val_acc: 0.4615\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.8743 - acc: 0.5905 - val_loss: 1.1120 - val_acc: 0.4615\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.8281 - acc: 0.6476 - val_loss: 1.1131 - val_acc: 0.4615\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.8590 - acc: 0.6762 - val_loss: 1.1141 - val_acc: 0.4615\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.8420 - acc: 0.6476 - val_loss: 1.1161 - val_acc: 0.4615\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.8113 - acc: 0.6857 - val_loss: 1.1182 - val_acc: 0.4615\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8211 - acc: 0.6762 - val_loss: 1.1200 - val_acc: 0.4615\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.8395 - acc: 0.6286 - val_loss: 1.1211 - val_acc: 0.4615\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8119 - acc: 0.6762 - val_loss: 1.1228 - val_acc: 0.4615\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8251 - acc: 0.6476 - val_loss: 1.1246 - val_acc: 0.4615\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.8198 - acc: 0.6381 - val_loss: 1.1263 - val_acc: 0.4615\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7935 - acc: 0.7143 - val_loss: 1.1285 - val_acc: 0.4615\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8271 - acc: 0.6476 - val_loss: 1.1308 - val_acc: 0.4615\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.8372 - acc: 0.6571 - val_loss: 1.1323 - val_acc: 0.4615\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8096 - acc: 0.6667 - val_loss: 1.1340 - val_acc: 0.5385\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8191 - acc: 0.6381 - val_loss: 1.1364 - val_acc: 0.5385\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8715 - acc: 0.6476 - val_loss: 1.1383 - val_acc: 0.5385\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.8136 - acc: 0.6381 - val_loss: 1.1396 - val_acc: 0.5385\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8137 - acc: 0.6476 - val_loss: 1.1407 - val_acc: 0.5385\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7830 - acc: 0.6762 - val_loss: 1.1419 - val_acc: 0.5385\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8179 - acc: 0.6762 - val_loss: 1.1429 - val_acc: 0.5385\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8286 - acc: 0.6952 - val_loss: 1.1437 - val_acc: 0.5385\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8204 - acc: 0.6762 - val_loss: 1.1439 - val_acc: 0.5385\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8081 - acc: 0.6952 - val_loss: 1.1436 - val_acc: 0.5385\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7992 - acc: 0.6762 - val_loss: 1.1428 - val_acc: 0.5385\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.8138 - acc: 0.6476 - val_loss: 1.1415 - val_acc: 0.5385\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.7679 - acc: 0.6286 - val_loss: 1.1401 - val_acc: 0.5385\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 94us/step - loss: 0.7998 - acc: 0.6476 - val_loss: 1.1389 - val_acc: 0.5385\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7803 - acc: 0.6667 - val_loss: 1.1377 - val_acc: 0.5385\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7846 - acc: 0.6857 - val_loss: 1.1365 - val_acc: 0.5385\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.7891 - acc: 0.6667 - val_loss: 1.1352 - val_acc: 0.5385\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7862 - acc: 0.6476 - val_loss: 1.1339 - val_acc: 0.5385\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7738 - acc: 0.7048 - val_loss: 1.1333 - val_acc: 0.5385\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7914 - acc: 0.6667 - val_loss: 1.1328 - val_acc: 0.5385\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7794 - acc: 0.6571 - val_loss: 1.1326 - val_acc: 0.5385\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7977 - acc: 0.6476 - val_loss: 1.1331 - val_acc: 0.5385\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7882 - acc: 0.7048 - val_loss: 1.1344 - val_acc: 0.5385\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7796 - acc: 0.6571 - val_loss: 1.1363 - val_acc: 0.5385\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.7909 - acc: 0.6286 - val_loss: 1.1395 - val_acc: 0.5385\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.8012 - acc: 0.6476 - val_loss: 1.1423 - val_acc: 0.5385\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7822 - acc: 0.6952 - val_loss: 1.1451 - val_acc: 0.5385\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.7502 - acc: 0.7238 - val_loss: 1.1478 - val_acc: 0.5385\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7703 - acc: 0.6381 - val_loss: 1.1497 - val_acc: 0.5385\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7710 - acc: 0.6667 - val_loss: 1.1518 - val_acc: 0.5385\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7700 - acc: 0.6857 - val_loss: 1.1543 - val_acc: 0.5385\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7846 - acc: 0.6857 - val_loss: 1.1555 - val_acc: 0.5385\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7737 - acc: 0.7048 - val_loss: 1.1567 - val_acc: 0.5385\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8019 - acc: 0.6190 - val_loss: 1.1577 - val_acc: 0.5385\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7943 - acc: 0.6476 - val_loss: 1.1586 - val_acc: 0.5385\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7648 - acc: 0.6952 - val_loss: 1.1598 - val_acc: 0.5385\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7937 - acc: 0.6762 - val_loss: 1.1626 - val_acc: 0.5385\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7671 - acc: 0.6476 - val_loss: 1.1653 - val_acc: 0.5385\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.7529 - acc: 0.6857 - val_loss: 1.1684 - val_acc: 0.5385\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7535 - acc: 0.6762 - val_loss: 1.1715 - val_acc: 0.5385\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7703 - acc: 0.6857 - val_loss: 1.1741 - val_acc: 0.5385\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7789 - acc: 0.6667 - val_loss: 1.1765 - val_acc: 0.5385\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7427 - acc: 0.6952 - val_loss: 1.1790 - val_acc: 0.5385\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7708 - acc: 0.6857 - val_loss: 1.1811 - val_acc: 0.5385\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7688 - acc: 0.6667 - val_loss: 1.1833 - val_acc: 0.5385\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7340 - acc: 0.6857 - val_loss: 1.1855 - val_acc: 0.5385\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7652 - acc: 0.6667 - val_loss: 1.1880 - val_acc: 0.5385\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7513 - acc: 0.6952 - val_loss: 1.1902 - val_acc: 0.5385\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.7477 - acc: 0.7238 - val_loss: 1.1922 - val_acc: 0.5385\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7301 - acc: 0.6762 - val_loss: 1.1931 - val_acc: 0.5385\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7562 - acc: 0.6762 - val_loss: 1.1939 - val_acc: 0.5385\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7302 - acc: 0.6952 - val_loss: 1.1946 - val_acc: 0.5385\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7691 - acc: 0.6762 - val_loss: 1.1936 - val_acc: 0.5385\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7397 - acc: 0.6476 - val_loss: 1.1940 - val_acc: 0.5385\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7599 - acc: 0.6762 - val_loss: 1.1947 - val_acc: 0.5385\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7391 - acc: 0.7143 - val_loss: 1.1962 - val_acc: 0.5385\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7408 - acc: 0.6952 - val_loss: 1.1986 - val_acc: 0.5385\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7232 - acc: 0.7143 - val_loss: 1.2028 - val_acc: 0.5385\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6749 - acc: 0.7429 - val_loss: 1.2082 - val_acc: 0.5385\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7693 - acc: 0.6857 - val_loss: 1.2132 - val_acc: 0.5385\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7094 - acc: 0.7333 - val_loss: 1.2177 - val_acc: 0.5385\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7315 - acc: 0.7048 - val_loss: 1.2232 - val_acc: 0.5385\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.7548 - acc: 0.6857 - val_loss: 1.2281 - val_acc: 0.5385\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.7686 - acc: 0.6476 - val_loss: 1.2322 - val_acc: 0.5385\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.7213 - acc: 0.7333 - val_loss: 1.2359 - val_acc: 0.5385\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.7382 - acc: 0.7048 - val_loss: 1.2393 - val_acc: 0.5385\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7471 - acc: 0.6571 - val_loss: 1.2424 - val_acc: 0.5385\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7310 - acc: 0.6762 - val_loss: 1.2448 - val_acc: 0.5385\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7324 - acc: 0.6857 - val_loss: 1.2472 - val_acc: 0.5385\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7453 - acc: 0.6762 - val_loss: 1.2487 - val_acc: 0.5385\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7220 - acc: 0.7524 - val_loss: 1.2501 - val_acc: 0.5385\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7497 - acc: 0.6857 - val_loss: 1.2505 - val_acc: 0.5385\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.7138 - acc: 0.7143 - val_loss: 1.2518 - val_acc: 0.5385\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.7193 - acc: 0.7238 - val_loss: 1.2527 - val_acc: 0.5385\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6726 - acc: 0.7238 - val_loss: 1.2533 - val_acc: 0.5385\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.7826 - acc: 0.6571 - val_loss: 1.2540 - val_acc: 0.5385\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7121 - acc: 0.6952 - val_loss: 1.2548 - val_acc: 0.5385\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7145 - acc: 0.7333 - val_loss: 1.2555 - val_acc: 0.5385\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7048 - acc: 0.7143 - val_loss: 1.2551 - val_acc: 0.5385\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6928 - acc: 0.7238 - val_loss: 1.2548 - val_acc: 0.5385\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7130 - acc: 0.7238 - val_loss: 1.2545 - val_acc: 0.5385\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7599 - acc: 0.6381 - val_loss: 1.2543 - val_acc: 0.5385\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 84us/step - loss: 0.7208 - acc: 0.7048 - val_loss: 1.2552 - val_acc: 0.5385\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6923 - acc: 0.7048 - val_loss: 1.2563 - val_acc: 0.5385\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 118us/step - loss: 0.7240 - acc: 0.6952 - val_loss: 1.2565 - val_acc: 0.5385\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6947 - acc: 0.7048 - val_loss: 1.2556 - val_acc: 0.5385\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6855 - acc: 0.7238 - val_loss: 1.2554 - val_acc: 0.5385\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.6892 - acc: 0.7143 - val_loss: 1.2551 - val_acc: 0.5385\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6834 - acc: 0.7048 - val_loss: 1.2557 - val_acc: 0.5385\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6698 - acc: 0.7048 - val_loss: 1.2566 - val_acc: 0.5385\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.7048 - acc: 0.6952 - val_loss: 1.2586 - val_acc: 0.5385\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.7161 - acc: 0.7048 - val_loss: 1.2602 - val_acc: 0.5385\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.7201 - acc: 0.6952 - val_loss: 1.2615 - val_acc: 0.5385\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6972 - acc: 0.7143 - val_loss: 1.2640 - val_acc: 0.5385\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6991 - acc: 0.7048 - val_loss: 1.2672 - val_acc: 0.5385\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6650 - acc: 0.7333 - val_loss: 1.2699 - val_acc: 0.5385\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7183 - acc: 0.6952 - val_loss: 1.2733 - val_acc: 0.5385\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.7213 - acc: 0.6667 - val_loss: 1.2757 - val_acc: 0.5385\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.7010 - acc: 0.7333 - val_loss: 1.2772 - val_acc: 0.5385\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6999 - acc: 0.7143 - val_loss: 1.2765 - val_acc: 0.5385\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.7220 - acc: 0.6476 - val_loss: 1.2752 - val_acc: 0.5385\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6826 - acc: 0.7143 - val_loss: 1.2738 - val_acc: 0.5385\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.7410 - acc: 0.6857 - val_loss: 1.2718 - val_acc: 0.5385\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.6517 - acc: 0.7429 - val_loss: 1.2709 - val_acc: 0.5385\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.6639 - acc: 0.7333 - val_loss: 1.2700 - val_acc: 0.5385\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.6542 - acc: 0.7333 - val_loss: 1.2696 - val_acc: 0.5385\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6968 - acc: 0.7524 - val_loss: 1.2702 - val_acc: 0.5385\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6997 - acc: 0.7238 - val_loss: 1.2704 - val_acc: 0.5385\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6729 - acc: 0.7333 - val_loss: 1.2713 - val_acc: 0.5385\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7051 - acc: 0.6857 - val_loss: 1.2737 - val_acc: 0.5385\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6701 - acc: 0.7333 - val_loss: 1.2760 - val_acc: 0.5385\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6708 - acc: 0.7333 - val_loss: 1.2783 - val_acc: 0.5385\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7074 - acc: 0.7048 - val_loss: 1.2811 - val_acc: 0.5385\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7198 - acc: 0.6381 - val_loss: 1.2852 - val_acc: 0.5385\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6496 - acc: 0.7048 - val_loss: 1.2886 - val_acc: 0.5385\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6415 - acc: 0.7524 - val_loss: 1.2935 - val_acc: 0.5385\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6897 - acc: 0.6952 - val_loss: 1.2998 - val_acc: 0.5385\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6333 - acc: 0.7333 - val_loss: 1.3054 - val_acc: 0.5385\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7299 - acc: 0.6952 - val_loss: 1.3083 - val_acc: 0.5385\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6579 - acc: 0.7048 - val_loss: 1.3105 - val_acc: 0.5385\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6675 - acc: 0.7048 - val_loss: 1.3115 - val_acc: 0.5385\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6464 - acc: 0.7524 - val_loss: 1.3133 - val_acc: 0.5385\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 97us/step - loss: 0.6910 - acc: 0.6857 - val_loss: 1.3158 - val_acc: 0.5385\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.6381 - acc: 0.7333 - val_loss: 1.3182 - val_acc: 0.5385\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6891 - acc: 0.7048 - val_loss: 1.3194 - val_acc: 0.5385\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6720 - acc: 0.7429 - val_loss: 1.3207 - val_acc: 0.5385\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6744 - acc: 0.7429 - val_loss: 1.3223 - val_acc: 0.5385\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6767 - acc: 0.7333 - val_loss: 1.3243 - val_acc: 0.5385\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6595 - acc: 0.7429 - val_loss: 1.3269 - val_acc: 0.5385\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6600 - acc: 0.7238 - val_loss: 1.3295 - val_acc: 0.5385\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6300 - acc: 0.7524 - val_loss: 1.3324 - val_acc: 0.5385\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6338 - acc: 0.7524 - val_loss: 1.3351 - val_acc: 0.5385\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6471 - acc: 0.7524 - val_loss: 1.3377 - val_acc: 0.5385\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6797 - acc: 0.7238 - val_loss: 1.3386 - val_acc: 0.5385\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.6787 - acc: 0.7048 - val_loss: 1.3374 - val_acc: 0.5385\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6139 - acc: 0.7048 - val_loss: 1.3378 - val_acc: 0.5385\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.6794 - acc: 0.7048 - val_loss: 1.3356 - val_acc: 0.5385\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.6441 - acc: 0.7619 - val_loss: 1.3350 - val_acc: 0.5385\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.6902 - acc: 0.6952 - val_loss: 1.3347 - val_acc: 0.5385\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.6468 - acc: 0.7048 - val_loss: 1.3335 - val_acc: 0.5385\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 93us/step - loss: 0.6421 - acc: 0.7238 - val_loss: 1.3324 - val_acc: 0.5385\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6056 - acc: 0.7524 - val_loss: 1.3305 - val_acc: 0.5385\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6547 - acc: 0.7143 - val_loss: 1.3295 - val_acc: 0.5385\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5889 - acc: 0.7810 - val_loss: 1.3287 - val_acc: 0.5385\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5959 - acc: 0.7714 - val_loss: 1.3297 - val_acc: 0.5385\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6216 - acc: 0.7333 - val_loss: 1.3310 - val_acc: 0.5385\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6170 - acc: 0.7619 - val_loss: 1.3334 - val_acc: 0.5385\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6281 - acc: 0.7333 - val_loss: 1.3343 - val_acc: 0.5385\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6789 - acc: 0.6857 - val_loss: 1.3350 - val_acc: 0.5385\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6299 - acc: 0.7524 - val_loss: 1.3373 - val_acc: 0.5385\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6072 - acc: 0.7429 - val_loss: 1.3383 - val_acc: 0.5385\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6260 - acc: 0.7333 - val_loss: 1.3397 - val_acc: 0.5385\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6250 - acc: 0.7619 - val_loss: 1.3419 - val_acc: 0.5385\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6300 - acc: 0.7238 - val_loss: 1.3442 - val_acc: 0.5385\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5830 - acc: 0.7429 - val_loss: 1.3476 - val_acc: 0.5385\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6619 - acc: 0.6952 - val_loss: 1.3495 - val_acc: 0.5385\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6543 - acc: 0.7048 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5845 - acc: 0.8000 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6009 - acc: 0.7905 - val_loss: 1.3494 - val_acc: 0.5385\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6165 - acc: 0.7714 - val_loss: 1.3496 - val_acc: 0.5385\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6066 - acc: 0.7619 - val_loss: 1.3495 - val_acc: 0.5385\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6540 - acc: 0.7048 - val_loss: 1.3494 - val_acc: 0.5385\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.6415 - acc: 0.7429 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5953 - acc: 0.7429 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5793 - acc: 0.8095 - val_loss: 1.3521 - val_acc: 0.5385\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6325 - acc: 0.7238 - val_loss: 1.3544 - val_acc: 0.5385\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.6172 - acc: 0.7714 - val_loss: 1.3569 - val_acc: 0.5385\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6241 - acc: 0.7143 - val_loss: 1.3608 - val_acc: 0.5385\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6430 - acc: 0.7333 - val_loss: 1.3658 - val_acc: 0.5385\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5897 - acc: 0.7714 - val_loss: 1.3693 - val_acc: 0.5385\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6032 - acc: 0.7048 - val_loss: 1.3733 - val_acc: 0.5385\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5561 - acc: 0.8000 - val_loss: 1.3781 - val_acc: 0.5385\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6201 - acc: 0.7714 - val_loss: 1.3815 - val_acc: 0.5385\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6079 - acc: 0.7048 - val_loss: 1.3860 - val_acc: 0.5385\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6292 - acc: 0.7524 - val_loss: 1.3926 - val_acc: 0.5385\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5971 - acc: 0.7143 - val_loss: 1.4013 - val_acc: 0.5385\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6148 - acc: 0.7524 - val_loss: 1.4101 - val_acc: 0.5385\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6462 - acc: 0.6857 - val_loss: 1.4185 - val_acc: 0.5385\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5951 - acc: 0.7524 - val_loss: 1.4273 - val_acc: 0.5385\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5939 - acc: 0.7714 - val_loss: 1.4337 - val_acc: 0.5385\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6513 - acc: 0.7238 - val_loss: 1.4373 - val_acc: 0.5385\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5503 - acc: 0.7619 - val_loss: 1.4421 - val_acc: 0.5385\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6365 - acc: 0.7143 - val_loss: 1.4456 - val_acc: 0.5385\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6105 - acc: 0.7905 - val_loss: 1.4485 - val_acc: 0.5385\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6142 - acc: 0.7333 - val_loss: 1.4535 - val_acc: 0.5385\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6000 - acc: 0.7905 - val_loss: 1.4598 - val_acc: 0.5385\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6384 - acc: 0.7619 - val_loss: 1.4655 - val_acc: 0.5385\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6027 - acc: 0.7524 - val_loss: 1.4703 - val_acc: 0.5385\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6383 - acc: 0.7429 - val_loss: 1.4742 - val_acc: 0.5385\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5770 - acc: 0.7810 - val_loss: 1.4771 - val_acc: 0.5385\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6116 - acc: 0.7429 - val_loss: 1.4791 - val_acc: 0.5385\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5617 - acc: 0.7524 - val_loss: 1.4809 - val_acc: 0.5385\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5943 - acc: 0.7714 - val_loss: 1.4856 - val_acc: 0.5385\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6228 - acc: 0.6952 - val_loss: 1.4866 - val_acc: 0.5385\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5331 - acc: 0.7810 - val_loss: 1.4875 - val_acc: 0.5385\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5982 - acc: 0.7524 - val_loss: 1.4898 - val_acc: 0.5385\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5526 - acc: 0.8095 - val_loss: 1.4914 - val_acc: 0.5385\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5824 - acc: 0.7714 - val_loss: 1.4930 - val_acc: 0.5385\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5559 - acc: 0.7905 - val_loss: 1.4960 - val_acc: 0.5385\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5633 - acc: 0.7714 - val_loss: 1.4953 - val_acc: 0.5385\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5873 - acc: 0.7714 - val_loss: 1.4946 - val_acc: 0.5385\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6025 - acc: 0.7143 - val_loss: 1.4924 - val_acc: 0.5385\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6322 - acc: 0.7048 - val_loss: 1.4890 - val_acc: 0.5385\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5549 - acc: 0.8381 - val_loss: 1.4858 - val_acc: 0.5385\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5490 - acc: 0.8286 - val_loss: 1.4836 - val_acc: 0.5385\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5951 - acc: 0.7619 - val_loss: 1.4811 - val_acc: 0.5385\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6068 - acc: 0.7524 - val_loss: 1.4789 - val_acc: 0.5385\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5717 - acc: 0.7905 - val_loss: 1.4774 - val_acc: 0.5385\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5967 - acc: 0.7333 - val_loss: 1.4772 - val_acc: 0.5385\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5272 - acc: 0.7810 - val_loss: 1.4766 - val_acc: 0.5385\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5351 - acc: 0.8000 - val_loss: 1.4757 - val_acc: 0.5385\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5949 - acc: 0.7619 - val_loss: 1.4735 - val_acc: 0.5385\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5584 - acc: 0.7905 - val_loss: 1.4717 - val_acc: 0.5385\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5823 - acc: 0.7619 - val_loss: 1.4670 - val_acc: 0.5385\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5351 - acc: 0.7905 - val_loss: 1.4639 - val_acc: 0.5385\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5876 - acc: 0.7524 - val_loss: 1.4591 - val_acc: 0.5385\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5454 - acc: 0.8000 - val_loss: 1.4546 - val_acc: 0.5385\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5311 - acc: 0.7905 - val_loss: 1.4499 - val_acc: 0.5385\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5780 - acc: 0.8000 - val_loss: 1.4448 - val_acc: 0.5385\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5754 - acc: 0.8000 - val_loss: 1.4395 - val_acc: 0.6154\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5331 - acc: 0.7905 - val_loss: 1.4351 - val_acc: 0.6154\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5146 - acc: 0.8095 - val_loss: 1.4329 - val_acc: 0.6154\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5241 - acc: 0.7619 - val_loss: 1.4317 - val_acc: 0.6154\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5799 - acc: 0.7714 - val_loss: 1.4339 - val_acc: 0.6154\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5267 - acc: 0.8000 - val_loss: 1.4364 - val_acc: 0.5385\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5605 - acc: 0.7905 - val_loss: 1.4418 - val_acc: 0.5385\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5447 - acc: 0.8000 - val_loss: 1.4485 - val_acc: 0.5385\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5385 - acc: 0.8000 - val_loss: 1.4570 - val_acc: 0.5385\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4938 - acc: 0.8190 - val_loss: 1.4674 - val_acc: 0.5385\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5100 - acc: 0.8000 - val_loss: 1.4797 - val_acc: 0.5385\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5644 - acc: 0.7524 - val_loss: 1.4890 - val_acc: 0.5385\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6036 - acc: 0.7714 - val_loss: 1.4972 - val_acc: 0.5385\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6183 - acc: 0.7429 - val_loss: 1.5040 - val_acc: 0.5385\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4928 - acc: 0.8286 - val_loss: 1.5092 - val_acc: 0.5385\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5769 - acc: 0.8095 - val_loss: 1.5122 - val_acc: 0.5385\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5117 - acc: 0.8000 - val_loss: 1.5137 - val_acc: 0.5385\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5127 - acc: 0.7810 - val_loss: 1.5137 - val_acc: 0.5385\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5290 - acc: 0.8286 - val_loss: 1.5151 - val_acc: 0.5385\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.4793 - acc: 0.8286 - val_loss: 1.5175 - val_acc: 0.5385\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.5078 - acc: 0.8000 - val_loss: 1.5195 - val_acc: 0.5385\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.5614 - acc: 0.7714 - val_loss: 1.5228 - val_acc: 0.5385\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5286 - acc: 0.8190 - val_loss: 1.5243 - val_acc: 0.5385\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.5003 - acc: 0.8286 - val_loss: 1.5300 - val_acc: 0.5385\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.5659 - acc: 0.7810 - val_loss: 1.5335 - val_acc: 0.5385\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5416 - acc: 0.8000 - val_loss: 1.5366 - val_acc: 0.5385\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5397 - acc: 0.7619 - val_loss: 1.5423 - val_acc: 0.5385\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5147 - acc: 0.7810 - val_loss: 1.5479 - val_acc: 0.5385\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5205 - acc: 0.8000 - val_loss: 1.5531 - val_acc: 0.5385\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5786 - acc: 0.7429 - val_loss: 1.5585 - val_acc: 0.5385\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5221 - acc: 0.8000 - val_loss: 1.5639 - val_acc: 0.5385\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5538 - acc: 0.7810 - val_loss: 1.5678 - val_acc: 0.5385\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5450 - acc: 0.7714 - val_loss: 1.5732 - val_acc: 0.6154\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5384 - acc: 0.7810 - val_loss: 1.5745 - val_acc: 0.6154\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5267 - acc: 0.8095 - val_loss: 1.5737 - val_acc: 0.6154\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4837 - acc: 0.8571 - val_loss: 1.5725 - val_acc: 0.6154\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5657 - acc: 0.7810 - val_loss: 1.5744 - val_acc: 0.6154\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5119 - acc: 0.8286 - val_loss: 1.5749 - val_acc: 0.6154\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4723 - acc: 0.8667 - val_loss: 1.5767 - val_acc: 0.6154\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4899 - acc: 0.8286 - val_loss: 1.5765 - val_acc: 0.6154\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4706 - acc: 0.8286 - val_loss: 1.5815 - val_acc: 0.6154\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5278 - acc: 0.7905 - val_loss: 1.5850 - val_acc: 0.6154\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5166 - acc: 0.7905 - val_loss: 1.5885 - val_acc: 0.6154\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5011 - acc: 0.7905 - val_loss: 1.5925 - val_acc: 0.6154\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.5150 - acc: 0.8571 - val_loss: 1.5947 - val_acc: 0.6154\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5837 - acc: 0.7524 - val_loss: 1.5985 - val_acc: 0.6154\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4765 - acc: 0.8286 - val_loss: 1.6020 - val_acc: 0.6154\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5526 - acc: 0.8190 - val_loss: 1.6068 - val_acc: 0.6154\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.5073 - acc: 0.8000 - val_loss: 1.6093 - val_acc: 0.6154\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5040 - acc: 0.8190 - val_loss: 1.6130 - val_acc: 0.6154\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.5058 - acc: 0.8190 - val_loss: 1.6171 - val_acc: 0.6154\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.5609 - acc: 0.7524 - val_loss: 1.6195 - val_acc: 0.6154\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.5504 - acc: 0.8095 - val_loss: 1.6184 - val_acc: 0.6154\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5166 - acc: 0.8476 - val_loss: 1.6152 - val_acc: 0.6154\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4834 - acc: 0.8190 - val_loss: 1.6134 - val_acc: 0.6154\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5408 - acc: 0.7810 - val_loss: 1.6097 - val_acc: 0.6154\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5078 - acc: 0.8190 - val_loss: 1.6058 - val_acc: 0.6154\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.4722 - acc: 0.8095 - val_loss: 1.6036 - val_acc: 0.6154\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.4351 - acc: 0.8476 - val_loss: 1.6034 - val_acc: 0.6154\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5464 - acc: 0.8381 - val_loss: 1.6040 - val_acc: 0.6154\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4568 - acc: 0.8381 - val_loss: 1.6067 - val_acc: 0.6154\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.4594 - acc: 0.8381 - val_loss: 1.6098 - val_acc: 0.6154\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.4593 - acc: 0.8571 - val_loss: 1.6144 - val_acc: 0.6154\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 87us/step - loss: 0.5190 - acc: 0.7905 - val_loss: 1.6214 - val_acc: 0.6154\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 101us/step - loss: 0.4812 - acc: 0.8286 - val_loss: 1.6271 - val_acc: 0.6154\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4904 - acc: 0.8476 - val_loss: 1.6317 - val_acc: 0.6154\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4884 - acc: 0.7905 - val_loss: 1.6383 - val_acc: 0.6154\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.5229 - acc: 0.8000 - val_loss: 1.6468 - val_acc: 0.6154\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.4435 - acc: 0.8762 - val_loss: 1.6531 - val_acc: 0.6154\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4623 - acc: 0.8667 - val_loss: 1.6619 - val_acc: 0.6154\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 94us/step - loss: 0.4690 - acc: 0.8571 - val_loss: 1.6707 - val_acc: 0.6154\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5087 - acc: 0.8286 - val_loss: 1.6799 - val_acc: 0.6154\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4585 - acc: 0.8571 - val_loss: 1.6888 - val_acc: 0.6154\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.4593 - acc: 0.8381 - val_loss: 1.6972 - val_acc: 0.6154\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4333 - acc: 0.8667 - val_loss: 1.7022 - val_acc: 0.6154\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5151 - acc: 0.8095 - val_loss: 1.7067 - val_acc: 0.6154\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4973 - acc: 0.8476 - val_loss: 1.7120 - val_acc: 0.6154\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4810 - acc: 0.8476 - val_loss: 1.7149 - val_acc: 0.6154\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4781 - acc: 0.8571 - val_loss: 1.7170 - val_acc: 0.6154\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4759 - acc: 0.8571 - val_loss: 1.7180 - val_acc: 0.6154\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5178 - acc: 0.7810 - val_loss: 1.7156 - val_acc: 0.6154\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4671 - acc: 0.8571 - val_loss: 1.7146 - val_acc: 0.6154\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5716 - acc: 0.7905 - val_loss: 1.7117 - val_acc: 0.6154\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4959 - acc: 0.8476 - val_loss: 1.7114 - val_acc: 0.6154\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5209 - acc: 0.7905 - val_loss: 1.7130 - val_acc: 0.6154\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.4757 - acc: 0.8286 - val_loss: 1.7125 - val_acc: 0.6154\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4718 - acc: 0.8667 - val_loss: 1.7077 - val_acc: 0.6154\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4888 - acc: 0.8190 - val_loss: 1.7035 - val_acc: 0.6154\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.4690 - acc: 0.8286 - val_loss: 1.7025 - val_acc: 0.6154\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4453 - acc: 0.8857 - val_loss: 1.7037 - val_acc: 0.6154\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4168 - acc: 0.8667 - val_loss: 1.7057 - val_acc: 0.6154\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4579 - acc: 0.8857 - val_loss: 1.7102 - val_acc: 0.6154\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5128 - acc: 0.8190 - val_loss: 1.7115 - val_acc: 0.6154\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5177 - acc: 0.8000 - val_loss: 1.7113 - val_acc: 0.6154\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4984 - acc: 0.8476 - val_loss: 1.7156 - val_acc: 0.6154\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5371 - acc: 0.8095 - val_loss: 1.7172 - val_acc: 0.6154\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4342 - acc: 0.8667 - val_loss: 1.7140 - val_acc: 0.6154\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4837 - acc: 0.8286 - val_loss: 1.7090 - val_acc: 0.6154\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.5337 - acc: 0.8000 - val_loss: 1.7012 - val_acc: 0.6154\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4760 - acc: 0.8000 - val_loss: 1.6911 - val_acc: 0.6154\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.4237 - acc: 0.8667 - val_loss: 1.6864 - val_acc: 0.6154\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.4832 - acc: 0.8476 - val_loss: 1.6843 - val_acc: 0.6154\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.4323 - acc: 0.9048 - val_loss: 1.6833 - val_acc: 0.6154\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.4318 - acc: 0.8286 - val_loss: 1.6831 - val_acc: 0.6154\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 114us/step - loss: 0.4505 - acc: 0.8762 - val_loss: 1.6837 - val_acc: 0.6154\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.4252 - acc: 0.8762 - val_loss: 1.6866 - val_acc: 0.6154\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4929 - acc: 0.8571 - val_loss: 1.6926 - val_acc: 0.6154\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4548 - acc: 0.8762 - val_loss: 1.6968 - val_acc: 0.6154\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4129 - acc: 0.9143 - val_loss: 1.7023 - val_acc: 0.6154\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4289 - acc: 0.8762 - val_loss: 1.7100 - val_acc: 0.6154\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.4362 - acc: 0.8667 - val_loss: 1.7204 - val_acc: 0.6154\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4240 - acc: 0.8667 - val_loss: 1.7339 - val_acc: 0.6154\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4735 - acc: 0.8381 - val_loss: 1.7491 - val_acc: 0.6154\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4450 - acc: 0.8667 - val_loss: 1.7631 - val_acc: 0.6154\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4541 - acc: 0.8571 - val_loss: 1.7768 - val_acc: 0.6154\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4174 - acc: 0.8857 - val_loss: 1.7882 - val_acc: 0.6154\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4420 - acc: 0.8571 - val_loss: 1.7985 - val_acc: 0.6154\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.8857 - val_loss: 1.8074 - val_acc: 0.6154\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4599 - acc: 0.8762 - val_loss: 1.8134 - val_acc: 0.6154\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4832 - acc: 0.8000 - val_loss: 1.8165 - val_acc: 0.6154\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4917 - acc: 0.8476 - val_loss: 1.8132 - val_acc: 0.6154\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4432 - acc: 0.8381 - val_loss: 1.8117 - val_acc: 0.6154\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4733 - acc: 0.8286 - val_loss: 1.8102 - val_acc: 0.6154\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 108us/step - loss: 0.4301 - acc: 0.8857 - val_loss: 1.8071 - val_acc: 0.6154\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 121us/step - loss: 0.4510 - acc: 0.8571 - val_loss: 1.8077 - val_acc: 0.6154\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4411 - acc: 0.8857 - val_loss: 1.8084 - val_acc: 0.6154\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.4150 - acc: 0.8857 - val_loss: 1.8133 - val_acc: 0.6154\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.4645 - acc: 0.8857 - val_loss: 1.8179 - val_acc: 0.6154\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5014 - acc: 0.8190 - val_loss: 1.8215 - val_acc: 0.6154\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.4448 - acc: 0.8571 - val_loss: 1.8197 - val_acc: 0.6154\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.4154 - acc: 0.8667 - val_loss: 1.8178 - val_acc: 0.6154\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4829 - acc: 0.8095 - val_loss: 1.8186 - val_acc: 0.6154\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.4609 - acc: 0.8476 - val_loss: 1.8187 - val_acc: 0.6154\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4028 - acc: 0.8857 - val_loss: 1.8195 - val_acc: 0.6154\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.4715 - acc: 0.8381 - val_loss: 1.8189 - val_acc: 0.6154\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4529 - acc: 0.8952 - val_loss: 1.8206 - val_acc: 0.6154\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4591 - acc: 0.8667 - val_loss: 1.8243 - val_acc: 0.6154\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4243 - acc: 0.8571 - val_loss: 1.8298 - val_acc: 0.6154\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4289 - acc: 0.8857 - val_loss: 1.8384 - val_acc: 0.6154\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4293 - acc: 0.8762 - val_loss: 1.8469 - val_acc: 0.6154\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4050 - acc: 0.8667 - val_loss: 1.8555 - val_acc: 0.6154\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5254 - acc: 0.8095 - val_loss: 1.8626 - val_acc: 0.6154\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4528 - acc: 0.8667 - val_loss: 1.8672 - val_acc: 0.6154\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4363 - acc: 0.8762 - val_loss: 1.8698 - val_acc: 0.6154\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4390 - acc: 0.8571 - val_loss: 1.8694 - val_acc: 0.6154\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4659 - acc: 0.8857 - val_loss: 1.8689 - val_acc: 0.6154\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4778 - acc: 0.8476 - val_loss: 1.8676 - val_acc: 0.6154\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4082 - acc: 0.8667 - val_loss: 1.8677 - val_acc: 0.6154\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3824 - acc: 0.9048 - val_loss: 1.8659 - val_acc: 0.6154\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4678 - acc: 0.8762 - val_loss: 1.8658 - val_acc: 0.6154\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4538 - acc: 0.8190 - val_loss: 1.8671 - val_acc: 0.6154\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4789 - acc: 0.8571 - val_loss: 1.8726 - val_acc: 0.6154\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4928 - acc: 0.8286 - val_loss: 1.8793 - val_acc: 0.6154\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4315 - acc: 0.9048 - val_loss: 1.8875 - val_acc: 0.6154\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4452 - acc: 0.8762 - val_loss: 1.8939 - val_acc: 0.6154\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4811 - acc: 0.8381 - val_loss: 1.8988 - val_acc: 0.6154\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.4244 - acc: 0.9238 - val_loss: 1.9052 - val_acc: 0.6154\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4257 - acc: 0.8667 - val_loss: 1.9050 - val_acc: 0.6154\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4105 - acc: 0.8762 - val_loss: 1.9038 - val_acc: 0.6154\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3951 - acc: 0.9048 - val_loss: 1.9011 - val_acc: 0.6154\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4060 - acc: 0.9143 - val_loss: 1.8947 - val_acc: 0.6154\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4142 - acc: 0.8857 - val_loss: 1.8884 - val_acc: 0.6154\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4191 - acc: 0.8571 - val_loss: 1.8832 - val_acc: 0.6154\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4450 - acc: 0.9048 - val_loss: 1.8763 - val_acc: 0.6154\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4430 - acc: 0.8667 - val_loss: 1.8711 - val_acc: 0.6154\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3834 - acc: 0.9048 - val_loss: 1.8671 - val_acc: 0.6154\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4044 - acc: 0.8571 - val_loss: 1.8659 - val_acc: 0.6154\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4169 - acc: 0.8762 - val_loss: 1.8637 - val_acc: 0.6154\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.4398 - acc: 0.8476 - val_loss: 1.8661 - val_acc: 0.6154\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.4211 - acc: 0.8857 - val_loss: 1.8716 - val_acc: 0.6154\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 88us/step - loss: 0.4506 - acc: 0.8762 - val_loss: 1.8781 - val_acc: 0.6154\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.4048 - acc: 0.9048 - val_loss: 1.8872 - val_acc: 0.6154\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4175 - acc: 0.8571 - val_loss: 1.8997 - val_acc: 0.6154\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.4339 - acc: 0.8762 - val_loss: 1.9150 - val_acc: 0.6154\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4285 - acc: 0.8381 - val_loss: 1.9254 - val_acc: 0.6154\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4041 - acc: 0.9048 - val_loss: 1.9367 - val_acc: 0.6154\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3823 - acc: 0.8762 - val_loss: 1.9483 - val_acc: 0.6154\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3793 - acc: 0.9048 - val_loss: 1.9591 - val_acc: 0.6154\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4550 - acc: 0.8381 - val_loss: 1.9695 - val_acc: 0.6154\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3827 - acc: 0.9143 - val_loss: 1.9792 - val_acc: 0.6154\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3835 - acc: 0.8762 - val_loss: 1.9873 - val_acc: 0.6154\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 90us/step - loss: 0.4233 - acc: 0.8571 - val_loss: 1.9941 - val_acc: 0.6154\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4191 - acc: 0.8857 - val_loss: 2.0034 - val_acc: 0.6154\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4188 - acc: 0.8667 - val_loss: 2.0129 - val_acc: 0.6154\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3946 - acc: 0.9333 - val_loss: 2.0233 - val_acc: 0.6154\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3945 - acc: 0.8952 - val_loss: 2.0300 - val_acc: 0.6154\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.3983 - acc: 0.8857 - val_loss: 2.0344 - val_acc: 0.6154\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3983 - acc: 0.8762 - val_loss: 2.0361 - val_acc: 0.6154\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3569 - acc: 0.9238 - val_loss: 2.0362 - val_acc: 0.6154\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3522 - acc: 0.8857 - val_loss: 2.0338 - val_acc: 0.6154\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3863 - acc: 0.8381 - val_loss: 2.0316 - val_acc: 0.6154\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4101 - acc: 0.8762 - val_loss: 2.0282 - val_acc: 0.6154\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3651 - acc: 0.9048 - val_loss: 2.0230 - val_acc: 0.6154\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3901 - acc: 0.8857 - val_loss: 2.0222 - val_acc: 0.6154\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.4117 - acc: 0.8857 - val_loss: 2.0238 - val_acc: 0.6154\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4077 - acc: 0.8667 - val_loss: 2.0306 - val_acc: 0.6154\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.3965 - acc: 0.8667 - val_loss: 2.0349 - val_acc: 0.6154\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3599 - acc: 0.9143 - val_loss: 2.0396 - val_acc: 0.6154\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.3960 - acc: 0.9143 - val_loss: 2.0413 - val_acc: 0.6154\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.4030 - acc: 0.9143 - val_loss: 2.0466 - val_acc: 0.6154\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4294 - acc: 0.9143 - val_loss: 2.0531 - val_acc: 0.6154\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4134 - acc: 0.8476 - val_loss: 2.0600 - val_acc: 0.6154\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3137 - acc: 0.9333 - val_loss: 2.0640 - val_acc: 0.6154\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.4164 - acc: 0.8952 - val_loss: 2.0655 - val_acc: 0.6154\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4062 - acc: 0.8952 - val_loss: 2.0622 - val_acc: 0.6154\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3857 - acc: 0.8952 - val_loss: 2.0568 - val_acc: 0.6154\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4351 - acc: 0.8381 - val_loss: 2.0521 - val_acc: 0.6154\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.3888 - acc: 0.9143 - val_loss: 2.0472 - val_acc: 0.6154\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3950 - acc: 0.9238 - val_loss: 2.0420 - val_acc: 0.6154\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3652 - acc: 0.9143 - val_loss: 2.0380 - val_acc: 0.6154\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4178 - acc: 0.8476 - val_loss: 2.0332 - val_acc: 0.6154\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4763 - acc: 0.8381 - val_loss: 2.0312 - val_acc: 0.6154\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3667 - acc: 0.9048 - val_loss: 2.0321 - val_acc: 0.6154\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 84us/step - loss: 0.3666 - acc: 0.9429 - val_loss: 2.0352 - val_acc: 0.6154\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.3991 - acc: 0.8857 - val_loss: 2.0394 - val_acc: 0.6154\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3509 - acc: 0.9048 - val_loss: 2.0429 - val_acc: 0.6154\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3370 - acc: 0.9429 - val_loss: 2.0477 - val_acc: 0.6154\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3485 - acc: 0.9143 - val_loss: 2.0513 - val_acc: 0.6154\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3548 - acc: 0.9429 - val_loss: 2.0563 - val_acc: 0.6154\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3588 - acc: 0.9333 - val_loss: 2.0672 - val_acc: 0.6154\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3447 - acc: 0.9333 - val_loss: 2.0738 - val_acc: 0.6154\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3918 - acc: 0.9048 - val_loss: 2.0750 - val_acc: 0.6154\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.3527 - acc: 0.9238 - val_loss: 2.0782 - val_acc: 0.6154\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 88us/step - loss: 0.3153 - acc: 0.9524 - val_loss: 2.0799 - val_acc: 0.6154\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4356 - acc: 0.8571 - val_loss: 2.0829 - val_acc: 0.6154\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.3342 - acc: 0.9333 - val_loss: 2.0847 - val_acc: 0.6154\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3681 - acc: 0.8952 - val_loss: 2.0860 - val_acc: 0.6154\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.4280 - acc: 0.8762 - val_loss: 2.0931 - val_acc: 0.5385\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3428 - acc: 0.9524 - val_loss: 2.1000 - val_acc: 0.5385\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.4009 - acc: 0.8762 - val_loss: 2.1063 - val_acc: 0.5385\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3526 - acc: 0.9143 - val_loss: 2.1125 - val_acc: 0.5385\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3561 - acc: 0.9048 - val_loss: 2.1164 - val_acc: 0.5385\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3585 - acc: 0.9238 - val_loss: 2.1206 - val_acc: 0.5385\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3448 - acc: 0.9333 - val_loss: 2.1254 - val_acc: 0.5385\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4312 - acc: 0.8571 - val_loss: 2.1314 - val_acc: 0.5385\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3195 - acc: 0.9333 - val_loss: 2.1383 - val_acc: 0.5385\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3443 - acc: 0.9048 - val_loss: 2.1451 - val_acc: 0.5385\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3661 - acc: 0.9333 - val_loss: 2.1539 - val_acc: 0.5385\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3838 - acc: 0.9143 - val_loss: 2.1606 - val_acc: 0.5385\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3852 - acc: 0.9143 - val_loss: 2.1613 - val_acc: 0.5385\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3214 - acc: 0.8952 - val_loss: 2.1630 - val_acc: 0.5385\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4195 - acc: 0.8857 - val_loss: 2.1659 - val_acc: 0.5385\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3390 - acc: 0.9238 - val_loss: 2.1675 - val_acc: 0.5385\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3933 - acc: 0.8476 - val_loss: 2.1678 - val_acc: 0.5385\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3802 - acc: 0.9238 - val_loss: 2.1723 - val_acc: 0.5385\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4225 - acc: 0.8571 - val_loss: 2.1717 - val_acc: 0.5385\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3181 - acc: 0.9333 - val_loss: 2.1728 - val_acc: 0.5385\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3410 - acc: 0.9143 - val_loss: 2.1738 - val_acc: 0.5385\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3835 - acc: 0.9143 - val_loss: 2.1743 - val_acc: 0.5385\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3684 - acc: 0.9048 - val_loss: 2.1734 - val_acc: 0.5385\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3584 - acc: 0.9429 - val_loss: 2.1725 - val_acc: 0.5385\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3258 - acc: 0.9524 - val_loss: 2.1749 - val_acc: 0.5385\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3925 - acc: 0.9048 - val_loss: 2.1776 - val_acc: 0.5385\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3506 - acc: 0.9143 - val_loss: 2.1808 - val_acc: 0.5385\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3496 - acc: 0.9143 - val_loss: 2.1878 - val_acc: 0.5385\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4483 - acc: 0.8571 - val_loss: 2.1909 - val_acc: 0.5385\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3929 - acc: 0.9143 - val_loss: 2.2010 - val_acc: 0.5385\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.3584 - acc: 0.9048 - val_loss: 2.2045 - val_acc: 0.5385\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3809 - acc: 0.8857 - val_loss: 2.2066 - val_acc: 0.5385\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4538 - acc: 0.8381 - val_loss: 2.2030 - val_acc: 0.5385\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3544 - acc: 0.8857 - val_loss: 2.2051 - val_acc: 0.5385\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3875 - acc: 0.9048 - val_loss: 2.2073 - val_acc: 0.5385\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3437 - acc: 0.8952 - val_loss: 2.2075 - val_acc: 0.5385\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3394 - acc: 0.9048 - val_loss: 2.2068 - val_acc: 0.5385\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3014 - acc: 0.9238 - val_loss: 2.2078 - val_acc: 0.5385\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4050 - acc: 0.8857 - val_loss: 2.2094 - val_acc: 0.5385\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3848 - acc: 0.9143 - val_loss: 2.2161 - val_acc: 0.5385\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3661 - acc: 0.9143 - val_loss: 2.2250 - val_acc: 0.5385\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3626 - acc: 0.9333 - val_loss: 2.2373 - val_acc: 0.5385\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3795 - acc: 0.8857 - val_loss: 2.2477 - val_acc: 0.5385\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3699 - acc: 0.9048 - val_loss: 2.2548 - val_acc: 0.5385\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3162 - acc: 0.9524 - val_loss: 2.2614 - val_acc: 0.5385\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3895 - acc: 0.8952 - val_loss: 2.2687 - val_acc: 0.5385\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3981 - acc: 0.8667 - val_loss: 2.2744 - val_acc: 0.5385\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4251 - acc: 0.8381 - val_loss: 2.2767 - val_acc: 0.5385\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3586 - acc: 0.9048 - val_loss: 2.2760 - val_acc: 0.5385\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3362 - acc: 0.9143 - val_loss: 2.2719 - val_acc: 0.5385\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3594 - acc: 0.8952 - val_loss: 2.2717 - val_acc: 0.5385\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3854 - acc: 0.8952 - val_loss: 2.2680 - val_acc: 0.5385\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3598 - acc: 0.9238 - val_loss: 2.2641 - val_acc: 0.5385\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3588 - acc: 0.8952 - val_loss: 2.2599 - val_acc: 0.5385\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3418 - acc: 0.9238 - val_loss: 2.2555 - val_acc: 0.5385\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3683 - acc: 0.9048 - val_loss: 2.2582 - val_acc: 0.5385\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3597 - acc: 0.8952 - val_loss: 2.2604 - val_acc: 0.5385\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 87us/step - loss: 0.3694 - acc: 0.9048 - val_loss: 2.2638 - val_acc: 0.5385\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.3508 - acc: 0.9143 - val_loss: 2.2663 - val_acc: 0.5385\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3756 - acc: 0.9048 - val_loss: 2.2710 - val_acc: 0.6154\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 105us/step - loss: 0.3131 - acc: 0.9429 - val_loss: 2.2800 - val_acc: 0.6154\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.3748 - acc: 0.9143 - val_loss: 2.2871 - val_acc: 0.6154\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 97us/step - loss: 0.3493 - acc: 0.9048 - val_loss: 2.2925 - val_acc: 0.6154\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3698 - acc: 0.9048 - val_loss: 2.2969 - val_acc: 0.6154\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3819 - acc: 0.9238 - val_loss: 2.2979 - val_acc: 0.6154\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3353 - acc: 0.9048 - val_loss: 2.2973 - val_acc: 0.6154\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.3422 - acc: 0.9143 - val_loss: 2.2919 - val_acc: 0.6154\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3251 - acc: 0.8952 - val_loss: 2.2825 - val_acc: 0.6154\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3217 - acc: 0.9333 - val_loss: 2.2769 - val_acc: 0.6154\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3360 - acc: 0.9048 - val_loss: 2.2737 - val_acc: 0.6154\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3524 - acc: 0.8762 - val_loss: 2.2754 - val_acc: 0.5385\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3211 - acc: 0.9143 - val_loss: 2.2810 - val_acc: 0.5385\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3139 - acc: 0.9333 - val_loss: 2.2937 - val_acc: 0.5385\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3777 - acc: 0.8952 - val_loss: 2.3055 - val_acc: 0.5385\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3076 - acc: 0.9429 - val_loss: 2.3178 - val_acc: 0.5385\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3242 - acc: 0.9238 - val_loss: 2.3327 - val_acc: 0.6154\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3632 - acc: 0.8762 - val_loss: 2.3447 - val_acc: 0.6154\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3433 - acc: 0.8952 - val_loss: 2.3564 - val_acc: 0.6154\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.3567 - acc: 0.9238 - val_loss: 2.3625 - val_acc: 0.6154\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.3606 - acc: 0.9048 - val_loss: 2.3662 - val_acc: 0.6154\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2900 - acc: 0.9429 - val_loss: 2.3671 - val_acc: 0.6154\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3051 - acc: 0.9143 - val_loss: 2.3647 - val_acc: 0.6154\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3090 - acc: 0.9333 - val_loss: 2.3653 - val_acc: 0.6154\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3166 - acc: 0.9333 - val_loss: 2.3641 - val_acc: 0.6154\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3162 - acc: 0.8952 - val_loss: 2.3622 - val_acc: 0.6154\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3291 - acc: 0.9238 - val_loss: 2.3579 - val_acc: 0.6154\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3363 - acc: 0.9333 - val_loss: 2.3538 - val_acc: 0.6154\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3145 - acc: 0.9333 - val_loss: 2.3506 - val_acc: 0.6154\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3130 - acc: 0.9238 - val_loss: 2.3481 - val_acc: 0.6154\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3450 - acc: 0.8857 - val_loss: 2.3421 - val_acc: 0.5385\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3344 - acc: 0.9333 - val_loss: 2.3349 - val_acc: 0.5385\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3598 - acc: 0.9333 - val_loss: 2.3318 - val_acc: 0.5385\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3982 - acc: 0.8762 - val_loss: 2.3284 - val_acc: 0.5385\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3557 - acc: 0.9048 - val_loss: 2.3247 - val_acc: 0.5385\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3446 - acc: 0.9429 - val_loss: 2.3266 - val_acc: 0.5385\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3207 - acc: 0.9143 - val_loss: 2.3273 - val_acc: 0.5385\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.2422 - acc: 0.9714 - val_loss: 2.3297 - val_acc: 0.5385\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.3488 - acc: 0.9048 - val_loss: 2.3343 - val_acc: 0.5385\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3816 - acc: 0.9048 - val_loss: 2.3413 - val_acc: 0.5385\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3063 - acc: 0.9429 - val_loss: 2.3473 - val_acc: 0.5385\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2977 - acc: 0.9333 - val_loss: 2.3504 - val_acc: 0.5385\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3365 - acc: 0.9524 - val_loss: 2.3490 - val_acc: 0.5385\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3380 - acc: 0.9333 - val_loss: 2.3493 - val_acc: 0.5385\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3304 - acc: 0.9238 - val_loss: 2.3537 - val_acc: 0.5385\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4035 - acc: 0.8762 - val_loss: 2.3589 - val_acc: 0.5385\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3328 - acc: 0.9143 - val_loss: 2.3670 - val_acc: 0.5385\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3285 - acc: 0.9143 - val_loss: 2.3738 - val_acc: 0.5385\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3318 - acc: 0.9333 - val_loss: 2.3788 - val_acc: 0.5385\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3555 - acc: 0.9333 - val_loss: 2.3860 - val_acc: 0.5385\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3169 - acc: 0.9048 - val_loss: 2.3951 - val_acc: 0.5385\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3246 - acc: 0.9238 - val_loss: 2.4041 - val_acc: 0.5385\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3831 - acc: 0.8762 - val_loss: 2.4153 - val_acc: 0.5385\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3370 - acc: 0.8762 - val_loss: 2.4260 - val_acc: 0.5385\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3685 - acc: 0.8857 - val_loss: 2.4375 - val_acc: 0.5385\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3283 - acc: 0.9238 - val_loss: 2.4432 - val_acc: 0.5385\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2713 - acc: 0.9524 - val_loss: 2.4496 - val_acc: 0.5385\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3208 - acc: 0.9143 - val_loss: 2.4499 - val_acc: 0.5385\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3659 - acc: 0.9048 - val_loss: 2.4419 - val_acc: 0.5385\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3179 - acc: 0.9143 - val_loss: 2.4301 - val_acc: 0.5385\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3131 - acc: 0.9524 - val_loss: 2.4195 - val_acc: 0.5385\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3561 - acc: 0.9238 - val_loss: 2.4130 - val_acc: 0.5385\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3437 - acc: 0.9333 - val_loss: 2.4093 - val_acc: 0.5385\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3303 - acc: 0.9333 - val_loss: 2.4061 - val_acc: 0.5385\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2755 - acc: 0.9810 - val_loss: 2.4049 - val_acc: 0.5385\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3080 - acc: 0.9619 - val_loss: 2.4068 - val_acc: 0.5385\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2819 - acc: 0.9714 - val_loss: 2.4091 - val_acc: 0.5385\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3341 - acc: 0.9143 - val_loss: 2.4121 - val_acc: 0.5385\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3176 - acc: 0.9333 - val_loss: 2.4184 - val_acc: 0.5385\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2736 - acc: 0.9619 - val_loss: 2.4236 - val_acc: 0.5385\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3368 - acc: 0.9048 - val_loss: 2.4316 - val_acc: 0.5385\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2990 - acc: 0.9619 - val_loss: 2.4378 - val_acc: 0.5385\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3504 - acc: 0.9048 - val_loss: 2.4378 - val_acc: 0.5385\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2979 - acc: 0.9524 - val_loss: 2.4355 - val_acc: 0.5385\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3639 - acc: 0.9143 - val_loss: 2.4364 - val_acc: 0.5385\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2746 - acc: 0.9524 - val_loss: 2.4310 - val_acc: 0.5385\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3541 - acc: 0.8952 - val_loss: 2.4232 - val_acc: 0.5385\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3269 - acc: 0.9238 - val_loss: 2.4157 - val_acc: 0.5385\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3383 - acc: 0.8952 - val_loss: 2.4110 - val_acc: 0.5385\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3383 - acc: 0.9048 - val_loss: 2.4093 - val_acc: 0.5385\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3164 - acc: 0.9524 - val_loss: 2.4080 - val_acc: 0.5385\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3258 - acc: 0.9143 - val_loss: 2.4047 - val_acc: 0.5385\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3354 - acc: 0.9238 - val_loss: 2.3995 - val_acc: 0.5385\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2837 - acc: 0.9524 - val_loss: 2.3960 - val_acc: 0.5385\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3120 - acc: 0.9333 - val_loss: 2.3911 - val_acc: 0.5385\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3276 - acc: 0.9524 - val_loss: 2.3883 - val_acc: 0.5385\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3005 - acc: 0.9143 - val_loss: 2.3854 - val_acc: 0.5385\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2747 - acc: 0.9429 - val_loss: 2.3854 - val_acc: 0.5385\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3149 - acc: 0.9524 - val_loss: 2.3876 - val_acc: 0.5385\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4060 - acc: 0.8762 - val_loss: 2.3910 - val_acc: 0.5385\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2728 - acc: 0.9714 - val_loss: 2.3959 - val_acc: 0.5385\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3728 - acc: 0.8857 - val_loss: 2.4025 - val_acc: 0.5385\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2732 - acc: 0.9524 - val_loss: 2.4103 - val_acc: 0.5385\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2839 - acc: 0.9524 - val_loss: 2.4183 - val_acc: 0.5385\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3565 - acc: 0.9333 - val_loss: 2.4290 - val_acc: 0.5385\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3608 - acc: 0.9333 - val_loss: 2.4420 - val_acc: 0.5385\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3350 - acc: 0.9333 - val_loss: 2.4573 - val_acc: 0.5385\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2743 - acc: 0.9524 - val_loss: 2.4746 - val_acc: 0.5385\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2762 - acc: 0.9429 - val_loss: 2.4930 - val_acc: 0.5385\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2763 - acc: 0.9429 - val_loss: 2.5037 - val_acc: 0.5385\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2724 - acc: 0.9333 - val_loss: 2.5111 - val_acc: 0.5385\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3073 - acc: 0.9143 - val_loss: 2.5108 - val_acc: 0.5385\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2812 - acc: 0.9238 - val_loss: 2.5098 - val_acc: 0.5385\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2992 - acc: 0.9619 - val_loss: 2.5130 - val_acc: 0.5385\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3208 - acc: 0.9429 - val_loss: 2.5132 - val_acc: 0.5385\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3096 - acc: 0.9524 - val_loss: 2.5112 - val_acc: 0.5385\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2984 - acc: 0.9333 - val_loss: 2.5113 - val_acc: 0.5385\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2994 - acc: 0.9524 - val_loss: 2.5131 - val_acc: 0.5385\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3271 - acc: 0.9429 - val_loss: 2.5164 - val_acc: 0.5385\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3368 - acc: 0.9048 - val_loss: 2.5159 - val_acc: 0.5385\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2978 - acc: 0.9619 - val_loss: 2.5169 - val_acc: 0.5385\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3204 - acc: 0.9429 - val_loss: 2.5162 - val_acc: 0.5385\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2858 - acc: 0.9429 - val_loss: 2.5118 - val_acc: 0.5385\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3882 - acc: 0.8857 - val_loss: 2.5055 - val_acc: 0.5385\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2828 - acc: 0.9429 - val_loss: 2.5036 - val_acc: 0.5385\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3020 - acc: 0.9238 - val_loss: 2.5030 - val_acc: 0.5385\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2479 - acc: 0.9810 - val_loss: 2.5027 - val_acc: 0.5385\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2676 - acc: 0.9524 - val_loss: 2.5048 - val_acc: 0.5385\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3384 - acc: 0.9238 - val_loss: 2.5030 - val_acc: 0.5385\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3463 - acc: 0.9048 - val_loss: 2.5021 - val_acc: 0.5385\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3184 - acc: 0.9048 - val_loss: 2.4986 - val_acc: 0.5385\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3132 - acc: 0.9524 - val_loss: 2.4959 - val_acc: 0.5385\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2704 - acc: 0.9333 - val_loss: 2.4958 - val_acc: 0.5385\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2938 - acc: 0.9429 - val_loss: 2.4966 - val_acc: 0.5385\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3156 - acc: 0.9143 - val_loss: 2.5003 - val_acc: 0.5385\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2701 - acc: 0.9524 - val_loss: 2.5060 - val_acc: 0.5385\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3019 - acc: 0.9524 - val_loss: 2.5118 - val_acc: 0.5385\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2547 - acc: 0.9619 - val_loss: 2.5186 - val_acc: 0.5385\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3303 - acc: 0.9238 - val_loss: 2.5265 - val_acc: 0.5385\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2794 - acc: 0.9333 - val_loss: 2.5315 - val_acc: 0.5385\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3641 - acc: 0.8952 - val_loss: 2.5263 - val_acc: 0.5385\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3212 - acc: 0.9143 - val_loss: 2.5208 - val_acc: 0.5385\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2911 - acc: 0.9333 - val_loss: 2.5129 - val_acc: 0.5385\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3033 - acc: 0.9429 - val_loss: 2.5082 - val_acc: 0.5385\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3051 - acc: 0.9238 - val_loss: 2.5067 - val_acc: 0.5385\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.3345 - acc: 0.9238 - val_loss: 2.5053 - val_acc: 0.5385\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 93us/step - loss: 0.2993 - acc: 0.9333 - val_loss: 2.5073 - val_acc: 0.5385\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.2771 - acc: 0.9619 - val_loss: 2.5125 - val_acc: 0.5385\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 84us/step - loss: 0.2467 - acc: 0.9524 - val_loss: 2.5174 - val_acc: 0.5385\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3433 - acc: 0.9143 - val_loss: 2.5252 - val_acc: 0.5385\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3406 - acc: 0.9429 - val_loss: 2.5351 - val_acc: 0.5385\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.2822 - acc: 0.9429 - val_loss: 2.5482 - val_acc: 0.5385\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3272 - acc: 0.9333 - val_loss: 2.5638 - val_acc: 0.5385\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2980 - acc: 0.9524 - val_loss: 2.5734 - val_acc: 0.5385\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2815 - acc: 0.9619 - val_loss: 2.5845 - val_acc: 0.5385\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3604 - acc: 0.8857 - val_loss: 2.5992 - val_acc: 0.5385\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.2942 - acc: 0.9524 - val_loss: 2.6112 - val_acc: 0.5385\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3373 - acc: 0.8952 - val_loss: 2.6178 - val_acc: 0.5385\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3215 - acc: 0.9238 - val_loss: 2.6174 - val_acc: 0.5385\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3097 - acc: 0.9238 - val_loss: 2.6159 - val_acc: 0.5385\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2867 - acc: 0.9333 - val_loss: 2.6160 - val_acc: 0.5385\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3621 - acc: 0.9143 - val_loss: 2.6111 - val_acc: 0.5385\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3357 - acc: 0.9143 - val_loss: 2.6050 - val_acc: 0.5385\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2467 - acc: 0.9619 - val_loss: 2.5966 - val_acc: 0.5385\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3345 - acc: 0.9048 - val_loss: 2.5886 - val_acc: 0.5385\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2866 - acc: 0.9333 - val_loss: 2.5775 - val_acc: 0.5385\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2805 - acc: 0.9333 - val_loss: 2.5698 - val_acc: 0.5385\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3099 - acc: 0.9333 - val_loss: 2.5618 - val_acc: 0.5385\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3670 - acc: 0.9048 - val_loss: 2.5524 - val_acc: 0.5385\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2530 - acc: 0.9524 - val_loss: 2.5477 - val_acc: 0.5385\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2341 - acc: 0.9524 - val_loss: 2.5430 - val_acc: 0.5385\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.3406 - acc: 0.8762 - val_loss: 2.5414 - val_acc: 0.5385\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2673 - acc: 0.9524 - val_loss: 2.5428 - val_acc: 0.5385\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2745 - acc: 0.9619 - val_loss: 2.5493 - val_acc: 0.5385\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2625 - acc: 0.9524 - val_loss: 2.5562 - val_acc: 0.5385\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2574 - acc: 0.9619 - val_loss: 2.5612 - val_acc: 0.5385\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2463 - acc: 0.9714 - val_loss: 2.5681 - val_acc: 0.5385\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3453 - acc: 0.9143 - val_loss: 2.5724 - val_acc: 0.5385\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2633 - acc: 0.9619 - val_loss: 2.5757 - val_acc: 0.5385\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3006 - acc: 0.9238 - val_loss: 2.5811 - val_acc: 0.5385\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2834 - acc: 0.9333 - val_loss: 2.5882 - val_acc: 0.5385\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2805 - acc: 0.9429 - val_loss: 2.5853 - val_acc: 0.5385\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3153 - acc: 0.9048 - val_loss: 2.5865 - val_acc: 0.5385\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.2342 - acc: 0.9810 - val_loss: 2.5838 - val_acc: 0.5385\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3029 - acc: 0.9429 - val_loss: 2.5794 - val_acc: 0.5385\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2720 - acc: 0.9524 - val_loss: 2.5761 - val_acc: 0.5385\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2620 - acc: 0.9143 - val_loss: 2.5720 - val_acc: 0.5385\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3012 - acc: 0.9238 - val_loss: 2.5647 - val_acc: 0.5385\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2611 - acc: 0.9429 - val_loss: 2.5633 - val_acc: 0.5385\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3196 - acc: 0.9143 - val_loss: 2.5607 - val_acc: 0.5385\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2413 - acc: 0.9524 - val_loss: 2.5601 - val_acc: 0.5385\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2707 - acc: 0.9619 - val_loss: 2.5554 - val_acc: 0.5385\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.3010 - acc: 0.9429 - val_loss: 2.5501 - val_acc: 0.5385\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3102 - acc: 0.9333 - val_loss: 2.5447 - val_acc: 0.5385\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2624 - acc: 0.9524 - val_loss: 2.5402 - val_acc: 0.5385\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.3349 - acc: 0.9238 - val_loss: 2.5373 - val_acc: 0.5385\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3022 - acc: 0.9238 - val_loss: 2.5401 - val_acc: 0.5385\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.2916 - acc: 0.9143 - val_loss: 2.5425 - val_acc: 0.5385\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.3548 - acc: 0.9048 - val_loss: 2.5485 - val_acc: 0.5385\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.2794 - acc: 0.9333 - val_loss: 2.5546 - val_acc: 0.5385\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2578 - acc: 0.9524 - val_loss: 2.5604 - val_acc: 0.5385\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2457 - acc: 0.9524 - val_loss: 2.5679 - val_acc: 0.5385\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2886 - acc: 0.9143 - val_loss: 2.5760 - val_acc: 0.5385\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2779 - acc: 0.9429 - val_loss: 2.5850 - val_acc: 0.5385\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2574 - acc: 0.9619 - val_loss: 2.5972 - val_acc: 0.5385\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3073 - acc: 0.9524 - val_loss: 2.6029 - val_acc: 0.5385\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2468 - acc: 0.9619 - val_loss: 2.6105 - val_acc: 0.5385\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2996 - acc: 0.9524 - val_loss: 2.6195 - val_acc: 0.5385\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3004 - acc: 0.9238 - val_loss: 2.6223 - val_acc: 0.5385\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 85us/step - loss: 0.2888 - acc: 0.9238 - val_loss: 2.6236 - val_acc: 0.5385\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3015 - acc: 0.9143 - val_loss: 2.6301 - val_acc: 0.5385\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2859 - acc: 0.9238 - val_loss: 2.6393 - val_acc: 0.5385\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2848 - acc: 0.9429 - val_loss: 2.6405 - val_acc: 0.5385\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2914 - acc: 0.9143 - val_loss: 2.6426 - val_acc: 0.5385\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2747 - acc: 0.9524 - val_loss: 2.6457 - val_acc: 0.5385\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2702 - acc: 0.9524 - val_loss: 2.6483 - val_acc: 0.5385\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2999 - acc: 0.9524 - val_loss: 2.6507 - val_acc: 0.5385\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3289 - acc: 0.9048 - val_loss: 2.6509 - val_acc: 0.5385\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2819 - acc: 0.9524 - val_loss: 2.6447 - val_acc: 0.5385\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3232 - acc: 0.9429 - val_loss: 2.6378 - val_acc: 0.5385\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2245 - acc: 0.9619 - val_loss: 2.6322 - val_acc: 0.5385\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3037 - acc: 0.9333 - val_loss: 2.6254 - val_acc: 0.5385\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2316 - acc: 0.9810 - val_loss: 2.6183 - val_acc: 0.5385\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3063 - acc: 0.9333 - val_loss: 2.6124 - val_acc: 0.5385\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2987 - acc: 0.9048 - val_loss: 2.6008 - val_acc: 0.5385\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2573 - acc: 0.9524 - val_loss: 2.5879 - val_acc: 0.5385\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2435 - acc: 0.9619 - val_loss: 2.5753 - val_acc: 0.5385\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2906 - acc: 0.9524 - val_loss: 2.5692 - val_acc: 0.5385\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2900 - acc: 0.9333 - val_loss: 2.5669 - val_acc: 0.5385\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2800 - acc: 0.9333 - val_loss: 2.5632 - val_acc: 0.5385\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3044 - acc: 0.9429 - val_loss: 2.5681 - val_acc: 0.5385\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3030 - acc: 0.9143 - val_loss: 2.5760 - val_acc: 0.5385\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3089 - acc: 0.9333 - val_loss: 2.5794 - val_acc: 0.5385\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2882 - acc: 0.9238 - val_loss: 2.5820 - val_acc: 0.5385\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2808 - acc: 0.9238 - val_loss: 2.5844 - val_acc: 0.5385\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2867 - acc: 0.9429 - val_loss: 2.5905 - val_acc: 0.5385\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2427 - acc: 0.9619 - val_loss: 2.5946 - val_acc: 0.5385\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2509 - acc: 0.9619 - val_loss: 2.5934 - val_acc: 0.5385\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2703 - acc: 0.9429 - val_loss: 2.5890 - val_acc: 0.5385\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2949 - acc: 0.9333 - val_loss: 2.5825 - val_acc: 0.5385\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3554 - acc: 0.9143 - val_loss: 2.5820 - val_acc: 0.5385\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3110 - acc: 0.9048 - val_loss: 2.5800 - val_acc: 0.5385\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.2959 - acc: 0.9429 - val_loss: 2.5732 - val_acc: 0.5385\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2778 - acc: 0.9429 - val_loss: 2.5660 - val_acc: 0.5385\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2363 - acc: 0.9810 - val_loss: 2.5596 - val_acc: 0.5385\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3138 - acc: 0.9143 - val_loss: 2.5532 - val_acc: 0.5385\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3283 - acc: 0.9143 - val_loss: 2.5496 - val_acc: 0.5385\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.2936 - acc: 0.9238 - val_loss: 2.5416 - val_acc: 0.5385\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3042 - acc: 0.9143 - val_loss: 2.5290 - val_acc: 0.5385\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2917 - acc: 0.9524 - val_loss: 2.5195 - val_acc: 0.5385\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3273 - acc: 0.8952 - val_loss: 2.5136 - val_acc: 0.5385\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2948 - acc: 0.9048 - val_loss: 2.5139 - val_acc: 0.5385\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2630 - acc: 0.9619 - val_loss: 2.5153 - val_acc: 0.5385\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2748 - acc: 0.9714 - val_loss: 2.5192 - val_acc: 0.5385\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3012 - acc: 0.9143 - val_loss: 2.5267 - val_acc: 0.5385\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2825 - acc: 0.9619 - val_loss: 2.5316 - val_acc: 0.5385\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2945 - acc: 0.9429 - val_loss: 2.5393 - val_acc: 0.5385\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2884 - acc: 0.9524 - val_loss: 2.5532 - val_acc: 0.5385\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3079 - acc: 0.9238 - val_loss: 2.5718 - val_acc: 0.5385\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2610 - acc: 0.9524 - val_loss: 2.5931 - val_acc: 0.5385\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2190 - acc: 0.9810 - val_loss: 2.6160 - val_acc: 0.5385\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2752 - acc: 0.9524 - val_loss: 2.6361 - val_acc: 0.5385\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.2476 - acc: 0.9429 - val_loss: 2.6566 - val_acc: 0.5385\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2428 - acc: 0.9714 - val_loss: 2.6767 - val_acc: 0.5385\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.2815 - acc: 0.9619 - val_loss: 2.6931 - val_acc: 0.5385\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.2466 - acc: 0.9619 - val_loss: 2.7080 - val_acc: 0.5385\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2881 - acc: 0.9524 - val_loss: 2.7213 - val_acc: 0.5385\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2635 - acc: 0.9333 - val_loss: 2.7316 - val_acc: 0.5385\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2662 - acc: 0.9429 - val_loss: 2.7373 - val_acc: 0.5385\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2479 - acc: 0.9619 - val_loss: 2.7447 - val_acc: 0.5385\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2628 - acc: 0.9714 - val_loss: 2.7492 - val_acc: 0.5385\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2837 - acc: 0.9524 - val_loss: 2.7509 - val_acc: 0.5385\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2081 - acc: 0.9810 - val_loss: 2.7520 - val_acc: 0.5385\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2297 - acc: 0.9619 - val_loss: 2.7536 - val_acc: 0.5385\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2331 - acc: 0.9810 - val_loss: 2.7542 - val_acc: 0.5385\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2283 - acc: 0.9619 - val_loss: 2.7541 - val_acc: 0.5385\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.2852 - acc: 0.9524 - val_loss: 2.7515 - val_acc: 0.5385\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.2608 - acc: 0.9333 - val_loss: 2.7485 - val_acc: 0.5385\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2635 - acc: 0.9429 - val_loss: 2.7405 - val_acc: 0.5385\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2757 - acc: 0.9429 - val_loss: 2.7300 - val_acc: 0.5385\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2793 - acc: 0.9429 - val_loss: 2.7198 - val_acc: 0.5385\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2287 - acc: 0.9714 - val_loss: 2.7143 - val_acc: 0.5385\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3225 - acc: 0.9333 - val_loss: 2.7104 - val_acc: 0.5385\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2600 - acc: 0.9524 - val_loss: 2.7097 - val_acc: 0.5385\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2663 - acc: 0.9524 - val_loss: 2.7083 - val_acc: 0.5385\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2631 - acc: 0.9429 - val_loss: 2.7091 - val_acc: 0.5385\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2461 - acc: 0.9714 - val_loss: 2.7114 - val_acc: 0.5385\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3109 - acc: 0.9333 - val_loss: 2.7123 - val_acc: 0.5385\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3121 - acc: 0.9429 - val_loss: 2.7131 - val_acc: 0.5385\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3023 - acc: 0.9333 - val_loss: 2.7096 - val_acc: 0.5385\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2711 - acc: 0.9429 - val_loss: 2.7035 - val_acc: 0.5385\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2857 - acc: 0.9429 - val_loss: 2.7000 - val_acc: 0.5385\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2461 - acc: 0.9429 - val_loss: 2.6994 - val_acc: 0.5385\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2149 - acc: 0.9810 - val_loss: 2.7009 - val_acc: 0.5385\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3285 - acc: 0.8952 - val_loss: 2.7005 - val_acc: 0.5385\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2947 - acc: 0.9238 - val_loss: 2.7016 - val_acc: 0.5385\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2781 - acc: 0.9333 - val_loss: 2.7011 - val_acc: 0.5385\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3081 - acc: 0.9143 - val_loss: 2.7002 - val_acc: 0.5385\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2557 - acc: 0.9524 - val_loss: 2.6977 - val_acc: 0.5385\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3077 - acc: 0.9238 - val_loss: 2.6980 - val_acc: 0.5385\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3069 - acc: 0.9143 - val_loss: 2.7003 - val_acc: 0.5385\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2040 - acc: 0.9810 - val_loss: 2.7061 - val_acc: 0.5385\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2771 - acc: 0.9429 - val_loss: 2.7117 - val_acc: 0.5385\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2437 - acc: 0.9619 - val_loss: 2.7180 - val_acc: 0.5385\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2405 - acc: 0.9619 - val_loss: 2.7253 - val_acc: 0.5385\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2668 - acc: 0.9714 - val_loss: 2.7337 - val_acc: 0.5385\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2227 - acc: 0.9714 - val_loss: 2.7401 - val_acc: 0.5385\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2915 - acc: 0.9524 - val_loss: 2.7417 - val_acc: 0.5385\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2219 - acc: 0.9714 - val_loss: 2.7438 - val_acc: 0.5385\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2677 - acc: 0.9429 - val_loss: 2.7447 - val_acc: 0.5385\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2648 - acc: 0.9524 - val_loss: 2.7467 - val_acc: 0.5385\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2639 - acc: 0.9524 - val_loss: 2.7496 - val_acc: 0.5385\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.3162 - acc: 0.8952 - val_loss: 2.7511 - val_acc: 0.5385\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2887 - acc: 0.9429 - val_loss: 2.7469 - val_acc: 0.5385\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3027 - acc: 0.8762 - val_loss: 2.7389 - val_acc: 0.5385\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.3166 - acc: 0.9143 - val_loss: 2.7320 - val_acc: 0.5385\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2792 - acc: 0.9429 - val_loss: 2.7273 - val_acc: 0.5385\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.2703 - acc: 0.9524 - val_loss: 2.7218 - val_acc: 0.5385\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.2753 - acc: 0.9429 - val_loss: 2.7121 - val_acc: 0.5385\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2620 - acc: 0.9619 - val_loss: 2.7046 - val_acc: 0.5385\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 86us/step - loss: 0.2555 - acc: 0.9333 - val_loss: 2.7025 - val_acc: 0.5385\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3731 - acc: 0.9143 - val_loss: 2.7029 - val_acc: 0.5385\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2891 - acc: 0.9333 - val_loss: 2.7053 - val_acc: 0.5385\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3159 - acc: 0.9333 - val_loss: 2.7098 - val_acc: 0.5385\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.2370 - acc: 0.9524 - val_loss: 2.7168 - val_acc: 0.5385\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2609 - acc: 0.9429 - val_loss: 2.7255 - val_acc: 0.5385\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2300 - acc: 0.9619 - val_loss: 2.7305 - val_acc: 0.5385\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2607 - acc: 0.9810 - val_loss: 2.7349 - val_acc: 0.5385\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2602 - acc: 0.9619 - val_loss: 2.7394 - val_acc: 0.5385\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.2722 - acc: 0.9524 - val_loss: 2.7445 - val_acc: 0.5385\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2940 - acc: 0.9143 - val_loss: 2.7523 - val_acc: 0.5385\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3089 - acc: 0.9238 - val_loss: 2.7641 - val_acc: 0.5385\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2975 - acc: 0.9333 - val_loss: 2.7692 - val_acc: 0.5385\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2402 - acc: 0.9905 - val_loss: 2.7733 - val_acc: 0.5385\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2252 - acc: 0.9619 - val_loss: 2.7752 - val_acc: 0.5385\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2366 - acc: 0.9619 - val_loss: 2.7760 - val_acc: 0.5385\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2794 - acc: 0.9524 - val_loss: 2.7712 - val_acc: 0.5385\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.2104 - acc: 0.9905 - val_loss: 2.7651 - val_acc: 0.5385\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2747 - acc: 0.9143 - val_loss: 2.7609 - val_acc: 0.5385\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2405 - acc: 0.9714 - val_loss: 2.7527 - val_acc: 0.5385\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2589 - acc: 0.9333 - val_loss: 2.7501 - val_acc: 0.5385\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3244 - acc: 0.9333 - val_loss: 2.7403 - val_acc: 0.5385\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2206 - acc: 0.9810 - val_loss: 2.7330 - val_acc: 0.5385\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 86us/step - loss: 0.3438 - acc: 0.9333 - val_loss: 2.7253 - val_acc: 0.5385\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2762 - acc: 0.9524 - val_loss: 2.7155 - val_acc: 0.5385\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2554 - acc: 0.9524 - val_loss: 2.7091 - val_acc: 0.5385\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2693 - acc: 0.9429 - val_loss: 2.7086 - val_acc: 0.5385\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2349 - acc: 0.9619 - val_loss: 2.7088 - val_acc: 0.5385\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2733 - acc: 0.9333 - val_loss: 2.7115 - val_acc: 0.5385\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.3235 - acc: 0.9048 - val_loss: 2.7115 - val_acc: 0.5385\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2611 - acc: 0.9524 - val_loss: 2.7140 - val_acc: 0.5385\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2964 - acc: 0.9143 - val_loss: 2.7223 - val_acc: 0.5385\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2341 - acc: 0.9524 - val_loss: 2.7327 - val_acc: 0.5385\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2302 - acc: 0.9524 - val_loss: 2.7413 - val_acc: 0.5385\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2267 - acc: 0.9524 - val_loss: 2.7531 - val_acc: 0.5385\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2440 - acc: 0.9429 - val_loss: 2.7660 - val_acc: 0.5385\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3180 - acc: 0.8857 - val_loss: 2.7673 - val_acc: 0.5385\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2310 - acc: 0.9429 - val_loss: 2.7711 - val_acc: 0.5385\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3100 - acc: 0.9524 - val_loss: 2.7765 - val_acc: 0.5385\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2443 - acc: 0.9429 - val_loss: 2.7829 - val_acc: 0.5385\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2868 - acc: 0.9429 - val_loss: 2.7886 - val_acc: 0.5385\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2640 - acc: 0.9333 - val_loss: 2.7944 - val_acc: 0.5385\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.2880 - acc: 0.9238 - val_loss: 2.7952 - val_acc: 0.5385\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.2701 - acc: 0.9238 - val_loss: 2.7906 - val_acc: 0.5385\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2456 - acc: 0.9619 - val_loss: 2.7884 - val_acc: 0.5385\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2794 - acc: 0.9143 - val_loss: 2.7821 - val_acc: 0.5385\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2511 - acc: 0.9714 - val_loss: 2.7768 - val_acc: 0.5385\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2331 - acc: 0.9619 - val_loss: 2.7742 - val_acc: 0.5385\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.2541 - acc: 0.9619 - val_loss: 2.7764 - val_acc: 0.5385\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.2480 - acc: 0.9524 - val_loss: 2.7849 - val_acc: 0.5385\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2662 - acc: 0.9143 - val_loss: 2.7934 - val_acc: 0.5385\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2867 - acc: 0.9143 - val_loss: 2.8005 - val_acc: 0.5385\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2715 - acc: 0.9143 - val_loss: 2.7999 - val_acc: 0.5385\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2362 - acc: 0.9905 - val_loss: 2.7985 - val_acc: 0.5385\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3306 - acc: 0.8952 - val_loss: 2.7862 - val_acc: 0.5385\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.2252 - acc: 0.9714 - val_loss: 2.7734 - val_acc: 0.5385\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2825 - acc: 0.9524 - val_loss: 2.7590 - val_acc: 0.5385\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.2413 - acc: 0.9524 - val_loss: 2.7509 - val_acc: 0.5385\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2511 - acc: 0.9810 - val_loss: 2.7446 - val_acc: 0.5385\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3233 - acc: 0.9143 - val_loss: 2.7449 - val_acc: 0.5385\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2554 - acc: 0.9238 - val_loss: 2.7473 - val_acc: 0.5385\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2168 - acc: 0.9810 - val_loss: 2.7486 - val_acc: 0.5385\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2170 - acc: 0.9619 - val_loss: 2.7512 - val_acc: 0.5385\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3092 - acc: 0.9429 - val_loss: 2.7512 - val_acc: 0.5385\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2379 - acc: 0.9429 - val_loss: 2.7475 - val_acc: 0.5385\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2340 - acc: 0.9524 - val_loss: 2.7420 - val_acc: 0.5385\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2688 - acc: 0.9238 - val_loss: 2.7359 - val_acc: 0.5385\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2797 - acc: 0.9429 - val_loss: 2.7324 - val_acc: 0.5385\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2362 - acc: 0.9714 - val_loss: 2.7288 - val_acc: 0.5385\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2000 - acc: 0.9905 - val_loss: 2.7300 - val_acc: 0.5385\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2272 - acc: 0.9619 - val_loss: 2.7344 - val_acc: 0.5385\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2322 - acc: 0.9810 - val_loss: 2.7372 - val_acc: 0.5385\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.2332 - acc: 0.9524 - val_loss: 2.7391 - val_acc: 0.5385\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.2537 - acc: 0.9524 - val_loss: 2.7437 - val_acc: 0.5385\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.2186 - acc: 0.9810 - val_loss: 2.7487 - val_acc: 0.5385\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2428 - acc: 0.9524 - val_loss: 2.7549 - val_acc: 0.5385\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.2177 - acc: 0.9714 - val_loss: 2.7611 - val_acc: 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5de62dc4-a760-4547-98d9-7c9c98079e7d",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d8a892e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3debxU8//A8de7RWnRSloVQvsqJals\n3ySRbC0IiX6IL/rapb587SGEElFZKkUqQgtFSqVoVSpKe1ppvff9++N9ppl7u2vduXPvnffz8ZjH\nzJxz5szn3KnzPuezvD+iqjjnnItf+WJdAOecc7HlgcA55+KcBwLnnItzHgiccy7OeSBwzrk454HA\nOefinAcCly4RyS8iu0WkSlZuG0sicqqIZHnfaRG5QERWR7xfJiItMrLtEXzXWyLy0JF+Po39PiEi\nQ7N6v6l8V5p/AxEZLiKPZ0dZ4lmBWBfAZT0R2R3xtgiwD0gI3t+qqiMysz9VTQCKZfW28UBVT8+K\n/YhId6CrqraK2Hf3rNi3cx4I8iBVPXQiDq62uqvq16ltLyIFVPVgdpTNOZfzeNVQHApu/T8SkQ9E\nZBfQVUSaicgPIrJdRNaLyAARKRhsX0BEVESqBu+HB+s/F5FdIjJTRKpldttg/cUi8quI7BCRV0Tk\nOxHplkq5M1LGW0VkhYhsE5EBEZ/NLyIvishWEVkJtEnj7/OwiHyYbNlrItI/eN1dRJYEx/NbcLWe\n2r7Wikir4HURERkWlG0R0CjZto+IyMpgv4tEpH2wvA7wKtAiqHbbEvG3fTzi87cFx75VRD4RkfIZ\n+dukR0Q6BOXZLiJTROT0iHUPicg6EdkpIksjjrWpiMwLlm8Ukecy+F2NRGR+8Df4ACgUsa6MiEwU\nkc3BMXwmIhUzehwuDarqjzz8AFYDFyRb9gSwH7gUuxg4FjgTOAu7SzwZ+BW4I9i+AKBA1eD9cGAL\n0BgoCHwEDD+CbU8AdgGXBevuAQ4A3VI5loyU8VOgBFAV+Ct07MAdwCKgElAG+Nb++af4PScDu4Gi\nEfveBDQO3l8abCPAecAeoG6w7gJgdcS+1gKtgtfPA9OAUsBJwOJk214NlA9+k85BGcoF67oD05KV\nczjwePD6oqCM9YHCwEBgSkb+Nikc/xPA0OB1jaAc5wW/0UPAsuB1LeB34MRg22rAycHrH4FOwevi\nwFmpfNehvxd20l8L9Ar2f23w7yF0jMcDHbB/r8cBY4DRsf4/lhcefkcQv2ao6meqmqiqe1T1R1Wd\npaoHVXUlMAhomcbnR6vqHFU9AIzATkCZ3bYdMF9VPw3WvYgFjRRlsIxPqeoOVV2NnXRD33U18KKq\nrlXVrcDTaXzPSmAhFqAALgS2qeqcYP1nqrpSzRRgMpBig3AyVwNPqOo2Vf0du8qP/N6Rqro++E3e\nx4J44wzsF6AL8JaqzlfVvcADQEsRqRSxTWp/m7RcC4xT1SnBb/Q0FkzOAg5iQadWUL24KvjbgZ3A\nq4tIGVXdpaqzMvBdzbGA9YqqHlDVD4GfQitVdbOqjg3+ve4E/kfa/0ZdBnkgiF9rIt+IyBkiMkFE\nNojITqAfUDaNz2+IeP0PaTcQp7ZthchyqKpiV4QpymAZM/Rd2JVsWt4HOgWvOwfvQ+VoJyKzROQv\nEdmOXY2n9bcKKZ9WGUSkm4gsCKpgtgNnZHC/YMd3aH/BiXIbEFl1kpnfLLX9JmK/UUVVXQbci/0O\nm4KqxhODTW8EagLLRGS2iLTN4HetDf4dhBz6bhEpJtZT6o/g959Cxv8+Lg0eCOJX8q6Tb2JXwaeq\n6nHAY1jVRzStx6pqABARIemJK7mjKeN6oHLE+/S6t44ELgjqoC8jCAQiciwwGngKq7YpCXyZwXJs\nSK0MInIy8DrQEygT7HdpxH7T6+q6DqtuCu2vOFYF9WcGypWZ/ebDfrM/AVR1uKo2x6qF8mN/F1R1\nmapei1X/vQB8LCKF0/muJP8eApG/U+/ge5oEv/95R3pQLikPBC6kOLAD+FtEagC3ZsN3jgcaisil\nIlIAuAurB45GGUcCd4tIRREpA9yf1saqugGYAQwFlqnq8mBVIeAYYDOQICLtgPMzUYaHRKSk2DiL\nOyLWFcNO9puxmHgLdkcQshGoFGocT8EHwM0iUldECmEn5OmqmuodVibK3F5EWgXf3Rtr15klIjVE\npHXwfXuCRyJ2ANeJSNngDmJHcGyJ6XzXDCCfiNwRNHBfDTSMWF8cu5PZFvyGjx3lsbmABwIXci9w\nA/af/E2sUTeqVHUjcA3QH9gKnILVCe+LQhlfx+ryf8EaMkdn4DPvY42Zh6qFVHU78G9gLNbgeiUW\n0DKiD3bVuxr4HHgvYr8/A68As4NtTgci69W/ApYDG0Uksoon9PkvsCqascHnq2DtBkdFVRdhf/PX\nsSDVBmgftBcUAp7F2nU2YHcgDwcfbQssEeuV9jxwjaruT+e79mGNwbdg1VodgE8iNumPtU9sBb7H\n/oYuC0jS6jjnYkdE8mNVEVeq6vRYl8e5eOF3BC6mRKRNUFVSCHgU620yO8bFci6ueCBwsXYOsBKr\ndvgX0CGoInDOZROvGnLOuTjndwTOORfncl3SubJly2rVqlVjXQznnMtV5s6du0VVU+yenesCQdWq\nVZkzZ06si+Gcc7mKiKQ6mt6rhpxzLs55IHDOuTjngcA55+JcrmsjSMmBAwdYu3Yte/fujXVRXDoK\nFy5MpUqVKFgwtZQ5zrnslicCwdq1aylevDhVq1bFEli6nEhV2bp1K2vXrqVatWrpf8A5ly3yRNXQ\n3r17KVOmjAeBHE5EKFOmjN+5OZfD5IlAAHgQyCX8d3Iu58kzgcA553IdVdizx14vXgwDBsDu3dle\nDA8EWWD79u0MHDjwiD7btm1btm/fnuY2jz32GF9//fUR7T+5qlWrsmVLqtMCO+ey0803Q+nS8M47\ncN11cNddcN992V4MDwRZIK1AcPDgwTQ/O3HiREqWLJnmNv369eOCCy444vI552IgIcGu8J94wq78\nk/voIwsAe/fCTTfBvHm2fPRoOHAgW4vqgSALPPDAA/z222/Ur1+f3r17M23aNFq0aEH79u2pWbMm\nAJdffjmNGjWiVq1aDBo06NBnQ1foq1evpkaNGtxyyy3UqlWLiy66iD3BLWO3bt0YPXr0oe379OlD\nw4YNqVOnDkuXLgVg8+bNXHjhhdSqVYvu3btz0kknpXvl379/f2rXrk3t2rV56aWXAPj777+55JJL\nqFevHrVr1+ajjz46dIw1a9akbt263BeDKxbncp2HH7Yr/EcfhWefTbruq6+ga1do2hQ2bYKGDeG8\n82DUKNi6FaZOPXx/jz8OP/4YlaLmie6jke6+G+bPz9p91q8PwXkyRU8//TQLFy5kfvDF06ZNY968\neSxcuPBQN8m3336b0qVLs2fPHs4880w6duxImTJlkuxn+fLlfPDBBwwePJirr76ajz/+mK5dux72\nfWXLlmXevHkMHDiQ559/nrfeeou+ffty3nnn8eCDD/LFF18wZMiQNI9p7ty5vPPOO8yaNQtV5ayz\nzqJly5asXLmSChUqMGHCBAB27NjB1q1bGTt2LEuXLkVE0q3Kci7XWrPGTiJffgnNm8PIkXDccalv\nv3UrvPGGfW7AADjmGFu+axe89hp06gT791swOP98aNzY1r/0EhQpAuPGwfHHw9y5tnzvXiheHG67\nDU47DW68Ea65BsaOhb597c7izDOz/LD9jiBKmjRpkqSv/IABA6hXrx5NmzZlzZo1LF++/LDPVKtW\njfr16wPQqFEjVq9eneK+r7jiisO2mTFjBtdeey0Abdq0oVSpUmmWb8aMGXTo0IGiRYtSrFgxrrji\nCqZPn06dOnX46quvuP/++5k+fTolSpSgRIkSFC5cmJtvvpkxY8ZQpEiRzP45nMv5EhKgSxeYNAna\ntrXnV19Nffsvv4RKleCRR+DNN5Nu++9/W6Pv3XdbgChZEnr3tnVbt8LXX0P37hYEIhUubCf/Vavs\n+7t0sX1df70FkYceyvrjJg/eEaR15Z6dihYteuj1tGnT+Prrr5k5cyZFihShVatWKfalL1So0KHX\n+fPnP1Q1lNp2+fPnT7cNIrNOO+005s2bx8SJE3nkkUc4//zzeeyxx5g9ezaTJ09m9OjRvPrqq0yZ\nMiVLv9e5mBszBqZPh7fftpPxtm0wcCD85z9QINmpcv9+6NULqlSxOv3eva3qpl07WLIEhgyBBx+E\nJk1s+wcfhHvugRkz7Dv277fvSMlzz9mJ/5RT4Nxz7aRWpw58/DFEnCOykt8RZIHixYuza9euVNfv\n2LGDUqVKUaRIEZYuXcoPP/yQ5WVo3rw5I0eOBODLL79k27ZtaW7fokULPvnkE/755x/+/vtvxo4d\nS4sWLVi3bh1FihSha9eu9O7dm3nz5rF792527NhB27ZtefHFF1mwYEGWl9+5mPvwQ6hY0U7CAHfc\nAX/+aSf3Zs3syj/U6PvMM7BsGbz4op2kBw+GfPmgRw+4/XZb1rdveN+33grlykGbNvDf/8KFF0Lt\n2imX45hjoFEju4uYMgU++AC+/96CTpTkuTuCWChTpgzNmzendu3aXHzxxVxyySVJ1rdp04Y33niD\nGjVqcPrpp9O0adMsL0OfPn3o1KkTw4YNo1mzZpx44okUL1481e0bNmxIt27daBJcsXTv3p0GDRow\nadIkevfuTb58+ShYsCCvv/46u3bt4rLLLmPv3r2oKv3798/y8jsXUwcOWHXNNddA/vy27NJLrSpm\n6FBb/8MP1uhbogRs2QLXXmtVSACVK1vgePJJO5GPHQuR+bSKFIH+/W2bmjXtdUaULWvfE2W5bs7i\nxo0ba/KJaZYsWUKNGjViVKKcYd++feTPn58CBQowc+ZMevbseajxOqfx38vlOD/8YFf9o0bBlVce\nvj4x0U7yq1bBzp3WnjBoUNI6/n377Oq9fn175DAiMldVG6e0zu8I8og//viDq6++msTERI455hgG\nDx4c6yI5l3uELi7POivl9fnyWc+ftBQqBN26ZWmxsosHgjyievXq/PTTT7EuhnOx9c8/drWeRrVo\niubNs6v7SpWiU64czhuLnXN5w8iRULUqVKtm/fozY+5ca6CN06SIHgicc7nfl19aQ2/RotZPv1ev\njH92zx5YtMhG98YpDwTOudgbPhzuvddOyJk1cSJccQXUqmUZPJ95Bj75BD79NGOf//lnq05q1Cjz\n351HeCBwzsXWxo3WyNq/v/Wv/+uvjH923z7L51OuHHz2GRx7rI3ErVPHBnllpFdkKL2DBwKX3YoV\nKwbAunXruDKl7mpAq1atSN5VNrmXXnqJf/7559D7jKS1zojHH3+c559//qj341y6xo+3K/J33oEN\nGw7vY79smfXxT0g4/LODB8OKFZbXJ5TSpWBBu7tYvtwGYh04YMGhXTvbf3Lz5kGZMlEdsJXTeSCI\nsQoVKhzKLHokkgeCjKS1di5H+eQTG5B1ww3QsaPl5gmNjF+3ztI0XHghXHaZ3QGEqFrwqFfPRuxG\n6tjRBnENHmyvX3oJJkywbJ8rViTdNs4bisEDQZZ44IEHeO211w69D11N7969m/PPP/9QyuhPU6iz\nXL16NbWDoeZ79uzh2muvpUaNGnTo0CFJrqGePXvSuHFjatWqRZ8+fQBLZLdu3Tpat25N69atgaQT\nz6SUZjqtdNepmT9/Pk2bNqVu3bp06NDhUPqKAQMGHEpNHUp4980331C/fn3q169PgwYN0ky94Rzr\n18Pnn0PnznYifuQRy9w5YIBl4mzTxgZwXXaZncivugo2b7bPTppkV/O33374fosVswDw7rtWZTRg\ngA0a27oVevYMVxnt2QMLF8Z1tRAAqpqrHo0aNdLkFi9eHH5z112qLVtm7eOuuw77zkjz5s3Tc889\n99D7GjVq6B9//KEHDhzQHTt2qKrq5s2b9ZRTTtHExERVVS1atKiqqq5atUpr1aqlqqovvPCC3njj\njaqqumDBAs2fP7/++OOPqqq6detWVVU9ePCgtmzZUhcsWKCqqieddJJu3rz50HeH3s+ZM0dr166t\nu3fv1l27dmnNmjV13rx5umrVKs2fP7/+9NNPqqp61VVX6bBhww47pj59+uhzzz2nqqp16tTRadOm\nqarqo48+qncFf4/y5cvr3r17VVV127Ztqqrarl07nTFjhqqq7tq1Sw8cOHDYvpP8Xi6+vfGGKqgu\nWhRedtllqiVL2v87UJ0wwZa/+qqqiGqFCqq//qp66qmqp5yiumdPyvv+9VfV009X7dw5vGzAANvn\n0KH2fsoUez9+fHSOLwcB5mgq51W/I8gCDRo0YNOmTaxbt44FCxZQqlQpKleujKry0EMPUbduXS64\n4AL+/PNPNm7cmOp+vv3220PzD9StW5e6deseWjdy5EgaNmxIgwYNWLRoEYsXL06zTKmlmYaMp7sG\nS5i3fft2WrZsCcANN9zAt99+e6iMXbp0Yfjw4RQIsjM2b96ce+65hwEDBrB9+/ZDy507JDEx/Hri\nRKsWikw58uijsH07vPyyTd8Yyudz++3wzTeW3vm006yK5403LHVzSqpXt0ygw4eHl/XsCWefDXfe\nad8xdaqNGj7nnKw/zlwkav9LRaQy8B5QDlBgkKq+nGybVsCnwKpg0RhV7XdUXxyjPNRXXXUVo0eP\nZsOGDVxzzTUAjBgxgs2bNzN37lwKFixI1apVU0w/nZ5Vq1bx/PPP8+OPP1KqVCm6det2RPsJyWi6\n6/RMmDCBb7/9ls8++4wnn3ySX375hQceeIBLLrmEiRMn0rx5cyZNmsQZZ5xxxGV1ecykSVYNNGKE\n1f1//rklYousn2/UCN57zxp6n3wy6edbtLCEbl262Mk8vSlck9f7Fyhg54gmTSxX0MCB0Lq1JZKL\nY9G8IzgI3KuqNYGmwO0iUjOF7aarav3gcXRBIIauueYaPvzwQ0aPHs1VV10F2NX0CSecQMGCBZk6\ndSq///57mvs499xzef/99wFYuHAhP//8MwA7d+6kaNGilChRgo0bN/L5558f+kxqKbBTSzOdWSVK\nlKBUqVKH7iaGDRtGy5YtSUxMZM2aNbRu3ZpnnnmGHTt2sHv3bn777Tfq1KnD/fffz5lnnnloKk0X\nBxIT4ZVXLI1z585QoYJd0Yfm3/37b7jlFuseesMNlo8/ISHl/DzXXQevv24Tuyd33nnWtnCkk7Q0\nbgxnnAH33293BZ5NN3p3BKq6HlgfvN4lIkuAikDadRq5VK1atdi1axcVK1akfPnyAHTp0oVLL72U\nOnXq0Lhx43SvjHv27MmNN95IjRo1qFGjBo2CBqx69erRoEEDzjjjDCpXrkzz5s0PfaZHjx60adOG\nChUqMDVintPU0kynVQ2UmnfffZfbbruNf/75h5NPPpl33nmHhIQEunbtyo4dO1BVevXqRcmSJXn0\n0UeZOnUq+fLlo1atWlx88cWZ/j6XS336aXhE73HH2Ujd4cPh/fett86xx1rqh2eesZP4uHHW1z+i\nCjRbiFjj8f/+Z7OEZff350DZkoZaRKoC3wK1VXVnxPJWwMfAWmAdcJ+qHja0UER6AD0AqlSp0ij5\nlbWnNc5d/PfKo266yWb52rAhXG8/YoT19pk2za7i+/aFxx6zEcCLFkGHDofP/uWiIqZpqEWkGHay\nvzsyCATmASep6m4RaQt8AlRPvg9VHQQMApuPIMpFds5llipMnmwTtEc23nbpYo99+2xswIkn2vKa\nNe3hcoSo9hoSkYJYEBihqmOSr1fVnaq6O3g9ESgoImWjWSbnXBSsXAl//GH19ykpVCgcBFyOE7VA\nICICDAGWqGqKrTEicmKwHSLSJCjP1iP5vuyo4nJHz3+nXOLjj22WrV9+ydj2kyfb8/nnR69MLmqi\neUfQHLgOOE9E5gePtiJym4jcFmxzJbBQRBYAA4Br9QjOFIULF2br1q1+ksnhVJWtW7dSOLV+3y5n\nCPXqWbAAnnrKlu3da1f7VatabqDkvvjCegmdfnq2FtVljWj2GpoBpJm8Q1VfBV492u+qVKkSa9eu\nZXNo6LnLsQoXLkylOJ0FKtcYOdK6ejZpYnmAdu2C55+3wVdgXUCXLIFQTqtdu2w8QPfucZ2vJzfL\nE831BQsWpFoo86Bz7ugMHWq5/V991YLBOefYib9zZ7jnHlvWvj289ZaN8B0yxO4YunSJdcndEfIU\nE865sHnzYNYs6NEDzjzT7gSWLrXkbwMG2KjfwYOt7aB1a5gxA/77X3vdtGmsS++OULaMI8hKjRs3\n1vRy9DvnjlDnzjYw7M8/w1U/iYmWjyfSwoV2p7Bjh6V7njvXRuu6HCutcQR+R+BcvPj9dxvM9cEH\nSfP6h0yZYuvuvTccBODwIABQuzbMmWMJ4mbN8iCQy/kdgXPxYP9+m8AllPupeXPr/RM64c+da/l9\n/v7bZgTznl15TkxHFjvncoDJky0IDBsGBw9az59WrWDmTLsLuPlm6/EzfrwHgTjkgcC5eDBmDBQv\nDldeaSf60qVt1q+zzrIqo1q1LC9QvXqxLqmLAW8jcC6vS0iwBuBLLglf7bdvD08/bb1/qlWzOwEP\nAnHLA4FzeZEqfPstbNoE331n8/x26JB0m/vvt0yhP/1kI4Zd3PJA4FxedN990LKljQUYMcKSvqU0\nN0S5cj4a2HkgcC7XW7rUcgONGGF3Ar/8YrNu1a5tGUEHDYILL7Q2AudS4I3FzuVmCQnQqRPMn2/z\n/E6bBrNn20n/m2/g8cdtlrCHH451SV0O5oHAudzsjTcsCAwfbm0CgwZB2bIwapT1DBowAF5+2at/\nXJp8QJlzuVViovX4Oekku/oH+Owzy/lzwgmxLZvLcXxAmXN50fTp1gbw5JPhK/727WNbJpcreSBw\nLjdJTLSr/lWrrDqoTJnDu4U6l0keCJzLTXr1gtdes9fHHguvvw5Fi8a2TC7X80DgXG4xeTIMHGiZ\nPt95xyaISSkzqHOZ5IHAuZwqMRHef98eixZZe8App1iiuMg00c4dJQ8EzuVEa9bA+efD8uVWBdS2\nrT369vUg4LKcBwLncqLHH4eVK60KqHNnOOaYWJfI5WEeCJzLaWbPtgDw739Dt26xLo2LA97S5FxO\nsnEjdOkC5ctDnz6xLo2LE35H4FxOoWpBYN06+PprOO64WJfIxQkPBM7lFCNGhLuINmsW69K4OOJV\nQ87F2tat0LOnTR7fuDH06BHrErk444HAueyyfz/897/wySfhZapw0UWWRbRXL5gwAfLnj10ZXVzy\nqiHnsssjj8Bzz9nrL76Af/3L5hKeNw9eeQXuuCO25XNxy+8InIuWefPsRJ+QAG+/bUHguuugTh2b\nTOaZZ6xxuGFDuOmmWJfWxbGo3RGISGXgPaAcoMAgVX052TYCvAy0Bf4BuqnqvGiVyblsM20atG5t\nr08+2bKFtmhhDcGbNkGrVvDAAxYUJk6EIkViWVoX56J5R3AQuFdVawJNgdtFpGaybS4GqgePHsDr\nUSyPc9mnb1+oUAGefdZyBJ1+OowfD8WKWWCYNQuGDLE5BcqVi3VpXZyLWiBQ1fWhq3tV3QUsASom\n2+wy4D01PwAlRaR8tMrkXLaYOdPuCO69F3r3hg0bbDrJyHEB5ctbdVCJEjErpnMh2dJGICJVgQbA\nrGSrKgJrIt6v5fBggYj0EJE5IjJn8+bN0Sqmc1njqadsvuBQN9AyZaBQodiWybk0RD0QiEgx4GPg\nblXdeST7UNVBqtpYVRsff/zxWVtA57LS0qU2g1ivXlYN5FwuENVAICIFsSAwQlXHpLDJn0DliPeV\ngmXO5U5Dh9o4gNtui3VJnMuwqAWCoEfQEGCJqvZPZbNxwPVimgI7VHV9tMrkXFQlJMCwYXDxxd4A\n7HKVaA4oaw5cB/wiIvODZQ8BVQBU9Q1gItZ1dAXWffTGKJbHuej6+mtLGDdgQKxL4lymRC0QqOoM\nQNLZRoHbo1UG57LVsGFQqhS0axfrkjiXKT6y2LnMOnjQRgp/91142f791kjcoYP3EHK5jgcC5zLr\nqafg5pvhnHPCCeSmTIGdO+Hyy2NbNueOgAcC5zLj99/hySetQbhBA7j1VtiyBd57z6qFLroo1iV0\nLtM8+6hz6Vm9Gu67DwoUgF9+gXz54M03Yds2mz/gvPNg4ULLHurVQi4X8kDgXFoSE63e/5dfLHfQ\nxo3w2mtQubI9Xn4Z7rwTKlb0OYZdruWBwLm0fPaZ5Ql69124/noLDPkialR79oSrr4bChaFo0diV\n07mj4IHAueQOHrSZw1Qti+jJJ0PnzrYuXwrNamXKZG/5nMti3ljsHMCXX1o20O++s3kDSpe2E/xP\nP8HTT1v7gHN5lP/rdm7bNrjySti1C955x5Y1a2aTxrRqBVddFdPiORdtHgicGzHCgsCkSZY9tEED\nuytwLk54IHDurbds3uCLLvJxAC4ueRuBi28rV8KCBTapvHNxygOBi2+TJtlz27axLYdzMeSBwMW3\nL76AatWgevVYl8S5mPFA4PKmv/6yUcCRdu2Cv/8Ov//nH5tDoE0bkDQzpjuXp3ljsct7Jk6Ejh3h\nwAGoWdPmB8iXD156ydJETJ9uM4iNH2/BwLuHujgnNjdM7tG4cWOdM2dOrIvhcqp58yw9dPXq0LQp\nTJ0Ky5fbupNOsuyht9wC/ftD3boWIJYts3mGncvDRGSuqjZOaZ3fEbi85T//geOOg6++ghNOsHmE\nR460O4GWLaFXL3jlFZgzx7KKfvONBwEX9zwQuLxj1iyYPBmef96CANhJvlOn8DaPPQYrVlg20Rde\n8IFjzuGBwOUlTz1lk8Pcemvq25Qta20IzrlDvNeQyxsWLoRPP4W77oJixWJdGudyFQ8ELvdThd69\nbT6AO++MdWmcy3U8ELjcZ+VKmyTm3XctCLz9tg0Me/ppSx/tnMsUbyNwucvMmXDZZbB5MwwbZkFg\nzhw491y4/fZYl865XClDdwQicoqIFApetxKRXiJSMrpFcy6ZkSOhdWvrHrpkCTzxhPX+Ofts+Ogj\nHx3s3BHK0IAyEZkPNAaqAhOBT4Faqprtmbp8QFmc2bzZsoOOH28TxTdvDp98Yr1/nHMZlhUDyhJV\n9aCIdABeUdVXROSnrCuic9DgNekAACAASURBVCmYPx/+9S/YtMneX3qp3RUULhzbcjmXx2Q0EBwQ\nkU7ADcClwbKC0SmSc9j0kZdfDsccYxPHXHghVKkS61I5lydlNBDcCNwGPKmqq0SkGjAsesVycS0x\nEbp3hz//tMnkmzSJdYmcy9My1FisqotVtZeqfiAipYDiqvpMWp8RkbdFZJOILExlfSsR2SEi84PH\nY0dQfpfXhLKBjhlj3UE9CDgXdRntNTRNRI4TkdLAPGCwiPRP52NDgTbpbDNdVesHj34ZKYvLo/75\nBx54AE47DcaOhRdfhHvuiXWpnIsLGR1QVkJVdwJXAO+p6lnABWl9QFW/Bf46yvK53EzVUj+sWJH+\ntjfeCM88AzVqwLRpcPfd3h3UuWyS0UBQQETKA1cD47Pw+5uJyAIR+VxEaqW2kYj0EJE5IjJn8+bN\nWfj1LipULdVzzZpQp47NDdCrF+zcmfL2P/xgvYH69rX00eeem73ldS7OZTQQ9AMmAb+p6o8icjKw\n/Ci/ex5wkqrWA14BPkltQ1UdpKqNVbXx8ccff5Rf66JqyxZo1cpO/CVKWEC4/XZ7rljR1n3/fdLP\nDBoExYt7VZBzMZKhXkOqOgoYFfF+JdDxaL44qGoKvZ4oIgNFpKyqbjma/boYSky03P+zZ8Mbb9hM\nYPmCa41u3eCdd+Czz2xswPTpUL++3SWMGmUNxJ411LmYyGhjcSURGRv0AtokIh+LSKWj+WIROVHE\nKoFFpElQlq1Hs08XQ6pw3302GfzLL9ucAPki/nk1bgyvvWa5gkqWhLZt4Y8/LFfQ7t1w222xK7tz\ncS6j4wjeAd4HQrN8dw2WXZjaB0TkA6AVUFZE1gJ9CAahqeobwJVATxE5COwBrtXcNoGyC7v/fuvp\n06uX3QmkpmJF+PxzSxXRrBn89Recd553E3UuhjKca0hV66e3LDt4rqEcaPFiqF3bBoG9+WbGevtM\nmwY33GBpo8eOhapVo11K5+JaVuQa2ioiXYEPgved8GocF/Lkk1CkCPzvfxnv8tmqFfz+e1SL5ZzL\nmIz2GroJ6zq6AViPVet0i1KZXG6ybBl8+CH83/95RlDncqmMppj4XVXbq+rxqnqCql7OUfYacnnA\n33/DtdfaFJH33hvr0jjnjtDRTFXpnb7j2YEDFgR+/tnuCMqVi3WJnHNH6GimqvTx//Hml1+sXv/g\nQXj9dfjySxg40LqCOudyraMJBN7VM69RtYFeu3bBOefYyODERBsg9txzlhE0pEgRGDzYego553K1\nNAOBiOwi5RO+AMdGpUQuNpYssRQPX3xh70Vs5O+2bbB6taWAePxxu/oXgZNPtq6fzrlcL81AoKrF\ns6sgLgZUYepUeP55G+RVvLh1AW3Y0BLBffklnHACPPYYXHmlrXfO5TlHUzXkcjNVuOkmGDoUypSB\n//4XevSwEz9YPqA+fWJaROdc9vBAEK8GDrQg0Ls39OvnE8I7F8c8EOR1Bw9a1c/ixXaVX6MGvPsu\nDBgA7drBU09B/vyxLqVzLoY8EOQle/fCvn32DNbb5/77rSG4eHEYNsyWi9hI4Bde8CDgnPNAkOvN\nnGmpnL/9FpYvt7r/SNWrw6ef2tX/jz/C2rVQt64td8454iwQ7Nhhc5/kmYvgfv2sQfe446B1a5sU\npkQJKBD8rOXKQYcOULCgvT/rLHs451yEuAkEI0ZA166WI+2002JdmizQv78Fgeuus1G+RYvGukTO\nuVzqaHIN5SqVK9vz6tUxLUbWeP99S/LWsaNN/+hBwDl3FOLmjqBaNXtetSq25ThimzbBhAmwbp31\n+T/3XLvNyTP1XM65WImbQFChglWV57pAsGsX9O1r/f737LFlZ50FH38MhQrFtmzOuTwhbqqG8ueH\nk06CFSsgISHWpcmgL76AWrWsPeDqq2HBAti61dI/+CQwzrksEjeBgKlTGb+5CV9/vJ1mzWJdmDTs\n22cjftu0gYsvtm5O339vy+rW9URvzrksFzdVQxx3HKfv+JEujGDgj7ezb1+Malb277d+/bNnQ6lS\ncNFFluQtXz4YNw5uuw3Wr7fsno8/Dg884FVAzrmoEk0+ACmHa9y4sc6ZM+eIPruzeiNWrThIfebT\npYswfHgWFy4tmzbBm29aV8/16+GYYywoAJx4ovX5X7AAGjSAZ5+F88/P+ETwzjmXDhGZq6qNU1oX\nP1VDQMHbe1CPn2nCbEaMyIYv3L4dpkyB66+3/quPPWbVOxMnwj//WHB47z0bDFaunOX9mTkTLrjA\ng4BzLtvE1R0BO3fCSSfxW6WWnLrwE9580zIvZ4nNm2HOHJg7N/xYs8bWFS9uA7/uuMOSvjnnXDZL\n644gftoIwFIx3HMPpzz2GA2Zy623NqJlSzj99KPY5/r18J//WJ/+UFA97TRo3tyqec44w67wixTJ\nkkNwzrmsFl93BGB3BdWqMZNmnP3XeKpXh19/PYL9rF9v2TvfeMNSPd95pyV2a9DAAo5zzuUg3kYQ\n6bjjoHdvmv01gZZMY/nyTH7+r78stfMpp8CLL8Lll8PChTa5e8uWHgScc7lO/AUCgF692FjsZAZz\nC8XZye7dGfhMQoL1+DnlFDvpd+xoGeyGD4dTT416kZ1zLlriMxAUKUKR94dQjVV8wuUcX3wPU6ak\nsq2q9fuvV88mc2nY0Lp5DhvmAcA5lydELRCIyNsisklEFqayXkRkgIisEJGfRaRhtMqSkuKXtiL/\ne0M5j6l8z9lMf2SS1fWH7NkDo0fD2Wdb9c+BAzBqFHz9NdSpk51Fdc65qIrmHcFQoE0a6y8GqgeP\nHsDrUSxLiuS6rrx0wXjKsZE+M9uQULostGhhSd1KloSrroING2DwYFi0CK680vv3O+fynKgFAlX9\nFvgrjU0uA95T8wNQUkTKR6s8qbnzi0t48uZVdGQ0b+26hr37xPr99+oFkyZZlrru3cOzfjnnXB4T\ny7NbRWBNxPu1wbL1yTcUkR7YXQNVqlTJ0kLkzw/PvFSI4kM6MoaOvFsAPh4Gf//tTQDOufiQKxqL\nVXWQqjZW1cbHH398lu+/WLHwIOCZM23uAp/b3TkXL2IZCP4EKke8rxQsi4lKleC++5Iu+/ln2LIF\n9u6NTZmccy47xDIQjAOuD3oPNQV2qOph1ULZ6amn4JVX4Jxz4Nhjrcfo8cfb6x9+sFkiVWHt2liW\n0jnnslY0u49+AMwETheRtSJys4jcJiK3BZtMBFYCK4DBwP9FqywZVaCA5YWbPt3mhI/UrBlUrGjZ\noStXtrFkzjmXF8RfrqFMaNkSvv025XVffgkXXpgtxXDOuaPmuYaO0IsvQpMmKa8bO5bM5ylyzrkc\nyANBGho2tLaBlLz+umWbFrF0Q7t2werV2Vo855zLEh4I0iFiyUUvvjj1ba67zpKOVqtmuej27Amv\ny2U1b865OOSBIANq1bLZJVXt8f770LRpytuOGQNLlljX00svtTnpN20Kr7/1Vuud5JxzOYU3Fh+F\n77+3iciSq1HDgkHI2WfDd9/Z61Cqolz2Z3fO5XLeWBwlZ59tYwzAEpc+84y9jgwCYAHjm29sWmPn\nnMtpPBAcpd9+s0bi/PnhhhtS365VKzjhhPD7hx+2fEbOORdrHgiOUvnycNJJ9vr446Fbt9R7GkX6\n3/+snWHr1qgWzznn0uWBIAvly2cjks86yzJYX3IJ/PRT6tsvXGhdVMHGJEyYALNnw8aN8OyzNheO\nc85FmweCKLnoIhg/HurXtxxFV19tyx99NOl2f/wBhQrZmIR27SyI3HUX3H8/HHMM7N+fse9LTEw6\nwZpzzmWUB4JsUL48vPWWJbR7/HFbdvLJ4fXJT/YffRR+3aKFjXAOdV1NTLTnhISkn+nUCQoWjErx\nnXN5nAeCbFK8uCW0y5fPehXNnn34NieeePiy2bPhnnvsc1ddZY3S995rCfJEYO5c227kSHtescIe\nzjmXUR4IYuCMM6BMGZg1y07koav9p5+29VWqWAqL5D7+2J5ffDG87Pbb4cknw++rV/dJdZxzmeMT\n8cZQ8oR27dtb28Kbb0LVqlbV07172vuYNcse6Zk61VJob9xovZsgXMXk0zE7F9/8jiAHKVXKehtV\nrWrvb74Zfv8dihbN/L4mTIDduy0AfP01nHeenfhDVUkLFsCdd1qw8d5JzsU3vxbM4apUsRP6jh0w\nbpw1HlerFl5/5pnw44+Hf65dO+uaOm9e0uXHHANz5tjnQnbutKoq51x88juCXKJECctyGrpbAJg2\nzRqTJ09O+TPJgwBY9VBkEAD4M4WZohMTj7SkzrncxgNBLtaypT2fc87R7adePeuBpGo9jm66yXon\nbdhgy37//ejL6pzLuTwQ5AHHHBN+/eefKTce16yZ/n4uvdRSbofma16+HF57ze5CFizIkqI653Ig\nDwS50NSph8+l/Oyz8PnnUKGC9UaaORMqVw73TJowIf39TpiQdHDb2LHwxRf2esYMG/381Vcpj4FI\nSIB9+47seJxzseXzEeRx//xjVTtVqkCxYrZs9+7w64zq0QMGDQq/nzLFqpG6dbOeSM2a2fI9eyyr\naunS4bkXnHOx5/MRxLEiRWyinKJFrfF4yxZ7nXyWtDp10t5PZBAA647aowcMHBgOAmDzM5QtC8OG\nhZe9+qo1bDvncia/I4hjb75pYxfOOAMqVbJRyh9+mHX7b9rUqpRCA9bS+qc2bpyl0WjXLuu+3zkX\nltYdgQcCl8SHH1oCu6yydKkFGoDnnoP77jt8G1ULAqHXzrms51VDLsOuvRbWr0/aGP3uu0e+v8su\nC7/u3dvaDcaNs0FsIT45j3Ox5YHAHebEE20Ec716Vr1z/fVQrlzq27dvH359xx1J1y1bdvj2l11m\nA+TeecfmZ9i4MbwucvrOjz5K2hVW1QLJCy9k7nicc2nzFBMuVXPmhHv+/PADjBoFzZtbbqKBA633\n0cSJcOONNr/CSy8lnWchPTfdZM+1aoWX9eoFHTta2ozQ3A0TJ1r31Nat7X3v3jb72+mnp90zqUED\nu9v444+Ml8m5eOSBwKUqMitp1ap2Ag5p2RJuucVeFywYTkmRLx9Mn253FBkV2Sbx9tv2iNS2rT2H\nUmGoWk+ohx+2OwpVKFz48P3On5/xMjgXz7xqyB2xF16wORQuvjg8FeeFF1rKi0svDW+XfHrOI7Vj\nR9L3Tz5pAaBEiYx9fv58+O47G+vgnAuLaiAQkTYiskxEVojIAyms7yYim0VkfvBIJ/u+y0mOO87m\nVs6Xz6qMVMOpLEJ3BN98A/36WTXS0Ypsi4i0f7/dkUyenHKvo9DdSoMGFqQap9hvInXffmvH4Vxe\nFbWqIRHJD7wGXAisBX4UkXGqujjZph+p6h2H7cDlavfeC+eeC2edZe+LFrU5m/fuhWuugf/+FwYP\nztw+05qCM39+e+7fH7ZvT9oA/fzzNk4iZHHyf4HpCCX3866tLq+KZhtBE2CFqq4EEJEPgcuATP43\ndLlRvnzhIBAS2aPotdesAbhbNwsYKbnpJrjiiswNMrvnnsOXTZx4+JwNqnanEAogkT74AHbtspHT\nKUlISPlzzuVW0awaqgisiXi/NliWXEcR+VlERotI5ZR2JCI9RGSOiMzZvHlzNMrqslnBgjBkSOop\ntPv1s/WhwWjJVamS8e/65pvwgLWQJ56wxvAtW+z9smXW3vD999C5M9x6K6xbd/i+XnnFPvfbbxn/\nfudyulg3Fn8GVFXVusBXQIpDl1R1kKo2VtXGx4cm3HV5gggsWmQ9gkKD2IoXDzcwly+f8ucyO8gt\neRvFY4/Z80cfWS+lN96w7KlXXBHepmLFpNVR5ctb91aAU089uik+f/jBGtl9mlCXE0QzEPwJRF7h\nVwqWHaKqW1U1lLz4LaBRFMvjcqiaNS19dosWdkW+ZEl4XZEisG1b+P1XX9l4hlatkia2S6506Yx9\n9x13hOeGhqRtCwDVq4dfb9iQdF1kuUIOHIBNm9L/3s6dLcX3kYxxOHjQqqecyyrRDAQ/AtVFpJqI\nHANcC4yL3EBEIq/32gNLcHGtWTO7Eo9UsqTV5yckwAUXwJVX2vJTTw1v8+9/J/3M//2fpcfeuTNj\nvYTGjs18Wf/6C8aPh1WrrPvs0KH2veXKpT83w5F0YVW1O5mCBdPPFutcZkStsVhVD4rIHcAkID/w\ntqouEpF+wBxVHQf0EpH2wEHgL6BbtMrjcjeRw0cRh6qNTjjBxjS8+GJ43Z9/QsOG9vrEE6NTpqVL\noUOH8PtRo8Kvd+2CQoWSbr9pk7VVDBkSvrsIBYSff4a33oL//c+Oc/bs8EjqkD17rLcVJL1rcu6o\nqWquejRq1EidU1VNSFC96SbVWbPs/e7dquvWqZYvrzp3bni7rl1VQbVLF3tO7TFpUtrrkz9KlEh9\nXb16qqtWJS1vStuFyn7CCeFl115rz8uXq77+umrPnqq//KK6eXPSz4YsX65atKjqkiVR/XO7XA67\nAE/xvBrrxmLnjljo6jo0HWfRonaXsG5d+G4A4PLL7blXL0tq17t3eApOCHcFvegiePDBlL+rUUTr\n1d699px8pHOkBQvgmWdsJjeR1LvIhu4IIuv8Q1nW+/WDnj3h9dctDUdkQj6wuR42b7Y7ib//hpEj\nUy9PRqxZk3K7R1q2bDmyajWXs3ggcHlex452wmrSxBqfn30W/vUvm9d5wQI7sYdO7k88YWMOhg61\n9/372/MNN4T3V6hQOCFeSErjCt54A84/315Pn55y2Vq1smAQmYo71FMpstG5cGEbDxGpRQuoXx/W\nrrX3Zcum/B2p+fDD8GA5sC65p52WuX20b289rVILIP/8k7n9udjwiWmcS8WqVZZsD5K2Uaja1fPN\nN8Ovv1qPo7lzrTdP8kF0GXHddWn3gMqMjz6ydovt223MQ9eu1qg+e7alFI8UOp6xY63sV11l7997\nzwJjKKlgWkqXtiCwcaO11URatcqy0Q4ZEs4062InrYlpYl7nn9mHtxG4WBk1SvXhh5MuW7xY9d//\nVj14UHXfPtUqVaz+fuxY1dtvz1hbw0knZa5tIjOPSy5RffFFez1kiLWrhKT32Y8/Vv3996TH++uv\nqnv2hN8XL27btm+fdN+qqhMm2LqLLz58H7t2Hf73PXDA2nk2bVJ9771M/zw5wplnqnbuHOtSpIw0\n2ghifmLP7MMDgcstHnkk5RPspZdG78Sf/NGpk2rJkkmXHTxo5cvoPkIN2hs3hpclJtqyY48NL2ve\nPOnxhwJB27ZJl4PqWWeF33/4oS3717/C+wFr+E/N7t2qTzyhun9/xn+PRYtUX3gh49sfieQN+TlJ\nWoHA2wici5KLL7bnMWMsBXZoVHLhwlYVc6TzJRQtGn5dOcWkLGEffGDVRJF27ICPP87494WquyJn\nqRs40MZKRDZyf/edZYIFOx2uWmWvp02ztodNm8IjqWfNsgbuHj2sig1g0qTwfsC64/brZ+k+kuvb\nFx55BN5/P+PH0bq1JUNM3uieHQ4csOqxtBInxlRqESKnPvyOwOUm+/aFX48aZVeLV15p7w8eTPkK\n/J57Ur8679DBPlutmibpahp6tGypWqdO2lf4N90UvTuQUBfWBx44fN2nn1qV2ZHsN9LCheHlb7yR\ndN3ixar9+iWtvgopVMg+8+67qm++aXci6dm/X3Xv3vD7AQNUV69OffvU7gjGjbPlTZuqrlmT+udH\njVJ96SXVDRvCd11ZBa8aci72kgcCVdU//7T68tAJZM8e1WXLUj8hzphhn+vY0d537hw+waxcaevu\nu8+WXXBB+ifYUDVMVj2mT1dt0iTldQ0aHPl+n3pKdcsWa08555zw8sGDk/6NQ20WZ54ZXrZnj51U\njzvu8P2uX5/2b1a9uqqI/d2XLLHPVKuW9CQ9a5bqhReqDhwY3u+OHdbmceCAbZP8e7/8UrVdO9U5\nc5J+X+Q2L7wQrsbLCh4InMsB1qyx/3GTJh2+7tZbVXv3tteJidbA+9NPqt9/rzpypD0iG2O7d7d9\nPf+81a1HDiabMcPWzZ1rJ5qWLVM+uV59dTiggOo116j+9pstz8rgEM1H3772fNppqqNHJ13Xtm14\nkGCfPqrlyh3++U6d7HnAgJR/s8hty5dP+n7x4sO3CT369VNt3Fg1dLpKvr5Fi/Dr1L4P7PfJKh4I\nnMtjFixQLVDg8NHLqRk3TvX99+1/fPXqFlQSE+3kH3kFGhJaNnRoyie61BrCc/KjQoXU15UrZ3di\nn3xivZa2b7feTWntb/Ro1RtuSHndU0+FX+/Zk/Z+Hn7Yfo/QhULyR1ZJKxB4Y7FzuVDdutYAGRrn\nkJ5LL7VHu3aWKC9fPhtHEBoI98QT4cbsSKefnvL+0ht49tNPSQfh5QQpzS8RsnGjHevll9t4iJIl\n0z/Gbt1ST4e+fHn49bHHpr2fJ5+03yG15Ij335/257OCBwLn4kSxYvDZZ0lPcAWCtJMVKoRfQzjD\na716UKZMeHmHDjYaumtXSxkeKTRZUOfONuL5nXdSnjEOwiO5wWarmz3bMsUejX79Ul4e6o0U6b33\nju67IOkcF7VqJV339tuZ31/yFOghzz6b+X1llgcC5+JY6I4g+fwGw4dbBtdjj7X0HKGKijFjYOpU\nu5to1syW7d1r3UZ//91Scw8fbvsQCZ8gO3RIGjgiM7PeeiuceWbSbrHpqVv38GWPPmoT/iR30UXh\n16EZ7xIS4O674aWXMn5XlZaHHoK77jr6/aTm8sttpHi0eCBwLo5Vq2bPkVf9YCfqChUyto9ChWyO\nBIBSpZKmCw9NRXrffRY4Jk+29N0QnhM6FIxC04l26pR0/8nzOoHliOrbN/w+lNMptRQfxYvbcyjN\nxt9/W9ryu+4Klz255PNihNKZP/fc4dt27mz7GzMmvCwrq8Y+/dSq7ubNy7p9JpFa40FOfXhjsXNZ\nZ/9+65GU1X3WMyIh4fDukQcPWllKl7YeM7t323ZFiqhecYV11Qx1oU1MtIbdhISUU2fUrBnu8790\nqTWwL11qaUDWrg1vX6OGbf/ss7af0OdPOcWe69ZV3blTtX59ez9zZtoNuqFlH3yQcuNv7dpJ3+/e\nbQ3/Gele+9RTR/73Jo3G4qhNTOOcy/kKFgwnm8tu+VKojwjdHWzZkvTOIqXRwCKQ0hTmn35qmVWH\nDAk31EY2eoemJQ25+mq7u+jaNWmZRo2yBuZGjeyOok4dGw2emJj086G7kZCEBEs/fv75sHIlPPxw\neN22bdYQPWyYZWa98kqrEtu9245n/XpruN++HUaPts8++WT488lHiWcVzz7qnItriYmW/iJU9XPB\nBXbCTX6a2b3bUnZ0724Bo0IFa0dJT/v2Fmy6ds14mTZvhm+/teytDRva46abLLiE2jkyK63sox4I\nnHMukxYvtm6mmZ0DIrMSE60R/NZbw72yjlRagcCrhpxzLpNq1sye78mXL2nVUNS+J/pf4ZxzLifz\nQOCcc3HOA4FzzsU5DwTOORfnPBA451yc80DgnHNxzgOBc87FOQ8EzjkX53LdyGIR2Qz8nu6GKSsL\nbMnC4uQGfszxwY85PhzNMZ+kqilkZ8qFgeBoiMic1IZY51V+zPHBjzk+ROuYvWrIOefinAcC55yL\nc/EWCAbFugAx4MccH/yY40NUjjmu2gicc84dLt7uCJxzziXjgcA55+Jc3AQCEWkjIstEZIWIPBDr\n8mQFEaksIlNFZLGILBKRu4LlpUXkKxFZHjyXCpaLiAwI/gY/i0jD2B7BkROR/CLyk4iMD95XE5FZ\nwbF9JCLHBMsLBe9XBOurxrLcR0pESorIaBFZKiJLRKRZXv+dReTfwb/rhSLygYgUzmu/s4i8LSKb\nRGRhxLJM/64ickOw/XIRuSGz5YiLQCAi+YHXgIuBmkAnEcmmOYai6iBwr6rWBJoCtwfH9QAwWVWr\nA5OD92DHXz149ABez/4iZ5m7gCUR758BXlTVU4FtwM3B8puBbcHyF4PtcqOXgS9U9QygHnbsefZ3\nFpGKQC+gsarWBvID15L3fuehQJtkyzL1u4pIaaAPcBbQBOgTCh4Zpqp5/gE0AyZFvH8QeDDW5YrC\ncX4KXAgsA8oHy8oDy4LXbwKdIrY/tF1uegCVgv8g5wHjAcFGWxZI/nsDk4BmwesCwXYS62PI5PGW\nAFYlL3de/p2BisAaoHTwu40H/pUXf2egKrDwSH9XoBPwZsTyJNtl5BEXdwSE/1GFrA2W5RnBrXAD\nYBZQTlXXB6s2AOWC13nl7/AS8B8gMXhfBtiuqgeD95HHdeiYg/U7gu1zk2rAZuCdoDrsLREpSh7+\nnVX1T+B54A9gPfa7zSVv/84hmf1dj/r3jpdAkKeJSDHgY+BuVd0ZuU7tEiHP9BEWkXbAJlWdG+uy\nZKMCQEPgdVVtAPxNuLoAyJO/cyngMiwIVgCKcngVSp6XXb9rvASCP4HKEe8rBctyPREpiAWBEao6\nJli8UUTKB+vLA5uC5Xnh79AcaC8iq4EPseqhl4GSIlIg2CbyuA4dc7C+BLA1OwucBdYCa1V1VvB+\nNBYY8vLvfAGwSlU3q+oBYAz22+fl3zkks7/rUf/e8RIIfgSqBz0OjsEancbFuExHTUQEGAIsUdX+\nEavGAaGeAzdgbQeh5dcHvQ+aAjsibkFzBVV9UFUrqWpV7HecoqpdgKnAlcFmyY859Le4Mtg+V105\nq+oGYI2InB4sOh9YTB7+nbEqoaYiUiT4dx465jz7O0fI7O86CbhIREoFd1IXBcsyLtYNJdnYINMW\n+BX4DXg41uXJomM6B7tt/BmYHzzaYnWjk4HlwNdA6WB7wXpP/Qb8gvXIiPlxHMXxtwLGB69PBmYD\nK4BRQKFgeeHg/Ypg/cmxLvcRHmt9YE7wW38ClMrrvzPQF1gKLASGAYXy2u8MfIC1gRzA7vxuPpLf\nFbgpOPYVwI2ZLYenmHDOuTgXL1VDzjnnUuGBwDnn4pwHAueci3MeCJxzLs55IHDOuTjngcC5gIgk\niMj8iEeWZakVkaqRxcq6LAAAAcRJREFUGSady0kKpL+Jc3Fjj6rWj3UhnMtufkfgXDpEZLWIPCsi\nv4jIbBE5NVheVUSmBLnhJ4tIlWB5OREZKyILgsfZwa7yi8jgIMf+lyJybLB9L7E5JX4WkQ9jdJgu\njnkgcC7s2GRVQ9dErNuhqnWAV7HspwCvAO+qal1gBDAgWD4A+EZV62E5gRYFy6sDr6lqLWA70DFY\n/gDQINjPbdE6OOdS4yOLnQuIyG5VLZbC8tXAeaq6Mkjyt0FVy4jIFixv/IFg+XpVLSsim4FKqrov\nYh9Vga/UJhtBRO4HCqrqEyLyBbAbSx3xiarujvKhOpeE3xE4lzGayuvM2BfxOoFwG90lWA6ZhsCP\nEdk1ncsWHgicy5hrIp5nBq+/xzKgAnQBpgevJwM94dDcyiVS26mI5AMqq+pU4H4sffJhdyXORZNf\neTgXdqyIzI94/4WqhrqQlhKRn7Gr+k7BsjuxWcN6YzOI3RgsvwsYJCI3Y1f+PbEMkynJDwwPgoUA\nA1R1e5YdkXMZ4G0EzqUjaCNorKpbYl0W56LBq4accy7O+R2Bc87FOb8jcM65OOeBwDnn4pwHAuec\ni3MeCJxzLs55IHDOuTj3/1oSznFf9Km4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2eadb653-9952-4361-f70f-4018d98f61ac",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d89ef6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAWOKQLDMIwC5/8DAkcGwuUf3u8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3b42c5fa-33d7-4012-b9ad-fec3e34b7dad",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=105, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "105/105 [==============================] - 3s 25ms/step - loss: 1.3937 - acc: 0.1905\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 1.4110 - acc: 0.1810\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.3383 - acc: 0.2571\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.3043 - acc: 0.2381\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.3153 - acc: 0.2286\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.3610 - acc: 0.2381\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2880 - acc: 0.1810\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.3193 - acc: 0.2286\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2666 - acc: 0.2381\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2252 - acc: 0.2952\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.2234 - acc: 0.3238\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1984 - acc: 0.3619\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2300 - acc: 0.3524\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.2067 - acc: 0.3810\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.1403 - acc: 0.3333\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1621 - acc: 0.3810\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1743 - acc: 0.3905\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1227 - acc: 0.4667\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.1019 - acc: 0.4571\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1252 - acc: 0.4667\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.1100 - acc: 0.5238\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0915 - acc: 0.4762\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.1084 - acc: 0.4762\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1050 - acc: 0.4857\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0968 - acc: 0.5143\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 1.0940 - acc: 0.4857\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0622 - acc: 0.5333\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0457 - acc: 0.4762\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0667 - acc: 0.5619\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0476 - acc: 0.5238\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0391 - acc: 0.5048\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0422 - acc: 0.4857\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0038 - acc: 0.5619\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0137 - acc: 0.5810\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0271 - acc: 0.5143\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.9952 - acc: 0.5810\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9775 - acc: 0.5905\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9867 - acc: 0.5524\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9978 - acc: 0.5333\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9850 - acc: 0.5524\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0007 - acc: 0.5238\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9849 - acc: 0.5714\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9770 - acc: 0.5333\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.9846 - acc: 0.5905\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.9231 - acc: 0.6286\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.9772 - acc: 0.6286\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9700 - acc: 0.5524\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9697 - acc: 0.5619\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.9433 - acc: 0.6000\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9482 - acc: 0.5905\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0039 - acc: 0.5238\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.9589 - acc: 0.5619\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9850 - acc: 0.5238\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9561 - acc: 0.6381\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9695 - acc: 0.6000\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.9591 - acc: 0.5714\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9592 - acc: 0.5810\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.9183 - acc: 0.6857\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9185 - acc: 0.6667\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9377 - acc: 0.6095\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9160 - acc: 0.6286\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.8839 - acc: 0.6571\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9107 - acc: 0.6095\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9032 - acc: 0.6000\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8975 - acc: 0.6476\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9169 - acc: 0.6381\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.9451 - acc: 0.5714\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9131 - acc: 0.5905\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9219 - acc: 0.6381\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9005 - acc: 0.6286\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9056 - acc: 0.6381\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9099 - acc: 0.6286\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.9113 - acc: 0.6286\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8849 - acc: 0.6476\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9197 - acc: 0.6381\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8924 - acc: 0.6667\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8921 - acc: 0.6571\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8889 - acc: 0.6381\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8970 - acc: 0.6286\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8806 - acc: 0.6286\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8833 - acc: 0.6667\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9167 - acc: 0.6000\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.8883 - acc: 0.6095\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8831 - acc: 0.6190\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8809 - acc: 0.6286\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8579 - acc: 0.6762\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8746 - acc: 0.6095\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.9011 - acc: 0.6095\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8654 - acc: 0.6381\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8840 - acc: 0.6286\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8554 - acc: 0.6667\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8368 - acc: 0.7048\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8601 - acc: 0.6571\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8930 - acc: 0.6190\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8767 - acc: 0.6762\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8916 - acc: 0.6190\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8798 - acc: 0.6476\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8708 - acc: 0.6571\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8654 - acc: 0.6762\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8620 - acc: 0.6476\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8637 - acc: 0.6286\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8318 - acc: 0.6667\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8762 - acc: 0.6762\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8185 - acc: 0.7048\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8340 - acc: 0.7048\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8421 - acc: 0.6476\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.8477 - acc: 0.6857\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8383 - acc: 0.6571\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.6667\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.8129 - acc: 0.6952\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8356 - acc: 0.6762\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8471 - acc: 0.6952\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8344 - acc: 0.6571\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8355 - acc: 0.6667\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8443 - acc: 0.6571\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8416 - acc: 0.6857\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8074 - acc: 0.7048\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8425 - acc: 0.6667\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8138 - acc: 0.6857\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8200 - acc: 0.6571\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8418 - acc: 0.6667\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8391 - acc: 0.6476\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8314 - acc: 0.7143\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7967 - acc: 0.7048\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7981 - acc: 0.7048\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8365 - acc: 0.6476\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8111 - acc: 0.6667\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8153 - acc: 0.6571\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8377 - acc: 0.6381\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.8188 - acc: 0.6667\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8090 - acc: 0.7333\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8031 - acc: 0.6762\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8067 - acc: 0.6667\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7981 - acc: 0.6952\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8168 - acc: 0.6571\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8258 - acc: 0.6762\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8166 - acc: 0.6762\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7743 - acc: 0.7333\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7744 - acc: 0.7048\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.7991 - acc: 0.7238\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8120 - acc: 0.6381\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7765 - acc: 0.7238\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7893 - acc: 0.7143\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7863 - acc: 0.6381\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8006 - acc: 0.6667\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7592 - acc: 0.7143\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7818 - acc: 0.6762\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7868 - acc: 0.6857\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7676 - acc: 0.6857\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.7504 - acc: 0.7143\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7969 - acc: 0.6952\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7639 - acc: 0.7143\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8001 - acc: 0.6571\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7951 - acc: 0.6667\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7913 - acc: 0.7238\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7812 - acc: 0.6857\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8231 - acc: 0.6476\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7422 - acc: 0.7333\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7702 - acc: 0.6667\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7592 - acc: 0.6762\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7420 - acc: 0.7143\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7269 - acc: 0.7714\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7695 - acc: 0.6667\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7669 - acc: 0.7333\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7546 - acc: 0.7048\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7713 - acc: 0.6667\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.7573 - acc: 0.6952\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7701 - acc: 0.7238\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7329 - acc: 0.7143\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7711 - acc: 0.7238\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7547 - acc: 0.6857\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7535 - acc: 0.7048\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7643 - acc: 0.6857\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7505 - acc: 0.6952\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7354 - acc: 0.7524\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7263 - acc: 0.7143\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7716 - acc: 0.6381\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7174 - acc: 0.7048\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7159 - acc: 0.7238\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.7346 - acc: 0.7143\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7297 - acc: 0.7143\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7288 - acc: 0.7143\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7298 - acc: 0.7143\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7516 - acc: 0.7714\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6870 - acc: 0.7238\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.7613 - acc: 0.6952\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6978 - acc: 0.7429\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7369 - acc: 0.6667\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7168 - acc: 0.7048\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7670 - acc: 0.6762\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7620 - acc: 0.6762\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7233 - acc: 0.7238\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7024 - acc: 0.7810\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7226 - acc: 0.6857\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7392 - acc: 0.7143\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7229 - acc: 0.7143\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7348 - acc: 0.6762\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7374 - acc: 0.7143\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7492 - acc: 0.7048\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7019 - acc: 0.6952\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7199 - acc: 0.7048\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6964 - acc: 0.7143\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7337 - acc: 0.7048\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7473 - acc: 0.6857\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7461 - acc: 0.6857\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7218 - acc: 0.7238\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7239 - acc: 0.7524\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7065 - acc: 0.7333\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7237 - acc: 0.7238\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7258 - acc: 0.7238\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7144 - acc: 0.7619\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7182 - acc: 0.6857\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6942 - acc: 0.7333\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7236 - acc: 0.6857\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6900 - acc: 0.7143\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7060 - acc: 0.7143\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6999 - acc: 0.7238\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6915 - acc: 0.7048\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6888 - acc: 0.7238\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6937 - acc: 0.7333\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7502 - acc: 0.6476\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6903 - acc: 0.7429\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6657 - acc: 0.7429\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6865 - acc: 0.7143\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7035 - acc: 0.7048\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6866 - acc: 0.6952\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.6632 - acc: 0.7238\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 108us/step - loss: 0.6999 - acc: 0.6762\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.6837 - acc: 0.7143\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.7002 - acc: 0.7238\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6856 - acc: 0.7524\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6607 - acc: 0.7333\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6844 - acc: 0.6952\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6536 - acc: 0.7619\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6380 - acc: 0.7333\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6792 - acc: 0.7143\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6552 - acc: 0.7714\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6618 - acc: 0.7619\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.6750 - acc: 0.7143\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6748 - acc: 0.6857\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6498 - acc: 0.7714\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6580 - acc: 0.7048\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6946 - acc: 0.7429\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.6465 - acc: 0.7619\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6815 - acc: 0.7333\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6653 - acc: 0.7429\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 110us/step - loss: 0.6445 - acc: 0.7524\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6649 - acc: 0.7333\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6202 - acc: 0.7905\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6587 - acc: 0.7333\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6323 - acc: 0.8000\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6880 - acc: 0.7524\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6480 - acc: 0.7619\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6189 - acc: 0.7619\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6628 - acc: 0.7048\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6104 - acc: 0.7810\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6605 - acc: 0.7619\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6486 - acc: 0.7143\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6182 - acc: 0.7905\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6133 - acc: 0.7714\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6410 - acc: 0.7429\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6380 - acc: 0.7810\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6291 - acc: 0.7714\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7031 - acc: 0.7048\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6810 - acc: 0.6857\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6209 - acc: 0.7714\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6352 - acc: 0.7143\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6085 - acc: 0.7619\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6462 - acc: 0.7524\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6233 - acc: 0.7714\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6891 - acc: 0.7429\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6082 - acc: 0.7714\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5983 - acc: 0.7810\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6324 - acc: 0.7524\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6702 - acc: 0.7333\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6971 - acc: 0.7143\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6366 - acc: 0.7619\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6421 - acc: 0.7810\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6061 - acc: 0.7619\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6071 - acc: 0.7905\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6328 - acc: 0.7429\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 109us/step - loss: 0.6007 - acc: 0.7714\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.5822 - acc: 0.7524\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6877 - acc: 0.7048\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6212 - acc: 0.7429\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6150 - acc: 0.7905\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6213 - acc: 0.7524\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5799 - acc: 0.7619\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5769 - acc: 0.7619\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5980 - acc: 0.7905\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5831 - acc: 0.7619\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6825 - acc: 0.6857\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6457 - acc: 0.7143\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.6066 - acc: 0.7524\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6260 - acc: 0.7905\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6378 - acc: 0.7333\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.6368 - acc: 0.7238\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6001 - acc: 0.7810\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6120 - acc: 0.7714\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.5727 - acc: 0.8000\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6005 - acc: 0.8000\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.5702 - acc: 0.8095\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6287 - acc: 0.7524\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.5972 - acc: 0.7714\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.6073 - acc: 0.7524\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5926 - acc: 0.7714\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5714 - acc: 0.7238\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.5869 - acc: 0.7810\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6113 - acc: 0.7810\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6140 - acc: 0.7429\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.5803 - acc: 0.7619\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5706 - acc: 0.7810\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6087 - acc: 0.7714\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5813 - acc: 0.7810\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5561 - acc: 0.7905\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5470 - acc: 0.7524\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5669 - acc: 0.7810\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.5655 - acc: 0.8000\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5719 - acc: 0.7429\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5305 - acc: 0.8190\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5607 - acc: 0.7810\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.6242 - acc: 0.7429\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5409 - acc: 0.8095\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.5532 - acc: 0.7714\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5900 - acc: 0.7524\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5708 - acc: 0.7714\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5650 - acc: 0.7810\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5533 - acc: 0.7619\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5497 - acc: 0.7810\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5423 - acc: 0.7905\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5761 - acc: 0.7810\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5994 - acc: 0.7619\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5869 - acc: 0.7714\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5826 - acc: 0.7524\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5619 - acc: 0.7810\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5462 - acc: 0.7810\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5880 - acc: 0.7619\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4954 - acc: 0.8000\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5612 - acc: 0.8190\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5274 - acc: 0.8095\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.5572 - acc: 0.8000\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.5916 - acc: 0.7524\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5793 - acc: 0.7714\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5331 - acc: 0.8190\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 92us/step - loss: 0.5683 - acc: 0.7810\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5823 - acc: 0.7524\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.5648 - acc: 0.8190\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5606 - acc: 0.7714\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5593 - acc: 0.7714\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5455 - acc: 0.8095\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5024 - acc: 0.7905\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5504 - acc: 0.8000\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5124 - acc: 0.8190\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5426 - acc: 0.7810\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5371 - acc: 0.7810\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5563 - acc: 0.8000\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5406 - acc: 0.7810\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5252 - acc: 0.8190\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6024 - acc: 0.7810\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5729 - acc: 0.7905\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5388 - acc: 0.8190\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5742 - acc: 0.7714\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5632 - acc: 0.7429\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5894 - acc: 0.7238\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5630 - acc: 0.8095\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5484 - acc: 0.7714\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5091 - acc: 0.8476\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5136 - acc: 0.8095\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5206 - acc: 0.8476\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5600 - acc: 0.8000\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5480 - acc: 0.7905\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4730 - acc: 0.8190\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5260 - acc: 0.8190\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5064 - acc: 0.8190\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5475 - acc: 0.7905\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4981 - acc: 0.8095\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5318 - acc: 0.8000\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5693 - acc: 0.7810\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5723 - acc: 0.7905\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5717 - acc: 0.8000\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5135 - acc: 0.8190\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5572 - acc: 0.8095\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5282 - acc: 0.7905\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5126 - acc: 0.8381\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4881 - acc: 0.8095\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4776 - acc: 0.8476\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5178 - acc: 0.7905\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5326 - acc: 0.8000\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5182 - acc: 0.8095\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4901 - acc: 0.8381\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4890 - acc: 0.8286\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4861 - acc: 0.8571\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4824 - acc: 0.8286\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5039 - acc: 0.7810\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5207 - acc: 0.8000\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5202 - acc: 0.8095\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4921 - acc: 0.8571\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5152 - acc: 0.7810\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5603 - acc: 0.7714\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5383 - acc: 0.8095\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6292 - acc: 0.7429\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5140 - acc: 0.8476\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5244 - acc: 0.7714\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4850 - acc: 0.8286\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4961 - acc: 0.8571\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5365 - acc: 0.8190\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5057 - acc: 0.8476\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5148 - acc: 0.8476\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5360 - acc: 0.8000\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4821 - acc: 0.8381\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5210 - acc: 0.8000\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4814 - acc: 0.8381\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5187 - acc: 0.8381\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5380 - acc: 0.7905\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4846 - acc: 0.8190\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4936 - acc: 0.8286\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5070 - acc: 0.8000\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5516 - acc: 0.7810\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5179 - acc: 0.7905\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4860 - acc: 0.8381\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5674 - acc: 0.8000\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5396 - acc: 0.7810\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4614 - acc: 0.8571\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5257 - acc: 0.8190\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4681 - acc: 0.8286\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5512 - acc: 0.7619\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4854 - acc: 0.8381\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4858 - acc: 0.8000\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4860 - acc: 0.8000\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4807 - acc: 0.8571\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4906 - acc: 0.8000\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4780 - acc: 0.7905\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5096 - acc: 0.8476\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4743 - acc: 0.8190\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4636 - acc: 0.8190\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4974 - acc: 0.7905\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5082 - acc: 0.8286\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5294 - acc: 0.7619\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4874 - acc: 0.8095\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4933 - acc: 0.8095\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4857 - acc: 0.8571\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5505 - acc: 0.7810\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4831 - acc: 0.8571\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5045 - acc: 0.8476\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5620 - acc: 0.7619\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4750 - acc: 0.8571\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4606 - acc: 0.8476\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4912 - acc: 0.8095\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4785 - acc: 0.8381\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5239 - acc: 0.8190\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.5097 - acc: 0.8190\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4812 - acc: 0.8381\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5310 - acc: 0.8571\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4752 - acc: 0.8286\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4298 - acc: 0.8762\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5146 - acc: 0.8095\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4754 - acc: 0.8286\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4081 - acc: 0.9048\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4854 - acc: 0.8571\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4772 - acc: 0.8381\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4651 - acc: 0.8952\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5254 - acc: 0.8190\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4216 - acc: 0.8857\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5029 - acc: 0.8000\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4360 - acc: 0.8667\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4753 - acc: 0.8381\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4594 - acc: 0.8190\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4676 - acc: 0.8381\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5211 - acc: 0.8190\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4348 - acc: 0.8952\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4550 - acc: 0.8667\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4293 - acc: 0.8667\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4544 - acc: 0.8381\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5068 - acc: 0.8667\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5010 - acc: 0.8286\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5277 - acc: 0.7714\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4518 - acc: 0.8762\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4479 - acc: 0.8667\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4102 - acc: 0.8381\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4426 - acc: 0.8857\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4721 - acc: 0.8286\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.4752 - acc: 0.8667\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4016 - acc: 0.8667\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4381 - acc: 0.8667\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4737 - acc: 0.8190\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4214 - acc: 0.9048\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5498 - acc: 0.8000\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4226 - acc: 0.8667\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4171 - acc: 0.8857\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4739 - acc: 0.8762\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5061 - acc: 0.8000\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4919 - acc: 0.8571\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4796 - acc: 0.8381\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3873 - acc: 0.8952\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4389 - acc: 0.8571\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.4484 - acc: 0.8571\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4019 - acc: 0.9048\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4456 - acc: 0.8476\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4179 - acc: 0.8952\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3975 - acc: 0.8762\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4385 - acc: 0.8762\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4336 - acc: 0.8762\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4994 - acc: 0.7905\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3745 - acc: 0.9333\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3977 - acc: 0.9048\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4423 - acc: 0.8762\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4538 - acc: 0.8762\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3897 - acc: 0.8952\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4028 - acc: 0.8476\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3861 - acc: 0.8857\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4313 - acc: 0.8762\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4624 - acc: 0.8381\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3772 - acc: 0.8952\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5134 - acc: 0.8286\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4695 - acc: 0.8762\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4434 - acc: 0.8476\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4539 - acc: 0.8000\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4144 - acc: 0.8571\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4762 - acc: 0.8286\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4579 - acc: 0.8190\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4248 - acc: 0.8952\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4672 - acc: 0.8857\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4722 - acc: 0.8762\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4828 - acc: 0.8000\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4192 - acc: 0.8571\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4599 - acc: 0.8190\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3661 - acc: 0.9048\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4184 - acc: 0.8952\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4072 - acc: 0.8667\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4245 - acc: 0.8571\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4678 - acc: 0.8476\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4224 - acc: 0.9048\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4046 - acc: 0.8571\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4076 - acc: 0.8857\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4369 - acc: 0.8857\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3940 - acc: 0.8667\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4163 - acc: 0.8476\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3502 - acc: 0.8952\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3672 - acc: 0.8952\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4026 - acc: 0.9048\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3913 - acc: 0.9048\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4593 - acc: 0.8286\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3873 - acc: 0.9048\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4513 - acc: 0.8476\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3996 - acc: 0.9048\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3739 - acc: 0.8857\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4336 - acc: 0.8762\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4501 - acc: 0.8571\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3540 - acc: 0.9143\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4088 - acc: 0.8857\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3663 - acc: 0.9238\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4599 - acc: 0.8571\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4180 - acc: 0.8857\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4177 - acc: 0.8476\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4711 - acc: 0.8381\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3752 - acc: 0.9048\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4234 - acc: 0.8571\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3704 - acc: 0.9238\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4490 - acc: 0.8286\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4360 - acc: 0.8762\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3578 - acc: 0.9143\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.4461 - acc: 0.8381\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3965 - acc: 0.8571\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4586 - acc: 0.8190\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4281 - acc: 0.8667\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4172 - acc: 0.9143\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4162 - acc: 0.8762\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3813 - acc: 0.8952\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3790 - acc: 0.9143\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4441 - acc: 0.8857\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3928 - acc: 0.8762\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4202 - acc: 0.8952\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3211 - acc: 0.9524\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4011 - acc: 0.8857\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4116 - acc: 0.8952\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4072 - acc: 0.8857\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4103 - acc: 0.8762\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4144 - acc: 0.8857\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4339 - acc: 0.8381\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4116 - acc: 0.8857\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4076 - acc: 0.8857\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3374 - acc: 0.8952\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4114 - acc: 0.8952\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3763 - acc: 0.9429\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.8857\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3930 - acc: 0.9048\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4283 - acc: 0.8667\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4002 - acc: 0.8952\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4396 - acc: 0.8476\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3580 - acc: 0.9143\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3413 - acc: 0.9333\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4069 - acc: 0.8762\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3863 - acc: 0.9048\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3636 - acc: 0.9238\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3624 - acc: 0.9238\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3756 - acc: 0.9143\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3441 - acc: 0.9143\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4243 - acc: 0.9048\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4019 - acc: 0.8476\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4213 - acc: 0.8762\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4197 - acc: 0.8762\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3770 - acc: 0.8952\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3680 - acc: 0.9048\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3686 - acc: 0.9238\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3992 - acc: 0.8857\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4400 - acc: 0.8762\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3825 - acc: 0.8857\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3245 - acc: 0.9619\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3952 - acc: 0.8667\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3555 - acc: 0.9048\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3488 - acc: 0.9143\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3560 - acc: 0.9143\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3519 - acc: 0.9238\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3646 - acc: 0.9048\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4008 - acc: 0.8667\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4256 - acc: 0.8762\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4372 - acc: 0.8286\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3627 - acc: 0.9048\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3695 - acc: 0.9048\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3597 - acc: 0.8952\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3990 - acc: 0.9048\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3542 - acc: 0.8952\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3681 - acc: 0.8952\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4121 - acc: 0.8667\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4011 - acc: 0.8952\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3884 - acc: 0.9238\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3514 - acc: 0.8952\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3564 - acc: 0.9143\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3340 - acc: 0.9238\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3491 - acc: 0.8857\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3444 - acc: 0.8952\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3400 - acc: 0.9238\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3887 - acc: 0.8667\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3900 - acc: 0.9048\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3497 - acc: 0.9238\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3638 - acc: 0.9048\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3723 - acc: 0.8571\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3636 - acc: 0.9333\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3626 - acc: 0.9143\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3728 - acc: 0.8762\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3782 - acc: 0.9238\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3645 - acc: 0.8857\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4445 - acc: 0.8667\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4511 - acc: 0.8857\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3846 - acc: 0.9333\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3771 - acc: 0.8857\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3995 - acc: 0.8952\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3425 - acc: 0.8952\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3519 - acc: 0.9238\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4161 - acc: 0.8476\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3776 - acc: 0.9048\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3395 - acc: 0.9143\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3927 - acc: 0.8762\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3972 - acc: 0.8381\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4097 - acc: 0.8667\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3954 - acc: 0.8667\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3211 - acc: 0.9619\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3614 - acc: 0.8762\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4170 - acc: 0.8571\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3720 - acc: 0.8857\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4011 - acc: 0.8571\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3415 - acc: 0.9048\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3250 - acc: 0.9143\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3016 - acc: 0.9333\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3384 - acc: 0.9238\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3508 - acc: 0.9143\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4041 - acc: 0.8952\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3876 - acc: 0.8667\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3593 - acc: 0.9048\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3421 - acc: 0.9333\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4155 - acc: 0.8857\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3745 - acc: 0.9143\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3582 - acc: 0.8952\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3687 - acc: 0.9048\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3498 - acc: 0.9143\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3376 - acc: 0.9429\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4174 - acc: 0.8857\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4081 - acc: 0.8762\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3452 - acc: 0.9333\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3209 - acc: 0.9143\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3170 - acc: 0.9238\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3024 - acc: 0.9524\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3640 - acc: 0.9048\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3165 - acc: 0.9333\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3376 - acc: 0.9143\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3761 - acc: 0.8952\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3442 - acc: 0.9048\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4584 - acc: 0.9238\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3818 - acc: 0.8857\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3093 - acc: 0.9524\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3184 - acc: 0.9238\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3437 - acc: 0.9048\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3521 - acc: 0.9048\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2882 - acc: 0.9714\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8476\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3566 - acc: 0.9143\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3377 - acc: 0.9143\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3698 - acc: 0.8952\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2954 - acc: 0.9143\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3197 - acc: 0.9143\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2871 - acc: 0.9333\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2806 - acc: 0.9619\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3126 - acc: 0.9333\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2936 - acc: 0.9333\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3742 - acc: 0.9143\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3663 - acc: 0.9048\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3087 - acc: 0.9238\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3344 - acc: 0.9429\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2963 - acc: 0.9524\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3995 - acc: 0.9048\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3300 - acc: 0.9143\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3349 - acc: 0.8857\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3319 - acc: 0.9238\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3100 - acc: 0.9333\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4045 - acc: 0.8571\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2995 - acc: 0.9333\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3400 - acc: 0.9429\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3172 - acc: 0.9429\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3277 - acc: 0.9048\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3377 - acc: 0.9238\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3058 - acc: 0.9524\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2997 - acc: 0.9429\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3298 - acc: 0.9429\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3733 - acc: 0.9048\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3700 - acc: 0.8952\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3314 - acc: 0.9238\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3104 - acc: 0.9143\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3294 - acc: 0.9333\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3817 - acc: 0.8952\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3914 - acc: 0.8952\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3228 - acc: 0.9333\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3240 - acc: 0.9048\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3127 - acc: 0.9429\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3161 - acc: 0.9048\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3123 - acc: 0.9238\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3153 - acc: 0.9143\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3129 - acc: 0.9143\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3502 - acc: 0.8667\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2868 - acc: 0.9524\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3523 - acc: 0.8857\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3407 - acc: 0.9238\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3311 - acc: 0.9238\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3410 - acc: 0.9333\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3568 - acc: 0.9238\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3504 - acc: 0.8762\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3587 - acc: 0.9333\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3769 - acc: 0.9238\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3099 - acc: 0.9524\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3133 - acc: 0.9333\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3678 - acc: 0.9048\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2864 - acc: 0.9333\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2896 - acc: 0.9429\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3903 - acc: 0.9048\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2851 - acc: 0.9429\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3082 - acc: 0.9048\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3461 - acc: 0.9048\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3200 - acc: 0.9238\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3359 - acc: 0.9143\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3264 - acc: 0.9333\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2983 - acc: 0.9429\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3267 - acc: 0.9524\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2996 - acc: 0.9238\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3666 - acc: 0.8667\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4223 - acc: 0.9143\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2862 - acc: 0.9429\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2958 - acc: 0.9333\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3446 - acc: 0.9333\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3221 - acc: 0.8952\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3007 - acc: 0.9238\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3186 - acc: 0.9429\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3486 - acc: 0.9143\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2811 - acc: 0.9429\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3619 - acc: 0.8857\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2931 - acc: 0.9429\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3619 - acc: 0.9048\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3024 - acc: 0.9238\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3093 - acc: 0.9333\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2788 - acc: 0.9429\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3064 - acc: 0.9238\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2678 - acc: 0.9619\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3292 - acc: 0.8952\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3468 - acc: 0.9238\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3353 - acc: 0.9238\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3012 - acc: 0.9238\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3243 - acc: 0.9333\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3304 - acc: 0.8952\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3782 - acc: 0.8762\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2678 - acc: 0.9333\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3010 - acc: 0.9143\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2659 - acc: 0.9619\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2936 - acc: 0.9429\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2897 - acc: 0.9429\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3110 - acc: 0.9238\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2843 - acc: 0.9333\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3317 - acc: 0.9143\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3197 - acc: 0.9143\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3111 - acc: 0.9333\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3524 - acc: 0.8857\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3328 - acc: 0.9238\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3213 - acc: 0.9048\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2380 - acc: 0.9714\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3274 - acc: 0.9048\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2990 - acc: 0.9333\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3002 - acc: 0.9333\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.2905 - acc: 0.9524\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3729 - acc: 0.8952\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3255 - acc: 0.9048\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3232 - acc: 0.9048\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3057 - acc: 0.9429\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3216 - acc: 0.9143\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3002 - acc: 0.9238\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3030 - acc: 0.9238\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3089 - acc: 0.9524\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3469 - acc: 0.9143\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3144 - acc: 0.9238\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3255 - acc: 0.9143\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3117 - acc: 0.9238\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2567 - acc: 0.9524\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3124 - acc: 0.9238\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2572 - acc: 0.9619\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3172 - acc: 0.9429\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3155 - acc: 0.8952\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2933 - acc: 0.9429\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3064 - acc: 0.9143\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2614 - acc: 0.9714\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2842 - acc: 0.9429\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3219 - acc: 0.9429\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3016 - acc: 0.9143\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3246 - acc: 0.9238\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3190 - acc: 0.9143\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3018 - acc: 0.9238\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2801 - acc: 0.9524\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2518 - acc: 0.9714\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3283 - acc: 0.9429\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3036 - acc: 0.9524\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2667 - acc: 0.9429\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3155 - acc: 0.9238\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3430 - acc: 0.8952\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3161 - acc: 0.9238\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3301 - acc: 0.8952\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2629 - acc: 0.9619\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2464 - acc: 0.9714\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2679 - acc: 0.9619\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2666 - acc: 0.9619\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3165 - acc: 0.9333\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2845 - acc: 0.9143\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3345 - acc: 0.9238\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2713 - acc: 0.9429\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2727 - acc: 0.9619\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2933 - acc: 0.9143\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2638 - acc: 0.9619\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3139 - acc: 0.9143\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2830 - acc: 0.9333\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2800 - acc: 0.9619\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2940 - acc: 0.9143\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3349 - acc: 0.9619\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2758 - acc: 0.9429\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3298 - acc: 0.9048\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3106 - acc: 0.9048\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2884 - acc: 0.9429\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2903 - acc: 0.9429\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2934 - acc: 0.9524\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3022 - acc: 0.9524\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3691 - acc: 0.8857\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2757 - acc: 0.9333\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2845 - acc: 0.9333\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3219 - acc: 0.9333\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2441 - acc: 0.9714\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2870 - acc: 0.9429\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3348 - acc: 0.9143\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2704 - acc: 0.9429\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3211 - acc: 0.9333\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2973 - acc: 0.9238\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3100 - acc: 0.9048\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2579 - acc: 0.9619\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3476 - acc: 0.9048\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2880 - acc: 0.9333\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2538 - acc: 0.9524\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2826 - acc: 0.9429\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2292 - acc: 0.9714\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3137 - acc: 0.9048\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2789 - acc: 0.9619\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2585 - acc: 0.9714\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3563 - acc: 0.9143\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2816 - acc: 0.9619\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2794 - acc: 0.9524\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3106 - acc: 0.9429\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2951 - acc: 0.9429\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2624 - acc: 0.9714\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2765 - acc: 0.9524\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2588 - acc: 0.9619\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2923 - acc: 0.9238\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2850 - acc: 0.9429\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2480 - acc: 0.9524\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2871 - acc: 0.9619\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2391 - acc: 0.9714\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2386 - acc: 0.9524\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2861 - acc: 0.9524\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2535 - acc: 0.9714\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2579 - acc: 0.9429\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2405 - acc: 0.9714\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2597 - acc: 0.9429\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2847 - acc: 0.9524\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2970 - acc: 0.9429\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2932 - acc: 0.9048\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3279 - acc: 0.9143\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3312 - acc: 0.9048\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3120 - acc: 0.9619\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2328 - acc: 0.9619\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3102 - acc: 0.9238\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3018 - acc: 0.9429\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2797 - acc: 0.9429\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3074 - acc: 0.9333\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2675 - acc: 0.9619\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3064 - acc: 0.9143\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2243 - acc: 0.9714\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2530 - acc: 0.9429\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2894 - acc: 0.9048\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2599 - acc: 0.9619\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2470 - acc: 0.9524\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2711 - acc: 0.9238\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2165 - acc: 0.9810\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2663 - acc: 0.9714\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2501 - acc: 0.9333\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2512 - acc: 0.9524\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3126 - acc: 0.9429\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2858 - acc: 0.9714\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2801 - acc: 0.9524\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2646 - acc: 0.9429\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2568 - acc: 0.9524\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2450 - acc: 0.9619\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2994 - acc: 0.9143\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2059 - acc: 0.9905\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2515 - acc: 0.9238\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2499 - acc: 0.9619\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2577 - acc: 0.9619\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2609 - acc: 0.9524\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2663 - acc: 0.9429\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2238 - acc: 0.9810\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3529 - acc: 0.8952\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.3644 - acc: 0.8857\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2540 - acc: 0.9429\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3209 - acc: 0.9143\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2728 - acc: 0.9333\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2832 - acc: 0.9333\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2770 - acc: 0.9429\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2314 - acc: 0.9810\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2286 - acc: 0.9619\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2739 - acc: 0.9524\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2516 - acc: 0.9714\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3039 - acc: 0.9619\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2512 - acc: 0.9619\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3096 - acc: 0.9333\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2533 - acc: 0.9429\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2335 - acc: 0.9619\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2975 - acc: 0.9238\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2402 - acc: 0.9619\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2494 - acc: 0.9429\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3083 - acc: 0.9238\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3057 - acc: 0.9524\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2864 - acc: 0.9429\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2509 - acc: 0.9524\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2604 - acc: 0.9429\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2255 - acc: 0.9810\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2244 - acc: 0.9619\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2286 - acc: 0.9619\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2938 - acc: 0.9429\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3292 - acc: 0.9143\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3720 - acc: 0.8667\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2987 - acc: 0.9238\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2663 - acc: 0.9524\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2483 - acc: 0.9524\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3037 - acc: 0.9238\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3137 - acc: 0.9048\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2818 - acc: 0.9238\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3126 - acc: 0.9048\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2672 - acc: 0.9714\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2308 - acc: 0.9714\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2599 - acc: 0.9429\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2695 - acc: 0.9524\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2321 - acc: 0.9714\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2632 - acc: 0.9333\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2566 - acc: 0.9524\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2715 - acc: 0.9429\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2441 - acc: 0.9619\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2301 - acc: 0.9619\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2331 - acc: 0.9714\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2937 - acc: 0.9238\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2546 - acc: 0.9524\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3295 - acc: 0.9143\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2731 - acc: 0.9619\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2780 - acc: 0.9333\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2737 - acc: 0.9333\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2695 - acc: 0.9238\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2373 - acc: 0.9619\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2809 - acc: 0.9429\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2419 - acc: 0.9524\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2392 - acc: 0.9524\n",
            "13/13 [==============================] - 1s 85ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff7576f8-bef3-4d03-d19d-4ff558c89c12",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6a12ba6a-9f05-4702-f549-37f98e0cff61",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HBt6_60xcak",
        "colab_type": "text"
      },
      "source": [
        "#NO FEATURES SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    }
  ]
}
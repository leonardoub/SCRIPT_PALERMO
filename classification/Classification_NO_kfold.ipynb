{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_NO_kfold.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgaxJc/JJttm5KKfrgp0BH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Classification_NO_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i4O_AtKVzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb00g1tWTqw",
        "colab_type": "code",
        "outputId": "d463e33b-5840-41d2-bdb6-463eac62fc6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "ad99beb6-dd0f-485f-e4c3-264700a9dc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "outputId": "b320ab97-6fab-4b46-f154-c539560487d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        }
      },
      "source": [
        "mean = X_train_1.mean(axis=0)\n",
        "train_data_stand = X_train - mean\n",
        "std = X_train.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-9fb161c492be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data_stand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data_stand\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaqeWiaXXGy0",
        "colab_type": "code",
        "outputId": "30b9da23-c3c5-4ebb-81cd-0db415b9c13e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "val_data_stand = X_val - mean\n",
        "val_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-1e8e2a7c3a80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_data_stand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_data_stand\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "outputId": "5db0f954-c012-4cb5-d5c2-b3a70c72a14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "test_data_stand = X_test - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-05b0ce8783a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data_stand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data_stand\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "713f09e6-5367-4c72-dc14-07aa57367b97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, y_train).transform(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3bbc96f90a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_stand_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_stand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_stand' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "1d339431-a448-4d37-add9-7d420fd4efaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-df608e2fb6f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_stand_lda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_stand_lda' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0UwBbCjf13g",
        "colab_type": "code",
        "outputId": "dfd6edd6-827c-4146-b051-1689bd755c74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "val_data_stand_lda = lda.transform(val_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-5a5dadae9898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mval_data_stand_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "outputId": "6ec5940f-790f-421c-f738-111a2b887d2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        }
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-37b65ed0c634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_data_stand_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5fsvoQwBqR",
        "colab_type": "text"
      },
      "source": [
        "#Z-score dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8_CwJZwBD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KW2c_RIXpWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = val_data_stand_lda - mean\n",
        "val_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3wGNwiWXvbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLYdHX-mYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAd2LU_dEO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_sSb3CnLM2",
        "colab_type": "code",
        "outputId": "9a7379ea-3dd5-4d02-aafe-e5bb8969e992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_val_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hc4rG203_u",
        "colab_type": "code",
        "outputId": "15a93300-06ad-43b7-fdb1-6102a62dff54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 180\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=1, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/180\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 1.3599 - acc: 0.1143 - val_loss: 4.0419 - val_acc: 0.0769\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 921us/step - loss: 1.1282 - acc: 0.1143 - val_loss: 3.6833 - val_acc: 0.1538\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 973us/step - loss: 0.9610 - acc: 0.1524 - val_loss: 3.2111 - val_acc: 0.2308\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.8387 - acc: 0.4571 - val_loss: 2.8336 - val_acc: 0.2308\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 977us/step - loss: 0.7469 - acc: 0.8667 - val_loss: 2.6660 - val_acc: 0.2308\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.6760 - acc: 0.9524 - val_loss: 2.6509 - val_acc: 0.2308\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.6194 - acc: 0.9810 - val_loss: 2.6473 - val_acc: 0.1538\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5729 - acc: 0.9905 - val_loss: 2.6447 - val_acc: 0.1538\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5338 - acc: 1.0000 - val_loss: 2.6416 - val_acc: 0.1538\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 1.0000 - val_loss: 2.6421 - val_acc: 0.1538\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.4703 - acc: 1.0000 - val_loss: 2.6448 - val_acc: 0.1538\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.4441 - acc: 1.0000 - val_loss: 2.6489 - val_acc: 0.1538\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.4204 - acc: 1.0000 - val_loss: 2.6547 - val_acc: 0.1538\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3988 - acc: 1.0000 - val_loss: 2.6621 - val_acc: 0.1538\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3791 - acc: 1.0000 - val_loss: 2.6689 - val_acc: 0.1538\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3610 - acc: 1.0000 - val_loss: 2.6774 - val_acc: 0.1538\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3444 - acc: 1.0000 - val_loss: 2.6862 - val_acc: 0.1538\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3289 - acc: 1.0000 - val_loss: 2.6942 - val_acc: 0.1538\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3146 - acc: 1.0000 - val_loss: 2.7043 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 978us/step - loss: 0.3068 - acc: 1.0000 - val_loss: 2.7053 - val_acc: 0.1538\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3054 - acc: 1.0000 - val_loss: 2.7062 - val_acc: 0.1538\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.3041 - acc: 1.0000 - val_loss: 2.7072 - val_acc: 0.1538\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3028 - acc: 1.0000 - val_loss: 2.7082 - val_acc: 0.1538\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3015 - acc: 1.0000 - val_loss: 2.7091 - val_acc: 0.1538\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3002 - acc: 1.0000 - val_loss: 2.7101 - val_acc: 0.1538\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.2990 - acc: 1.0000 - val_loss: 2.7112 - val_acc: 0.1538\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2977 - acc: 1.0000 - val_loss: 2.7122 - val_acc: 0.1538\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2964 - acc: 1.0000 - val_loss: 2.7133 - val_acc: 0.1538\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2952 - acc: 1.0000 - val_loss: 2.7142 - val_acc: 0.1538\n",
            "\n",
            "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2939 - acc: 1.0000 - val_loss: 2.7152 - val_acc: 0.1538\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2927 - acc: 1.0000 - val_loss: 2.7162 - val_acc: 0.1538\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2915 - acc: 1.0000 - val_loss: 2.7173 - val_acc: 0.1538\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2902 - acc: 1.0000 - val_loss: 2.7183 - val_acc: 0.1538\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2890 - acc: 1.0000 - val_loss: 2.7193 - val_acc: 0.1538\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2878 - acc: 1.0000 - val_loss: 2.7205 - val_acc: 0.1538\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 944us/step - loss: 0.2866 - acc: 1.0000 - val_loss: 2.7215 - val_acc: 0.1538\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2854 - acc: 1.0000 - val_loss: 2.7224 - val_acc: 0.1538\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2843 - acc: 1.0000 - val_loss: 2.7235 - val_acc: 0.1538\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 958us/step - loss: 0.2831 - acc: 1.0000 - val_loss: 2.7246 - val_acc: 0.1538\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 965us/step - loss: 0.2819 - acc: 1.0000 - val_loss: 2.7257 - val_acc: 0.1538\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2808 - acc: 1.0000 - val_loss: 2.7268 - val_acc: 0.1538\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2796 - acc: 1.0000 - val_loss: 2.7278 - val_acc: 0.1538\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2785 - acc: 1.0000 - val_loss: 2.7289 - val_acc: 0.1538\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2773 - acc: 1.0000 - val_loss: 2.7300 - val_acc: 0.1538\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2762 - acc: 1.0000 - val_loss: 2.7311 - val_acc: 0.1538\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2751 - acc: 1.0000 - val_loss: 2.7322 - val_acc: 0.1538\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2740 - acc: 1.0000 - val_loss: 2.7333 - val_acc: 0.1538\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2729 - acc: 1.0000 - val_loss: 2.7345 - val_acc: 0.1538\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2718 - acc: 1.0000 - val_loss: 2.7356 - val_acc: 0.1538\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2707 - acc: 1.0000 - val_loss: 2.7367 - val_acc: 0.1538\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.2696 - acc: 1.0000 - val_loss: 2.7378 - val_acc: 0.1538\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 994us/step - loss: 0.2685 - acc: 1.0000 - val_loss: 2.7389 - val_acc: 0.1538\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2675 - acc: 1.0000 - val_loss: 2.7400 - val_acc: 0.1538\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2664 - acc: 1.0000 - val_loss: 2.7412 - val_acc: 0.1538\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2653 - acc: 1.0000 - val_loss: 2.7424 - val_acc: 0.1538\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2643 - acc: 1.0000 - val_loss: 2.7434 - val_acc: 0.1538\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2632 - acc: 1.0000 - val_loss: 2.7446 - val_acc: 0.1538\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2622 - acc: 1.0000 - val_loss: 2.7458 - val_acc: 0.1538\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2612 - acc: 1.0000 - val_loss: 2.7470 - val_acc: 0.1538\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.2601 - acc: 1.0000 - val_loss: 2.7482 - val_acc: 0.1538\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2591 - acc: 1.0000 - val_loss: 2.7493 - val_acc: 0.1538\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2581 - acc: 1.0000 - val_loss: 2.7505 - val_acc: 0.1538\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2571 - acc: 1.0000 - val_loss: 2.7517 - val_acc: 0.1538\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 987us/step - loss: 0.2561 - acc: 1.0000 - val_loss: 2.7528 - val_acc: 0.1538\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2551 - acc: 1.0000 - val_loss: 2.7540 - val_acc: 0.1538\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2541 - acc: 1.0000 - val_loss: 2.7552 - val_acc: 0.1538\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2532 - acc: 1.0000 - val_loss: 2.7564 - val_acc: 0.1538\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2522 - acc: 1.0000 - val_loss: 2.7576 - val_acc: 0.1538\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2512 - acc: 1.0000 - val_loss: 2.7588 - val_acc: 0.1538\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2502 - acc: 1.0000 - val_loss: 2.7599 - val_acc: 0.1538\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2493 - acc: 1.0000 - val_loss: 2.7612 - val_acc: 0.1538\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2483 - acc: 1.0000 - val_loss: 2.7624 - val_acc: 0.1538\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2474 - acc: 1.0000 - val_loss: 2.7636 - val_acc: 0.1538\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2465 - acc: 1.0000 - val_loss: 2.7648 - val_acc: 0.1538\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2455 - acc: 1.0000 - val_loss: 2.7661 - val_acc: 0.2308\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2446 - acc: 1.0000 - val_loss: 2.7673 - val_acc: 0.2308\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 948us/step - loss: 0.2437 - acc: 1.0000 - val_loss: 2.7685 - val_acc: 0.2308\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.2428 - acc: 1.0000 - val_loss: 2.7698 - val_acc: 0.2308\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2418 - acc: 1.0000 - val_loss: 2.7710 - val_acc: 0.2308\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 1000us/step - loss: 0.2409 - acc: 1.0000 - val_loss: 2.7722 - val_acc: 0.2308\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2400 - acc: 1.0000 - val_loss: 2.7735 - val_acc: 0.2308\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.2391 - acc: 1.0000 - val_loss: 2.7747 - val_acc: 0.2308\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2383 - acc: 1.0000 - val_loss: 2.7759 - val_acc: 0.2308\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2374 - acc: 1.0000 - val_loss: 2.7772 - val_acc: 0.2308\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2365 - acc: 1.0000 - val_loss: 2.7785 - val_acc: 0.2308\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2356 - acc: 1.0000 - val_loss: 2.7797 - val_acc: 0.2308\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2348 - acc: 1.0000 - val_loss: 2.7810 - val_acc: 0.2308\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2339 - acc: 1.0000 - val_loss: 2.7823 - val_acc: 0.3077\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2330 - acc: 1.0000 - val_loss: 2.7835 - val_acc: 0.3077\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2322 - acc: 1.0000 - val_loss: 2.7848 - val_acc: 0.3077\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2313 - acc: 1.0000 - val_loss: 2.7861 - val_acc: 0.3077\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2305 - acc: 1.0000 - val_loss: 2.7873 - val_acc: 0.3077\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2297 - acc: 1.0000 - val_loss: 2.7887 - val_acc: 0.3077\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 996us/step - loss: 0.2288 - acc: 1.0000 - val_loss: 2.7899 - val_acc: 0.3077\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2280 - acc: 1.0000 - val_loss: 2.7912 - val_acc: 0.3077\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.2272 - acc: 1.0000 - val_loss: 2.7925 - val_acc: 0.3077\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2264 - acc: 1.0000 - val_loss: 2.7938 - val_acc: 0.3077\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2256 - acc: 1.0000 - val_loss: 2.7951 - val_acc: 0.3077\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2247 - acc: 1.0000 - val_loss: 2.7964 - val_acc: 0.3077\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2239 - acc: 1.0000 - val_loss: 2.7977 - val_acc: 0.3077\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2231 - acc: 1.0000 - val_loss: 2.7990 - val_acc: 0.3077\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2224 - acc: 1.0000 - val_loss: 2.8003 - val_acc: 0.3077\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2216 - acc: 1.0000 - val_loss: 2.8016 - val_acc: 0.3077\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2208 - acc: 1.0000 - val_loss: 2.8029 - val_acc: 0.3077\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2200 - acc: 1.0000 - val_loss: 2.8043 - val_acc: 0.3077\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2192 - acc: 1.0000 - val_loss: 2.8056 - val_acc: 0.3077\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2185 - acc: 1.0000 - val_loss: 2.8069 - val_acc: 0.3077\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2177 - acc: 1.0000 - val_loss: 2.8082 - val_acc: 0.3077\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2169 - acc: 1.0000 - val_loss: 2.8096 - val_acc: 0.3077\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2162 - acc: 1.0000 - val_loss: 2.8109 - val_acc: 0.3077\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2154 - acc: 1.0000 - val_loss: 2.8123 - val_acc: 0.3077\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 983us/step - loss: 0.2147 - acc: 1.0000 - val_loss: 2.8135 - val_acc: 0.3077\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2139 - acc: 1.0000 - val_loss: 2.8149 - val_acc: 0.3077\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2132 - acc: 1.0000 - val_loss: 2.8162 - val_acc: 0.3077\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2124 - acc: 1.0000 - val_loss: 2.8176 - val_acc: 0.3077\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2117 - acc: 1.0000 - val_loss: 2.8189 - val_acc: 0.3077\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2110 - acc: 1.0000 - val_loss: 2.8203 - val_acc: 0.3077\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2103 - acc: 1.0000 - val_loss: 2.8217 - val_acc: 0.3077\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2095 - acc: 1.0000 - val_loss: 2.8230 - val_acc: 0.3077\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2088 - acc: 1.0000 - val_loss: 2.8243 - val_acc: 0.3077\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2081 - acc: 1.0000 - val_loss: 2.8257 - val_acc: 0.3077\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2074 - acc: 1.0000 - val_loss: 2.8270 - val_acc: 0.3077\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2067 - acc: 1.0000 - val_loss: 2.8284 - val_acc: 0.3077\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2060 - acc: 1.0000 - val_loss: 2.8298 - val_acc: 0.3077\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2053 - acc: 1.0000 - val_loss: 2.8311 - val_acc: 0.3077\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 1000us/step - loss: 0.2046 - acc: 1.0000 - val_loss: 2.8324 - val_acc: 0.3077\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2039 - acc: 1.0000 - val_loss: 2.8339 - val_acc: 0.3077\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2033 - acc: 1.0000 - val_loss: 2.8352 - val_acc: 0.3077\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 998us/step - loss: 0.2026 - acc: 1.0000 - val_loss: 2.8366 - val_acc: 0.3077\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 998us/step - loss: 0.2019 - acc: 1.0000 - val_loss: 2.8380 - val_acc: 0.3077\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2012 - acc: 1.0000 - val_loss: 2.8394 - val_acc: 0.3077\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2006 - acc: 1.0000 - val_loss: 2.8408 - val_acc: 0.3077\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1999 - acc: 1.0000 - val_loss: 2.8422 - val_acc: 0.3077\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1992 - acc: 1.0000 - val_loss: 2.8436 - val_acc: 0.3077\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1986 - acc: 1.0000 - val_loss: 2.8449 - val_acc: 0.3077\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1979 - acc: 1.0000 - val_loss: 2.8463 - val_acc: 0.3077\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1973 - acc: 1.0000 - val_loss: 2.8477 - val_acc: 0.3077\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1966 - acc: 1.0000 - val_loss: 2.8491 - val_acc: 0.3077\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1960 - acc: 1.0000 - val_loss: 2.8505 - val_acc: 0.3077\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1953 - acc: 1.0000 - val_loss: 2.8519 - val_acc: 0.3077\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1947 - acc: 1.0000 - val_loss: 2.8533 - val_acc: 0.3077\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1941 - acc: 1.0000 - val_loss: 2.8547 - val_acc: 0.3077\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1934 - acc: 1.0000 - val_loss: 2.8561 - val_acc: 0.3077\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1928 - acc: 1.0000 - val_loss: 2.8575 - val_acc: 0.3077\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1922 - acc: 1.0000 - val_loss: 2.8589 - val_acc: 0.3077\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1916 - acc: 1.0000 - val_loss: 2.8603 - val_acc: 0.3077\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1910 - acc: 1.0000 - val_loss: 2.8617 - val_acc: 0.3077\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1904 - acc: 1.0000 - val_loss: 2.8631 - val_acc: 0.3077\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1897 - acc: 1.0000 - val_loss: 2.8645 - val_acc: 0.3077\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1891 - acc: 1.0000 - val_loss: 2.8660 - val_acc: 0.3077\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 994us/step - loss: 0.1885 - acc: 1.0000 - val_loss: 2.8674 - val_acc: 0.3077\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1879 - acc: 1.0000 - val_loss: 2.8688 - val_acc: 0.3077\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.1873 - acc: 1.0000 - val_loss: 2.8702 - val_acc: 0.3077\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 978us/step - loss: 0.1867 - acc: 1.0000 - val_loss: 2.8716 - val_acc: 0.3077\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1862 - acc: 1.0000 - val_loss: 2.8731 - val_acc: 0.3077\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1856 - acc: 1.0000 - val_loss: 2.8745 - val_acc: 0.3077\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1850 - acc: 1.0000 - val_loss: 2.8759 - val_acc: 0.3077\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1844 - acc: 1.0000 - val_loss: 2.8773 - val_acc: 0.3077\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1838 - acc: 1.0000 - val_loss: 2.8788 - val_acc: 0.3077\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1833 - acc: 1.0000 - val_loss: 2.8802 - val_acc: 0.3077\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1827 - acc: 1.0000 - val_loss: 2.8816 - val_acc: 0.3077\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1821 - acc: 1.0000 - val_loss: 2.8830 - val_acc: 0.3077\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1815 - acc: 1.0000 - val_loss: 2.8845 - val_acc: 0.3077\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1810 - acc: 1.0000 - val_loss: 2.8860 - val_acc: 0.3077\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1804 - acc: 1.0000 - val_loss: 2.8874 - val_acc: 0.3077\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1799 - acc: 1.0000 - val_loss: 2.8888 - val_acc: 0.3077\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1793 - acc: 1.0000 - val_loss: 2.8902 - val_acc: 0.3077\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1788 - acc: 1.0000 - val_loss: 2.8917 - val_acc: 0.3077\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1782 - acc: 1.0000 - val_loss: 2.8932 - val_acc: 0.3077\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1777 - acc: 1.0000 - val_loss: 2.8946 - val_acc: 0.3077\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 980us/step - loss: 0.1771 - acc: 1.0000 - val_loss: 2.8960 - val_acc: 0.3077\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1766 - acc: 1.0000 - val_loss: 2.8975 - val_acc: 0.3077\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1761 - acc: 1.0000 - val_loss: 2.8989 - val_acc: 0.3077\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1755 - acc: 1.0000 - val_loss: 2.9003 - val_acc: 0.3077\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1750 - acc: 1.0000 - val_loss: 2.9018 - val_acc: 0.3077\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1745 - acc: 1.0000 - val_loss: 2.9033 - val_acc: 0.3077\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1739 - acc: 1.0000 - val_loss: 2.9047 - val_acc: 0.3077\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.1734 - acc: 1.0000 - val_loss: 2.9061 - val_acc: 0.3077\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1729 - acc: 1.0000 - val_loss: 2.9076 - val_acc: 0.3077\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 958us/step - loss: 0.1724 - acc: 1.0000 - val_loss: 2.9090 - val_acc: 0.3077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f2b950ee-a464-4753-c603-0d28f216275b",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd27d2132b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ac20a0b8-7e10-40f1-f774-4ccf815b4d41",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd27d18a9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZGI+/9HZapcvtYvAQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dbf3be95-3d26-4190-ca35-2cdcb8463887",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=1, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/180\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.9294 - acc: 0.4952\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 921us/step - loss: 0.8590 - acc: 0.4762\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 889us/step - loss: 0.8044 - acc: 0.4762\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.7607 - acc: 0.4952\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.7243 - acc: 0.5143\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 961us/step - loss: 0.6932 - acc: 0.5524\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 952us/step - loss: 0.6658 - acc: 0.6476\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 920us/step - loss: 0.6414 - acc: 0.7524\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 990us/step - loss: 0.6194 - acc: 0.7905\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 951us/step - loss: 0.5993 - acc: 0.8571\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 962us/step - loss: 0.5806 - acc: 0.8667\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.5633 - acc: 0.8667\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.5471 - acc: 0.8667\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.5319 - acc: 0.8667\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.5176 - acc: 0.8667\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 941us/step - loss: 0.5039 - acc: 0.8667\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.4910 - acc: 0.8667\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 885us/step - loss: 0.4786 - acc: 0.8762\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.4669 - acc: 0.8762\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.4556 - acc: 0.8762\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.4448 - acc: 0.8762\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.4343 - acc: 0.8762\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.4242 - acc: 0.8762\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 967us/step - loss: 0.4144 - acc: 0.8762\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.4049 - acc: 0.8857\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 916us/step - loss: 0.3958 - acc: 0.8857\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.3869 - acc: 0.8857\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 965us/step - loss: 0.3782 - acc: 0.8857\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3698 - acc: 0.8857\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.3616 - acc: 0.8857\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 919us/step - loss: 0.3535 - acc: 0.8857\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3457 - acc: 0.8857\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.3381 - acc: 0.8857\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.3306 - acc: 0.8857\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 959us/step - loss: 0.3233 - acc: 0.8857\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3160 - acc: 0.8857\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.3088 - acc: 0.8857\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.3017 - acc: 0.8857\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2948 - acc: 0.8857\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2881 - acc: 0.8857\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 971us/step - loss: 0.2814 - acc: 0.8857\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.2750 - acc: 0.8857\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2687 - acc: 0.8857\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.2625 - acc: 0.8857\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.2566 - acc: 0.8857\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2507 - acc: 0.8857\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2451 - acc: 0.8857\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2396 - acc: 0.8857\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 975us/step - loss: 0.2342 - acc: 0.8857\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2290 - acc: 0.8857\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 992us/step - loss: 0.2239 - acc: 0.8857\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2190 - acc: 0.8857\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 933us/step - loss: 0.2143 - acc: 0.8857\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.2096 - acc: 0.8857\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.2051 - acc: 0.8857\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 943us/step - loss: 0.2008 - acc: 0.8857\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.1965 - acc: 0.8857\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1925 - acc: 0.8857\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 929us/step - loss: 0.1885 - acc: 0.8857\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 904us/step - loss: 0.1846 - acc: 0.8857\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 925us/step - loss: 0.1808 - acc: 0.8857\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1772 - acc: 0.8857\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 937us/step - loss: 0.1737 - acc: 0.8857\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 980us/step - loss: 0.1703 - acc: 0.8857\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1669 - acc: 0.8857\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.1637 - acc: 0.8857\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 971us/step - loss: 0.1606 - acc: 0.8857\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.1575 - acc: 0.8857\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1546 - acc: 0.8857\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 975us/step - loss: 0.1517 - acc: 0.8857\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 957us/step - loss: 0.1489 - acc: 0.8857\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 981us/step - loss: 0.1462 - acc: 0.8857\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1436 - acc: 0.8857\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1410 - acc: 0.8857\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.1385 - acc: 0.8857\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.1361 - acc: 0.8857\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 970us/step - loss: 0.1338 - acc: 0.8857\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 958us/step - loss: 0.1314 - acc: 0.8857\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.1292 - acc: 0.8857\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 997us/step - loss: 0.1270 - acc: 0.8857\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 957us/step - loss: 0.1249 - acc: 0.9429\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1228 - acc: 1.0000\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1208 - acc: 1.0000\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.1189 - acc: 1.0000\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.1170 - acc: 1.0000\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1151 - acc: 1.0000\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 970us/step - loss: 0.1133 - acc: 1.0000\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.1115 - acc: 1.0000\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 930us/step - loss: 0.1098 - acc: 1.0000\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 994us/step - loss: 0.1081 - acc: 1.0000\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1064 - acc: 1.0000\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 944us/step - loss: 0.1048 - acc: 1.0000\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.1032 - acc: 1.0000\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1017 - acc: 1.0000\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 945us/step - loss: 0.1002 - acc: 1.0000\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.0987 - acc: 1.0000\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 977us/step - loss: 0.0973 - acc: 1.0000\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 986us/step - loss: 0.0959 - acc: 1.0000\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 961us/step - loss: 0.0945 - acc: 1.0000\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 999us/step - loss: 0.0932 - acc: 1.0000\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0919 - acc: 1.0000\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0906 - acc: 1.0000\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 882us/step - loss: 0.0893 - acc: 1.0000\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 945us/step - loss: 0.0881 - acc: 1.0000\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0869 - acc: 1.0000\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0858 - acc: 1.0000\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.0846 - acc: 1.0000\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0835 - acc: 1.0000\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 935us/step - loss: 0.0824 - acc: 1.0000\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 887us/step - loss: 0.0813 - acc: 1.0000\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.0802 - acc: 1.0000\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0792 - acc: 1.0000\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.0782 - acc: 1.0000\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.0772 - acc: 1.0000\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.0762 - acc: 1.0000\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0753 - acc: 1.0000\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 989us/step - loss: 0.0744 - acc: 1.0000\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.0734 - acc: 1.0000\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.0726 - acc: 1.0000\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 991us/step - loss: 0.0717 - acc: 1.0000\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0708 - acc: 1.0000\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 979us/step - loss: 0.0700 - acc: 1.0000\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 964us/step - loss: 0.0691 - acc: 1.0000\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.0683 - acc: 1.0000\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 988us/step - loss: 0.0675 - acc: 1.0000\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 928us/step - loss: 0.0668 - acc: 1.0000\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.0660 - acc: 1.0000\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0652 - acc: 1.0000\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 961us/step - loss: 0.0645 - acc: 1.0000\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 936us/step - loss: 0.0638 - acc: 1.0000\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 936us/step - loss: 0.0631 - acc: 1.0000\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0624 - acc: 1.0000\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 905us/step - loss: 0.0617 - acc: 1.0000\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 941us/step - loss: 0.0610 - acc: 1.0000\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.0603 - acc: 1.0000\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.0597 - acc: 1.0000\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 960us/step - loss: 0.0591 - acc: 1.0000\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.0584 - acc: 1.0000\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 981us/step - loss: 0.0578 - acc: 1.0000\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.0572 - acc: 1.0000\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 951us/step - loss: 0.0566 - acc: 1.0000\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 912us/step - loss: 0.0560 - acc: 1.0000\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.0555 - acc: 1.0000\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0549 - acc: 1.0000\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0543 - acc: 1.0000\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0538 - acc: 1.0000\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0533 - acc: 1.0000\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 905us/step - loss: 0.0527 - acc: 1.0000\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 931us/step - loss: 0.0522 - acc: 1.0000\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 974us/step - loss: 0.0517 - acc: 1.0000\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 945us/step - loss: 0.0512 - acc: 1.0000\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.0507 - acc: 1.0000\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 940us/step - loss: 0.0502 - acc: 1.0000\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.0498 - acc: 1.0000\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.0493 - acc: 1.0000\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.0488 - acc: 1.0000\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.0484 - acc: 1.0000\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.0479 - acc: 1.0000\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 956us/step - loss: 0.0475 - acc: 1.0000\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.0471 - acc: 1.0000\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 971us/step - loss: 0.0466 - acc: 1.0000\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 976us/step - loss: 0.0462 - acc: 1.0000\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 950us/step - loss: 0.0458 - acc: 1.0000\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0454 - acc: 1.0000\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0450 - acc: 1.0000\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 921us/step - loss: 0.0446 - acc: 1.0000\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.0442 - acc: 1.0000\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 900us/step - loss: 0.0438 - acc: 1.0000\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 968us/step - loss: 0.0434 - acc: 1.0000\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 978us/step - loss: 0.0431 - acc: 1.0000\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 943us/step - loss: 0.0427 - acc: 1.0000\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 984us/step - loss: 0.0424 - acc: 1.0000\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.0420 - acc: 1.0000\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 967us/step - loss: 0.0416 - acc: 1.0000\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 930us/step - loss: 0.0413 - acc: 1.0000\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 956us/step - loss: 0.0410 - acc: 1.0000\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.0406 - acc: 1.0000\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 968us/step - loss: 0.0403 - acc: 1.0000\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 822us/step - loss: 0.0400 - acc: 1.0000\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 869us/step - loss: 0.0396 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 4ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8ce4a441-e681-4e80-c687-ab180852425a",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b4a4f604-cf03-4f29-a20e-a8ff2d90b883",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_NO_kfold.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOzQtGNku9rx43TiivW/OZk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Classification_NO_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i4O_AtKVzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb00g1tWTqw",
        "colab_type": "code",
        "outputId": "95f6d6ad-7f31-461c-b977-5a230843aac7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        }
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "55f13ec2-8156-4902-a539-4230ddd8b681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "17951ad9-3f91-41a0-8e44-09b35270d0d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, y_train).transform(train_data_stand)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "e33278f6-8bcd-4840-ea80-cb05f024294c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0UwBbCjf13g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = lda.transform(val_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5fsvoQwBqR",
        "colab_type": "text"
      },
      "source": [
        "#Z-score dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYbsLEB_6nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_lda = scaler_2.fit_transform(train_data_stand_lda)\n",
        "val_data_stand_lda = scaler_2.transform(val_data_stand_lda)\n",
        "test_data_stand_lda = scaler_2.transform(test_data_stand_lda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8_CwJZwBD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KW2c_RIXpWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = val_data_stand_lda - mean\n",
        "val_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3wGNwiWXvbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLYdHX-mYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(8, activation='relu', input_shape=(2,), kernel_regularizer=regularizers.l2(l=0.1)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAd2LU_dEO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_sSb3CnLM2",
        "colab_type": "code",
        "outputId": "8934bbb8-c7cf-43d1-e221-ccc94aa26fb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "one_hot_val_labels.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hc4rG203_u",
        "colab_type": "code",
        "outputId": "15a012a5-04de-450f-e161-237e31fbed43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 180\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=10, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/180\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 1.3480 - acc: 0.8667 - val_loss: 1.6233 - val_acc: 0.3077\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 137us/step - loss: 1.3283 - acc: 0.8857 - val_loss: 1.5953 - val_acc: 0.3077\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 119us/step - loss: 1.3086 - acc: 0.8857 - val_loss: 1.5706 - val_acc: 0.3077\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 147us/step - loss: 1.2896 - acc: 0.8857 - val_loss: 1.5472 - val_acc: 0.3077\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 173us/step - loss: 1.2717 - acc: 0.8857 - val_loss: 1.5253 - val_acc: 0.3077\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 158us/step - loss: 1.2536 - acc: 0.8857 - val_loss: 1.5066 - val_acc: 0.3077\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 133us/step - loss: 1.2362 - acc: 0.8857 - val_loss: 1.4893 - val_acc: 0.3077\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 1.2194 - acc: 0.8857 - val_loss: 1.4737 - val_acc: 0.3077\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 1.2030 - acc: 0.8857 - val_loss: 1.4594 - val_acc: 0.3077\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 142us/step - loss: 1.1869 - acc: 0.8857 - val_loss: 1.4461 - val_acc: 0.4615\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 124us/step - loss: 1.1711 - acc: 0.8857 - val_loss: 1.4341 - val_acc: 0.5385\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 1.1561 - acc: 0.8857 - val_loss: 1.4239 - val_acc: 0.4615\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 133us/step - loss: 1.1415 - acc: 0.8857 - val_loss: 1.4145 - val_acc: 0.4615\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 128us/step - loss: 1.1271 - acc: 0.8857 - val_loss: 1.4065 - val_acc: 0.4615\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 133us/step - loss: 1.1133 - acc: 0.8857 - val_loss: 1.3992 - val_acc: 0.4615\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 141us/step - loss: 1.0997 - acc: 0.8857 - val_loss: 1.3935 - val_acc: 0.4615\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 1.0864 - acc: 0.8857 - val_loss: 1.3883 - val_acc: 0.4615\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 128us/step - loss: 1.0734 - acc: 0.8857 - val_loss: 1.3840 - val_acc: 0.4615\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 149us/step - loss: 1.0608 - acc: 0.8857 - val_loss: 1.3808 - val_acc: 0.4615\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 147us/step - loss: 1.0485 - acc: 0.8857 - val_loss: 1.3783 - val_acc: 0.5385\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 150us/step - loss: 1.0365 - acc: 0.8857 - val_loss: 1.3768 - val_acc: 0.5385\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 140us/step - loss: 1.0249 - acc: 0.8857 - val_loss: 1.3759 - val_acc: 0.5385\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 1.0133 - acc: 0.8857 - val_loss: 1.3757 - val_acc: 0.5385\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 120us/step - loss: 1.0022 - acc: 0.8857 - val_loss: 1.3760 - val_acc: 0.5385\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.9914 - acc: 0.8857 - val_loss: 1.3770 - val_acc: 0.5385\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.9807 - acc: 0.8857 - val_loss: 1.3786 - val_acc: 0.5385\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.9703 - acc: 0.8857 - val_loss: 1.3804 - val_acc: 0.5385\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.9602 - acc: 0.8857 - val_loss: 1.3826 - val_acc: 0.5385\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.9501 - acc: 0.8857 - val_loss: 1.3851 - val_acc: 0.5385\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.9404 - acc: 0.8857 - val_loss: 1.3881 - val_acc: 0.5385\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.9307 - acc: 0.8857 - val_loss: 1.3911 - val_acc: 0.5385\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.9213 - acc: 0.8857 - val_loss: 1.3947 - val_acc: 0.5385\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.9121 - acc: 0.8857 - val_loss: 1.3987 - val_acc: 0.5385\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.9059 - acc: 0.8857 - val_loss: 1.3997 - val_acc: 0.6154\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.9049 - acc: 0.8857 - val_loss: 1.4001 - val_acc: 0.6154\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.9040 - acc: 0.8857 - val_loss: 1.4005 - val_acc: 0.6154\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.9031 - acc: 0.8857 - val_loss: 1.4010 - val_acc: 0.6154\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.9022 - acc: 0.8857 - val_loss: 1.4014 - val_acc: 0.6154\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.9013 - acc: 0.8857 - val_loss: 1.4018 - val_acc: 0.6154\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.9004 - acc: 0.8857 - val_loss: 1.4022 - val_acc: 0.6154\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.8996 - acc: 0.8857 - val_loss: 1.4027 - val_acc: 0.6154\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.8987 - acc: 0.8857 - val_loss: 1.4031 - val_acc: 0.6154\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.8978 - acc: 0.8857 - val_loss: 1.4035 - val_acc: 0.6154\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.8969 - acc: 0.8857 - val_loss: 1.4040 - val_acc: 0.6154\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.8960 - acc: 0.8857 - val_loss: 1.4044 - val_acc: 0.6154\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.8952 - acc: 0.8857 - val_loss: 1.4048 - val_acc: 0.6154\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8943 - acc: 0.8857 - val_loss: 1.4053 - val_acc: 0.6154\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.8935 - acc: 0.8857 - val_loss: 1.4057 - val_acc: 0.6154\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.8926 - acc: 0.8857 - val_loss: 1.4062 - val_acc: 0.6154\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.8918 - acc: 0.8857 - val_loss: 1.4067 - val_acc: 0.6154\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.8909 - acc: 0.8857 - val_loss: 1.4071 - val_acc: 0.6154\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.8901 - acc: 0.8857 - val_loss: 1.4076 - val_acc: 0.6154\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.8892 - acc: 0.8857 - val_loss: 1.4081 - val_acc: 0.6154\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8884 - acc: 0.8857 - val_loss: 1.4085 - val_acc: 0.6154\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.8875 - acc: 0.8857 - val_loss: 1.4090 - val_acc: 0.6154\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.8867 - acc: 0.8857 - val_loss: 1.4094 - val_acc: 0.6154\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.8859 - acc: 0.8857 - val_loss: 1.4099 - val_acc: 0.6154\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.8850 - acc: 0.8857 - val_loss: 1.4104 - val_acc: 0.6154\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.8842 - acc: 0.8857 - val_loss: 1.4108 - val_acc: 0.6154\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.8834 - acc: 0.8857 - val_loss: 1.4113 - val_acc: 0.6154\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.8826 - acc: 0.8857 - val_loss: 1.4118 - val_acc: 0.6154\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 124us/step - loss: 0.8817 - acc: 0.8857 - val_loss: 1.4122 - val_acc: 0.6154\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8809 - acc: 0.8857 - val_loss: 1.4128 - val_acc: 0.6154\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.8801 - acc: 0.8857 - val_loss: 1.4132 - val_acc: 0.6154\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.8793 - acc: 0.8857 - val_loss: 1.4138 - val_acc: 0.6154\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8785 - acc: 0.8857 - val_loss: 1.4143 - val_acc: 0.6154\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.8777 - acc: 0.8857 - val_loss: 1.4148 - val_acc: 0.6154\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.8769 - acc: 0.8857 - val_loss: 1.4153 - val_acc: 0.6154\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.8761 - acc: 0.8857 - val_loss: 1.4158 - val_acc: 0.6154\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.8753 - acc: 0.8857 - val_loss: 1.4163 - val_acc: 0.6154\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.8745 - acc: 0.8857 - val_loss: 1.4168 - val_acc: 0.6154\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.8737 - acc: 0.8857 - val_loss: 1.4174 - val_acc: 0.5385\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8729 - acc: 0.8857 - val_loss: 1.4179 - val_acc: 0.5385\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8721 - acc: 0.8857 - val_loss: 1.4184 - val_acc: 0.5385\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.8713 - acc: 0.8857 - val_loss: 1.4189 - val_acc: 0.5385\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 142us/step - loss: 0.8705 - acc: 0.8857 - val_loss: 1.4194 - val_acc: 0.5385\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.8697 - acc: 0.8857 - val_loss: 1.4200 - val_acc: 0.5385\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.8690 - acc: 0.8857 - val_loss: 1.4205 - val_acc: 0.5385\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.8682 - acc: 0.8857 - val_loss: 1.4210 - val_acc: 0.5385\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.8674 - acc: 0.8857 - val_loss: 1.4215 - val_acc: 0.5385\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8666 - acc: 0.8857 - val_loss: 1.4220 - val_acc: 0.5385\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.8658 - acc: 0.8857 - val_loss: 1.4226 - val_acc: 0.5385\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 132us/step - loss: 0.8650 - acc: 0.8857 - val_loss: 1.4231 - val_acc: 0.5385\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 124us/step - loss: 0.8643 - acc: 0.8857 - val_loss: 1.4236 - val_acc: 0.5385\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.8635 - acc: 0.8857 - val_loss: 1.4242 - val_acc: 0.5385\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.8627 - acc: 0.8857 - val_loss: 1.4247 - val_acc: 0.5385\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 129us/step - loss: 0.8619 - acc: 0.8857 - val_loss: 1.4252 - val_acc: 0.5385\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 124us/step - loss: 0.8611 - acc: 0.8857 - val_loss: 1.4258 - val_acc: 0.5385\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8604 - acc: 0.8857 - val_loss: 1.4263 - val_acc: 0.5385\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.8596 - acc: 0.8857 - val_loss: 1.4269 - val_acc: 0.5385\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.8588 - acc: 0.8857 - val_loss: 1.4274 - val_acc: 0.5385\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.8581 - acc: 0.8857 - val_loss: 1.4279 - val_acc: 0.5385\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.8573 - acc: 0.8857 - val_loss: 1.4285 - val_acc: 0.5385\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.8565 - acc: 0.8857 - val_loss: 1.4290 - val_acc: 0.5385\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8558 - acc: 0.8857 - val_loss: 1.4296 - val_acc: 0.5385\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.8550 - acc: 0.8857 - val_loss: 1.4302 - val_acc: 0.5385\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.8542 - acc: 0.8857 - val_loss: 1.4307 - val_acc: 0.5385\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8535 - acc: 0.8857 - val_loss: 1.4313 - val_acc: 0.5385\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 133us/step - loss: 0.8527 - acc: 0.8857 - val_loss: 1.4318 - val_acc: 0.5385\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.8520 - acc: 0.8857 - val_loss: 1.4324 - val_acc: 0.5385\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.8512 - acc: 0.8857 - val_loss: 1.4329 - val_acc: 0.5385\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8505 - acc: 0.8857 - val_loss: 1.4335 - val_acc: 0.5385\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.8497 - acc: 0.8857 - val_loss: 1.4341 - val_acc: 0.5385\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.8490 - acc: 0.8857 - val_loss: 1.4347 - val_acc: 0.5385\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8482 - acc: 0.8857 - val_loss: 1.4352 - val_acc: 0.5385\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8475 - acc: 0.8857 - val_loss: 1.4358 - val_acc: 0.5385\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 119us/step - loss: 0.8467 - acc: 0.8857 - val_loss: 1.4363 - val_acc: 0.5385\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8460 - acc: 0.8857 - val_loss: 1.4369 - val_acc: 0.5385\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.8453 - acc: 0.8857 - val_loss: 1.4375 - val_acc: 0.5385\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.8445 - acc: 0.8857 - val_loss: 1.4381 - val_acc: 0.5385\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8438 - acc: 0.8857 - val_loss: 1.4387 - val_acc: 0.5385\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.8431 - acc: 0.8857 - val_loss: 1.4393 - val_acc: 0.5385\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.8423 - acc: 0.8857 - val_loss: 1.4399 - val_acc: 0.5385\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.8416 - acc: 0.8857 - val_loss: 1.4405 - val_acc: 0.5385\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.8409 - acc: 0.8857 - val_loss: 1.4411 - val_acc: 0.5385\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.8401 - acc: 0.8857 - val_loss: 1.4417 - val_acc: 0.5385\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.8394 - acc: 0.8857 - val_loss: 1.4423 - val_acc: 0.5385\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.8387 - acc: 0.8857 - val_loss: 1.4429 - val_acc: 0.5385\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 120us/step - loss: 0.8379 - acc: 0.8857 - val_loss: 1.4435 - val_acc: 0.5385\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 118us/step - loss: 0.8372 - acc: 0.8857 - val_loss: 1.4441 - val_acc: 0.5385\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.8365 - acc: 0.8857 - val_loss: 1.4447 - val_acc: 0.5385\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 125us/step - loss: 0.8358 - acc: 0.8857 - val_loss: 1.4453 - val_acc: 0.5385\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.8351 - acc: 0.8857 - val_loss: 1.4458 - val_acc: 0.5385\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 110us/step - loss: 0.8343 - acc: 0.8857 - val_loss: 1.4464 - val_acc: 0.5385\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8336 - acc: 0.8857 - val_loss: 1.4470 - val_acc: 0.5385\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 122us/step - loss: 0.8329 - acc: 0.8857 - val_loss: 1.4476 - val_acc: 0.5385\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.8322 - acc: 0.8857 - val_loss: 1.4483 - val_acc: 0.5385\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 123us/step - loss: 0.8315 - acc: 0.8857 - val_loss: 1.4489 - val_acc: 0.5385\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.8308 - acc: 0.8857 - val_loss: 1.4495 - val_acc: 0.5385\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.8301 - acc: 0.8857 - val_loss: 1.4501 - val_acc: 0.5385\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.8294 - acc: 0.8857 - val_loss: 1.4507 - val_acc: 0.5385\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8287 - acc: 0.8857 - val_loss: 1.4513 - val_acc: 0.5385\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.8280 - acc: 0.8857 - val_loss: 1.4519 - val_acc: 0.5385\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.8273 - acc: 0.8857 - val_loss: 1.4525 - val_acc: 0.5385\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.8266 - acc: 0.8857 - val_loss: 1.4531 - val_acc: 0.5385\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.8259 - acc: 0.8857 - val_loss: 1.4537 - val_acc: 0.5385\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.8252 - acc: 0.8857 - val_loss: 1.4544 - val_acc: 0.5385\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.8245 - acc: 0.8857 - val_loss: 1.4550 - val_acc: 0.5385\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 146us/step - loss: 0.8238 - acc: 0.8857 - val_loss: 1.4556 - val_acc: 0.5385\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.8231 - acc: 0.8857 - val_loss: 1.4562 - val_acc: 0.5385\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.8224 - acc: 0.8857 - val_loss: 1.4568 - val_acc: 0.5385\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 122us/step - loss: 0.8217 - acc: 0.8857 - val_loss: 1.4575 - val_acc: 0.5385\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.8211 - acc: 0.8857 - val_loss: 1.4581 - val_acc: 0.5385\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 125us/step - loss: 0.8204 - acc: 0.8857 - val_loss: 1.4587 - val_acc: 0.5385\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.8197 - acc: 0.8857 - val_loss: 1.4594 - val_acc: 0.5385\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.8190 - acc: 0.8857 - val_loss: 1.4600 - val_acc: 0.5385\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.8183 - acc: 0.8857 - val_loss: 1.4607 - val_acc: 0.5385\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 126us/step - loss: 0.8177 - acc: 0.8857 - val_loss: 1.4613 - val_acc: 0.5385\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.8170 - acc: 0.8857 - val_loss: 1.4620 - val_acc: 0.5385\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.8163 - acc: 0.8857 - val_loss: 1.4626 - val_acc: 0.5385\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8156 - acc: 0.8857 - val_loss: 1.4632 - val_acc: 0.5385\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 119us/step - loss: 0.8150 - acc: 0.8857 - val_loss: 1.4639 - val_acc: 0.5385\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 144us/step - loss: 0.8143 - acc: 0.8857 - val_loss: 1.4645 - val_acc: 0.5385\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 138us/step - loss: 0.8136 - acc: 0.8857 - val_loss: 1.4652 - val_acc: 0.5385\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 139us/step - loss: 0.8130 - acc: 0.8857 - val_loss: 1.4658 - val_acc: 0.5385\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.8123 - acc: 0.8857 - val_loss: 1.4665 - val_acc: 0.5385\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.8116 - acc: 0.8857 - val_loss: 1.4671 - val_acc: 0.5385\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 134us/step - loss: 0.8110 - acc: 0.8857 - val_loss: 1.4678 - val_acc: 0.5385\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.8103 - acc: 0.8857 - val_loss: 1.4684 - val_acc: 0.5385\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.8096 - acc: 0.8857 - val_loss: 1.4691 - val_acc: 0.5385\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.8090 - acc: 0.8857 - val_loss: 1.4697 - val_acc: 0.5385\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 135us/step - loss: 0.8083 - acc: 0.8857 - val_loss: 1.4704 - val_acc: 0.5385\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 143us/step - loss: 0.8076 - acc: 0.8857 - val_loss: 1.4710 - val_acc: 0.5385\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8070 - acc: 0.8857 - val_loss: 1.4717 - val_acc: 0.5385\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.8063 - acc: 0.8857 - val_loss: 1.4724 - val_acc: 0.5385\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 122us/step - loss: 0.8057 - acc: 0.8857 - val_loss: 1.4730 - val_acc: 0.5385\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 128us/step - loss: 0.8050 - acc: 0.8857 - val_loss: 1.4737 - val_acc: 0.5385\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 137us/step - loss: 0.8043 - acc: 0.8857 - val_loss: 1.4744 - val_acc: 0.5385\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 121us/step - loss: 0.8037 - acc: 0.8857 - val_loss: 1.4750 - val_acc: 0.5385\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 125us/step - loss: 0.8030 - acc: 0.8857 - val_loss: 1.4757 - val_acc: 0.5385\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 130us/step - loss: 0.8024 - acc: 0.8857 - val_loss: 1.4763 - val_acc: 0.5385\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 125us/step - loss: 0.8017 - acc: 0.8857 - val_loss: 1.4770 - val_acc: 0.5385\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 127us/step - loss: 0.8011 - acc: 0.8857 - val_loss: 1.4777 - val_acc: 0.5385\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.8004 - acc: 0.8857 - val_loss: 1.4784 - val_acc: 0.5385\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.7998 - acc: 0.8857 - val_loss: 1.4790 - val_acc: 0.5385\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.7991 - acc: 0.8857 - val_loss: 1.4797 - val_acc: 0.5385\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.7985 - acc: 0.8857 - val_loss: 1.4804 - val_acc: 0.5385\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 140us/step - loss: 0.7978 - acc: 0.8857 - val_loss: 1.4811 - val_acc: 0.5385\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.7972 - acc: 0.8857 - val_loss: 1.4817 - val_acc: 0.5385\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 129us/step - loss: 0.7966 - acc: 0.8857 - val_loss: 1.4824 - val_acc: 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ce0c4169-ec20-4e4d-b3e8-e8ccfbaafc1b",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff4c426bb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np0QgIlLglAhERAqcEoGISIH7///PM+uSwvaaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0bc96451-b2cb-4f7f-daf6-0efea7891062",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff4c41eff60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c8a40eee-cdb9-47ae-c1af-99ea63785ce4",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=1, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/180\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 1.1976 - acc: 0.4476\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 895us/step - loss: 1.0908 - acc: 0.5810\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 1.0153 - acc: 0.5810\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 917us/step - loss: 0.9541 - acc: 0.5905\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 787us/step - loss: 0.9022 - acc: 0.7714\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 799us/step - loss: 0.8565 - acc: 0.9810\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.8157 - acc: 0.9905\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 852us/step - loss: 0.7787 - acc: 0.9905\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 817us/step - loss: 0.7448 - acc: 1.0000\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 809us/step - loss: 0.7135 - acc: 1.0000\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 803us/step - loss: 0.6844 - acc: 1.0000\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 822us/step - loss: 0.6575 - acc: 1.0000\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 841us/step - loss: 0.6323 - acc: 1.0000\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 842us/step - loss: 0.6090 - acc: 1.0000\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 806us/step - loss: 0.5872 - acc: 1.0000\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 832us/step - loss: 0.5666 - acc: 1.0000\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 807us/step - loss: 0.5471 - acc: 1.0000\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 901us/step - loss: 0.5287 - acc: 1.0000\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 870us/step - loss: 0.5111 - acc: 1.0000\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 917us/step - loss: 0.4944 - acc: 1.0000\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 909us/step - loss: 0.4783 - acc: 1.0000\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 899us/step - loss: 0.4628 - acc: 1.0000\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 913us/step - loss: 0.4479 - acc: 1.0000\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.4337 - acc: 1.0000\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 871us/step - loss: 0.4205 - acc: 1.0000\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 994us/step - loss: 0.4080 - acc: 1.0000\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 876us/step - loss: 0.3960 - acc: 1.0000\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 908us/step - loss: 0.3848 - acc: 1.0000\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 864us/step - loss: 0.3742 - acc: 1.0000\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 813us/step - loss: 0.3641 - acc: 1.0000\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 844us/step - loss: 0.3544 - acc: 1.0000\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 824us/step - loss: 0.3453 - acc: 1.0000\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 812us/step - loss: 0.3367 - acc: 1.0000\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 842us/step - loss: 0.3285 - acc: 1.0000\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 828us/step - loss: 0.3208 - acc: 1.0000\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.3136 - acc: 1.0000\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.3070 - acc: 1.0000\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 816us/step - loss: 0.3007 - acc: 1.0000\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 853us/step - loss: 0.2948 - acc: 1.0000\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 825us/step - loss: 0.2894 - acc: 1.0000\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.2842 - acc: 1.0000\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.2793 - acc: 1.0000\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 818us/step - loss: 0.2748 - acc: 1.0000\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.2704 - acc: 1.0000\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 828us/step - loss: 0.2663 - acc: 1.0000\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 808us/step - loss: 0.2625 - acc: 1.0000\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 908us/step - loss: 0.2587 - acc: 1.0000\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 846us/step - loss: 0.2552 - acc: 1.0000\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 826us/step - loss: 0.2519 - acc: 1.0000\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 889us/step - loss: 0.2488 - acc: 1.0000\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 838us/step - loss: 0.2458 - acc: 1.0000\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 874us/step - loss: 0.2428 - acc: 1.0000\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 858us/step - loss: 0.2401 - acc: 1.0000\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 887us/step - loss: 0.2374 - acc: 1.0000\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 867us/step - loss: 0.2349 - acc: 1.0000\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 852us/step - loss: 0.2325 - acc: 1.0000\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.2302 - acc: 1.0000\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 824us/step - loss: 0.2279 - acc: 1.0000\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 916us/step - loss: 0.2257 - acc: 1.0000\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 899us/step - loss: 0.2237 - acc: 1.0000\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 886us/step - loss: 0.2216 - acc: 1.0000\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 876us/step - loss: 0.2197 - acc: 1.0000\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 862us/step - loss: 0.2178 - acc: 1.0000\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 902us/step - loss: 0.2159 - acc: 1.0000\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.2142 - acc: 1.0000\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 977us/step - loss: 0.2125 - acc: 1.0000\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 860us/step - loss: 0.2108 - acc: 1.0000\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 952us/step - loss: 0.2092 - acc: 1.0000\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 935us/step - loss: 0.2076 - acc: 1.0000\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 833us/step - loss: 0.2061 - acc: 1.0000\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 878us/step - loss: 0.2046 - acc: 1.0000\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 876us/step - loss: 0.2032 - acc: 1.0000\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 838us/step - loss: 0.2018 - acc: 1.0000\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 813us/step - loss: 0.2005 - acc: 1.0000\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 827us/step - loss: 0.1992 - acc: 1.0000\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.1978 - acc: 1.0000\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 842us/step - loss: 0.1966 - acc: 1.0000\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.1953 - acc: 1.0000\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 802us/step - loss: 0.1942 - acc: 1.0000\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 858us/step - loss: 0.1930 - acc: 1.0000\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 805us/step - loss: 0.1919 - acc: 1.0000\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 781us/step - loss: 0.1907 - acc: 1.0000\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.1896 - acc: 1.0000\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 778us/step - loss: 0.1885 - acc: 1.0000\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 837us/step - loss: 0.1874 - acc: 1.0000\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 808us/step - loss: 0.1864 - acc: 1.0000\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 798us/step - loss: 0.1854 - acc: 1.0000\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 810us/step - loss: 0.1844 - acc: 1.0000\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 804us/step - loss: 0.1834 - acc: 1.0000\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 802us/step - loss: 0.1825 - acc: 1.0000\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 827us/step - loss: 0.1816 - acc: 1.0000\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 937us/step - loss: 0.1806 - acc: 1.0000\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 952us/step - loss: 0.1797 - acc: 1.0000\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.1788 - acc: 1.0000\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 845us/step - loss: 0.1780 - acc: 1.0000\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 944us/step - loss: 0.1771 - acc: 1.0000\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 800us/step - loss: 0.1763 - acc: 1.0000\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 866us/step - loss: 0.1755 - acc: 1.0000\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 863us/step - loss: 0.1747 - acc: 1.0000\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 819us/step - loss: 0.1739 - acc: 1.0000\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 848us/step - loss: 0.1731 - acc: 1.0000\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.1723 - acc: 1.0000\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.1716 - acc: 1.0000\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 837us/step - loss: 0.1708 - acc: 1.0000\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 833us/step - loss: 0.1701 - acc: 1.0000\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 893us/step - loss: 0.1694 - acc: 1.0000\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 856us/step - loss: 0.1687 - acc: 1.0000\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.1680 - acc: 1.0000\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 881us/step - loss: 0.1673 - acc: 1.0000\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 827us/step - loss: 0.1666 - acc: 1.0000\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 904us/step - loss: 0.1659 - acc: 1.0000\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 848us/step - loss: 0.1653 - acc: 1.0000\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 847us/step - loss: 0.1646 - acc: 1.0000\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 877us/step - loss: 0.1640 - acc: 1.0000\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 864us/step - loss: 0.1634 - acc: 1.0000\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.1627 - acc: 1.0000\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.1621 - acc: 1.0000\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.1615 - acc: 1.0000\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.1609 - acc: 1.0000\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.1603 - acc: 1.0000\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 895us/step - loss: 0.1597 - acc: 1.0000\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.1592 - acc: 1.0000\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 834us/step - loss: 0.1586 - acc: 1.0000\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 838us/step - loss: 0.1581 - acc: 1.0000\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 868us/step - loss: 0.1575 - acc: 1.0000\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 869us/step - loss: 0.1569 - acc: 1.0000\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 861us/step - loss: 0.1564 - acc: 1.0000\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 966us/step - loss: 0.1559 - acc: 1.0000\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.1553 - acc: 1.0000\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.1548 - acc: 1.0000\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 890us/step - loss: 0.1543 - acc: 1.0000\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.1538 - acc: 1.0000\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 909us/step - loss: 0.1533 - acc: 1.0000\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 907us/step - loss: 0.1528 - acc: 1.0000\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.1523 - acc: 1.0000\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 902us/step - loss: 0.1518 - acc: 1.0000\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 918us/step - loss: 0.1513 - acc: 1.0000\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 924us/step - loss: 0.1509 - acc: 1.0000\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 852us/step - loss: 0.1504 - acc: 1.0000\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 858us/step - loss: 0.1499 - acc: 1.0000\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 896us/step - loss: 0.1495 - acc: 1.0000\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 853us/step - loss: 0.1490 - acc: 1.0000\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 930us/step - loss: 0.1485 - acc: 1.0000\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 872us/step - loss: 0.1481 - acc: 1.0000\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.1476 - acc: 1.0000\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 868us/step - loss: 0.1472 - acc: 1.0000\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.1468 - acc: 1.0000\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 903us/step - loss: 0.1463 - acc: 1.0000\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 874us/step - loss: 0.1459 - acc: 1.0000\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 931us/step - loss: 0.1455 - acc: 1.0000\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 821us/step - loss: 0.1451 - acc: 1.0000\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 867us/step - loss: 0.1447 - acc: 1.0000\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 867us/step - loss: 0.1443 - acc: 1.0000\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 865us/step - loss: 0.1439 - acc: 1.0000\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 901us/step - loss: 0.1435 - acc: 1.0000\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 905us/step - loss: 0.1431 - acc: 1.0000\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 903us/step - loss: 0.1427 - acc: 1.0000\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 933us/step - loss: 0.1423 - acc: 1.0000\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.1419 - acc: 1.0000\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1415 - acc: 1.0000\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 861us/step - loss: 0.1412 - acc: 1.0000\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 937us/step - loss: 0.1408 - acc: 1.0000\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.1405 - acc: 1.0000\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 918us/step - loss: 0.1401 - acc: 1.0000\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 987us/step - loss: 0.1397 - acc: 1.0000\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.1393 - acc: 1.0000\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1390 - acc: 1.0000\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 927us/step - loss: 0.1386 - acc: 1.0000\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1383 - acc: 1.0000\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 904us/step - loss: 0.1379 - acc: 1.0000\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 927us/step - loss: 0.1376 - acc: 1.0000\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 992us/step - loss: 0.1372 - acc: 1.0000\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.1369 - acc: 1.0000\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.1366 - acc: 1.0000\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.1363 - acc: 1.0000\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1359 - acc: 1.0000\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.1356 - acc: 1.0000\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.1352 - acc: 1.0000\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.1349 - acc: 1.0000\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1346 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1e0fd245-3e13-4025-f307-48f228c13a5a",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "515455ab-de01-4627-c667-5239e9544c4d",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_NO_kfold.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPwlqBGXh+rDECAtL+Y/8ye",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Classification_NO_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i4O_AtKVzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb00g1tWTqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "c86d3170-1cad-459c-941c-476d38e2763d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "#LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "e65557b5-1393-4dd9-f14a-87df3375e607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, y_train).transform(train_data_stand)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "236376ff-ef8e-4862-b64f-e5774d61632d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0UwBbCjf13g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = lda.transform(val_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5fsvoQwBqR",
        "colab_type": "text"
      },
      "source": [
        "##Z-score dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYbsLEB_6nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_lda = scaler_2.fit_transform(train_data_stand_lda)\n",
        "val_data_stand_lda = scaler_2.transform(val_data_stand_lda)\n",
        "test_data_stand_lda = scaler_2.transform(test_data_stand_lda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLYdHX-mYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2EW9Sb0FDCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "#def build_model():\n",
        "#  model = keras.models.Sequential()\n",
        "#  model.add(layers.Dense(8, activation='relu', input_shape=(2,), kernel_regularizer=regularizers.l2(l=0.1)))\n",
        "#  model.add(layers.Dense(7, activation='relu'))\n",
        "#  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "#  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "#  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "#  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "#  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "#  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAd2LU_dEO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.callbacks import ReduceLROnPlateau\n",
        "#red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRXO8cdfbUO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_sSb3CnLM2",
        "colab_type": "code",
        "outputId": "4d8a0b41-ba28-4487-e344-8838045a7445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_val_labels.shape"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hc4rG203_u",
        "colab_type": "code",
        "outputId": "be7d040a-c044-4f57-bce7-0d42d344051e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=8)\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "105/105 [==============================] - 1s 5ms/step - loss: 1.5448 - acc: 0.0000e+00 - val_loss: 2.6692 - val_acc: 0.3846\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 1.4945 - acc: 0.0000e+00 - val_loss: 2.6371 - val_acc: 0.3846\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 1.4395 - acc: 0.0000e+00 - val_loss: 2.6043 - val_acc: 0.3846\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 1.3885 - acc: 0.0000e+00 - val_loss: 2.5744 - val_acc: 0.3846\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 1.3454 - acc: 0.0000e+00 - val_loss: 2.5473 - val_acc: 0.3846\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 1.3067 - acc: 0.0000e+00 - val_loss: 2.5216 - val_acc: 0.3846\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 1.2692 - acc: 0.0000e+00 - val_loss: 2.4971 - val_acc: 0.3846\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 1.2345 - acc: 0.0000e+00 - val_loss: 2.4734 - val_acc: 0.3846\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 1.2004 - acc: 0.0000e+00 - val_loss: 2.4510 - val_acc: 0.3846\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 1.1652 - acc: 0.0000e+00 - val_loss: 2.4131 - val_acc: 0.3846\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 1.1355 - acc: 0.0000e+00 - val_loss: 2.3191 - val_acc: 0.3846\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 1.1037 - acc: 0.0000e+00 - val_loss: 2.2207 - val_acc: 0.3846\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 1.0743 - acc: 0.0000e+00 - val_loss: 2.1351 - val_acc: 0.3846\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 1.0495 - acc: 0.0000e+00 - val_loss: 2.0589 - val_acc: 0.3846\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 1.0272 - acc: 0.0000e+00 - val_loss: 1.9863 - val_acc: 0.3846\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 1.0056 - acc: 0.0286 - val_loss: 1.9163 - val_acc: 0.3077\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.9849 - acc: 0.0857 - val_loss: 1.8500 - val_acc: 0.3077\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 213us/step - loss: 0.9662 - acc: 0.1905 - val_loss: 1.7853 - val_acc: 0.3077\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.9476 - acc: 0.3524 - val_loss: 1.7231 - val_acc: 0.3077\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.9301 - acc: 0.4190 - val_loss: 1.6636 - val_acc: 0.3077\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.9134 - acc: 0.4667 - val_loss: 1.6065 - val_acc: 0.3077\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.8976 - acc: 0.4952 - val_loss: 1.5490 - val_acc: 0.3077\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.8814 - acc: 0.5143 - val_loss: 1.4851 - val_acc: 0.3077\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.8659 - acc: 0.5429 - val_loss: 1.4234 - val_acc: 0.3077\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.8513 - acc: 0.5905 - val_loss: 1.3658 - val_acc: 0.3846\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.8377 - acc: 0.6286 - val_loss: 1.3129 - val_acc: 0.3846\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.8264 - acc: 0.6762 - val_loss: 1.2633 - val_acc: 0.3846\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.8143 - acc: 0.7238 - val_loss: 1.2125 - val_acc: 0.3846\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.8026 - acc: 0.7714 - val_loss: 1.1679 - val_acc: 0.3846\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.7924 - acc: 0.8190 - val_loss: 1.1276 - val_acc: 0.3846\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.7821 - acc: 0.8286 - val_loss: 1.0886 - val_acc: 0.3846\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.7720 - acc: 0.8286 - val_loss: 1.0546 - val_acc: 0.3846\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 209us/step - loss: 0.7626 - acc: 0.8476 - val_loss: 1.0278 - val_acc: 0.3846\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.7537 - acc: 0.8476 - val_loss: 1.0074 - val_acc: 0.4615\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.7455 - acc: 0.8571 - val_loss: 0.9922 - val_acc: 0.4615\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.7374 - acc: 0.8667 - val_loss: 0.9809 - val_acc: 0.4615\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.7299 - acc: 0.8667 - val_loss: 0.9728 - val_acc: 0.4615\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 0.7224 - acc: 0.8762 - val_loss: 0.9667 - val_acc: 0.4615\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 203us/step - loss: 0.7159 - acc: 0.8857 - val_loss: 0.9623 - val_acc: 0.4615\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.7089 - acc: 0.8857 - val_loss: 0.9595 - val_acc: 0.4615\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.7022 - acc: 0.8857 - val_loss: 0.9572 - val_acc: 0.4615\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.6960 - acc: 0.8857 - val_loss: 0.9548 - val_acc: 0.5385\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 236us/step - loss: 0.6898 - acc: 0.8857 - val_loss: 0.9530 - val_acc: 0.5385\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 242us/step - loss: 0.6839 - acc: 0.8857 - val_loss: 0.9510 - val_acc: 0.5385\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 207us/step - loss: 0.6782 - acc: 0.8857 - val_loss: 0.9496 - val_acc: 0.5385\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.6726 - acc: 0.8857 - val_loss: 0.9484 - val_acc: 0.5385\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.6672 - acc: 0.8857 - val_loss: 0.9471 - val_acc: 0.5385\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.6620 - acc: 0.8857 - val_loss: 0.9457 - val_acc: 0.5385\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.6569 - acc: 0.8857 - val_loss: 0.9444 - val_acc: 0.5385\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 200us/step - loss: 0.6518 - acc: 0.8857 - val_loss: 0.9436 - val_acc: 0.5385\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.6470 - acc: 0.8857 - val_loss: 0.9435 - val_acc: 0.5385\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.6423 - acc: 0.8857 - val_loss: 0.9434 - val_acc: 0.5385\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 224us/step - loss: 0.6377 - acc: 0.8857 - val_loss: 0.9429 - val_acc: 0.6154\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.6332 - acc: 0.8857 - val_loss: 0.9421 - val_acc: 0.5385\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.6288 - acc: 0.8857 - val_loss: 0.9416 - val_acc: 0.5385\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 206us/step - loss: 0.6246 - acc: 0.8857 - val_loss: 0.9417 - val_acc: 0.5385\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.6204 - acc: 0.8857 - val_loss: 0.9418 - val_acc: 0.5385\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.6163 - acc: 0.8857 - val_loss: 0.9415 - val_acc: 0.5385\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.6123 - acc: 0.8857 - val_loss: 0.9410 - val_acc: 0.5385\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.6084 - acc: 0.8857 - val_loss: 0.9406 - val_acc: 0.5385\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.6049 - acc: 0.8857 - val_loss: 0.9401 - val_acc: 0.5385\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.6011 - acc: 0.8857 - val_loss: 0.9402 - val_acc: 0.5385\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.5974 - acc: 0.8857 - val_loss: 0.9399 - val_acc: 0.5385\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.5938 - acc: 0.8857 - val_loss: 0.9398 - val_acc: 0.5385\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.5902 - acc: 0.8857 - val_loss: 0.9398 - val_acc: 0.6154\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 200us/step - loss: 0.5867 - acc: 0.8857 - val_loss: 0.9397 - val_acc: 0.6154\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.5832 - acc: 0.8857 - val_loss: 0.9395 - val_acc: 0.6154\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5798 - acc: 0.8857 - val_loss: 0.9397 - val_acc: 0.6154\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.5765 - acc: 0.8857 - val_loss: 0.9399 - val_acc: 0.6154\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 198us/step - loss: 0.5732 - acc: 0.8857 - val_loss: 0.9400 - val_acc: 0.6154\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 198us/step - loss: 0.5700 - acc: 0.8857 - val_loss: 0.9404 - val_acc: 0.6154\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.5668 - acc: 0.8857 - val_loss: 0.9407 - val_acc: 0.6154\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.5638 - acc: 0.8857 - val_loss: 0.9411 - val_acc: 0.6154\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.5606 - acc: 0.8857 - val_loss: 0.9413 - val_acc: 0.6154\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.5576 - acc: 0.8857 - val_loss: 0.9416 - val_acc: 0.6154\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.5547 - acc: 0.8857 - val_loss: 0.9424 - val_acc: 0.6154\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.5518 - acc: 0.8857 - val_loss: 0.9430 - val_acc: 0.6154\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5489 - acc: 0.8857 - val_loss: 0.9435 - val_acc: 0.6154\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.5461 - acc: 0.8857 - val_loss: 0.9441 - val_acc: 0.6154\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.5436 - acc: 0.8857 - val_loss: 0.9448 - val_acc: 0.6154\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.5410 - acc: 0.8857 - val_loss: 0.9450 - val_acc: 0.6154\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.5385 - acc: 0.8857 - val_loss: 0.9450 - val_acc: 0.6154\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.5358 - acc: 0.8857 - val_loss: 0.9457 - val_acc: 0.6154\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.5335 - acc: 0.8857 - val_loss: 0.9464 - val_acc: 0.6154\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.5309 - acc: 0.8857 - val_loss: 0.9467 - val_acc: 0.6154\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.5285 - acc: 0.8857 - val_loss: 0.9467 - val_acc: 0.6154\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.5259 - acc: 0.8857 - val_loss: 0.9475 - val_acc: 0.6154\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.5234 - acc: 0.8857 - val_loss: 0.9483 - val_acc: 0.6154\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.5208 - acc: 0.8857 - val_loss: 0.9490 - val_acc: 0.6154\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.5183 - acc: 0.8857 - val_loss: 0.9497 - val_acc: 0.6154\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.5157 - acc: 0.8857 - val_loss: 0.9504 - val_acc: 0.6154\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.5132 - acc: 0.8857 - val_loss: 0.9513 - val_acc: 0.6154\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.5108 - acc: 0.8857 - val_loss: 0.9526 - val_acc: 0.6154\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.5085 - acc: 0.8857 - val_loss: 0.9538 - val_acc: 0.6154\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.5063 - acc: 0.8857 - val_loss: 0.9550 - val_acc: 0.6154\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.5040 - acc: 0.8857 - val_loss: 0.9561 - val_acc: 0.6154\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.5019 - acc: 0.8857 - val_loss: 0.9570 - val_acc: 0.6154\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.4999 - acc: 0.8857 - val_loss: 0.9579 - val_acc: 0.6154\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.4976 - acc: 0.8857 - val_loss: 0.9589 - val_acc: 0.6154\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.4952 - acc: 0.8857 - val_loss: 0.9600 - val_acc: 0.6154\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4931 - acc: 0.8857 - val_loss: 0.9612 - val_acc: 0.6154\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.4908 - acc: 0.8857 - val_loss: 0.9625 - val_acc: 0.6154\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.4887 - acc: 0.8857 - val_loss: 0.9637 - val_acc: 0.6154\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.4867 - acc: 0.8857 - val_loss: 0.9649 - val_acc: 0.6154\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.4847 - acc: 0.8857 - val_loss: 0.9662 - val_acc: 0.6154\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4824 - acc: 0.8857 - val_loss: 0.9674 - val_acc: 0.6154\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4802 - acc: 0.8857 - val_loss: 0.9688 - val_acc: 0.6154\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.4780 - acc: 0.8857 - val_loss: 0.9703 - val_acc: 0.6154\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.4760 - acc: 0.8857 - val_loss: 0.9720 - val_acc: 0.6154\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.4740 - acc: 0.8857 - val_loss: 0.9738 - val_acc: 0.6154\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.4720 - acc: 0.8857 - val_loss: 0.9755 - val_acc: 0.6154\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4700 - acc: 0.8857 - val_loss: 0.9772 - val_acc: 0.6154\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.4681 - acc: 0.8857 - val_loss: 0.9788 - val_acc: 0.6154\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.4660 - acc: 0.8857 - val_loss: 0.9804 - val_acc: 0.6154\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4639 - acc: 0.8857 - val_loss: 0.9819 - val_acc: 0.6154\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.4619 - acc: 0.8857 - val_loss: 0.9832 - val_acc: 0.6154\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.4600 - acc: 0.8857 - val_loss: 0.9845 - val_acc: 0.6154\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.4580 - acc: 0.8857 - val_loss: 0.9863 - val_acc: 0.6154\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.4561 - acc: 0.8857 - val_loss: 0.9880 - val_acc: 0.6154\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.4541 - acc: 0.8857 - val_loss: 0.9898 - val_acc: 0.6154\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.4523 - acc: 0.8857 - val_loss: 0.9917 - val_acc: 0.6154\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 212us/step - loss: 0.4503 - acc: 0.8857 - val_loss: 0.9934 - val_acc: 0.6154\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4483 - acc: 0.8857 - val_loss: 0.9952 - val_acc: 0.6154\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.4465 - acc: 0.8857 - val_loss: 0.9972 - val_acc: 0.6154\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4447 - acc: 0.8857 - val_loss: 0.9991 - val_acc: 0.6154\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.4428 - acc: 0.8857 - val_loss: 1.0009 - val_acc: 0.6154\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.4408 - acc: 0.8857 - val_loss: 1.0028 - val_acc: 0.6154\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4389 - acc: 0.8857 - val_loss: 1.0048 - val_acc: 0.6154\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.4371 - acc: 0.8857 - val_loss: 1.0068 - val_acc: 0.6154\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.4353 - acc: 0.8857 - val_loss: 1.0087 - val_acc: 0.6154\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 211us/step - loss: 0.4335 - acc: 0.8857 - val_loss: 1.0109 - val_acc: 0.6154\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 211us/step - loss: 0.4318 - acc: 0.8857 - val_loss: 1.0129 - val_acc: 0.6154\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 213us/step - loss: 0.4300 - acc: 0.8857 - val_loss: 1.0149 - val_acc: 0.6154\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 211us/step - loss: 0.4281 - acc: 0.8857 - val_loss: 1.0169 - val_acc: 0.6154\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.4263 - acc: 0.8857 - val_loss: 1.0190 - val_acc: 0.6154\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.4246 - acc: 0.8857 - val_loss: 1.0212 - val_acc: 0.6154\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.4227 - acc: 0.8857 - val_loss: 1.0233 - val_acc: 0.6154\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.4209 - acc: 0.8857 - val_loss: 1.0254 - val_acc: 0.6154\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.4193 - acc: 0.8857 - val_loss: 1.0277 - val_acc: 0.6154\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 0.4177 - acc: 0.8857 - val_loss: 1.0299 - val_acc: 0.6154\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 198us/step - loss: 0.4160 - acc: 0.8857 - val_loss: 1.0321 - val_acc: 0.6154\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 260us/step - loss: 0.4143 - acc: 0.8857 - val_loss: 1.0343 - val_acc: 0.6154\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.4126 - acc: 0.8857 - val_loss: 1.0366 - val_acc: 0.6154\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.4109 - acc: 0.8857 - val_loss: 1.0389 - val_acc: 0.6154\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.4091 - acc: 0.8857 - val_loss: 1.0411 - val_acc: 0.6154\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.4074 - acc: 0.8857 - val_loss: 1.0434 - val_acc: 0.6154\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.4057 - acc: 0.8857 - val_loss: 1.0457 - val_acc: 0.6154\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.4041 - acc: 0.8857 - val_loss: 1.0481 - val_acc: 0.6154\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.4024 - acc: 0.8857 - val_loss: 1.0503 - val_acc: 0.6154\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.4009 - acc: 0.8857 - val_loss: 1.0525 - val_acc: 0.6154\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.3992 - acc: 0.8857 - val_loss: 1.0547 - val_acc: 0.6154\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.3977 - acc: 0.8857 - val_loss: 1.0569 - val_acc: 0.6154\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.3961 - acc: 0.8857 - val_loss: 1.0594 - val_acc: 0.6154\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.3944 - acc: 0.8857 - val_loss: 1.0618 - val_acc: 0.6154\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.3927 - acc: 0.8857 - val_loss: 1.0643 - val_acc: 0.6154\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.3911 - acc: 0.8857 - val_loss: 1.0668 - val_acc: 0.6154\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.3896 - acc: 0.8857 - val_loss: 1.0693 - val_acc: 0.6154\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.3881 - acc: 0.8857 - val_loss: 1.0718 - val_acc: 0.6154\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.3865 - acc: 0.8857 - val_loss: 1.0744 - val_acc: 0.6154\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.3848 - acc: 0.8857 - val_loss: 1.0769 - val_acc: 0.6154\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.3834 - acc: 0.8857 - val_loss: 1.0795 - val_acc: 0.6154\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.3818 - acc: 0.8857 - val_loss: 1.0820 - val_acc: 0.6154\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 220us/step - loss: 0.3802 - acc: 0.8857 - val_loss: 1.0846 - val_acc: 0.6154\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.3787 - acc: 0.8857 - val_loss: 1.0872 - val_acc: 0.6154\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.3771 - acc: 0.8857 - val_loss: 1.0898 - val_acc: 0.6154\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.3756 - acc: 0.8857 - val_loss: 1.0924 - val_acc: 0.6154\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.3740 - acc: 0.8857 - val_loss: 1.0949 - val_acc: 0.6154\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.3725 - acc: 0.8857 - val_loss: 1.0975 - val_acc: 0.6154\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 217us/step - loss: 0.3709 - acc: 0.8857 - val_loss: 1.1000 - val_acc: 0.6154\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.3695 - acc: 0.8857 - val_loss: 1.1025 - val_acc: 0.6154\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 204us/step - loss: 0.3682 - acc: 0.8857 - val_loss: 1.1051 - val_acc: 0.6154\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.3668 - acc: 0.8857 - val_loss: 1.1076 - val_acc: 0.6154\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 208us/step - loss: 0.3653 - acc: 0.8857 - val_loss: 1.1102 - val_acc: 0.6154\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 235us/step - loss: 0.3638 - acc: 0.8857 - val_loss: 1.1128 - val_acc: 0.6154\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.3624 - acc: 0.8857 - val_loss: 1.1153 - val_acc: 0.6154\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 237us/step - loss: 0.3611 - acc: 0.8857 - val_loss: 1.1179 - val_acc: 0.6154\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 209us/step - loss: 0.3597 - acc: 0.8857 - val_loss: 1.1205 - val_acc: 0.6154\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 205us/step - loss: 0.3583 - acc: 0.8857 - val_loss: 1.1231 - val_acc: 0.6154\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.3570 - acc: 0.8857 - val_loss: 1.1257 - val_acc: 0.6154\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 224us/step - loss: 0.3556 - acc: 0.8857 - val_loss: 1.1284 - val_acc: 0.6154\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.3542 - acc: 0.8857 - val_loss: 1.1311 - val_acc: 0.6154\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.3527 - acc: 0.8857 - val_loss: 1.1336 - val_acc: 0.6154\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.3514 - acc: 0.8857 - val_loss: 1.1360 - val_acc: 0.6154\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.3502 - acc: 0.8857 - val_loss: 1.1384 - val_acc: 0.6154\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.3489 - acc: 0.8857 - val_loss: 1.1412 - val_acc: 0.6154\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.3475 - acc: 0.8857 - val_loss: 1.1437 - val_acc: 0.6154\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.3462 - acc: 0.8857 - val_loss: 1.1463 - val_acc: 0.6154\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.3448 - acc: 0.8857 - val_loss: 1.1490 - val_acc: 0.6154\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.3435 - acc: 0.8857 - val_loss: 1.1517 - val_acc: 0.6154\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.3422 - acc: 0.8857 - val_loss: 1.1544 - val_acc: 0.6154\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 230us/step - loss: 0.3409 - acc: 0.8857 - val_loss: 1.1572 - val_acc: 0.6154\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.3396 - acc: 0.8857 - val_loss: 1.1599 - val_acc: 0.6154\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.3384 - acc: 0.8857 - val_loss: 1.1626 - val_acc: 0.6154\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.3370 - acc: 0.8857 - val_loss: 1.1654 - val_acc: 0.6154\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.3358 - acc: 0.8857 - val_loss: 1.1683 - val_acc: 0.6154\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.3345 - acc: 0.8857 - val_loss: 1.1711 - val_acc: 0.6154\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.3331 - acc: 0.8857 - val_loss: 1.1739 - val_acc: 0.6154\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.3318 - acc: 0.8857 - val_loss: 1.1766 - val_acc: 0.6154\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.3305 - acc: 0.8857 - val_loss: 1.1791 - val_acc: 0.6154\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.3293 - acc: 0.8857 - val_loss: 1.1817 - val_acc: 0.6154\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 200us/step - loss: 0.3280 - acc: 0.8857 - val_loss: 1.1843 - val_acc: 0.6154\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.3268 - acc: 0.8857 - val_loss: 1.1870 - val_acc: 0.6154\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.3256 - acc: 0.8857 - val_loss: 1.1895 - val_acc: 0.6154\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.3244 - acc: 0.8857 - val_loss: 1.1921 - val_acc: 0.6154\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.3231 - acc: 0.8857 - val_loss: 1.1948 - val_acc: 0.6154\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.3219 - acc: 0.8857 - val_loss: 1.1975 - val_acc: 0.6154\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.3206 - acc: 0.8857 - val_loss: 1.2002 - val_acc: 0.6154\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.3194 - acc: 0.8857 - val_loss: 1.2028 - val_acc: 0.6154\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.3183 - acc: 0.8857 - val_loss: 1.2054 - val_acc: 0.6154\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.3171 - acc: 0.8857 - val_loss: 1.2080 - val_acc: 0.6154\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.3160 - acc: 0.8857 - val_loss: 1.2106 - val_acc: 0.6154\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 214us/step - loss: 0.3149 - acc: 0.8857 - val_loss: 1.2132 - val_acc: 0.6154\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.3138 - acc: 0.8857 - val_loss: 1.2159 - val_acc: 0.6154\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.3126 - acc: 0.8857 - val_loss: 1.2186 - val_acc: 0.6154\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.3115 - acc: 0.8857 - val_loss: 1.2213 - val_acc: 0.6154\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.3103 - acc: 0.8857 - val_loss: 1.2240 - val_acc: 0.6154\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.3092 - acc: 0.8857 - val_loss: 1.2267 - val_acc: 0.6154\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.3080 - acc: 0.8857 - val_loss: 1.2293 - val_acc: 0.6154\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 201us/step - loss: 0.3070 - acc: 0.8857 - val_loss: 1.2319 - val_acc: 0.6154\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 212us/step - loss: 0.3058 - acc: 0.8857 - val_loss: 1.2347 - val_acc: 0.6154\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.3048 - acc: 0.8857 - val_loss: 1.2373 - val_acc: 0.6154\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.3037 - acc: 0.8857 - val_loss: 1.2399 - val_acc: 0.6154\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.3026 - acc: 0.8857 - val_loss: 1.2427 - val_acc: 0.6154\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.3015 - acc: 0.8857 - val_loss: 1.2455 - val_acc: 0.6154\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.3005 - acc: 0.8857 - val_loss: 1.2480 - val_acc: 0.6154\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.2994 - acc: 0.8857 - val_loss: 1.2505 - val_acc: 0.6154\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.2984 - acc: 0.8857 - val_loss: 1.2533 - val_acc: 0.6154\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.2974 - acc: 0.8857 - val_loss: 1.2560 - val_acc: 0.6154\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.2963 - acc: 0.8857 - val_loss: 1.2589 - val_acc: 0.6154\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.2952 - acc: 0.8857 - val_loss: 1.2616 - val_acc: 0.6154\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.2942 - acc: 0.8857 - val_loss: 1.2641 - val_acc: 0.6154\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.2932 - acc: 0.8857 - val_loss: 1.2667 - val_acc: 0.6154\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.2921 - acc: 0.8857 - val_loss: 1.2693 - val_acc: 0.6154\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.2911 - acc: 0.8857 - val_loss: 1.2721 - val_acc: 0.6154\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.2901 - acc: 0.8857 - val_loss: 1.2749 - val_acc: 0.6154\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.2891 - acc: 0.8857 - val_loss: 1.2778 - val_acc: 0.6154\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2881 - acc: 0.8857 - val_loss: 1.2806 - val_acc: 0.6154\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2871 - acc: 0.8857 - val_loss: 1.2836 - val_acc: 0.6154\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.2861 - acc: 0.8857 - val_loss: 1.2864 - val_acc: 0.6154\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.2851 - acc: 0.8857 - val_loss: 1.2893 - val_acc: 0.6154\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.2841 - acc: 0.8857 - val_loss: 1.2922 - val_acc: 0.6154\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.2832 - acc: 0.8857 - val_loss: 1.2948 - val_acc: 0.6154\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2822 - acc: 0.8857 - val_loss: 1.2975 - val_acc: 0.6154\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.2813 - acc: 0.8857 - val_loss: 1.3002 - val_acc: 0.6154\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.2804 - acc: 0.8857 - val_loss: 1.3031 - val_acc: 0.6154\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.2794 - acc: 0.8857 - val_loss: 1.3060 - val_acc: 0.6154\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2784 - acc: 0.8857 - val_loss: 1.3091 - val_acc: 0.6154\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2774 - acc: 0.8857 - val_loss: 1.3120 - val_acc: 0.6154\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.2765 - acc: 0.8857 - val_loss: 1.3149 - val_acc: 0.6154\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.2757 - acc: 0.8857 - val_loss: 1.3176 - val_acc: 0.6154\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.2748 - acc: 0.8857 - val_loss: 1.3203 - val_acc: 0.6154\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.2739 - acc: 0.8857 - val_loss: 1.3230 - val_acc: 0.6154\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.2729 - acc: 0.8857 - val_loss: 1.3258 - val_acc: 0.6154\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.2720 - acc: 0.8857 - val_loss: 1.3285 - val_acc: 0.6154\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.2711 - acc: 0.8857 - val_loss: 1.3313 - val_acc: 0.6154\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.2702 - acc: 0.8857 - val_loss: 1.3343 - val_acc: 0.6154\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 209us/step - loss: 0.2694 - acc: 0.8857 - val_loss: 1.3371 - val_acc: 0.6154\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.2685 - acc: 0.8857 - val_loss: 1.3400 - val_acc: 0.6154\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 147us/step - loss: 0.2676 - acc: 0.8857 - val_loss: 1.3429 - val_acc: 0.6154\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2668 - acc: 0.8857 - val_loss: 1.3457 - val_acc: 0.6154\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.2659 - acc: 0.8857 - val_loss: 1.3483 - val_acc: 0.6154\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.2651 - acc: 0.8857 - val_loss: 1.3512 - val_acc: 0.6154\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2642 - acc: 0.8857 - val_loss: 1.3542 - val_acc: 0.6154\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.2633 - acc: 0.8857 - val_loss: 1.3572 - val_acc: 0.6154\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.2625 - acc: 0.8857 - val_loss: 1.3600 - val_acc: 0.6154\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.2617 - acc: 0.8857 - val_loss: 1.3630 - val_acc: 0.6154\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.2608 - acc: 0.8857 - val_loss: 1.3661 - val_acc: 0.6154\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.2600 - acc: 0.8857 - val_loss: 1.3691 - val_acc: 0.6154\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 214us/step - loss: 0.2591 - acc: 0.8857 - val_loss: 1.3719 - val_acc: 0.6154\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.2583 - acc: 0.8857 - val_loss: 1.3746 - val_acc: 0.6154\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 236us/step - loss: 0.2575 - acc: 0.8857 - val_loss: 1.3775 - val_acc: 0.6154\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.2566 - acc: 0.8857 - val_loss: 1.3805 - val_acc: 0.6154\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.2559 - acc: 0.8857 - val_loss: 1.3834 - val_acc: 0.6154\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.2551 - acc: 0.8857 - val_loss: 1.3866 - val_acc: 0.6154\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.2542 - acc: 0.8857 - val_loss: 1.3896 - val_acc: 0.6154\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.2535 - acc: 0.8857 - val_loss: 1.3923 - val_acc: 0.6154\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.2526 - acc: 0.8857 - val_loss: 1.3949 - val_acc: 0.6154\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.2519 - acc: 0.8857 - val_loss: 1.3978 - val_acc: 0.6154\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.2511 - acc: 0.8857 - val_loss: 1.4009 - val_acc: 0.6154\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2503 - acc: 0.8857 - val_loss: 1.4040 - val_acc: 0.6154\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.2495 - acc: 0.8857 - val_loss: 1.4071 - val_acc: 0.6154\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.2488 - acc: 0.8857 - val_loss: 1.4100 - val_acc: 0.6154\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.2480 - acc: 0.8857 - val_loss: 1.4130 - val_acc: 0.6154\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.2473 - acc: 0.8857 - val_loss: 1.4160 - val_acc: 0.6154\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.2465 - acc: 0.8857 - val_loss: 1.4191 - val_acc: 0.6154\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.2457 - acc: 0.8857 - val_loss: 1.4221 - val_acc: 0.6154\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.2450 - acc: 0.8857 - val_loss: 1.4250 - val_acc: 0.6154\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.2443 - acc: 0.8857 - val_loss: 1.4280 - val_acc: 0.6154\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.2436 - acc: 0.8857 - val_loss: 1.4309 - val_acc: 0.6154\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.2428 - acc: 0.8857 - val_loss: 1.4336 - val_acc: 0.6154\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.2421 - acc: 0.8857 - val_loss: 1.4361 - val_acc: 0.6154\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.2413 - acc: 0.8857 - val_loss: 1.4391 - val_acc: 0.6154\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.2407 - acc: 0.8857 - val_loss: 1.4420 - val_acc: 0.6154\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.2399 - acc: 0.8857 - val_loss: 1.4450 - val_acc: 0.6154\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.2393 - acc: 0.8857 - val_loss: 1.4479 - val_acc: 0.6154\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.2386 - acc: 0.8857 - val_loss: 1.4509 - val_acc: 0.6154\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.2379 - acc: 0.8857 - val_loss: 1.4536 - val_acc: 0.6154\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2371 - acc: 0.8857 - val_loss: 1.4563 - val_acc: 0.6154\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.2365 - acc: 0.8857 - val_loss: 1.4591 - val_acc: 0.6154\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2358 - acc: 0.8857 - val_loss: 1.4620 - val_acc: 0.6154\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.2351 - acc: 0.8857 - val_loss: 1.4651 - val_acc: 0.6154\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.2344 - acc: 0.8857 - val_loss: 1.4681 - val_acc: 0.6154\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.2338 - acc: 0.8857 - val_loss: 1.4710 - val_acc: 0.6154\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.2331 - acc: 0.8857 - val_loss: 1.4738 - val_acc: 0.6154\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.2325 - acc: 0.8857 - val_loss: 1.4767 - val_acc: 0.6154\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.2318 - acc: 0.8857 - val_loss: 1.4799 - val_acc: 0.6154\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.2312 - acc: 0.8857 - val_loss: 1.4829 - val_acc: 0.6154\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.2305 - acc: 0.8857 - val_loss: 1.4861 - val_acc: 0.6154\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.2299 - acc: 0.8857 - val_loss: 1.4890 - val_acc: 0.6154\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.2292 - acc: 0.8857 - val_loss: 1.4920 - val_acc: 0.6154\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.2286 - acc: 0.8857 - val_loss: 1.4951 - val_acc: 0.6154\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2279 - acc: 0.8857 - val_loss: 1.4982 - val_acc: 0.6154\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.2273 - acc: 0.8857 - val_loss: 1.5013 - val_acc: 0.6154\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.2266 - acc: 0.8857 - val_loss: 1.5043 - val_acc: 0.6154\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.2260 - acc: 0.8857 - val_loss: 1.5073 - val_acc: 0.6154\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.2254 - acc: 0.8857 - val_loss: 1.5100 - val_acc: 0.6154\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.2248 - acc: 0.8857 - val_loss: 1.5128 - val_acc: 0.6154\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.2242 - acc: 0.8857 - val_loss: 1.5155 - val_acc: 0.6154\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.2235 - acc: 0.8857 - val_loss: 1.5182 - val_acc: 0.6154\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.2229 - acc: 0.8857 - val_loss: 1.5211 - val_acc: 0.6154\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.2223 - acc: 0.8857 - val_loss: 1.5239 - val_acc: 0.6154\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.2217 - acc: 0.8857 - val_loss: 1.5265 - val_acc: 0.6154\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.2211 - acc: 0.8857 - val_loss: 1.5291 - val_acc: 0.6154\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 208us/step - loss: 0.2205 - acc: 0.8857 - val_loss: 1.5320 - val_acc: 0.6154\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 213us/step - loss: 0.2199 - acc: 0.8857 - val_loss: 1.5349 - val_acc: 0.6154\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.2193 - acc: 0.8857 - val_loss: 1.5376 - val_acc: 0.6154\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.2188 - acc: 0.8857 - val_loss: 1.5403 - val_acc: 0.6154\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.2182 - acc: 0.8857 - val_loss: 1.5432 - val_acc: 0.6154\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.2176 - acc: 0.8857 - val_loss: 1.5462 - val_acc: 0.6154\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.2170 - acc: 0.8857 - val_loss: 1.5493 - val_acc: 0.6154\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.2164 - acc: 0.8857 - val_loss: 1.5522 - val_acc: 0.6154\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 208us/step - loss: 0.2159 - acc: 0.8857 - val_loss: 1.5549 - val_acc: 0.6154\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.2153 - acc: 0.8857 - val_loss: 1.5576 - val_acc: 0.6154\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2147 - acc: 0.8857 - val_loss: 1.5601 - val_acc: 0.6154\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.2141 - acc: 0.8857 - val_loss: 1.5628 - val_acc: 0.6154\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.2136 - acc: 0.8857 - val_loss: 1.5654 - val_acc: 0.6154\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.2130 - acc: 0.8857 - val_loss: 1.5679 - val_acc: 0.6154\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.2124 - acc: 0.8857 - val_loss: 1.5706 - val_acc: 0.6154\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.2119 - acc: 0.8857 - val_loss: 1.5732 - val_acc: 0.6154\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.2113 - acc: 0.8857 - val_loss: 1.5756 - val_acc: 0.6154\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.2107 - acc: 0.8857 - val_loss: 1.5783 - val_acc: 0.6154\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.2101 - acc: 0.8857 - val_loss: 1.5813 - val_acc: 0.6154\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2095 - acc: 0.8857 - val_loss: 1.5842 - val_acc: 0.6154\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.2090 - acc: 0.8857 - val_loss: 1.5869 - val_acc: 0.6154\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.2085 - acc: 0.8857 - val_loss: 1.5896 - val_acc: 0.6154\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.2080 - acc: 0.8857 - val_loss: 1.5924 - val_acc: 0.6154\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.2074 - acc: 0.8857 - val_loss: 1.5953 - val_acc: 0.6154\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.2069 - acc: 0.8857 - val_loss: 1.5981 - val_acc: 0.6154\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.2064 - acc: 0.8857 - val_loss: 1.6010 - val_acc: 0.6154\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.2059 - acc: 0.8857 - val_loss: 1.6037 - val_acc: 0.6154\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.2054 - acc: 0.8857 - val_loss: 1.6068 - val_acc: 0.6154\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.2049 - acc: 0.8857 - val_loss: 1.6094 - val_acc: 0.6154\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.2042 - acc: 0.8857 - val_loss: 1.6121 - val_acc: 0.6154\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.2037 - acc: 0.8857 - val_loss: 1.6149 - val_acc: 0.6154\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.2032 - acc: 0.8857 - val_loss: 1.6175 - val_acc: 0.6154\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.2026 - acc: 0.8857 - val_loss: 1.6202 - val_acc: 0.6154\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.2021 - acc: 0.8857 - val_loss: 1.6230 - val_acc: 0.6154\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.2016 - acc: 0.8857 - val_loss: 1.6258 - val_acc: 0.6154\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.2011 - acc: 0.8857 - val_loss: 1.6285 - val_acc: 0.6154\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 219us/step - loss: 0.2005 - acc: 0.8857 - val_loss: 1.6311 - val_acc: 0.6154\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.2000 - acc: 0.8857 - val_loss: 1.6339 - val_acc: 0.6154\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.1995 - acc: 0.8857 - val_loss: 1.6366 - val_acc: 0.6154\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1990 - acc: 0.8857 - val_loss: 1.6395 - val_acc: 0.6154\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1986 - acc: 0.8857 - val_loss: 1.6420 - val_acc: 0.6154\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1980 - acc: 0.8857 - val_loss: 1.6444 - val_acc: 0.6154\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1975 - acc: 0.8857 - val_loss: 1.6470 - val_acc: 0.6154\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.1971 - acc: 0.8857 - val_loss: 1.6496 - val_acc: 0.6154\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1966 - acc: 0.8857 - val_loss: 1.6521 - val_acc: 0.6154\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.1960 - acc: 0.8857 - val_loss: 1.6545 - val_acc: 0.6154\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1956 - acc: 0.8857 - val_loss: 1.6573 - val_acc: 0.6154\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1951 - acc: 0.8857 - val_loss: 1.6599 - val_acc: 0.6154\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.1945 - acc: 0.8857 - val_loss: 1.6625 - val_acc: 0.6154\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1940 - acc: 0.8857 - val_loss: 1.6653 - val_acc: 0.6154\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1936 - acc: 0.8857 - val_loss: 1.6681 - val_acc: 0.6154\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.1931 - acc: 0.8857 - val_loss: 1.6707 - val_acc: 0.6154\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1927 - acc: 0.8857 - val_loss: 1.6735 - val_acc: 0.6154\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1922 - acc: 0.8857 - val_loss: 1.6764 - val_acc: 0.6154\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1917 - acc: 0.8857 - val_loss: 1.6793 - val_acc: 0.6154\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1913 - acc: 0.8857 - val_loss: 1.6820 - val_acc: 0.6154\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.1908 - acc: 0.8857 - val_loss: 1.6847 - val_acc: 0.6154\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1904 - acc: 0.8857 - val_loss: 1.6874 - val_acc: 0.6154\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.1899 - acc: 0.8857 - val_loss: 1.6900 - val_acc: 0.6154\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1895 - acc: 0.8857 - val_loss: 1.6928 - val_acc: 0.6154\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1890 - acc: 0.8857 - val_loss: 1.6952 - val_acc: 0.6154\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1885 - acc: 0.8857 - val_loss: 1.6977 - val_acc: 0.6154\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1881 - acc: 0.8857 - val_loss: 1.7005 - val_acc: 0.6154\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1876 - acc: 0.8857 - val_loss: 1.7033 - val_acc: 0.6154\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.1872 - acc: 0.8857 - val_loss: 1.7061 - val_acc: 0.6154\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.1867 - acc: 0.8857 - val_loss: 1.7089 - val_acc: 0.6154\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1863 - acc: 0.8857 - val_loss: 1.7117 - val_acc: 0.6154\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1859 - acc: 0.8857 - val_loss: 1.7141 - val_acc: 0.6154\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.1853 - acc: 0.8857 - val_loss: 1.7165 - val_acc: 0.6154\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1849 - acc: 0.8857 - val_loss: 1.7192 - val_acc: 0.6154\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.1845 - acc: 0.8857 - val_loss: 1.7218 - val_acc: 0.6154\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1841 - acc: 0.8857 - val_loss: 1.7244 - val_acc: 0.6154\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1836 - acc: 0.8857 - val_loss: 1.7269 - val_acc: 0.6154\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1832 - acc: 0.8857 - val_loss: 1.7296 - val_acc: 0.6154\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 205us/step - loss: 0.1828 - acc: 0.8857 - val_loss: 1.7323 - val_acc: 0.6154\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 220us/step - loss: 0.1824 - acc: 0.8857 - val_loss: 1.7350 - val_acc: 0.6154\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.1820 - acc: 0.8857 - val_loss: 1.7374 - val_acc: 0.6154\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.1816 - acc: 0.8857 - val_loss: 1.7399 - val_acc: 0.6154\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1812 - acc: 0.8857 - val_loss: 1.7425 - val_acc: 0.6154\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.1808 - acc: 0.8857 - val_loss: 1.7452 - val_acc: 0.6154\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1804 - acc: 0.8857 - val_loss: 1.7477 - val_acc: 0.6154\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1800 - acc: 0.8857 - val_loss: 1.7503 - val_acc: 0.6154\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1796 - acc: 0.8857 - val_loss: 1.7529 - val_acc: 0.6154\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.1792 - acc: 0.8857 - val_loss: 1.7555 - val_acc: 0.6154\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1787 - acc: 0.8857 - val_loss: 1.7578 - val_acc: 0.6154\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1783 - acc: 0.8857 - val_loss: 1.7602 - val_acc: 0.6154\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.1779 - acc: 0.8857 - val_loss: 1.7627 - val_acc: 0.6154\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.1775 - acc: 0.8857 - val_loss: 1.7651 - val_acc: 0.6154\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.1771 - acc: 0.8857 - val_loss: 1.7676 - val_acc: 0.6154\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1767 - acc: 0.8857 - val_loss: 1.7701 - val_acc: 0.6154\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1763 - acc: 0.8857 - val_loss: 1.7728 - val_acc: 0.6154\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.1759 - acc: 0.8857 - val_loss: 1.7754 - val_acc: 0.6154\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1755 - acc: 0.8857 - val_loss: 1.7778 - val_acc: 0.6154\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1752 - acc: 0.8857 - val_loss: 1.7802 - val_acc: 0.6154\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1748 - acc: 0.8857 - val_loss: 1.7828 - val_acc: 0.6154\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1744 - acc: 0.8857 - val_loss: 1.7853 - val_acc: 0.6154\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.1740 - acc: 0.8857 - val_loss: 1.7877 - val_acc: 0.6154\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1736 - acc: 0.8857 - val_loss: 1.7902 - val_acc: 0.6154\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1733 - acc: 0.8857 - val_loss: 1.7927 - val_acc: 0.6154\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1729 - acc: 0.8857 - val_loss: 1.7954 - val_acc: 0.6154\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1725 - acc: 0.8857 - val_loss: 1.7979 - val_acc: 0.6154\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1721 - acc: 0.8857 - val_loss: 1.8003 - val_acc: 0.6154\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1718 - acc: 0.8857 - val_loss: 1.8026 - val_acc: 0.6154\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1713 - acc: 0.8857 - val_loss: 1.8049 - val_acc: 0.6154\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1709 - acc: 0.8857 - val_loss: 1.8075 - val_acc: 0.6154\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1705 - acc: 0.8857 - val_loss: 1.8102 - val_acc: 0.6154\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 212us/step - loss: 0.1702 - acc: 0.8857 - val_loss: 1.8128 - val_acc: 0.6154\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1698 - acc: 0.8857 - val_loss: 1.8154 - val_acc: 0.6154\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1694 - acc: 0.8857 - val_loss: 1.8179 - val_acc: 0.6154\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1691 - acc: 0.8857 - val_loss: 1.8203 - val_acc: 0.6154\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1687 - acc: 0.8857 - val_loss: 1.8228 - val_acc: 0.6154\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1683 - acc: 0.8857 - val_loss: 1.8254 - val_acc: 0.6154\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1680 - acc: 0.8857 - val_loss: 1.8279 - val_acc: 0.6154\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.1676 - acc: 0.8857 - val_loss: 1.8303 - val_acc: 0.6154\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1673 - acc: 0.8857 - val_loss: 1.8327 - val_acc: 0.6154\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.1669 - acc: 0.8857 - val_loss: 1.8352 - val_acc: 0.6154\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1666 - acc: 0.8857 - val_loss: 1.8374 - val_acc: 0.6154\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 234us/step - loss: 0.1661 - acc: 0.8857 - val_loss: 1.8395 - val_acc: 0.6154\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 212us/step - loss: 0.1658 - acc: 0.8857 - val_loss: 1.8419 - val_acc: 0.6154\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.1654 - acc: 0.8857 - val_loss: 1.8445 - val_acc: 0.6154\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1650 - acc: 0.8857 - val_loss: 1.8471 - val_acc: 0.6154\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1647 - acc: 0.8857 - val_loss: 1.8495 - val_acc: 0.6154\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1642 - acc: 0.8857 - val_loss: 1.8516 - val_acc: 0.6154\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 226us/step - loss: 0.1639 - acc: 0.8857 - val_loss: 1.8539 - val_acc: 0.6154\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 230us/step - loss: 0.1636 - acc: 0.8857 - val_loss: 1.8563 - val_acc: 0.6154\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.1632 - acc: 0.8857 - val_loss: 1.8589 - val_acc: 0.6154\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1629 - acc: 0.8857 - val_loss: 1.8614 - val_acc: 0.6154\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.1625 - acc: 0.8857 - val_loss: 1.8639 - val_acc: 0.6154\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1622 - acc: 0.8857 - val_loss: 1.8663 - val_acc: 0.6154\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1618 - acc: 0.8857 - val_loss: 1.8687 - val_acc: 0.6154\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1615 - acc: 0.8857 - val_loss: 1.8712 - val_acc: 0.6154\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1612 - acc: 0.8857 - val_loss: 1.8736 - val_acc: 0.6154\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1608 - acc: 0.8857 - val_loss: 1.8759 - val_acc: 0.6154\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1605 - acc: 0.8857 - val_loss: 1.8782 - val_acc: 0.6154\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1602 - acc: 0.8857 - val_loss: 1.8806 - val_acc: 0.6154\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1598 - acc: 0.8857 - val_loss: 1.8831 - val_acc: 0.6154\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1595 - acc: 0.8857 - val_loss: 1.8855 - val_acc: 0.6154\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.1592 - acc: 0.8857 - val_loss: 1.8879 - val_acc: 0.6154\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1588 - acc: 0.8857 - val_loss: 1.8905 - val_acc: 0.6154\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.1585 - acc: 0.8857 - val_loss: 1.8927 - val_acc: 0.6154\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1581 - acc: 0.8857 - val_loss: 1.8947 - val_acc: 0.6154\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1578 - acc: 0.8857 - val_loss: 1.8970 - val_acc: 0.6154\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1574 - acc: 0.8857 - val_loss: 1.8993 - val_acc: 0.6154\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.1571 - acc: 0.8857 - val_loss: 1.9018 - val_acc: 0.6154\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1568 - acc: 0.8857 - val_loss: 1.9041 - val_acc: 0.6154\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1565 - acc: 0.8857 - val_loss: 1.9063 - val_acc: 0.6154\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1562 - acc: 0.8857 - val_loss: 1.9085 - val_acc: 0.6154\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 201us/step - loss: 0.1559 - acc: 0.8857 - val_loss: 1.9107 - val_acc: 0.6154\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.1555 - acc: 0.8857 - val_loss: 1.9129 - val_acc: 0.6154\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1552 - acc: 0.8857 - val_loss: 1.9153 - val_acc: 0.6154\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.1549 - acc: 0.8857 - val_loss: 1.9177 - val_acc: 0.6154\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1546 - acc: 0.8857 - val_loss: 1.9201 - val_acc: 0.6154\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1543 - acc: 0.8857 - val_loss: 1.9226 - val_acc: 0.6154\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.1540 - acc: 0.8857 - val_loss: 1.9249 - val_acc: 0.6154\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1537 - acc: 0.8857 - val_loss: 1.9269 - val_acc: 0.6154\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1533 - acc: 0.8857 - val_loss: 1.9290 - val_acc: 0.6154\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1531 - acc: 0.8857 - val_loss: 1.9311 - val_acc: 0.6154\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1527 - acc: 0.8857 - val_loss: 1.9335 - val_acc: 0.6154\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1524 - acc: 0.8857 - val_loss: 1.9356 - val_acc: 0.6154\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1521 - acc: 0.8857 - val_loss: 1.9377 - val_acc: 0.6154\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1518 - acc: 0.8857 - val_loss: 1.9399 - val_acc: 0.6154\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1515 - acc: 0.8857 - val_loss: 1.9419 - val_acc: 0.6154\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1511 - acc: 0.8857 - val_loss: 1.9439 - val_acc: 0.6154\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1508 - acc: 0.8857 - val_loss: 1.9460 - val_acc: 0.6154\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1505 - acc: 0.8857 - val_loss: 1.9479 - val_acc: 0.6154\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1501 - acc: 0.8857 - val_loss: 1.9498 - val_acc: 0.6154\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1498 - acc: 0.8857 - val_loss: 1.9520 - val_acc: 0.6154\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.1495 - acc: 0.8857 - val_loss: 1.9541 - val_acc: 0.6154\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1492 - acc: 0.8857 - val_loss: 1.9561 - val_acc: 0.6154\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1489 - acc: 0.8857 - val_loss: 1.9582 - val_acc: 0.6154\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1486 - acc: 0.8857 - val_loss: 1.9604 - val_acc: 0.6154\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1483 - acc: 0.8857 - val_loss: 1.9627 - val_acc: 0.6154\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.1480 - acc: 0.8857 - val_loss: 1.9648 - val_acc: 0.6154\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.1477 - acc: 0.8857 - val_loss: 1.9669 - val_acc: 0.6154\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1474 - acc: 0.8857 - val_loss: 1.9689 - val_acc: 0.6154\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.1471 - acc: 0.8857 - val_loss: 1.9708 - val_acc: 0.6154\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1468 - acc: 0.8857 - val_loss: 1.9729 - val_acc: 0.6154\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1465 - acc: 0.8857 - val_loss: 1.9750 - val_acc: 0.6154\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.1462 - acc: 0.8857 - val_loss: 1.9771 - val_acc: 0.6154\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.1459 - acc: 0.8857 - val_loss: 1.9793 - val_acc: 0.6154\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1456 - acc: 0.8857 - val_loss: 1.9813 - val_acc: 0.6154\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1453 - acc: 0.8857 - val_loss: 1.9834 - val_acc: 0.6154\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1450 - acc: 0.8857 - val_loss: 1.9853 - val_acc: 0.6154\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1447 - acc: 0.8857 - val_loss: 1.9872 - val_acc: 0.6154\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1444 - acc: 0.8857 - val_loss: 1.9894 - val_acc: 0.6154\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1441 - acc: 0.8857 - val_loss: 1.9916 - val_acc: 0.6154\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.1438 - acc: 0.8857 - val_loss: 1.9936 - val_acc: 0.6154\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.1434 - acc: 0.8857 - val_loss: 1.9955 - val_acc: 0.6154\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.1431 - acc: 0.8857 - val_loss: 1.9977 - val_acc: 0.6154\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1429 - acc: 0.8857 - val_loss: 1.9998 - val_acc: 0.6154\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.1426 - acc: 0.8857 - val_loss: 2.0019 - val_acc: 0.6154\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1423 - acc: 0.8857 - val_loss: 2.0040 - val_acc: 0.6154\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1420 - acc: 0.8857 - val_loss: 2.0063 - val_acc: 0.6154\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1417 - acc: 0.8857 - val_loss: 2.0083 - val_acc: 0.6154\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.1414 - acc: 0.8857 - val_loss: 2.0100 - val_acc: 0.6154\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1411 - acc: 0.8857 - val_loss: 2.0118 - val_acc: 0.6154\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1407 - acc: 0.8857 - val_loss: 2.0135 - val_acc: 0.6154\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1404 - acc: 0.8857 - val_loss: 2.0154 - val_acc: 0.6154\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1401 - acc: 0.8857 - val_loss: 2.0173 - val_acc: 0.6154\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.1397 - acc: 0.8857 - val_loss: 2.0192 - val_acc: 0.6154\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1395 - acc: 0.8857 - val_loss: 2.0212 - val_acc: 0.6154\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.1392 - acc: 0.8857 - val_loss: 2.0232 - val_acc: 0.6154\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 0.1389 - acc: 0.8857 - val_loss: 2.0252 - val_acc: 0.6154\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.1387 - acc: 0.8857 - val_loss: 2.0274 - val_acc: 0.6154\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1384 - acc: 0.8857 - val_loss: 2.0295 - val_acc: 0.6154\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.1381 - acc: 0.8857 - val_loss: 2.0315 - val_acc: 0.6154\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1379 - acc: 0.8857 - val_loss: 2.0335 - val_acc: 0.6154\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1376 - acc: 0.8857 - val_loss: 2.0353 - val_acc: 0.6154\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1372 - acc: 0.8857 - val_loss: 2.0370 - val_acc: 0.6154\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1370 - acc: 0.8857 - val_loss: 2.0390 - val_acc: 0.6154\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1367 - acc: 0.8857 - val_loss: 2.0411 - val_acc: 0.6154\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.1364 - acc: 0.8857 - val_loss: 2.0428 - val_acc: 0.6154\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1361 - acc: 0.8857 - val_loss: 2.0444 - val_acc: 0.6154\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1358 - acc: 0.8857 - val_loss: 2.0463 - val_acc: 0.6154\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1355 - acc: 0.8857 - val_loss: 2.0482 - val_acc: 0.6154\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1352 - acc: 0.8857 - val_loss: 2.0500 - val_acc: 0.6154\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1349 - acc: 0.8857 - val_loss: 2.0517 - val_acc: 0.6154\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.1346 - acc: 0.8857 - val_loss: 2.0538 - val_acc: 0.6154\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1344 - acc: 0.8857 - val_loss: 2.0558 - val_acc: 0.6154\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1341 - acc: 0.8857 - val_loss: 2.0577 - val_acc: 0.6154\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.1339 - acc: 0.8857 - val_loss: 2.0594 - val_acc: 0.6154\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.1335 - acc: 0.8857 - val_loss: 2.0612 - val_acc: 0.6154\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1333 - acc: 0.8857 - val_loss: 2.0631 - val_acc: 0.6154\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.1330 - acc: 0.8952 - val_loss: 2.0649 - val_acc: 0.6154\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 244us/step - loss: 0.1328 - acc: 0.8952 - val_loss: 2.0668 - val_acc: 0.6154\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1325 - acc: 0.8952 - val_loss: 2.0689 - val_acc: 0.6154\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.1323 - acc: 0.8952 - val_loss: 2.0707 - val_acc: 0.6154\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1320 - acc: 0.8952 - val_loss: 2.0725 - val_acc: 0.6154\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1318 - acc: 0.8952 - val_loss: 2.0744 - val_acc: 0.6154\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.1315 - acc: 0.8952 - val_loss: 2.0763 - val_acc: 0.6154\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1313 - acc: 0.8952 - val_loss: 2.0782 - val_acc: 0.6154\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1310 - acc: 0.8952 - val_loss: 2.0802 - val_acc: 0.6154\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1308 - acc: 0.8952 - val_loss: 2.0824 - val_acc: 0.6154\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1305 - acc: 0.8952 - val_loss: 2.0844 - val_acc: 0.6154\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1303 - acc: 0.8952 - val_loss: 2.0863 - val_acc: 0.6154\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1301 - acc: 0.8952 - val_loss: 2.0881 - val_acc: 0.6154\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 274us/step - loss: 0.1298 - acc: 0.8952 - val_loss: 2.0900 - val_acc: 0.6154\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1296 - acc: 0.8952 - val_loss: 2.0919 - val_acc: 0.6154\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1293 - acc: 0.8952 - val_loss: 2.0938 - val_acc: 0.6154\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1291 - acc: 0.8952 - val_loss: 2.0956 - val_acc: 0.6154\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1289 - acc: 0.8952 - val_loss: 2.0974 - val_acc: 0.6154\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1286 - acc: 0.8952 - val_loss: 2.0993 - val_acc: 0.6154\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.1284 - acc: 0.8952 - val_loss: 2.1011 - val_acc: 0.6154\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1281 - acc: 0.8952 - val_loss: 2.1028 - val_acc: 0.6154\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1278 - acc: 0.8952 - val_loss: 2.1045 - val_acc: 0.6154\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.1275 - acc: 0.8952 - val_loss: 2.1062 - val_acc: 0.6154\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1273 - acc: 0.8952 - val_loss: 2.1082 - val_acc: 0.6154\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1270 - acc: 0.8952 - val_loss: 2.1101 - val_acc: 0.6154\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1268 - acc: 0.8952 - val_loss: 2.1121 - val_acc: 0.6154\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.1266 - acc: 0.8952 - val_loss: 2.1138 - val_acc: 0.6154\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1263 - acc: 0.8952 - val_loss: 2.1155 - val_acc: 0.6154\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1260 - acc: 0.8952 - val_loss: 2.1173 - val_acc: 0.6154\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.1258 - acc: 0.8952 - val_loss: 2.1193 - val_acc: 0.6154\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1256 - acc: 0.8952 - val_loss: 2.1210 - val_acc: 0.6154\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.1253 - acc: 0.8952 - val_loss: 2.1229 - val_acc: 0.6154\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.1251 - acc: 0.8952 - val_loss: 2.1248 - val_acc: 0.6154\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.1249 - acc: 0.8952 - val_loss: 2.1267 - val_acc: 0.6154\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1246 - acc: 0.8952 - val_loss: 2.1286 - val_acc: 0.6154\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1244 - acc: 0.8952 - val_loss: 2.1304 - val_acc: 0.6154\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.1242 - acc: 0.8952 - val_loss: 2.1323 - val_acc: 0.6154\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1240 - acc: 0.8952 - val_loss: 2.1339 - val_acc: 0.6154\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1236 - acc: 0.8952 - val_loss: 2.1354 - val_acc: 0.6154\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1233 - acc: 0.8952 - val_loss: 2.1370 - val_acc: 0.6154\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1230 - acc: 0.8952 - val_loss: 2.1386 - val_acc: 0.6154\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1228 - acc: 0.8952 - val_loss: 2.1403 - val_acc: 0.6154\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1226 - acc: 0.8952 - val_loss: 2.1421 - val_acc: 0.6154\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1224 - acc: 0.8952 - val_loss: 2.1438 - val_acc: 0.6154\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.1221 - acc: 0.8952 - val_loss: 2.1457 - val_acc: 0.6154\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.1219 - acc: 0.8952 - val_loss: 2.1477 - val_acc: 0.6154\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1217 - acc: 0.8952 - val_loss: 2.1496 - val_acc: 0.6154\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1215 - acc: 0.8952 - val_loss: 2.1514 - val_acc: 0.6154\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 198us/step - loss: 0.1213 - acc: 0.8952 - val_loss: 2.1533 - val_acc: 0.6154\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1210 - acc: 0.8952 - val_loss: 2.1550 - val_acc: 0.6154\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1208 - acc: 0.8952 - val_loss: 2.1567 - val_acc: 0.6154\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 207us/step - loss: 0.1206 - acc: 0.8952 - val_loss: 2.1584 - val_acc: 0.6154\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.1204 - acc: 0.8952 - val_loss: 2.1602 - val_acc: 0.6154\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.1202 - acc: 0.8952 - val_loss: 2.1621 - val_acc: 0.6154\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.1200 - acc: 0.8952 - val_loss: 2.1640 - val_acc: 0.6154\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1197 - acc: 0.8952 - val_loss: 2.1659 - val_acc: 0.6154\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.1195 - acc: 0.8952 - val_loss: 2.1677 - val_acc: 0.6154\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 219us/step - loss: 0.1193 - acc: 0.8952 - val_loss: 2.1696 - val_acc: 0.6154\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.1191 - acc: 0.8952 - val_loss: 2.1713 - val_acc: 0.6154\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1189 - acc: 0.8952 - val_loss: 2.1731 - val_acc: 0.6154\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.1187 - acc: 0.8952 - val_loss: 2.1749 - val_acc: 0.6154\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1185 - acc: 0.8952 - val_loss: 2.1766 - val_acc: 0.6154\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1183 - acc: 0.8952 - val_loss: 2.1785 - val_acc: 0.6154\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1180 - acc: 0.8952 - val_loss: 2.1804 - val_acc: 0.6154\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.1178 - acc: 0.8952 - val_loss: 2.1822 - val_acc: 0.6154\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 206us/step - loss: 0.1176 - acc: 0.8952 - val_loss: 2.1839 - val_acc: 0.6154\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1173 - acc: 0.8952 - val_loss: 2.1854 - val_acc: 0.6154\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.1171 - acc: 0.8952 - val_loss: 2.1872 - val_acc: 0.6154\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1169 - acc: 0.8952 - val_loss: 2.1889 - val_acc: 0.6154\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.1166 - acc: 0.8952 - val_loss: 2.1906 - val_acc: 0.6154\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1164 - acc: 0.8952 - val_loss: 2.1923 - val_acc: 0.6154\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.1162 - acc: 0.9048 - val_loss: 2.1940 - val_acc: 0.6154\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 153us/step - loss: 0.1160 - acc: 0.9048 - val_loss: 2.1956 - val_acc: 0.6154\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.1157 - acc: 0.9048 - val_loss: 2.1972 - val_acc: 0.6154\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1155 - acc: 0.9048 - val_loss: 2.1990 - val_acc: 0.6154\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1153 - acc: 0.9048 - val_loss: 2.2009 - val_acc: 0.6154\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1151 - acc: 0.9048 - val_loss: 2.2026 - val_acc: 0.6154\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1149 - acc: 0.9048 - val_loss: 2.2044 - val_acc: 0.6154\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1147 - acc: 0.9048 - val_loss: 2.2061 - val_acc: 0.6154\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1145 - acc: 0.9048 - val_loss: 2.2079 - val_acc: 0.6154\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.1143 - acc: 0.9048 - val_loss: 2.2096 - val_acc: 0.5385\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1141 - acc: 1.0000 - val_loss: 2.2115 - val_acc: 0.5385\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.1139 - acc: 1.0000 - val_loss: 2.2133 - val_acc: 0.5385\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.1137 - acc: 1.0000 - val_loss: 2.2151 - val_acc: 0.5385\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.1135 - acc: 1.0000 - val_loss: 2.2170 - val_acc: 0.5385\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.1133 - acc: 1.0000 - val_loss: 2.2187 - val_acc: 0.5385\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 221us/step - loss: 0.1131 - acc: 1.0000 - val_loss: 2.2202 - val_acc: 0.5385\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1128 - acc: 1.0000 - val_loss: 2.2218 - val_acc: 0.5385\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 203us/step - loss: 0.1126 - acc: 1.0000 - val_loss: 2.2236 - val_acc: 0.5385\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 198us/step - loss: 0.1124 - acc: 1.0000 - val_loss: 2.2255 - val_acc: 0.5385\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 291us/step - loss: 0.1122 - acc: 1.0000 - val_loss: 2.2272 - val_acc: 0.5385\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 239us/step - loss: 0.1120 - acc: 1.0000 - val_loss: 2.2287 - val_acc: 0.5385\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.1118 - acc: 1.0000 - val_loss: 2.2304 - val_acc: 0.5385\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 226us/step - loss: 0.1116 - acc: 1.0000 - val_loss: 2.2323 - val_acc: 0.5385\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 203us/step - loss: 0.1114 - acc: 1.0000 - val_loss: 2.2341 - val_acc: 0.5385\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 204us/step - loss: 0.1112 - acc: 1.0000 - val_loss: 2.2358 - val_acc: 0.5385\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 205us/step - loss: 0.1109 - acc: 1.0000 - val_loss: 2.2373 - val_acc: 0.5385\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1107 - acc: 1.0000 - val_loss: 2.2391 - val_acc: 0.5385\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1105 - acc: 1.0000 - val_loss: 2.2409 - val_acc: 0.5385\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1103 - acc: 1.0000 - val_loss: 2.2425 - val_acc: 0.5385\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.1101 - acc: 1.0000 - val_loss: 2.2441 - val_acc: 0.5385\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.1100 - acc: 1.0000 - val_loss: 2.2458 - val_acc: 0.5385\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.1098 - acc: 1.0000 - val_loss: 2.2475 - val_acc: 0.5385\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.1096 - acc: 1.0000 - val_loss: 2.2493 - val_acc: 0.5385\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 198us/step - loss: 0.1094 - acc: 1.0000 - val_loss: 2.2511 - val_acc: 0.5385\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.1092 - acc: 1.0000 - val_loss: 2.2528 - val_acc: 0.5385\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 215us/step - loss: 0.1090 - acc: 1.0000 - val_loss: 2.2545 - val_acc: 0.5385\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 222us/step - loss: 0.1088 - acc: 1.0000 - val_loss: 2.2561 - val_acc: 0.5385\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.1086 - acc: 1.0000 - val_loss: 2.2578 - val_acc: 0.5385\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1085 - acc: 1.0000 - val_loss: 2.2595 - val_acc: 0.5385\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.1083 - acc: 1.0000 - val_loss: 2.2612 - val_acc: 0.5385\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.1081 - acc: 1.0000 - val_loss: 2.2629 - val_acc: 0.5385\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.1079 - acc: 1.0000 - val_loss: 2.2646 - val_acc: 0.5385\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1077 - acc: 1.0000 - val_loss: 2.2663 - val_acc: 0.5385\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1075 - acc: 1.0000 - val_loss: 2.2679 - val_acc: 0.5385\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1073 - acc: 1.0000 - val_loss: 2.2696 - val_acc: 0.5385\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.1072 - acc: 1.0000 - val_loss: 2.2712 - val_acc: 0.5385\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1069 - acc: 1.0000 - val_loss: 2.2727 - val_acc: 0.5385\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.1067 - acc: 1.0000 - val_loss: 2.2744 - val_acc: 0.5385\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.1065 - acc: 1.0000 - val_loss: 2.2760 - val_acc: 0.5385\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1064 - acc: 1.0000 - val_loss: 2.2776 - val_acc: 0.5385\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.1062 - acc: 1.0000 - val_loss: 2.2793 - val_acc: 0.5385\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.1060 - acc: 1.0000 - val_loss: 2.2809 - val_acc: 0.5385\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1057 - acc: 1.0000 - val_loss: 2.2823 - val_acc: 0.5385\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.1056 - acc: 1.0000 - val_loss: 2.2840 - val_acc: 0.5385\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1054 - acc: 1.0000 - val_loss: 2.2855 - val_acc: 0.5385\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.1051 - acc: 1.0000 - val_loss: 2.2869 - val_acc: 0.5385\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.1050 - acc: 1.0000 - val_loss: 2.2885 - val_acc: 0.5385\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.1048 - acc: 1.0000 - val_loss: 2.2900 - val_acc: 0.5385\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.1046 - acc: 1.0000 - val_loss: 2.2917 - val_acc: 0.5385\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.1044 - acc: 1.0000 - val_loss: 2.2935 - val_acc: 0.5385\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.1042 - acc: 1.0000 - val_loss: 2.2952 - val_acc: 0.5385\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.1041 - acc: 1.0000 - val_loss: 2.2968 - val_acc: 0.5385\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.1039 - acc: 1.0000 - val_loss: 2.2985 - val_acc: 0.5385\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.1037 - acc: 1.0000 - val_loss: 2.2999 - val_acc: 0.5385\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.1035 - acc: 1.0000 - val_loss: 2.3014 - val_acc: 0.5385\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.1033 - acc: 1.0000 - val_loss: 2.3031 - val_acc: 0.5385\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1031 - acc: 1.0000 - val_loss: 2.3046 - val_acc: 0.5385\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.1030 - acc: 1.0000 - val_loss: 2.3062 - val_acc: 0.5385\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1028 - acc: 1.0000 - val_loss: 2.3079 - val_acc: 0.5385\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 204us/step - loss: 0.1026 - acc: 1.0000 - val_loss: 2.3093 - val_acc: 0.5385\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.1024 - acc: 1.0000 - val_loss: 2.3106 - val_acc: 0.5385\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 226us/step - loss: 0.1021 - acc: 1.0000 - val_loss: 2.3121 - val_acc: 0.5385\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 222us/step - loss: 0.1020 - acc: 1.0000 - val_loss: 2.3136 - val_acc: 0.5385\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 193us/step - loss: 0.1018 - acc: 1.0000 - val_loss: 2.3150 - val_acc: 0.5385\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.1016 - acc: 1.0000 - val_loss: 2.3164 - val_acc: 0.5385\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 208us/step - loss: 0.1014 - acc: 1.0000 - val_loss: 2.3179 - val_acc: 0.5385\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 242us/step - loss: 0.1012 - acc: 1.0000 - val_loss: 2.3195 - val_acc: 0.5385\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.1010 - acc: 1.0000 - val_loss: 2.3212 - val_acc: 0.5385\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.1009 - acc: 1.0000 - val_loss: 2.3226 - val_acc: 0.5385\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 204us/step - loss: 0.1007 - acc: 1.0000 - val_loss: 2.3241 - val_acc: 0.5385\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 205us/step - loss: 0.1005 - acc: 1.0000 - val_loss: 2.3256 - val_acc: 0.5385\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.1004 - acc: 1.0000 - val_loss: 2.3271 - val_acc: 0.5385\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.1002 - acc: 1.0000 - val_loss: 2.3287 - val_acc: 0.5385\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.1001 - acc: 1.0000 - val_loss: 2.3303 - val_acc: 0.5385\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0999 - acc: 1.0000 - val_loss: 2.3319 - val_acc: 0.5385\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 211us/step - loss: 0.0997 - acc: 1.0000 - val_loss: 2.3334 - val_acc: 0.5385\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0996 - acc: 1.0000 - val_loss: 2.3349 - val_acc: 0.5385\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.0993 - acc: 1.0000 - val_loss: 2.3362 - val_acc: 0.5385\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.0991 - acc: 1.0000 - val_loss: 2.3376 - val_acc: 0.5385\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0989 - acc: 1.0000 - val_loss: 2.3392 - val_acc: 0.5385\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0988 - acc: 1.0000 - val_loss: 2.3407 - val_acc: 0.5385\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0986 - acc: 1.0000 - val_loss: 2.3421 - val_acc: 0.5385\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0985 - acc: 1.0000 - val_loss: 2.3436 - val_acc: 0.5385\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0983 - acc: 1.0000 - val_loss: 2.3451 - val_acc: 0.5385\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0981 - acc: 1.0000 - val_loss: 2.3465 - val_acc: 0.5385\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0980 - acc: 1.0000 - val_loss: 2.3478 - val_acc: 0.5385\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0977 - acc: 1.0000 - val_loss: 2.3491 - val_acc: 0.5385\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.0976 - acc: 1.0000 - val_loss: 2.3508 - val_acc: 0.5385\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.0974 - acc: 1.0000 - val_loss: 2.3524 - val_acc: 0.5385\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0973 - acc: 1.0000 - val_loss: 2.3538 - val_acc: 0.5385\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.0971 - acc: 1.0000 - val_loss: 2.3553 - val_acc: 0.5385\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0970 - acc: 1.0000 - val_loss: 2.3568 - val_acc: 0.5385\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0968 - acc: 1.0000 - val_loss: 2.3585 - val_acc: 0.5385\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0966 - acc: 1.0000 - val_loss: 2.3601 - val_acc: 0.5385\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.0965 - acc: 1.0000 - val_loss: 2.3617 - val_acc: 0.5385\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0963 - acc: 1.0000 - val_loss: 2.3631 - val_acc: 0.5385\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.0962 - acc: 1.0000 - val_loss: 2.3646 - val_acc: 0.5385\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 203us/step - loss: 0.0960 - acc: 1.0000 - val_loss: 2.3659 - val_acc: 0.5385\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0958 - acc: 1.0000 - val_loss: 2.3672 - val_acc: 0.5385\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 207us/step - loss: 0.0956 - acc: 1.0000 - val_loss: 2.3687 - val_acc: 0.5385\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0955 - acc: 1.0000 - val_loss: 2.3701 - val_acc: 0.5385\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0953 - acc: 1.0000 - val_loss: 2.3716 - val_acc: 0.5385\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0951 - acc: 1.0000 - val_loss: 2.3730 - val_acc: 0.5385\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0949 - acc: 1.0000 - val_loss: 2.3746 - val_acc: 0.5385\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0948 - acc: 1.0000 - val_loss: 2.3762 - val_acc: 0.5385\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0946 - acc: 1.0000 - val_loss: 2.3778 - val_acc: 0.5385\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0945 - acc: 1.0000 - val_loss: 2.3792 - val_acc: 0.5385\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.0943 - acc: 1.0000 - val_loss: 2.3805 - val_acc: 0.5385\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0941 - acc: 1.0000 - val_loss: 2.3820 - val_acc: 0.5385\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.0940 - acc: 1.0000 - val_loss: 2.3834 - val_acc: 0.5385\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0938 - acc: 1.0000 - val_loss: 2.3848 - val_acc: 0.5385\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0937 - acc: 1.0000 - val_loss: 2.3863 - val_acc: 0.5385\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 220us/step - loss: 0.0935 - acc: 1.0000 - val_loss: 2.3878 - val_acc: 0.5385\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0934 - acc: 1.0000 - val_loss: 2.3892 - val_acc: 0.5385\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.0932 - acc: 1.0000 - val_loss: 2.3906 - val_acc: 0.5385\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.0930 - acc: 1.0000 - val_loss: 2.3922 - val_acc: 0.5385\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0929 - acc: 1.0000 - val_loss: 2.3938 - val_acc: 0.5385\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0927 - acc: 1.0000 - val_loss: 2.3951 - val_acc: 0.5385\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0925 - acc: 1.0000 - val_loss: 2.3964 - val_acc: 0.5385\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.0923 - acc: 1.0000 - val_loss: 2.3976 - val_acc: 0.5385\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 222us/step - loss: 0.0921 - acc: 1.0000 - val_loss: 2.3990 - val_acc: 0.5385\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0919 - acc: 1.0000 - val_loss: 2.4003 - val_acc: 0.5385\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 235us/step - loss: 0.0917 - acc: 1.0000 - val_loss: 2.4016 - val_acc: 0.5385\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.0916 - acc: 1.0000 - val_loss: 2.4030 - val_acc: 0.5385\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.0914 - acc: 1.0000 - val_loss: 2.4044 - val_acc: 0.5385\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 0.0913 - acc: 1.0000 - val_loss: 2.4059 - val_acc: 0.5385\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0912 - acc: 1.0000 - val_loss: 2.4074 - val_acc: 0.5385\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0910 - acc: 1.0000 - val_loss: 2.4089 - val_acc: 0.5385\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0909 - acc: 1.0000 - val_loss: 2.4103 - val_acc: 0.5385\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0907 - acc: 1.0000 - val_loss: 2.4118 - val_acc: 0.5385\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0906 - acc: 1.0000 - val_loss: 2.4133 - val_acc: 0.5385\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0904 - acc: 1.0000 - val_loss: 2.4147 - val_acc: 0.5385\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0903 - acc: 1.0000 - val_loss: 2.4163 - val_acc: 0.5385\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.0902 - acc: 1.0000 - val_loss: 2.4177 - val_acc: 0.5385\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0900 - acc: 1.0000 - val_loss: 2.4192 - val_acc: 0.5385\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0899 - acc: 1.0000 - val_loss: 2.4207 - val_acc: 0.5385\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 149us/step - loss: 0.0897 - acc: 1.0000 - val_loss: 2.4221 - val_acc: 0.5385\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.0895 - acc: 1.0000 - val_loss: 2.4235 - val_acc: 0.5385\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0894 - acc: 1.0000 - val_loss: 2.4249 - val_acc: 0.5385\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 205us/step - loss: 0.0892 - acc: 1.0000 - val_loss: 2.4263 - val_acc: 0.5385\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 200us/step - loss: 0.0890 - acc: 1.0000 - val_loss: 2.4276 - val_acc: 0.5385\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.0889 - acc: 1.0000 - val_loss: 2.4290 - val_acc: 0.5385\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0888 - acc: 1.0000 - val_loss: 2.4304 - val_acc: 0.5385\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0886 - acc: 1.0000 - val_loss: 2.4319 - val_acc: 0.5385\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0885 - acc: 1.0000 - val_loss: 2.4334 - val_acc: 0.5385\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0884 - acc: 1.0000 - val_loss: 2.4348 - val_acc: 0.5385\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0882 - acc: 1.0000 - val_loss: 2.4361 - val_acc: 0.5385\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0880 - acc: 1.0000 - val_loss: 2.4373 - val_acc: 0.5385\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.0878 - acc: 1.0000 - val_loss: 2.4386 - val_acc: 0.5385\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.0877 - acc: 1.0000 - val_loss: 2.4400 - val_acc: 0.5385\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 207us/step - loss: 0.0876 - acc: 1.0000 - val_loss: 2.4413 - val_acc: 0.5385\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0874 - acc: 1.0000 - val_loss: 2.4428 - val_acc: 0.5385\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.0873 - acc: 1.0000 - val_loss: 2.4443 - val_acc: 0.5385\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0871 - acc: 1.0000 - val_loss: 2.4455 - val_acc: 0.5385\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0870 - acc: 1.0000 - val_loss: 2.4467 - val_acc: 0.5385\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0868 - acc: 1.0000 - val_loss: 2.4481 - val_acc: 0.5385\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.0867 - acc: 1.0000 - val_loss: 2.4495 - val_acc: 0.5385\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 202us/step - loss: 0.0866 - acc: 1.0000 - val_loss: 2.4510 - val_acc: 0.5385\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.0864 - acc: 1.0000 - val_loss: 2.4523 - val_acc: 0.5385\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.0863 - acc: 1.0000 - val_loss: 2.4537 - val_acc: 0.5385\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.0862 - acc: 1.0000 - val_loss: 2.4553 - val_acc: 0.5385\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0860 - acc: 1.0000 - val_loss: 2.4568 - val_acc: 0.5385\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0859 - acc: 1.0000 - val_loss: 2.4582 - val_acc: 0.5385\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.0858 - acc: 1.0000 - val_loss: 2.4596 - val_acc: 0.5385\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0856 - acc: 1.0000 - val_loss: 2.4610 - val_acc: 0.5385\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0855 - acc: 1.0000 - val_loss: 2.4624 - val_acc: 0.5385\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.0854 - acc: 1.0000 - val_loss: 2.4638 - val_acc: 0.5385\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0852 - acc: 1.0000 - val_loss: 2.4652 - val_acc: 0.5385\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0851 - acc: 1.0000 - val_loss: 2.4665 - val_acc: 0.5385\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.0849 - acc: 1.0000 - val_loss: 2.4677 - val_acc: 0.5385\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 211us/step - loss: 0.0848 - acc: 1.0000 - val_loss: 2.4691 - val_acc: 0.5385\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0847 - acc: 1.0000 - val_loss: 2.4704 - val_acc: 0.5385\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0845 - acc: 1.0000 - val_loss: 2.4717 - val_acc: 0.5385\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0844 - acc: 1.0000 - val_loss: 2.4732 - val_acc: 0.5385\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.0843 - acc: 1.0000 - val_loss: 2.4746 - val_acc: 0.5385\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0842 - acc: 1.0000 - val_loss: 2.4760 - val_acc: 0.5385\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.0840 - acc: 1.0000 - val_loss: 2.4774 - val_acc: 0.5385\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.0839 - acc: 1.0000 - val_loss: 2.4788 - val_acc: 0.5385\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0838 - acc: 1.0000 - val_loss: 2.4802 - val_acc: 0.5385\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0836 - acc: 1.0000 - val_loss: 2.4816 - val_acc: 0.5385\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.0835 - acc: 1.0000 - val_loss: 2.4829 - val_acc: 0.5385\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 219us/step - loss: 0.0833 - acc: 1.0000 - val_loss: 2.4842 - val_acc: 0.5385\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0832 - acc: 1.0000 - val_loss: 2.4856 - val_acc: 0.5385\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0831 - acc: 1.0000 - val_loss: 2.4869 - val_acc: 0.5385\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0830 - acc: 1.0000 - val_loss: 2.4882 - val_acc: 0.5385\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0828 - acc: 1.0000 - val_loss: 2.4895 - val_acc: 0.5385\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0827 - acc: 1.0000 - val_loss: 2.4910 - val_acc: 0.5385\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.0826 - acc: 1.0000 - val_loss: 2.4924 - val_acc: 0.5385\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0825 - acc: 1.0000 - val_loss: 2.4938 - val_acc: 0.5385\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0823 - acc: 1.0000 - val_loss: 2.4951 - val_acc: 0.5385\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.0822 - acc: 1.0000 - val_loss: 2.4964 - val_acc: 0.5385\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0821 - acc: 1.0000 - val_loss: 2.4978 - val_acc: 0.5385\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 191us/step - loss: 0.0820 - acc: 1.0000 - val_loss: 2.4991 - val_acc: 0.5385\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0818 - acc: 1.0000 - val_loss: 2.5004 - val_acc: 0.5385\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0817 - acc: 1.0000 - val_loss: 2.5017 - val_acc: 0.5385\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0816 - acc: 1.0000 - val_loss: 2.5030 - val_acc: 0.5385\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0814 - acc: 1.0000 - val_loss: 2.5044 - val_acc: 0.5385\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0813 - acc: 1.0000 - val_loss: 2.5056 - val_acc: 0.5385\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.0811 - acc: 1.0000 - val_loss: 2.5069 - val_acc: 0.5385\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0810 - acc: 1.0000 - val_loss: 2.5081 - val_acc: 0.5385\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.0808 - acc: 1.0000 - val_loss: 2.5094 - val_acc: 0.5385\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.0807 - acc: 1.0000 - val_loss: 2.5108 - val_acc: 0.5385\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 154us/step - loss: 0.0806 - acc: 1.0000 - val_loss: 2.5122 - val_acc: 0.5385\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0805 - acc: 1.0000 - val_loss: 2.5135 - val_acc: 0.5385\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.0804 - acc: 1.0000 - val_loss: 2.5148 - val_acc: 0.5385\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0802 - acc: 1.0000 - val_loss: 2.5160 - val_acc: 0.5385\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0801 - acc: 1.0000 - val_loss: 2.5173 - val_acc: 0.5385\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0800 - acc: 1.0000 - val_loss: 2.5187 - val_acc: 0.5385\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.0799 - acc: 1.0000 - val_loss: 2.5201 - val_acc: 0.5385\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0798 - acc: 1.0000 - val_loss: 2.5215 - val_acc: 0.5385\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0797 - acc: 1.0000 - val_loss: 2.5228 - val_acc: 0.5385\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0795 - acc: 1.0000 - val_loss: 2.5241 - val_acc: 0.5385\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0794 - acc: 1.0000 - val_loss: 2.5255 - val_acc: 0.5385\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 197us/step - loss: 0.0793 - acc: 1.0000 - val_loss: 2.5267 - val_acc: 0.5385\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 195us/step - loss: 0.0791 - acc: 1.0000 - val_loss: 2.5279 - val_acc: 0.5385\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0790 - acc: 1.0000 - val_loss: 2.5291 - val_acc: 0.5385\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0789 - acc: 1.0000 - val_loss: 2.5304 - val_acc: 0.5385\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0787 - acc: 1.0000 - val_loss: 2.5316 - val_acc: 0.5385\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.0786 - acc: 1.0000 - val_loss: 2.5330 - val_acc: 0.5385\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.0785 - acc: 1.0000 - val_loss: 2.5343 - val_acc: 0.5385\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0784 - acc: 1.0000 - val_loss: 2.5356 - val_acc: 0.5385\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0783 - acc: 1.0000 - val_loss: 2.5369 - val_acc: 0.5385\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.0782 - acc: 1.0000 - val_loss: 2.5381 - val_acc: 0.5385\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.0781 - acc: 1.0000 - val_loss: 2.5395 - val_acc: 0.5385\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0779 - acc: 1.0000 - val_loss: 2.5408 - val_acc: 0.5385\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0778 - acc: 1.0000 - val_loss: 2.5421 - val_acc: 0.5385\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0777 - acc: 1.0000 - val_loss: 2.5435 - val_acc: 0.5385\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0776 - acc: 1.0000 - val_loss: 2.5447 - val_acc: 0.5385\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0774 - acc: 1.0000 - val_loss: 2.5459 - val_acc: 0.5385\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.0773 - acc: 1.0000 - val_loss: 2.5473 - val_acc: 0.5385\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.0772 - acc: 1.0000 - val_loss: 2.5486 - val_acc: 0.5385\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.0771 - acc: 1.0000 - val_loss: 2.5498 - val_acc: 0.5385\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.0769 - acc: 1.0000 - val_loss: 2.5511 - val_acc: 0.5385\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.0768 - acc: 1.0000 - val_loss: 2.5524 - val_acc: 0.5385\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 148us/step - loss: 0.0767 - acc: 1.0000 - val_loss: 2.5536 - val_acc: 0.5385\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0766 - acc: 1.0000 - val_loss: 2.5546 - val_acc: 0.5385\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.0764 - acc: 1.0000 - val_loss: 2.5559 - val_acc: 0.5385\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.0763 - acc: 1.0000 - val_loss: 2.5572 - val_acc: 0.5385\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0762 - acc: 1.0000 - val_loss: 2.5584 - val_acc: 0.5385\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 211us/step - loss: 0.0761 - acc: 1.0000 - val_loss: 2.5597 - val_acc: 0.5385\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.0760 - acc: 1.0000 - val_loss: 2.5611 - val_acc: 0.5385\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.0758 - acc: 1.0000 - val_loss: 2.5624 - val_acc: 0.5385\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.0757 - acc: 1.0000 - val_loss: 2.5637 - val_acc: 0.5385\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0756 - acc: 1.0000 - val_loss: 2.5652 - val_acc: 0.5385\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 155us/step - loss: 0.0755 - acc: 1.0000 - val_loss: 2.5666 - val_acc: 0.5385\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.0754 - acc: 1.0000 - val_loss: 2.5679 - val_acc: 0.5385\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.0753 - acc: 1.0000 - val_loss: 2.5692 - val_acc: 0.5385\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.0752 - acc: 1.0000 - val_loss: 2.5705 - val_acc: 0.5385\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0751 - acc: 1.0000 - val_loss: 2.5718 - val_acc: 0.5385\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.0750 - acc: 1.0000 - val_loss: 2.5729 - val_acc: 0.5385\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0748 - acc: 1.0000 - val_loss: 2.5741 - val_acc: 0.5385\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.0747 - acc: 1.0000 - val_loss: 2.5755 - val_acc: 0.5385\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0746 - acc: 1.0000 - val_loss: 2.5768 - val_acc: 0.5385\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0745 - acc: 1.0000 - val_loss: 2.5781 - val_acc: 0.5385\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 156us/step - loss: 0.0744 - acc: 1.0000 - val_loss: 2.5793 - val_acc: 0.5385\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0743 - acc: 1.0000 - val_loss: 2.5805 - val_acc: 0.5385\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0742 - acc: 1.0000 - val_loss: 2.5819 - val_acc: 0.5385\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.0741 - acc: 1.0000 - val_loss: 2.5833 - val_acc: 0.5385\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0740 - acc: 1.0000 - val_loss: 2.5847 - val_acc: 0.5385\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0739 - acc: 1.0000 - val_loss: 2.5860 - val_acc: 0.5385\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0738 - acc: 1.0000 - val_loss: 2.5873 - val_acc: 0.5385\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 167us/step - loss: 0.0737 - acc: 1.0000 - val_loss: 2.5885 - val_acc: 0.5385\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0736 - acc: 1.0000 - val_loss: 2.5898 - val_acc: 0.5385\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 151us/step - loss: 0.0735 - acc: 1.0000 - val_loss: 2.5911 - val_acc: 0.5385\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0734 - acc: 1.0000 - val_loss: 2.5923 - val_acc: 0.5385\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0733 - acc: 1.0000 - val_loss: 2.5935 - val_acc: 0.5385\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.0731 - acc: 1.0000 - val_loss: 2.5949 - val_acc: 0.5385\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0730 - acc: 1.0000 - val_loss: 2.5961 - val_acc: 0.5385\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.0729 - acc: 1.0000 - val_loss: 2.5972 - val_acc: 0.5385\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.0728 - acc: 1.0000 - val_loss: 2.5985 - val_acc: 0.5385\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.0727 - acc: 1.0000 - val_loss: 2.5996 - val_acc: 0.5385\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0726 - acc: 1.0000 - val_loss: 2.6010 - val_acc: 0.5385\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 160us/step - loss: 0.0725 - acc: 1.0000 - val_loss: 2.6023 - val_acc: 0.5385\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 175us/step - loss: 0.0724 - acc: 1.0000 - val_loss: 2.6035 - val_acc: 0.5385\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0723 - acc: 1.0000 - val_loss: 2.6048 - val_acc: 0.5385\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0722 - acc: 1.0000 - val_loss: 2.6061 - val_acc: 0.5385\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0721 - acc: 1.0000 - val_loss: 2.6073 - val_acc: 0.5385\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.0720 - acc: 1.0000 - val_loss: 2.6085 - val_acc: 0.5385\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0719 - acc: 1.0000 - val_loss: 2.6099 - val_acc: 0.5385\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.0718 - acc: 1.0000 - val_loss: 2.6112 - val_acc: 0.5385\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 212us/step - loss: 0.0717 - acc: 1.0000 - val_loss: 2.6123 - val_acc: 0.5385\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 250us/step - loss: 0.0716 - acc: 1.0000 - val_loss: 2.6136 - val_acc: 0.5385\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0715 - acc: 1.0000 - val_loss: 2.6148 - val_acc: 0.5385\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0714 - acc: 1.0000 - val_loss: 2.6161 - val_acc: 0.5385\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0713 - acc: 1.0000 - val_loss: 2.6173 - val_acc: 0.5385\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0712 - acc: 1.0000 - val_loss: 2.6185 - val_acc: 0.5385\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.0711 - acc: 1.0000 - val_loss: 2.6197 - val_acc: 0.5385\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 183us/step - loss: 0.0710 - acc: 1.0000 - val_loss: 2.6210 - val_acc: 0.5385\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0709 - acc: 1.0000 - val_loss: 2.6224 - val_acc: 0.5385\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0708 - acc: 1.0000 - val_loss: 2.6236 - val_acc: 0.5385\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 150us/step - loss: 0.0707 - acc: 1.0000 - val_loss: 2.6247 - val_acc: 0.5385\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 163us/step - loss: 0.0706 - acc: 1.0000 - val_loss: 2.6258 - val_acc: 0.5385\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 210us/step - loss: 0.0705 - acc: 1.0000 - val_loss: 2.6270 - val_acc: 0.5385\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.0704 - acc: 1.0000 - val_loss: 2.6282 - val_acc: 0.5385\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0703 - acc: 1.0000 - val_loss: 2.6295 - val_acc: 0.5385\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.0702 - acc: 1.0000 - val_loss: 2.6306 - val_acc: 0.5385\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0701 - acc: 1.0000 - val_loss: 2.6318 - val_acc: 0.5385\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.0700 - acc: 1.0000 - val_loss: 2.6330 - val_acc: 0.5385\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0698 - acc: 1.0000 - val_loss: 2.6341 - val_acc: 0.5385\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0697 - acc: 1.0000 - val_loss: 2.6353 - val_acc: 0.5385\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.0696 - acc: 1.0000 - val_loss: 2.6365 - val_acc: 0.5385\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0696 - acc: 1.0000 - val_loss: 2.6377 - val_acc: 0.5385\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.0695 - acc: 1.0000 - val_loss: 2.6389 - val_acc: 0.5385\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0694 - acc: 1.0000 - val_loss: 2.6400 - val_acc: 0.5385\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 173us/step - loss: 0.0692 - acc: 1.0000 - val_loss: 2.6411 - val_acc: 0.5385\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0691 - acc: 1.0000 - val_loss: 2.6422 - val_acc: 0.5385\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0690 - acc: 1.0000 - val_loss: 2.6434 - val_acc: 0.5385\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 188us/step - loss: 0.0689 - acc: 1.0000 - val_loss: 2.6445 - val_acc: 0.5385\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 186us/step - loss: 0.0689 - acc: 1.0000 - val_loss: 2.6457 - val_acc: 0.5385\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 201us/step - loss: 0.0687 - acc: 1.0000 - val_loss: 2.6467 - val_acc: 0.5385\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 172us/step - loss: 0.0686 - acc: 1.0000 - val_loss: 2.6479 - val_acc: 0.5385\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 214us/step - loss: 0.0685 - acc: 1.0000 - val_loss: 2.6491 - val_acc: 0.5385\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 218us/step - loss: 0.0684 - acc: 1.0000 - val_loss: 2.6503 - val_acc: 0.5385\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 216us/step - loss: 0.0683 - acc: 1.0000 - val_loss: 2.6515 - val_acc: 0.5385\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 212us/step - loss: 0.0683 - acc: 1.0000 - val_loss: 2.6527 - val_acc: 0.5385\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0682 - acc: 1.0000 - val_loss: 2.6538 - val_acc: 0.5385\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0681 - acc: 1.0000 - val_loss: 2.6551 - val_acc: 0.5385\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.0680 - acc: 1.0000 - val_loss: 2.6564 - val_acc: 0.5385\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0679 - acc: 1.0000 - val_loss: 2.6577 - val_acc: 0.5385\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 194us/step - loss: 0.0678 - acc: 1.0000 - val_loss: 2.6589 - val_acc: 0.5385\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 184us/step - loss: 0.0677 - acc: 1.0000 - val_loss: 2.6601 - val_acc: 0.5385\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 196us/step - loss: 0.0676 - acc: 1.0000 - val_loss: 2.6613 - val_acc: 0.5385\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 207us/step - loss: 0.0675 - acc: 1.0000 - val_loss: 2.6625 - val_acc: 0.5385\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 177us/step - loss: 0.0674 - acc: 1.0000 - val_loss: 2.6637 - val_acc: 0.5385\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 182us/step - loss: 0.0673 - acc: 1.0000 - val_loss: 2.6648 - val_acc: 0.5385\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0673 - acc: 1.0000 - val_loss: 2.6660 - val_acc: 0.5385\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 162us/step - loss: 0.0672 - acc: 1.0000 - val_loss: 2.6672 - val_acc: 0.5385\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0671 - acc: 1.0000 - val_loss: 2.6683 - val_acc: 0.5385\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.0669 - acc: 1.0000 - val_loss: 2.6694 - val_acc: 0.5385\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0669 - acc: 1.0000 - val_loss: 2.6707 - val_acc: 0.5385\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.0668 - acc: 1.0000 - val_loss: 2.6719 - val_acc: 0.5385\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 174us/step - loss: 0.0667 - acc: 1.0000 - val_loss: 2.6731 - val_acc: 0.5385\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0666 - acc: 1.0000 - val_loss: 2.6743 - val_acc: 0.5385\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0665 - acc: 1.0000 - val_loss: 2.6754 - val_acc: 0.5385\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 204us/step - loss: 0.0664 - acc: 1.0000 - val_loss: 2.6765 - val_acc: 0.5385\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 181us/step - loss: 0.0663 - acc: 1.0000 - val_loss: 2.6776 - val_acc: 0.5385\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.0662 - acc: 1.0000 - val_loss: 2.6786 - val_acc: 0.5385\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0661 - acc: 1.0000 - val_loss: 2.6798 - val_acc: 0.5385\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0660 - acc: 1.0000 - val_loss: 2.6809 - val_acc: 0.5385\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 189us/step - loss: 0.0659 - acc: 1.0000 - val_loss: 2.6819 - val_acc: 0.5385\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0658 - acc: 1.0000 - val_loss: 2.6828 - val_acc: 0.5385\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.0657 - acc: 1.0000 - val_loss: 2.6837 - val_acc: 0.5385\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.0656 - acc: 1.0000 - val_loss: 2.6848 - val_acc: 0.5385\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 168us/step - loss: 0.0655 - acc: 1.0000 - val_loss: 2.6860 - val_acc: 0.5385\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.0654 - acc: 1.0000 - val_loss: 2.6873 - val_acc: 0.5385\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 180us/step - loss: 0.0653 - acc: 1.0000 - val_loss: 2.6885 - val_acc: 0.5385\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0652 - acc: 1.0000 - val_loss: 2.6896 - val_acc: 0.5385\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.0652 - acc: 1.0000 - val_loss: 2.6909 - val_acc: 0.5385\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0651 - acc: 1.0000 - val_loss: 2.6921 - val_acc: 0.5385\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 207us/step - loss: 0.0650 - acc: 1.0000 - val_loss: 2.6932 - val_acc: 0.5385\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 190us/step - loss: 0.0649 - acc: 1.0000 - val_loss: 2.6943 - val_acc: 0.5385\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0648 - acc: 1.0000 - val_loss: 2.6954 - val_acc: 0.5385\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 185us/step - loss: 0.0647 - acc: 1.0000 - val_loss: 2.6966 - val_acc: 0.5385\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 179us/step - loss: 0.0646 - acc: 1.0000 - val_loss: 2.6978 - val_acc: 0.5385\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 199us/step - loss: 0.0645 - acc: 1.0000 - val_loss: 2.6990 - val_acc: 0.5385\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 176us/step - loss: 0.0644 - acc: 1.0000 - val_loss: 2.7001 - val_acc: 0.5385\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.0644 - acc: 1.0000 - val_loss: 2.7013 - val_acc: 0.5385\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 170us/step - loss: 0.0643 - acc: 1.0000 - val_loss: 2.7025 - val_acc: 0.5385\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.0642 - acc: 1.0000 - val_loss: 2.7037 - val_acc: 0.5385\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 166us/step - loss: 0.0641 - acc: 1.0000 - val_loss: 2.7048 - val_acc: 0.5385\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 178us/step - loss: 0.0640 - acc: 1.0000 - val_loss: 2.7060 - val_acc: 0.5385\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 169us/step - loss: 0.0639 - acc: 1.0000 - val_loss: 2.7072 - val_acc: 0.5385\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 161us/step - loss: 0.0639 - acc: 1.0000 - val_loss: 2.7084 - val_acc: 0.5385\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.0638 - acc: 1.0000 - val_loss: 2.7095 - val_acc: 0.5385\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 171us/step - loss: 0.0637 - acc: 1.0000 - val_loss: 2.7106 - val_acc: 0.5385\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 158us/step - loss: 0.0636 - acc: 1.0000 - val_loss: 2.7118 - val_acc: 0.5385\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0635 - acc: 1.0000 - val_loss: 2.7129 - val_acc: 0.5385\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 152us/step - loss: 0.0634 - acc: 1.0000 - val_loss: 2.7140 - val_acc: 0.5385\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 157us/step - loss: 0.0634 - acc: 1.0000 - val_loss: 2.7151 - val_acc: 0.5385\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 159us/step - loss: 0.0633 - acc: 1.0000 - val_loss: 2.7163 - val_acc: 0.5385\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 187us/step - loss: 0.0632 - acc: 1.0000 - val_loss: 2.7173 - val_acc: 0.5385\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 165us/step - loss: 0.0631 - acc: 1.0000 - val_loss: 2.7185 - val_acc: 0.5385\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 192us/step - loss: 0.0630 - acc: 1.0000 - val_loss: 2.7196 - val_acc: 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0fc4ffd4-3621-4212-f3e1-db2ea286aa1f",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3c38e7fb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "acdb5528-5b73-45b7-fcad-a810e004d2cb",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3c38fc14e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "013bb9b9-0e63-4860-be0e-4688000e4bd1",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=105, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 1.0928 - acc: 0.8667\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0922 - acc: 0.8857\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0912 - acc: 0.8857\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0901 - acc: 0.8857\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0890 - acc: 0.8857\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.0878 - acc: 0.8857\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.0865 - acc: 0.8857\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0853 - acc: 0.8857\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0841 - acc: 0.8857\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0828 - acc: 0.8857\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 1.0816 - acc: 0.8857\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 1.0804 - acc: 0.8857\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0791 - acc: 0.8857\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0779 - acc: 0.8857\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0767 - acc: 0.8857\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0755 - acc: 0.8857\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0742 - acc: 0.8857\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0730 - acc: 0.8857\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0718 - acc: 0.8857\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0706 - acc: 0.8857\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0694 - acc: 0.8857\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0682 - acc: 0.8857\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0670 - acc: 0.8857\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0658 - acc: 0.8857\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0645 - acc: 0.8857\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0633 - acc: 0.8857\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0622 - acc: 0.8857\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0610 - acc: 0.8857\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0598 - acc: 0.8857\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0586 - acc: 0.8857\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0574 - acc: 0.8857\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.0562 - acc: 0.8857\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0550 - acc: 0.8857\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0538 - acc: 0.8857\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0526 - acc: 0.8857\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0515 - acc: 0.8857\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0503 - acc: 0.8857\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0491 - acc: 0.8857\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0479 - acc: 0.8857\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0468 - acc: 0.8857\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0456 - acc: 0.8857\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0444 - acc: 0.8857\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0433 - acc: 0.8857\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.0421 - acc: 0.8857\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0410 - acc: 0.8857\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0398 - acc: 0.8857\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0387 - acc: 0.8857\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0375 - acc: 0.8857\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0364 - acc: 0.8857\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0352 - acc: 0.8857\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0341 - acc: 0.8857\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0329 - acc: 0.8857\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0318 - acc: 0.8857\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0307 - acc: 0.8857\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0295 - acc: 0.8857\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0284 - acc: 0.8857\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0273 - acc: 0.8857\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.0261 - acc: 0.8857\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.0250 - acc: 0.8857\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0239 - acc: 0.8857\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0228 - acc: 0.8857\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.0217 - acc: 0.8857\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.0206 - acc: 0.8857\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0195 - acc: 0.8857\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0184 - acc: 0.8857\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0173 - acc: 0.8857\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0162 - acc: 0.8857\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0151 - acc: 0.8857\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0140 - acc: 0.8857\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.0130 - acc: 0.8857\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0119 - acc: 0.8857\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0108 - acc: 0.8857\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0097 - acc: 0.8857\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0086 - acc: 0.8857\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.0076 - acc: 0.8857\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0065 - acc: 0.8857\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0054 - acc: 0.8857\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0043 - acc: 0.8857\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0033 - acc: 0.8857\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0022 - acc: 0.8857\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.0011 - acc: 0.8857\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.0001 - acc: 0.8857\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9990 - acc: 0.8857\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9980 - acc: 0.8857\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9969 - acc: 0.8857\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9959 - acc: 0.8857\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9948 - acc: 0.8857\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.9938 - acc: 0.8857\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9927 - acc: 0.8857\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9917 - acc: 0.8857\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9906 - acc: 0.8857\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9896 - acc: 0.8857\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9886 - acc: 0.8857\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9875 - acc: 0.8857\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9865 - acc: 0.8857\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9855 - acc: 0.8857\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9844 - acc: 0.8857\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9834 - acc: 0.8857\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9824 - acc: 0.8857\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9814 - acc: 0.8857\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9803 - acc: 0.8857\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9793 - acc: 0.8857\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9783 - acc: 0.8857\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9773 - acc: 0.8857\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9763 - acc: 0.8857\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9753 - acc: 0.8857\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9743 - acc: 0.8857\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9732 - acc: 0.8857\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.9722 - acc: 0.8857\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9712 - acc: 0.8857\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9702 - acc: 0.8857\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9692 - acc: 0.8857\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9682 - acc: 0.8857\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9673 - acc: 0.8857\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9663 - acc: 0.8857\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9653 - acc: 0.8857\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9643 - acc: 0.8857\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9633 - acc: 0.8857\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9623 - acc: 0.8857\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9613 - acc: 0.8857\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9603 - acc: 0.8857\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9594 - acc: 0.8857\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9584 - acc: 0.8857\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9574 - acc: 0.8857\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9564 - acc: 0.8857\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9555 - acc: 0.8857\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9545 - acc: 0.8857\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9535 - acc: 0.8857\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9526 - acc: 0.8857\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9516 - acc: 0.8857\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9506 - acc: 0.8857\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9497 - acc: 0.8857\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9487 - acc: 0.8857\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9478 - acc: 0.8857\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9469 - acc: 0.8857\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9459 - acc: 0.8857\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9450 - acc: 0.8857\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9440 - acc: 0.8857\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9431 - acc: 0.8857\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9422 - acc: 0.8857\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.9412 - acc: 0.8857\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9403 - acc: 0.8857\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9394 - acc: 0.8857\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9385 - acc: 0.8857\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9375 - acc: 0.8857\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9366 - acc: 0.8857\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9357 - acc: 0.8857\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9348 - acc: 0.8857\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9339 - acc: 0.8857\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9329 - acc: 0.8857\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9320 - acc: 0.8857\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9311 - acc: 0.8857\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9302 - acc: 0.8857\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9293 - acc: 0.8857\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9284 - acc: 0.8857\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9275 - acc: 0.8857\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9266 - acc: 0.8857\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9257 - acc: 0.8857\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9248 - acc: 0.8857\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9239 - acc: 0.8857\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9230 - acc: 0.8857\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9221 - acc: 0.8857\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9212 - acc: 0.8857\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9203 - acc: 0.8857\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9194 - acc: 0.8857\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9185 - acc: 0.8857\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.9176 - acc: 0.8857\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9167 - acc: 0.8857\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9159 - acc: 0.8857\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9150 - acc: 0.8857\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9141 - acc: 0.8857\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9132 - acc: 0.8857\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9123 - acc: 0.8857\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.9115 - acc: 0.8857\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9106 - acc: 0.8857\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9097 - acc: 0.8857\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9088 - acc: 0.8857\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9080 - acc: 0.8857\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9071 - acc: 0.8857\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9062 - acc: 0.8857\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9054 - acc: 0.8857\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9045 - acc: 0.8857\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9037 - acc: 0.8857\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9028 - acc: 0.8857\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9019 - acc: 0.8857\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9011 - acc: 0.8857\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9002 - acc: 0.8857\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8994 - acc: 0.8857\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8985 - acc: 0.8857\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8977 - acc: 0.8857\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8968 - acc: 0.8857\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8960 - acc: 0.8857\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.8951 - acc: 0.8857\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8943 - acc: 0.8857\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.8935 - acc: 0.8857\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.8926 - acc: 0.8857\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.8918 - acc: 0.8857\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8909 - acc: 0.8857\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8901 - acc: 0.8857\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8893 - acc: 0.8857\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8885 - acc: 0.8857\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8876 - acc: 0.8857\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8868 - acc: 0.8857\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8860 - acc: 0.8857\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8851 - acc: 0.8857\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8843 - acc: 0.8857\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8835 - acc: 0.8857\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8827 - acc: 0.8857\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8819 - acc: 0.8857\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8810 - acc: 0.8857\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8802 - acc: 0.8857\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8794 - acc: 0.8857\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 90us/step - loss: 0.8786 - acc: 0.8857\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8778 - acc: 0.8857\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8770 - acc: 0.8857\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8762 - acc: 0.8857\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.8754 - acc: 0.8857\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.8746 - acc: 0.8857\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8738 - acc: 0.8857\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.8730 - acc: 0.8857\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8722 - acc: 0.8857\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8714 - acc: 0.8857\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8706 - acc: 0.8857\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8698 - acc: 0.8857\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8690 - acc: 0.8857\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8682 - acc: 0.8857\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.8674 - acc: 0.8857\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8666 - acc: 0.8857\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8658 - acc: 0.8857\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8651 - acc: 0.8857\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8643 - acc: 0.8857\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8635 - acc: 0.8857\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8627 - acc: 0.8857\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.8857\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8612 - acc: 0.8857\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8604 - acc: 0.8857\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8597 - acc: 0.8857\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8589 - acc: 0.8857\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8581 - acc: 0.8857\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8574 - acc: 0.8857\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8566 - acc: 0.8857\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8558 - acc: 0.8857\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8551 - acc: 0.8857\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8543 - acc: 0.8857\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8535 - acc: 0.8857\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8528 - acc: 0.8857\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8520 - acc: 0.8857\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.8513 - acc: 0.8857\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8505 - acc: 0.8857\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8498 - acc: 0.8857\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8490 - acc: 0.8857\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8483 - acc: 0.8857\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8475 - acc: 0.8857\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8468 - acc: 0.8857\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8460 - acc: 0.8857\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8453 - acc: 0.8857\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8446 - acc: 0.8857\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8438 - acc: 0.8857\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8431 - acc: 0.8857\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8424 - acc: 0.8857\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8416 - acc: 0.8857\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8409 - acc: 0.8857\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8402 - acc: 0.8857\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8394 - acc: 0.8857\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8387 - acc: 0.8857\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8380 - acc: 0.8857\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8373 - acc: 0.8857\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8365 - acc: 0.8857\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8358 - acc: 0.8857\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8351 - acc: 0.8857\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8344 - acc: 0.8857\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8337 - acc: 0.8857\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8329 - acc: 0.8857\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8322 - acc: 0.8857\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8315 - acc: 0.8857\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8308 - acc: 0.8857\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8301 - acc: 0.8857\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8294 - acc: 0.8857\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.8857\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8280 - acc: 0.8857\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8273 - acc: 0.8857\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8266 - acc: 0.8857\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8259 - acc: 0.8857\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8252 - acc: 0.8857\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8245 - acc: 0.8857\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8238 - acc: 0.8857\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8231 - acc: 0.8857\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8224 - acc: 0.8857\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8217 - acc: 0.8857\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8210 - acc: 0.8857\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8203 - acc: 0.8857\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8196 - acc: 0.8857\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8189 - acc: 0.8857\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8182 - acc: 0.8857\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8175 - acc: 0.8857\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8168 - acc: 0.8857\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8161 - acc: 0.8857\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8154 - acc: 0.8857\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8148 - acc: 0.8857\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.8141 - acc: 0.8857\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8134 - acc: 0.8857\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8127 - acc: 0.8857\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8121 - acc: 0.8857\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8114 - acc: 0.8857\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.8107 - acc: 0.8857\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.8100 - acc: 0.8857\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.8094 - acc: 0.8857\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8087 - acc: 0.8857\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8080 - acc: 0.8857\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8074 - acc: 0.8857\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8067 - acc: 0.8857\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8060 - acc: 0.8857\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8054 - acc: 0.8857\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8047 - acc: 0.8857\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8040 - acc: 0.8857\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8034 - acc: 0.8857\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8027 - acc: 0.8857\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.8021 - acc: 0.8857\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.8014 - acc: 0.8857\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.8008 - acc: 0.8857\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8001 - acc: 0.8857\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7994 - acc: 0.8857\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7988 - acc: 0.8857\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7981 - acc: 0.8857\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7975 - acc: 0.8857\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7969 - acc: 0.8857\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7962 - acc: 0.8857\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7956 - acc: 0.8857\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7949 - acc: 0.8857\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7943 - acc: 0.8857\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7936 - acc: 0.8857\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7930 - acc: 0.8857\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7924 - acc: 0.8857\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7917 - acc: 0.8857\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7911 - acc: 0.8857\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7904 - acc: 0.8857\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7898 - acc: 0.8857\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7892 - acc: 0.8857\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7885 - acc: 0.8857\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7879 - acc: 0.8857\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7873 - acc: 0.8857\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7867 - acc: 0.8857\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7860 - acc: 0.8857\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7854 - acc: 0.8857\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7848 - acc: 0.8857\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7841 - acc: 0.8857\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7835 - acc: 0.8857\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7829 - acc: 0.8857\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7823 - acc: 0.8857\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7816 - acc: 0.8857\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7810 - acc: 0.8857\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7804 - acc: 0.8857\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7798 - acc: 0.8857\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7792 - acc: 0.8857\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7786 - acc: 0.8857\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7779 - acc: 0.8857\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7773 - acc: 0.8857\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7767 - acc: 0.8857\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7761 - acc: 0.8857\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7755 - acc: 0.8857\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7749 - acc: 0.8857\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7743 - acc: 0.8857\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7737 - acc: 0.8857\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7730 - acc: 0.8857\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7724 - acc: 0.8857\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7718 - acc: 0.8857\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7712 - acc: 0.8857\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7706 - acc: 0.8857\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7700 - acc: 0.8857\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7694 - acc: 0.8857\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7688 - acc: 0.8857\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7682 - acc: 0.8857\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7676 - acc: 0.8857\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7670 - acc: 0.8857\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7664 - acc: 0.8857\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7658 - acc: 0.8857\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.7652 - acc: 0.8857\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7646 - acc: 0.8857\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7640 - acc: 0.8857\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7634 - acc: 0.8857\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7628 - acc: 0.8857\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7622 - acc: 0.8857\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7617 - acc: 0.8857\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7611 - acc: 0.8857\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7605 - acc: 0.8857\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7599 - acc: 0.8857\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7593 - acc: 0.8857\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7587 - acc: 0.8857\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7581 - acc: 0.8857\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7575 - acc: 0.8857\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7570 - acc: 0.8857\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7564 - acc: 0.8857\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7558 - acc: 0.8857\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7552 - acc: 0.8857\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7546 - acc: 0.8857\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7541 - acc: 0.8857\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7535 - acc: 0.8857\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7529 - acc: 0.8857\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7523 - acc: 0.8857\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7518 - acc: 0.8857\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7512 - acc: 0.8857\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7506 - acc: 0.8857\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7501 - acc: 0.8857\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7495 - acc: 0.8857\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.7489 - acc: 0.8857\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7483 - acc: 0.8857\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7478 - acc: 0.8857\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7472 - acc: 0.8857\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7466 - acc: 0.8857\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7461 - acc: 0.8857\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7455 - acc: 0.8857\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7449 - acc: 0.8857\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7444 - acc: 0.8857\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7438 - acc: 0.8857\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7433 - acc: 0.8857\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7427 - acc: 0.8857\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7421 - acc: 0.8857\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7416 - acc: 0.8857\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7410 - acc: 0.8857\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7405 - acc: 0.8857\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7399 - acc: 0.8857\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7394 - acc: 0.8857\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7388 - acc: 0.8857\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7383 - acc: 0.8857\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7377 - acc: 0.8857\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7372 - acc: 0.8857\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7366 - acc: 0.8857\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7361 - acc: 0.8857\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7355 - acc: 0.8857\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7350 - acc: 0.8857\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7344 - acc: 0.8857\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7339 - acc: 0.8857\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7333 - acc: 0.8857\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7328 - acc: 0.8857\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7322 - acc: 0.8857\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7317 - acc: 0.8857\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7312 - acc: 0.8857\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7306 - acc: 0.8857\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7301 - acc: 0.8857\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7295 - acc: 0.8857\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7290 - acc: 0.8857\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7285 - acc: 0.8857\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7279 - acc: 0.8857\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7274 - acc: 0.8857\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7269 - acc: 0.8857\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7263 - acc: 0.8857\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7258 - acc: 0.8857\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7253 - acc: 0.8857\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.7247 - acc: 0.8857\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7242 - acc: 0.8857\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7237 - acc: 0.8857\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7231 - acc: 0.8857\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7226 - acc: 0.8857\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7221 - acc: 0.8857\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7216 - acc: 0.8857\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7210 - acc: 0.8857\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7205 - acc: 0.8857\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7200 - acc: 0.8857\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7195 - acc: 0.8857\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7189 - acc: 0.8857\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7184 - acc: 0.8857\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7179 - acc: 0.8857\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7174 - acc: 0.8857\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7169 - acc: 0.8857\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.7163 - acc: 0.8857\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7158 - acc: 0.8857\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7153 - acc: 0.8857\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7148 - acc: 0.8857\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7143 - acc: 0.8857\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7138 - acc: 0.8857\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7132 - acc: 0.8857\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7127 - acc: 0.8857\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7122 - acc: 0.8857\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.7117 - acc: 0.8857\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7112 - acc: 0.8857\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7107 - acc: 0.8857\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7102 - acc: 0.8857\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7097 - acc: 0.8857\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.7092 - acc: 0.8857\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.7087 - acc: 0.8857\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7082 - acc: 0.8857\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7077 - acc: 0.8857\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7071 - acc: 0.8857\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7066 - acc: 0.8857\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7061 - acc: 0.8857\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7056 - acc: 0.8857\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7051 - acc: 0.8857\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7046 - acc: 0.8857\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7041 - acc: 0.8857\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7036 - acc: 0.8857\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.7031 - acc: 0.8857\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.7026 - acc: 0.8857\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7021 - acc: 0.8857\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.7016 - acc: 0.8857\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7012 - acc: 0.8857\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7007 - acc: 0.8857\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7002 - acc: 0.8857\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6997 - acc: 0.8857\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6992 - acc: 0.8857\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6987 - acc: 0.8857\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6982 - acc: 0.8857\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6977 - acc: 0.8857\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6972 - acc: 0.8857\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6967 - acc: 0.8857\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6962 - acc: 0.8857\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6957 - acc: 0.8857\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6953 - acc: 0.8857\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6948 - acc: 0.8857\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6943 - acc: 0.8857\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6938 - acc: 0.8857\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6933 - acc: 0.8857\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6928 - acc: 0.8857\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6924 - acc: 0.8857\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6919 - acc: 0.8857\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6914 - acc: 0.8857\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6909 - acc: 0.8857\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6904 - acc: 0.8857\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6900 - acc: 0.8857\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6895 - acc: 0.8857\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6890 - acc: 0.8857\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6885 - acc: 0.8857\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6880 - acc: 0.8857\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6876 - acc: 0.8857\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6871 - acc: 0.8857\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6866 - acc: 0.8857\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6861 - acc: 0.8857\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6857 - acc: 0.8857\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6852 - acc: 0.8857\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.6847 - acc: 0.8857\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6842 - acc: 0.8857\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6838 - acc: 0.8857\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6833 - acc: 0.8857\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6828 - acc: 0.8857\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6824 - acc: 0.8857\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.6819 - acc: 0.8857\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.6814 - acc: 0.8857\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6810 - acc: 0.8857\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.6805 - acc: 0.8857\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.6800 - acc: 0.8857\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6795 - acc: 0.8857\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6791 - acc: 0.8857\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6786 - acc: 0.8857\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6781 - acc: 0.8857\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6777 - acc: 0.8857\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6772 - acc: 0.8857\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6767 - acc: 0.8857\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6763 - acc: 0.8857\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6758 - acc: 0.8857\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6754 - acc: 0.8857\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6749 - acc: 0.8857\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6744 - acc: 0.8857\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6740 - acc: 0.8857\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6735 - acc: 0.8857\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.6730 - acc: 0.8857\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6726 - acc: 0.8857\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6721 - acc: 0.8857\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6717 - acc: 0.8857\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6712 - acc: 0.8857\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6708 - acc: 0.8857\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6703 - acc: 0.8857\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6698 - acc: 0.8857\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6694 - acc: 0.8857\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6689 - acc: 0.8857\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6685 - acc: 0.8857\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6680 - acc: 0.8857\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6676 - acc: 0.8857\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6671 - acc: 0.8857\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6667 - acc: 0.8857\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6662 - acc: 0.8857\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6658 - acc: 0.8857\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6653 - acc: 0.8857\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6649 - acc: 0.8857\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6644 - acc: 0.8857\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6640 - acc: 0.8857\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6635 - acc: 0.8857\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6631 - acc: 0.8857\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6626 - acc: 0.8857\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6622 - acc: 0.8857\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6617 - acc: 0.8857\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6613 - acc: 0.8857\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6608 - acc: 0.8857\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6604 - acc: 0.8857\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6599 - acc: 0.8857\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6595 - acc: 0.8857\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6591 - acc: 0.8857\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6586 - acc: 0.8857\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6582 - acc: 0.8857\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6577 - acc: 0.8857\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6573 - acc: 0.8857\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6569 - acc: 0.8857\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6564 - acc: 0.8857\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6560 - acc: 0.8857\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6555 - acc: 0.8857\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6551 - acc: 0.8857\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6547 - acc: 0.8857\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6542 - acc: 0.8857\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6538 - acc: 0.8857\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6534 - acc: 0.8857\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6529 - acc: 0.8857\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6525 - acc: 0.8857\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6521 - acc: 0.8857\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6516 - acc: 0.8857\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6512 - acc: 0.8857\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6508 - acc: 0.8857\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6503 - acc: 0.8857\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6499 - acc: 0.8857\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6495 - acc: 0.8857\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6490 - acc: 0.8857\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6486 - acc: 0.8857\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6482 - acc: 0.8857\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6478 - acc: 0.8857\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6473 - acc: 0.8857\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6469 - acc: 0.8857\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6465 - acc: 0.8857\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6460 - acc: 0.8857\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6456 - acc: 0.8857\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6452 - acc: 0.8857\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6448 - acc: 0.8857\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6443 - acc: 0.8857\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6439 - acc: 0.8857\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6435 - acc: 0.8857\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6430 - acc: 0.8857\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6426 - acc: 0.8857\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6422 - acc: 0.8857\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6418 - acc: 0.8857\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6413 - acc: 0.8857\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6409 - acc: 0.8857\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6405 - acc: 0.8857\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.6401 - acc: 0.8857\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.6396 - acc: 0.8857\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6392 - acc: 0.8857\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6388 - acc: 0.8857\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6384 - acc: 0.8857\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6380 - acc: 0.8857\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6375 - acc: 0.8857\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6371 - acc: 0.8857\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6367 - acc: 0.8857\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6363 - acc: 0.8857\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6358 - acc: 0.8857\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6354 - acc: 0.8857\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6350 - acc: 0.8857\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6346 - acc: 0.8857\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6342 - acc: 0.8857\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.6337 - acc: 0.8857\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6333 - acc: 0.8857\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6329 - acc: 0.8857\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6325 - acc: 0.8857\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6321 - acc: 0.8857\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6316 - acc: 0.8857\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6312 - acc: 0.8857\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6308 - acc: 0.8857\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6304 - acc: 0.8857\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6300 - acc: 0.8857\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6295 - acc: 0.8857\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6291 - acc: 0.8857\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6287 - acc: 0.8857\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6283 - acc: 0.8857\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6278 - acc: 0.8857\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6274 - acc: 0.8857\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6270 - acc: 0.8857\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6266 - acc: 0.8857\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6261 - acc: 0.8857\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6257 - acc: 0.8857\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6253 - acc: 0.8857\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6248 - acc: 0.8857\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6244 - acc: 0.8857\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6240 - acc: 0.8857\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6236 - acc: 0.8857\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6231 - acc: 0.8857\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6227 - acc: 0.8857\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 13us/step - loss: 0.6223 - acc: 0.8857\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6219 - acc: 0.8857\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6215 - acc: 0.8857\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6210 - acc: 0.8857\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6206 - acc: 0.8857\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6202 - acc: 0.8857\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6198 - acc: 0.8857\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6194 - acc: 0.8857\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6189 - acc: 0.8857\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6185 - acc: 0.8857\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6181 - acc: 0.8857\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6177 - acc: 0.8857\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6173 - acc: 0.8857\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6168 - acc: 0.8857\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6164 - acc: 0.8857\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6160 - acc: 0.8857\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6156 - acc: 0.8857\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6152 - acc: 0.8857\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6148 - acc: 0.8857\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6143 - acc: 0.8857\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6139 - acc: 0.8857\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6135 - acc: 0.8857\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6131 - acc: 0.8857\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6127 - acc: 0.8857\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6123 - acc: 0.8857\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6119 - acc: 0.8857\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6115 - acc: 0.8857\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6111 - acc: 0.8857\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6106 - acc: 0.8857\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6102 - acc: 0.8857\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6098 - acc: 0.8857\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6094 - acc: 0.8857\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.6090 - acc: 0.8857\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6086 - acc: 0.8857\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6082 - acc: 0.8857\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6078 - acc: 0.8857\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6074 - acc: 0.8857\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6070 - acc: 0.8857\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6066 - acc: 0.8857\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.6062 - acc: 0.8857\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6058 - acc: 0.8857\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6054 - acc: 0.8857\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6050 - acc: 0.8857\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6046 - acc: 0.8857\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6042 - acc: 0.8857\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6038 - acc: 0.8857\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6034 - acc: 0.8857\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6030 - acc: 0.8857\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.6026 - acc: 0.8857\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6022 - acc: 0.8857\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.6018 - acc: 0.8857\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6014 - acc: 0.8857\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6010 - acc: 0.8857\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6007 - acc: 0.8857\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6003 - acc: 0.8857\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5999 - acc: 0.8857\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5995 - acc: 0.8857\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5991 - acc: 0.8857\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5987 - acc: 0.8857\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5983 - acc: 0.8857\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5979 - acc: 0.8857\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5975 - acc: 0.8857\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5972 - acc: 0.8857\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5968 - acc: 0.8857\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5964 - acc: 0.8857\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5960 - acc: 0.8857\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5956 - acc: 0.8857\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5952 - acc: 0.8857\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5949 - acc: 0.8857\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5945 - acc: 0.8857\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5941 - acc: 0.8857\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5937 - acc: 0.8857\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5933 - acc: 0.8857\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5929 - acc: 0.8857\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5926 - acc: 0.8857\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5922 - acc: 0.8857\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5918 - acc: 0.8857\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5914 - acc: 0.8857\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5911 - acc: 0.8857\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5907 - acc: 0.8857\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5903 - acc: 0.8857\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5899 - acc: 0.8857\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5896 - acc: 0.8857\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5892 - acc: 0.8857\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5888 - acc: 0.8857\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5884 - acc: 0.8857\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5881 - acc: 0.8857\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5877 - acc: 0.8857\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5873 - acc: 0.8857\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5870 - acc: 0.8857\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5866 - acc: 0.8857\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5862 - acc: 0.8857\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.5858 - acc: 0.8857\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5855 - acc: 0.8857\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5851 - acc: 0.8857\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5847 - acc: 0.8857\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5844 - acc: 0.8857\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5840 - acc: 0.8857\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5836 - acc: 0.8857\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5833 - acc: 0.8857\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5829 - acc: 0.8857\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5825 - acc: 0.8857\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5822 - acc: 0.8857\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5818 - acc: 0.8857\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5814 - acc: 0.8857\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5811 - acc: 0.8857\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5807 - acc: 0.8857\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5803 - acc: 0.8857\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5800 - acc: 0.8857\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5796 - acc: 0.8857\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5793 - acc: 0.8857\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5789 - acc: 0.8857\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5785 - acc: 0.8857\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5782 - acc: 0.8857\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5778 - acc: 0.8857\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5775 - acc: 0.8857\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5771 - acc: 0.8857\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5767 - acc: 0.8857\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5764 - acc: 0.8857\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5760 - acc: 0.8857\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5757 - acc: 0.8857\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5753 - acc: 0.8857\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5750 - acc: 0.8857\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5746 - acc: 0.8857\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5742 - acc: 0.8857\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5739 - acc: 0.8857\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5735 - acc: 0.8857\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5732 - acc: 0.8857\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5728 - acc: 0.8857\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5725 - acc: 0.8857\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5721 - acc: 0.8857\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5718 - acc: 0.8857\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5714 - acc: 0.8857\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5711 - acc: 0.8857\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5707 - acc: 0.8857\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5704 - acc: 0.8857\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5700 - acc: 0.8857\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5697 - acc: 0.8857\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5694 - acc: 0.8857\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5690 - acc: 0.8857\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5687 - acc: 0.8857\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5683 - acc: 0.8857\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5680 - acc: 0.8857\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5676 - acc: 0.8857\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5673 - acc: 0.8857\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5670 - acc: 0.8857\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5666 - acc: 0.8857\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5663 - acc: 0.8857\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5659 - acc: 0.8857\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5656 - acc: 0.8857\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5653 - acc: 0.8857\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5649 - acc: 0.8857\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5646 - acc: 0.8857\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5643 - acc: 0.8857\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5639 - acc: 0.8857\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5636 - acc: 0.8857\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5633 - acc: 0.8857\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5629 - acc: 0.8857\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5626 - acc: 0.8857\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5623 - acc: 0.8857\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5619 - acc: 0.8857\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5616 - acc: 0.8857\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5613 - acc: 0.8857\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5609 - acc: 0.8857\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5606 - acc: 0.8857\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5603 - acc: 0.8857\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5600 - acc: 0.8857\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5596 - acc: 0.8857\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5593 - acc: 0.8857\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5590 - acc: 0.8857\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5586 - acc: 0.8857\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5583 - acc: 0.8857\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5580 - acc: 0.8857\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5577 - acc: 0.8857\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5574 - acc: 0.8857\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5570 - acc: 0.8857\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5567 - acc: 0.8857\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5564 - acc: 0.8857\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5561 - acc: 0.8857\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5557 - acc: 0.8857\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5554 - acc: 0.8857\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5551 - acc: 0.8857\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5548 - acc: 0.8857\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5545 - acc: 0.8857\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5542 - acc: 0.8857\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5538 - acc: 0.8857\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5535 - acc: 0.8857\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5532 - acc: 0.8857\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5529 - acc: 0.8857\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5526 - acc: 0.8857\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5523 - acc: 0.8857\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5520 - acc: 0.8857\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5516 - acc: 0.8857\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5513 - acc: 0.8857\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5510 - acc: 0.8857\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5507 - acc: 0.8857\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5504 - acc: 0.8857\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5501 - acc: 0.8857\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5498 - acc: 0.8857\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5495 - acc: 0.8857\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.5492 - acc: 0.8857\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5489 - acc: 0.8857\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5485 - acc: 0.8857\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5482 - acc: 0.8857\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5479 - acc: 0.8857\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5476 - acc: 0.8857\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5473 - acc: 0.8857\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5470 - acc: 0.8857\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5467 - acc: 0.8857\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5464 - acc: 0.8857\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5461 - acc: 0.8857\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5458 - acc: 0.8857\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5455 - acc: 0.8857\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5452 - acc: 0.8857\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5449 - acc: 0.8857\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5446 - acc: 0.8857\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5443 - acc: 0.8857\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5440 - acc: 0.8857\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5437 - acc: 0.8857\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5434 - acc: 0.8857\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5431 - acc: 0.8857\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5428 - acc: 0.8857\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5425 - acc: 0.8857\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5422 - acc: 0.8857\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5419 - acc: 0.8857\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5416 - acc: 0.8857\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5414 - acc: 0.8857\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5411 - acc: 0.8857\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5408 - acc: 0.8857\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5405 - acc: 0.8857\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5402 - acc: 0.8857\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5399 - acc: 0.8857\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5396 - acc: 0.8857\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5393 - acc: 0.8857\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5390 - acc: 0.8857\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5387 - acc: 0.8857\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5384 - acc: 0.8857\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5382 - acc: 0.8857\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5379 - acc: 0.8857\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5376 - acc: 0.8857\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5373 - acc: 0.8857\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5370 - acc: 0.8857\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5367 - acc: 0.8857\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5364 - acc: 0.8857\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5362 - acc: 0.8857\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5359 - acc: 0.8857\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5356 - acc: 0.8857\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5353 - acc: 0.8857\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5350 - acc: 0.8857\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5348 - acc: 0.8857\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5345 - acc: 0.8857\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5342 - acc: 0.8857\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5339 - acc: 0.8857\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5336 - acc: 0.8857\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5334 - acc: 0.8857\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5331 - acc: 0.8857\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5328 - acc: 0.8857\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5325 - acc: 0.8857\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5322 - acc: 0.8857\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5320 - acc: 0.8857\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5317 - acc: 0.8857\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5314 - acc: 0.8857\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5311 - acc: 0.8857\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5309 - acc: 0.8857\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5306 - acc: 0.8857\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5303 - acc: 0.8857\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5300 - acc: 0.8857\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5298 - acc: 0.8857\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5295 - acc: 0.8857\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5292 - acc: 0.8857\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5289 - acc: 0.8857\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5287 - acc: 0.8857\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5284 - acc: 0.8857\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5281 - acc: 0.8857\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5279 - acc: 0.8857\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5276 - acc: 0.8857\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5273 - acc: 0.8857\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5271 - acc: 0.8857\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5268 - acc: 0.8857\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5265 - acc: 0.8857\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5263 - acc: 0.8857\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5260 - acc: 0.8857\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5257 - acc: 0.8857\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5255 - acc: 0.8857\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5252 - acc: 0.8857\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5249 - acc: 0.8857\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5247 - acc: 0.8857\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5244 - acc: 0.8857\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5241 - acc: 0.8857\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5239 - acc: 0.8857\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5236 - acc: 0.8857\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5233 - acc: 0.8857\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5231 - acc: 0.8857\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5228 - acc: 0.8857\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5226 - acc: 0.8857\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5223 - acc: 0.8857\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5220 - acc: 0.8857\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5218 - acc: 0.8857\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5215 - acc: 0.8857\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5213 - acc: 0.8857\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.5210 - acc: 0.8857\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5207 - acc: 0.8857\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 14us/step - loss: 0.5205 - acc: 0.8857\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5202 - acc: 0.8857\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5200 - acc: 0.8857\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5197 - acc: 0.8857\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.5195 - acc: 0.8857\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5192 - acc: 0.8857\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5189 - acc: 0.8857\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5187 - acc: 0.8857\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5184 - acc: 0.8857\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5182 - acc: 0.8857\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5179 - acc: 0.8857\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.5177 - acc: 0.8857\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5174 - acc: 0.8857\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5172 - acc: 0.8857\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5169 - acc: 0.8857\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5167 - acc: 0.8857\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5164 - acc: 0.8857\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5162 - acc: 0.8857\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5159 - acc: 0.8857\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5157 - acc: 0.8857\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5154 - acc: 0.8857\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5152 - acc: 0.8857\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.5149 - acc: 0.8857\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5147 - acc: 0.8857\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5144 - acc: 0.8857\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5142 - acc: 0.8857\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5139 - acc: 0.8857\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5137 - acc: 0.8857\n",
            "13/13 [==============================] - 0s 23ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff7576f8-bef3-4d03-d19d-4ff558c89c12",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "28738a68-0eee-4160-cc82-541d912187a2",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZWMB9Smta3",
        "colab_type": "code",
        "outputId": "ad8f5b4f-4590-4cf7-8c44-0a5868e102f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=0.9, svd_solver='full')\n",
        "pca.fit(train_data_stand)\n",
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "val_data_stand_pca = pca.transform(val_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)\n",
        "train_data_stand_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xY_6GSELqt62"
      },
      "source": [
        "##Z-score dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yGFxr_Rzqt7C",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_pca = scaler_2.fit_transform(train_data_stand_pca)\n",
        "val_data_stand_pca = scaler_2.transform(val_data_stand_pca)\n",
        "test_data_stand_pca = scaler_2.transform(test_data_stand_pca)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cZJkkVO1qfR7"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pbXLDHyAqfSH",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "54cjt6jQqfSe",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KUZ-gNDwqfSu",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jG_v2EVGqfS6",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TBJjibPuqfTF",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OxjsDNt_qfTR",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oReRAccqrEtY"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O6mpn7ugrEti",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-uMZaxirEt2",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSsTXouFFW6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3YDEfMtrEuB",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xMmd6vmCrEuM",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8s8-_E4TrEuY",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(9,), kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkjlnTtdrEui",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "714e6ad1-0a94-4a09-8de6-ae6db00c2339",
        "id": "Ut6pUmx6rEuu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4451c311-89b8-49b3-e2ad-4474dda20eef",
        "id": "xVxJ7QLKrEu4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_pca, one_hot_train_labels, validation_data=(val_data_stand_pca, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=105)\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "105/105 [==============================] - 3s 27ms/step - loss: 1.2288 - acc: 0.3714 - val_loss: 1.1590 - val_acc: 0.5385\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 1.1973 - acc: 0.3524 - val_loss: 1.1482 - val_acc: 0.5385\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.1665 - acc: 0.4190 - val_loss: 1.1378 - val_acc: 0.5385\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.1567 - acc: 0.4476 - val_loss: 1.1277 - val_acc: 0.5385\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.1307 - acc: 0.4667 - val_loss: 1.1186 - val_acc: 0.5385\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.1641 - acc: 0.3524 - val_loss: 1.1103 - val_acc: 0.5385\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1336 - acc: 0.4476 - val_loss: 1.1021 - val_acc: 0.5385\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1309 - acc: 0.4571 - val_loss: 1.0938 - val_acc: 0.5385\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1387 - acc: 0.4000 - val_loss: 1.0860 - val_acc: 0.5385\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.1251 - acc: 0.4381 - val_loss: 1.0785 - val_acc: 0.5385\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.1069 - acc: 0.4476 - val_loss: 1.0713 - val_acc: 0.5385\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.1086 - acc: 0.4667 - val_loss: 1.0645 - val_acc: 0.5385\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0954 - acc: 0.5333 - val_loss: 1.0583 - val_acc: 0.5385\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0564 - acc: 0.5143 - val_loss: 1.0527 - val_acc: 0.5385\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0927 - acc: 0.4667 - val_loss: 1.0479 - val_acc: 0.5385\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0678 - acc: 0.5524 - val_loss: 1.0434 - val_acc: 0.5385\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0493 - acc: 0.5524 - val_loss: 1.0391 - val_acc: 0.5385\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0493 - acc: 0.5048 - val_loss: 1.0353 - val_acc: 0.4615\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0409 - acc: 0.5524 - val_loss: 1.0319 - val_acc: 0.4615\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0224 - acc: 0.5714 - val_loss: 1.0290 - val_acc: 0.4615\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0453 - acc: 0.4857 - val_loss: 1.0262 - val_acc: 0.4615\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0334 - acc: 0.5238 - val_loss: 1.0239 - val_acc: 0.4615\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0136 - acc: 0.5238 - val_loss: 1.0222 - val_acc: 0.4615\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0048 - acc: 0.5524 - val_loss: 1.0208 - val_acc: 0.4615\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0128 - acc: 0.5619 - val_loss: 1.0193 - val_acc: 0.3846\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0043 - acc: 0.5524 - val_loss: 1.0181 - val_acc: 0.3846\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9904 - acc: 0.5333 - val_loss: 1.0169 - val_acc: 0.3846\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9967 - acc: 0.6381 - val_loss: 1.0161 - val_acc: 0.3846\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.9961 - acc: 0.5714 - val_loss: 1.0153 - val_acc: 0.4615\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.9692 - acc: 0.6000 - val_loss: 1.0146 - val_acc: 0.4615\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.9797 - acc: 0.5810 - val_loss: 1.0140 - val_acc: 0.4615\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.9707 - acc: 0.5714 - val_loss: 1.0138 - val_acc: 0.4615\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.9609 - acc: 0.5905 - val_loss: 1.0139 - val_acc: 0.4615\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.9787 - acc: 0.5714 - val_loss: 1.0145 - val_acc: 0.4615\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.9704 - acc: 0.5619 - val_loss: 1.0152 - val_acc: 0.3846\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.9828 - acc: 0.5333 - val_loss: 1.0159 - val_acc: 0.3846\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.9608 - acc: 0.5714 - val_loss: 1.0166 - val_acc: 0.3846\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9595 - acc: 0.5524 - val_loss: 1.0177 - val_acc: 0.3846\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.9549 - acc: 0.6095 - val_loss: 1.0187 - val_acc: 0.3846\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.9455 - acc: 0.6571 - val_loss: 1.0202 - val_acc: 0.3846\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.9487 - acc: 0.6286 - val_loss: 1.0216 - val_acc: 0.3846\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9524 - acc: 0.6095 - val_loss: 1.0232 - val_acc: 0.3846\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.9240 - acc: 0.5905 - val_loss: 1.0250 - val_acc: 0.3846\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9214 - acc: 0.5905 - val_loss: 1.0269 - val_acc: 0.3846\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.9548 - acc: 0.6190 - val_loss: 1.0288 - val_acc: 0.3846\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9302 - acc: 0.6286 - val_loss: 1.0304 - val_acc: 0.3846\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9516 - acc: 0.5714 - val_loss: 1.0323 - val_acc: 0.3846\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.9104 - acc: 0.5619 - val_loss: 1.0345 - val_acc: 0.3846\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9143 - acc: 0.6190 - val_loss: 1.0367 - val_acc: 0.3846\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8959 - acc: 0.6000 - val_loss: 1.0391 - val_acc: 0.3846\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.9187 - acc: 0.5810 - val_loss: 1.0412 - val_acc: 0.3846\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.9434 - acc: 0.5714 - val_loss: 1.0435 - val_acc: 0.3846\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.9152 - acc: 0.6095 - val_loss: 1.0459 - val_acc: 0.3846\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8941 - acc: 0.6190 - val_loss: 1.0487 - val_acc: 0.3846\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9346 - acc: 0.5810 - val_loss: 1.0515 - val_acc: 0.3846\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9295 - acc: 0.5905 - val_loss: 1.0540 - val_acc: 0.3846\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8923 - acc: 0.6286 - val_loss: 1.0560 - val_acc: 0.3846\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.9143 - acc: 0.6095 - val_loss: 1.0580 - val_acc: 0.3846\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8971 - acc: 0.6571 - val_loss: 1.0603 - val_acc: 0.3846\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.9419 - acc: 0.5905 - val_loss: 1.0627 - val_acc: 0.3846\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8862 - acc: 0.6286 - val_loss: 1.0654 - val_acc: 0.3846\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.9013 - acc: 0.6381 - val_loss: 1.0675 - val_acc: 0.3846\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.8907 - acc: 0.6095 - val_loss: 1.0694 - val_acc: 0.4615\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.9046 - acc: 0.6286 - val_loss: 1.0711 - val_acc: 0.4615\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.8802 - acc: 0.6571 - val_loss: 1.0728 - val_acc: 0.4615\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8915 - acc: 0.6000 - val_loss: 1.0745 - val_acc: 0.4615\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9022 - acc: 0.6286 - val_loss: 1.0766 - val_acc: 0.4615\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8939 - acc: 0.6190 - val_loss: 1.0782 - val_acc: 0.4615\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.8977 - acc: 0.6190 - val_loss: 1.0801 - val_acc: 0.4615\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8665 - acc: 0.6095 - val_loss: 1.0820 - val_acc: 0.4615\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.8987 - acc: 0.6095 - val_loss: 1.0843 - val_acc: 0.4615\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.8490 - acc: 0.6476 - val_loss: 1.0872 - val_acc: 0.4615\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8689 - acc: 0.6381 - val_loss: 1.0896 - val_acc: 0.4615\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.8841 - acc: 0.6190 - val_loss: 1.0921 - val_acc: 0.4615\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.8870 - acc: 0.6190 - val_loss: 1.0941 - val_acc: 0.4615\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.8678 - acc: 0.6286 - val_loss: 1.0962 - val_acc: 0.4615\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8671 - acc: 0.6190 - val_loss: 1.0978 - val_acc: 0.4615\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8729 - acc: 0.6095 - val_loss: 1.0987 - val_acc: 0.4615\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8900 - acc: 0.6286 - val_loss: 1.0997 - val_acc: 0.4615\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.8612 - acc: 0.6286 - val_loss: 1.1011 - val_acc: 0.4615\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.8695 - acc: 0.6667 - val_loss: 1.1025 - val_acc: 0.4615\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8809 - acc: 0.6286 - val_loss: 1.1033 - val_acc: 0.4615\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8835 - acc: 0.6571 - val_loss: 1.1040 - val_acc: 0.4615\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.8298 - acc: 0.6476 - val_loss: 1.1044 - val_acc: 0.4615\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8437 - acc: 0.6381 - val_loss: 1.1053 - val_acc: 0.4615\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.8473 - acc: 0.6571 - val_loss: 1.1054 - val_acc: 0.4615\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8418 - acc: 0.6762 - val_loss: 1.1060 - val_acc: 0.4615\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8585 - acc: 0.6381 - val_loss: 1.1064 - val_acc: 0.4615\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.8431 - acc: 0.6571 - val_loss: 1.1070 - val_acc: 0.4615\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.8468 - acc: 0.6476 - val_loss: 1.1080 - val_acc: 0.4615\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8219 - acc: 0.6762 - val_loss: 1.1091 - val_acc: 0.4615\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 85us/step - loss: 0.8674 - acc: 0.6190 - val_loss: 1.1100 - val_acc: 0.4615\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.8173 - acc: 0.6571 - val_loss: 1.1106 - val_acc: 0.4615\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8496 - acc: 0.6476 - val_loss: 1.1107 - val_acc: 0.4615\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.8376 - acc: 0.6476 - val_loss: 1.1115 - val_acc: 0.4615\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.8743 - acc: 0.5905 - val_loss: 1.1120 - val_acc: 0.4615\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.8281 - acc: 0.6476 - val_loss: 1.1131 - val_acc: 0.4615\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.8590 - acc: 0.6762 - val_loss: 1.1141 - val_acc: 0.4615\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.8420 - acc: 0.6476 - val_loss: 1.1161 - val_acc: 0.4615\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.8113 - acc: 0.6857 - val_loss: 1.1182 - val_acc: 0.4615\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8211 - acc: 0.6762 - val_loss: 1.1200 - val_acc: 0.4615\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.8395 - acc: 0.6286 - val_loss: 1.1211 - val_acc: 0.4615\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8119 - acc: 0.6762 - val_loss: 1.1228 - val_acc: 0.4615\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8251 - acc: 0.6476 - val_loss: 1.1246 - val_acc: 0.4615\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.8198 - acc: 0.6381 - val_loss: 1.1263 - val_acc: 0.4615\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7935 - acc: 0.7143 - val_loss: 1.1285 - val_acc: 0.4615\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8271 - acc: 0.6476 - val_loss: 1.1308 - val_acc: 0.4615\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.8372 - acc: 0.6571 - val_loss: 1.1323 - val_acc: 0.4615\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8096 - acc: 0.6667 - val_loss: 1.1340 - val_acc: 0.5385\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8191 - acc: 0.6381 - val_loss: 1.1364 - val_acc: 0.5385\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8715 - acc: 0.6476 - val_loss: 1.1383 - val_acc: 0.5385\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.8136 - acc: 0.6381 - val_loss: 1.1396 - val_acc: 0.5385\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8137 - acc: 0.6476 - val_loss: 1.1407 - val_acc: 0.5385\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7830 - acc: 0.6762 - val_loss: 1.1419 - val_acc: 0.5385\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8179 - acc: 0.6762 - val_loss: 1.1429 - val_acc: 0.5385\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8286 - acc: 0.6952 - val_loss: 1.1437 - val_acc: 0.5385\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8204 - acc: 0.6762 - val_loss: 1.1439 - val_acc: 0.5385\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8081 - acc: 0.6952 - val_loss: 1.1436 - val_acc: 0.5385\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7992 - acc: 0.6762 - val_loss: 1.1428 - val_acc: 0.5385\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.8138 - acc: 0.6476 - val_loss: 1.1415 - val_acc: 0.5385\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.7679 - acc: 0.6286 - val_loss: 1.1401 - val_acc: 0.5385\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 94us/step - loss: 0.7998 - acc: 0.6476 - val_loss: 1.1389 - val_acc: 0.5385\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7803 - acc: 0.6667 - val_loss: 1.1377 - val_acc: 0.5385\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7846 - acc: 0.6857 - val_loss: 1.1365 - val_acc: 0.5385\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.7891 - acc: 0.6667 - val_loss: 1.1352 - val_acc: 0.5385\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7862 - acc: 0.6476 - val_loss: 1.1339 - val_acc: 0.5385\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7738 - acc: 0.7048 - val_loss: 1.1333 - val_acc: 0.5385\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7914 - acc: 0.6667 - val_loss: 1.1328 - val_acc: 0.5385\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7794 - acc: 0.6571 - val_loss: 1.1326 - val_acc: 0.5385\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7977 - acc: 0.6476 - val_loss: 1.1331 - val_acc: 0.5385\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7882 - acc: 0.7048 - val_loss: 1.1344 - val_acc: 0.5385\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7796 - acc: 0.6571 - val_loss: 1.1363 - val_acc: 0.5385\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.7909 - acc: 0.6286 - val_loss: 1.1395 - val_acc: 0.5385\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.8012 - acc: 0.6476 - val_loss: 1.1423 - val_acc: 0.5385\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7822 - acc: 0.6952 - val_loss: 1.1451 - val_acc: 0.5385\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.7502 - acc: 0.7238 - val_loss: 1.1478 - val_acc: 0.5385\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7703 - acc: 0.6381 - val_loss: 1.1497 - val_acc: 0.5385\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7710 - acc: 0.6667 - val_loss: 1.1518 - val_acc: 0.5385\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7700 - acc: 0.6857 - val_loss: 1.1543 - val_acc: 0.5385\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7846 - acc: 0.6857 - val_loss: 1.1555 - val_acc: 0.5385\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7737 - acc: 0.7048 - val_loss: 1.1567 - val_acc: 0.5385\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.8019 - acc: 0.6190 - val_loss: 1.1577 - val_acc: 0.5385\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7943 - acc: 0.6476 - val_loss: 1.1586 - val_acc: 0.5385\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7648 - acc: 0.6952 - val_loss: 1.1598 - val_acc: 0.5385\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7937 - acc: 0.6762 - val_loss: 1.1626 - val_acc: 0.5385\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7671 - acc: 0.6476 - val_loss: 1.1653 - val_acc: 0.5385\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.7529 - acc: 0.6857 - val_loss: 1.1684 - val_acc: 0.5385\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7535 - acc: 0.6762 - val_loss: 1.1715 - val_acc: 0.5385\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7703 - acc: 0.6857 - val_loss: 1.1741 - val_acc: 0.5385\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7789 - acc: 0.6667 - val_loss: 1.1765 - val_acc: 0.5385\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7427 - acc: 0.6952 - val_loss: 1.1790 - val_acc: 0.5385\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7708 - acc: 0.6857 - val_loss: 1.1811 - val_acc: 0.5385\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7688 - acc: 0.6667 - val_loss: 1.1833 - val_acc: 0.5385\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7340 - acc: 0.6857 - val_loss: 1.1855 - val_acc: 0.5385\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7652 - acc: 0.6667 - val_loss: 1.1880 - val_acc: 0.5385\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7513 - acc: 0.6952 - val_loss: 1.1902 - val_acc: 0.5385\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.7477 - acc: 0.7238 - val_loss: 1.1922 - val_acc: 0.5385\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7301 - acc: 0.6762 - val_loss: 1.1931 - val_acc: 0.5385\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7562 - acc: 0.6762 - val_loss: 1.1939 - val_acc: 0.5385\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7302 - acc: 0.6952 - val_loss: 1.1946 - val_acc: 0.5385\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7691 - acc: 0.6762 - val_loss: 1.1936 - val_acc: 0.5385\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7397 - acc: 0.6476 - val_loss: 1.1940 - val_acc: 0.5385\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.7599 - acc: 0.6762 - val_loss: 1.1947 - val_acc: 0.5385\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7391 - acc: 0.7143 - val_loss: 1.1962 - val_acc: 0.5385\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7408 - acc: 0.6952 - val_loss: 1.1986 - val_acc: 0.5385\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7232 - acc: 0.7143 - val_loss: 1.2028 - val_acc: 0.5385\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6749 - acc: 0.7429 - val_loss: 1.2082 - val_acc: 0.5385\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7693 - acc: 0.6857 - val_loss: 1.2132 - val_acc: 0.5385\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7094 - acc: 0.7333 - val_loss: 1.2177 - val_acc: 0.5385\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7315 - acc: 0.7048 - val_loss: 1.2232 - val_acc: 0.5385\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.7548 - acc: 0.6857 - val_loss: 1.2281 - val_acc: 0.5385\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.7686 - acc: 0.6476 - val_loss: 1.2322 - val_acc: 0.5385\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.7213 - acc: 0.7333 - val_loss: 1.2359 - val_acc: 0.5385\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.7382 - acc: 0.7048 - val_loss: 1.2393 - val_acc: 0.5385\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7471 - acc: 0.6571 - val_loss: 1.2424 - val_acc: 0.5385\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7310 - acc: 0.6762 - val_loss: 1.2448 - val_acc: 0.5385\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7324 - acc: 0.6857 - val_loss: 1.2472 - val_acc: 0.5385\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7453 - acc: 0.6762 - val_loss: 1.2487 - val_acc: 0.5385\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7220 - acc: 0.7524 - val_loss: 1.2501 - val_acc: 0.5385\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7497 - acc: 0.6857 - val_loss: 1.2505 - val_acc: 0.5385\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.7138 - acc: 0.7143 - val_loss: 1.2518 - val_acc: 0.5385\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.7193 - acc: 0.7238 - val_loss: 1.2527 - val_acc: 0.5385\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6726 - acc: 0.7238 - val_loss: 1.2533 - val_acc: 0.5385\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.7826 - acc: 0.6571 - val_loss: 1.2540 - val_acc: 0.5385\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7121 - acc: 0.6952 - val_loss: 1.2548 - val_acc: 0.5385\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.7145 - acc: 0.7333 - val_loss: 1.2555 - val_acc: 0.5385\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.7048 - acc: 0.7143 - val_loss: 1.2551 - val_acc: 0.5385\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6928 - acc: 0.7238 - val_loss: 1.2548 - val_acc: 0.5385\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7130 - acc: 0.7238 - val_loss: 1.2545 - val_acc: 0.5385\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7599 - acc: 0.6381 - val_loss: 1.2543 - val_acc: 0.5385\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 84us/step - loss: 0.7208 - acc: 0.7048 - val_loss: 1.2552 - val_acc: 0.5385\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6923 - acc: 0.7048 - val_loss: 1.2563 - val_acc: 0.5385\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 118us/step - loss: 0.7240 - acc: 0.6952 - val_loss: 1.2565 - val_acc: 0.5385\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6947 - acc: 0.7048 - val_loss: 1.2556 - val_acc: 0.5385\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6855 - acc: 0.7238 - val_loss: 1.2554 - val_acc: 0.5385\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.6892 - acc: 0.7143 - val_loss: 1.2551 - val_acc: 0.5385\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6834 - acc: 0.7048 - val_loss: 1.2557 - val_acc: 0.5385\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6698 - acc: 0.7048 - val_loss: 1.2566 - val_acc: 0.5385\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.7048 - acc: 0.6952 - val_loss: 1.2586 - val_acc: 0.5385\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.7161 - acc: 0.7048 - val_loss: 1.2602 - val_acc: 0.5385\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.7201 - acc: 0.6952 - val_loss: 1.2615 - val_acc: 0.5385\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6972 - acc: 0.7143 - val_loss: 1.2640 - val_acc: 0.5385\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6991 - acc: 0.7048 - val_loss: 1.2672 - val_acc: 0.5385\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6650 - acc: 0.7333 - val_loss: 1.2699 - val_acc: 0.5385\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7183 - acc: 0.6952 - val_loss: 1.2733 - val_acc: 0.5385\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.7213 - acc: 0.6667 - val_loss: 1.2757 - val_acc: 0.5385\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.7010 - acc: 0.7333 - val_loss: 1.2772 - val_acc: 0.5385\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6999 - acc: 0.7143 - val_loss: 1.2765 - val_acc: 0.5385\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.7220 - acc: 0.6476 - val_loss: 1.2752 - val_acc: 0.5385\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6826 - acc: 0.7143 - val_loss: 1.2738 - val_acc: 0.5385\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.7410 - acc: 0.6857 - val_loss: 1.2718 - val_acc: 0.5385\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.6517 - acc: 0.7429 - val_loss: 1.2709 - val_acc: 0.5385\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.6639 - acc: 0.7333 - val_loss: 1.2700 - val_acc: 0.5385\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.6542 - acc: 0.7333 - val_loss: 1.2696 - val_acc: 0.5385\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6968 - acc: 0.7524 - val_loss: 1.2702 - val_acc: 0.5385\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6997 - acc: 0.7238 - val_loss: 1.2704 - val_acc: 0.5385\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6729 - acc: 0.7333 - val_loss: 1.2713 - val_acc: 0.5385\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7051 - acc: 0.6857 - val_loss: 1.2737 - val_acc: 0.5385\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6701 - acc: 0.7333 - val_loss: 1.2760 - val_acc: 0.5385\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6708 - acc: 0.7333 - val_loss: 1.2783 - val_acc: 0.5385\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.7074 - acc: 0.7048 - val_loss: 1.2811 - val_acc: 0.5385\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7198 - acc: 0.6381 - val_loss: 1.2852 - val_acc: 0.5385\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6496 - acc: 0.7048 - val_loss: 1.2886 - val_acc: 0.5385\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6415 - acc: 0.7524 - val_loss: 1.2935 - val_acc: 0.5385\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6897 - acc: 0.6952 - val_loss: 1.2998 - val_acc: 0.5385\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6333 - acc: 0.7333 - val_loss: 1.3054 - val_acc: 0.5385\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7299 - acc: 0.6952 - val_loss: 1.3083 - val_acc: 0.5385\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6579 - acc: 0.7048 - val_loss: 1.3105 - val_acc: 0.5385\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6675 - acc: 0.7048 - val_loss: 1.3115 - val_acc: 0.5385\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6464 - acc: 0.7524 - val_loss: 1.3133 - val_acc: 0.5385\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 97us/step - loss: 0.6910 - acc: 0.6857 - val_loss: 1.3158 - val_acc: 0.5385\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.6381 - acc: 0.7333 - val_loss: 1.3182 - val_acc: 0.5385\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6891 - acc: 0.7048 - val_loss: 1.3194 - val_acc: 0.5385\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6720 - acc: 0.7429 - val_loss: 1.3207 - val_acc: 0.5385\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6744 - acc: 0.7429 - val_loss: 1.3223 - val_acc: 0.5385\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6767 - acc: 0.7333 - val_loss: 1.3243 - val_acc: 0.5385\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6595 - acc: 0.7429 - val_loss: 1.3269 - val_acc: 0.5385\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6600 - acc: 0.7238 - val_loss: 1.3295 - val_acc: 0.5385\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6300 - acc: 0.7524 - val_loss: 1.3324 - val_acc: 0.5385\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6338 - acc: 0.7524 - val_loss: 1.3351 - val_acc: 0.5385\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6471 - acc: 0.7524 - val_loss: 1.3377 - val_acc: 0.5385\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6797 - acc: 0.7238 - val_loss: 1.3386 - val_acc: 0.5385\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.6787 - acc: 0.7048 - val_loss: 1.3374 - val_acc: 0.5385\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6139 - acc: 0.7048 - val_loss: 1.3378 - val_acc: 0.5385\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.6794 - acc: 0.7048 - val_loss: 1.3356 - val_acc: 0.5385\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.6441 - acc: 0.7619 - val_loss: 1.3350 - val_acc: 0.5385\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.6902 - acc: 0.6952 - val_loss: 1.3347 - val_acc: 0.5385\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.6468 - acc: 0.7048 - val_loss: 1.3335 - val_acc: 0.5385\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 93us/step - loss: 0.6421 - acc: 0.7238 - val_loss: 1.3324 - val_acc: 0.5385\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6056 - acc: 0.7524 - val_loss: 1.3305 - val_acc: 0.5385\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6547 - acc: 0.7143 - val_loss: 1.3295 - val_acc: 0.5385\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5889 - acc: 0.7810 - val_loss: 1.3287 - val_acc: 0.5385\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5959 - acc: 0.7714 - val_loss: 1.3297 - val_acc: 0.5385\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6216 - acc: 0.7333 - val_loss: 1.3310 - val_acc: 0.5385\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6170 - acc: 0.7619 - val_loss: 1.3334 - val_acc: 0.5385\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6281 - acc: 0.7333 - val_loss: 1.3343 - val_acc: 0.5385\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6789 - acc: 0.6857 - val_loss: 1.3350 - val_acc: 0.5385\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6299 - acc: 0.7524 - val_loss: 1.3373 - val_acc: 0.5385\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6072 - acc: 0.7429 - val_loss: 1.3383 - val_acc: 0.5385\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6260 - acc: 0.7333 - val_loss: 1.3397 - val_acc: 0.5385\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6250 - acc: 0.7619 - val_loss: 1.3419 - val_acc: 0.5385\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6300 - acc: 0.7238 - val_loss: 1.3442 - val_acc: 0.5385\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5830 - acc: 0.7429 - val_loss: 1.3476 - val_acc: 0.5385\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6619 - acc: 0.6952 - val_loss: 1.3495 - val_acc: 0.5385\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6543 - acc: 0.7048 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5845 - acc: 0.8000 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6009 - acc: 0.7905 - val_loss: 1.3494 - val_acc: 0.5385\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6165 - acc: 0.7714 - val_loss: 1.3496 - val_acc: 0.5385\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6066 - acc: 0.7619 - val_loss: 1.3495 - val_acc: 0.5385\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6540 - acc: 0.7048 - val_loss: 1.3494 - val_acc: 0.5385\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.6415 - acc: 0.7429 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5953 - acc: 0.7429 - val_loss: 1.3506 - val_acc: 0.5385\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5793 - acc: 0.8095 - val_loss: 1.3521 - val_acc: 0.5385\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6325 - acc: 0.7238 - val_loss: 1.3544 - val_acc: 0.5385\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.6172 - acc: 0.7714 - val_loss: 1.3569 - val_acc: 0.5385\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6241 - acc: 0.7143 - val_loss: 1.3608 - val_acc: 0.5385\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6430 - acc: 0.7333 - val_loss: 1.3658 - val_acc: 0.5385\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5897 - acc: 0.7714 - val_loss: 1.3693 - val_acc: 0.5385\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6032 - acc: 0.7048 - val_loss: 1.3733 - val_acc: 0.5385\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5561 - acc: 0.8000 - val_loss: 1.3781 - val_acc: 0.5385\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6201 - acc: 0.7714 - val_loss: 1.3815 - val_acc: 0.5385\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6079 - acc: 0.7048 - val_loss: 1.3860 - val_acc: 0.5385\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6292 - acc: 0.7524 - val_loss: 1.3926 - val_acc: 0.5385\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5971 - acc: 0.7143 - val_loss: 1.4013 - val_acc: 0.5385\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6148 - acc: 0.7524 - val_loss: 1.4101 - val_acc: 0.5385\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6462 - acc: 0.6857 - val_loss: 1.4185 - val_acc: 0.5385\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5951 - acc: 0.7524 - val_loss: 1.4273 - val_acc: 0.5385\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5939 - acc: 0.7714 - val_loss: 1.4337 - val_acc: 0.5385\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6513 - acc: 0.7238 - val_loss: 1.4373 - val_acc: 0.5385\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5503 - acc: 0.7619 - val_loss: 1.4421 - val_acc: 0.5385\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6365 - acc: 0.7143 - val_loss: 1.4456 - val_acc: 0.5385\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6105 - acc: 0.7905 - val_loss: 1.4485 - val_acc: 0.5385\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6142 - acc: 0.7333 - val_loss: 1.4535 - val_acc: 0.5385\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6000 - acc: 0.7905 - val_loss: 1.4598 - val_acc: 0.5385\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6384 - acc: 0.7619 - val_loss: 1.4655 - val_acc: 0.5385\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6027 - acc: 0.7524 - val_loss: 1.4703 - val_acc: 0.5385\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6383 - acc: 0.7429 - val_loss: 1.4742 - val_acc: 0.5385\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5770 - acc: 0.7810 - val_loss: 1.4771 - val_acc: 0.5385\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6116 - acc: 0.7429 - val_loss: 1.4791 - val_acc: 0.5385\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5617 - acc: 0.7524 - val_loss: 1.4809 - val_acc: 0.5385\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5943 - acc: 0.7714 - val_loss: 1.4856 - val_acc: 0.5385\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6228 - acc: 0.6952 - val_loss: 1.4866 - val_acc: 0.5385\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5331 - acc: 0.7810 - val_loss: 1.4875 - val_acc: 0.5385\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5982 - acc: 0.7524 - val_loss: 1.4898 - val_acc: 0.5385\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5526 - acc: 0.8095 - val_loss: 1.4914 - val_acc: 0.5385\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5824 - acc: 0.7714 - val_loss: 1.4930 - val_acc: 0.5385\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5559 - acc: 0.7905 - val_loss: 1.4960 - val_acc: 0.5385\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5633 - acc: 0.7714 - val_loss: 1.4953 - val_acc: 0.5385\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5873 - acc: 0.7714 - val_loss: 1.4946 - val_acc: 0.5385\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6025 - acc: 0.7143 - val_loss: 1.4924 - val_acc: 0.5385\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6322 - acc: 0.7048 - val_loss: 1.4890 - val_acc: 0.5385\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5549 - acc: 0.8381 - val_loss: 1.4858 - val_acc: 0.5385\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5490 - acc: 0.8286 - val_loss: 1.4836 - val_acc: 0.5385\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5951 - acc: 0.7619 - val_loss: 1.4811 - val_acc: 0.5385\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6068 - acc: 0.7524 - val_loss: 1.4789 - val_acc: 0.5385\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5717 - acc: 0.7905 - val_loss: 1.4774 - val_acc: 0.5385\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5967 - acc: 0.7333 - val_loss: 1.4772 - val_acc: 0.5385\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5272 - acc: 0.7810 - val_loss: 1.4766 - val_acc: 0.5385\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5351 - acc: 0.8000 - val_loss: 1.4757 - val_acc: 0.5385\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5949 - acc: 0.7619 - val_loss: 1.4735 - val_acc: 0.5385\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5584 - acc: 0.7905 - val_loss: 1.4717 - val_acc: 0.5385\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5823 - acc: 0.7619 - val_loss: 1.4670 - val_acc: 0.5385\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5351 - acc: 0.7905 - val_loss: 1.4639 - val_acc: 0.5385\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5876 - acc: 0.7524 - val_loss: 1.4591 - val_acc: 0.5385\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5454 - acc: 0.8000 - val_loss: 1.4546 - val_acc: 0.5385\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5311 - acc: 0.7905 - val_loss: 1.4499 - val_acc: 0.5385\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5780 - acc: 0.8000 - val_loss: 1.4448 - val_acc: 0.5385\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5754 - acc: 0.8000 - val_loss: 1.4395 - val_acc: 0.6154\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5331 - acc: 0.7905 - val_loss: 1.4351 - val_acc: 0.6154\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5146 - acc: 0.8095 - val_loss: 1.4329 - val_acc: 0.6154\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5241 - acc: 0.7619 - val_loss: 1.4317 - val_acc: 0.6154\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5799 - acc: 0.7714 - val_loss: 1.4339 - val_acc: 0.6154\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5267 - acc: 0.8000 - val_loss: 1.4364 - val_acc: 0.5385\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5605 - acc: 0.7905 - val_loss: 1.4418 - val_acc: 0.5385\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5447 - acc: 0.8000 - val_loss: 1.4485 - val_acc: 0.5385\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5385 - acc: 0.8000 - val_loss: 1.4570 - val_acc: 0.5385\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4938 - acc: 0.8190 - val_loss: 1.4674 - val_acc: 0.5385\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5100 - acc: 0.8000 - val_loss: 1.4797 - val_acc: 0.5385\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5644 - acc: 0.7524 - val_loss: 1.4890 - val_acc: 0.5385\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.6036 - acc: 0.7714 - val_loss: 1.4972 - val_acc: 0.5385\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6183 - acc: 0.7429 - val_loss: 1.5040 - val_acc: 0.5385\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4928 - acc: 0.8286 - val_loss: 1.5092 - val_acc: 0.5385\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5769 - acc: 0.8095 - val_loss: 1.5122 - val_acc: 0.5385\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5117 - acc: 0.8000 - val_loss: 1.5137 - val_acc: 0.5385\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5127 - acc: 0.7810 - val_loss: 1.5137 - val_acc: 0.5385\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5290 - acc: 0.8286 - val_loss: 1.5151 - val_acc: 0.5385\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.4793 - acc: 0.8286 - val_loss: 1.5175 - val_acc: 0.5385\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.5078 - acc: 0.8000 - val_loss: 1.5195 - val_acc: 0.5385\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.5614 - acc: 0.7714 - val_loss: 1.5228 - val_acc: 0.5385\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5286 - acc: 0.8190 - val_loss: 1.5243 - val_acc: 0.5385\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.5003 - acc: 0.8286 - val_loss: 1.5300 - val_acc: 0.5385\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.5659 - acc: 0.7810 - val_loss: 1.5335 - val_acc: 0.5385\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5416 - acc: 0.8000 - val_loss: 1.5366 - val_acc: 0.5385\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5397 - acc: 0.7619 - val_loss: 1.5423 - val_acc: 0.5385\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5147 - acc: 0.7810 - val_loss: 1.5479 - val_acc: 0.5385\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5205 - acc: 0.8000 - val_loss: 1.5531 - val_acc: 0.5385\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5786 - acc: 0.7429 - val_loss: 1.5585 - val_acc: 0.5385\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5221 - acc: 0.8000 - val_loss: 1.5639 - val_acc: 0.5385\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5538 - acc: 0.7810 - val_loss: 1.5678 - val_acc: 0.5385\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5450 - acc: 0.7714 - val_loss: 1.5732 - val_acc: 0.6154\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5384 - acc: 0.7810 - val_loss: 1.5745 - val_acc: 0.6154\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5267 - acc: 0.8095 - val_loss: 1.5737 - val_acc: 0.6154\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4837 - acc: 0.8571 - val_loss: 1.5725 - val_acc: 0.6154\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5657 - acc: 0.7810 - val_loss: 1.5744 - val_acc: 0.6154\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5119 - acc: 0.8286 - val_loss: 1.5749 - val_acc: 0.6154\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4723 - acc: 0.8667 - val_loss: 1.5767 - val_acc: 0.6154\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4899 - acc: 0.8286 - val_loss: 1.5765 - val_acc: 0.6154\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4706 - acc: 0.8286 - val_loss: 1.5815 - val_acc: 0.6154\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5278 - acc: 0.7905 - val_loss: 1.5850 - val_acc: 0.6154\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5166 - acc: 0.7905 - val_loss: 1.5885 - val_acc: 0.6154\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5011 - acc: 0.7905 - val_loss: 1.5925 - val_acc: 0.6154\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.5150 - acc: 0.8571 - val_loss: 1.5947 - val_acc: 0.6154\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5837 - acc: 0.7524 - val_loss: 1.5985 - val_acc: 0.6154\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4765 - acc: 0.8286 - val_loss: 1.6020 - val_acc: 0.6154\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5526 - acc: 0.8190 - val_loss: 1.6068 - val_acc: 0.6154\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.5073 - acc: 0.8000 - val_loss: 1.6093 - val_acc: 0.6154\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5040 - acc: 0.8190 - val_loss: 1.6130 - val_acc: 0.6154\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.5058 - acc: 0.8190 - val_loss: 1.6171 - val_acc: 0.6154\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.5609 - acc: 0.7524 - val_loss: 1.6195 - val_acc: 0.6154\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.5504 - acc: 0.8095 - val_loss: 1.6184 - val_acc: 0.6154\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5166 - acc: 0.8476 - val_loss: 1.6152 - val_acc: 0.6154\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4834 - acc: 0.8190 - val_loss: 1.6134 - val_acc: 0.6154\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5408 - acc: 0.7810 - val_loss: 1.6097 - val_acc: 0.6154\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5078 - acc: 0.8190 - val_loss: 1.6058 - val_acc: 0.6154\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.4722 - acc: 0.8095 - val_loss: 1.6036 - val_acc: 0.6154\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.4351 - acc: 0.8476 - val_loss: 1.6034 - val_acc: 0.6154\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5464 - acc: 0.8381 - val_loss: 1.6040 - val_acc: 0.6154\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4568 - acc: 0.8381 - val_loss: 1.6067 - val_acc: 0.6154\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.4594 - acc: 0.8381 - val_loss: 1.6098 - val_acc: 0.6154\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.4593 - acc: 0.8571 - val_loss: 1.6144 - val_acc: 0.6154\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 87us/step - loss: 0.5190 - acc: 0.7905 - val_loss: 1.6214 - val_acc: 0.6154\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 101us/step - loss: 0.4812 - acc: 0.8286 - val_loss: 1.6271 - val_acc: 0.6154\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4904 - acc: 0.8476 - val_loss: 1.6317 - val_acc: 0.6154\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4884 - acc: 0.7905 - val_loss: 1.6383 - val_acc: 0.6154\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.5229 - acc: 0.8000 - val_loss: 1.6468 - val_acc: 0.6154\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.4435 - acc: 0.8762 - val_loss: 1.6531 - val_acc: 0.6154\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4623 - acc: 0.8667 - val_loss: 1.6619 - val_acc: 0.6154\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 94us/step - loss: 0.4690 - acc: 0.8571 - val_loss: 1.6707 - val_acc: 0.6154\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5087 - acc: 0.8286 - val_loss: 1.6799 - val_acc: 0.6154\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4585 - acc: 0.8571 - val_loss: 1.6888 - val_acc: 0.6154\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.4593 - acc: 0.8381 - val_loss: 1.6972 - val_acc: 0.6154\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4333 - acc: 0.8667 - val_loss: 1.7022 - val_acc: 0.6154\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5151 - acc: 0.8095 - val_loss: 1.7067 - val_acc: 0.6154\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4973 - acc: 0.8476 - val_loss: 1.7120 - val_acc: 0.6154\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4810 - acc: 0.8476 - val_loss: 1.7149 - val_acc: 0.6154\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4781 - acc: 0.8571 - val_loss: 1.7170 - val_acc: 0.6154\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4759 - acc: 0.8571 - val_loss: 1.7180 - val_acc: 0.6154\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5178 - acc: 0.7810 - val_loss: 1.7156 - val_acc: 0.6154\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4671 - acc: 0.8571 - val_loss: 1.7146 - val_acc: 0.6154\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5716 - acc: 0.7905 - val_loss: 1.7117 - val_acc: 0.6154\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4959 - acc: 0.8476 - val_loss: 1.7114 - val_acc: 0.6154\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5209 - acc: 0.7905 - val_loss: 1.7130 - val_acc: 0.6154\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.4757 - acc: 0.8286 - val_loss: 1.7125 - val_acc: 0.6154\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4718 - acc: 0.8667 - val_loss: 1.7077 - val_acc: 0.6154\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4888 - acc: 0.8190 - val_loss: 1.7035 - val_acc: 0.6154\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.4690 - acc: 0.8286 - val_loss: 1.7025 - val_acc: 0.6154\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4453 - acc: 0.8857 - val_loss: 1.7037 - val_acc: 0.6154\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4168 - acc: 0.8667 - val_loss: 1.7057 - val_acc: 0.6154\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4579 - acc: 0.8857 - val_loss: 1.7102 - val_acc: 0.6154\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5128 - acc: 0.8190 - val_loss: 1.7115 - val_acc: 0.6154\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5177 - acc: 0.8000 - val_loss: 1.7113 - val_acc: 0.6154\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4984 - acc: 0.8476 - val_loss: 1.7156 - val_acc: 0.6154\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5371 - acc: 0.8095 - val_loss: 1.7172 - val_acc: 0.6154\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4342 - acc: 0.8667 - val_loss: 1.7140 - val_acc: 0.6154\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4837 - acc: 0.8286 - val_loss: 1.7090 - val_acc: 0.6154\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.5337 - acc: 0.8000 - val_loss: 1.7012 - val_acc: 0.6154\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4760 - acc: 0.8000 - val_loss: 1.6911 - val_acc: 0.6154\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.4237 - acc: 0.8667 - val_loss: 1.6864 - val_acc: 0.6154\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.4832 - acc: 0.8476 - val_loss: 1.6843 - val_acc: 0.6154\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.4323 - acc: 0.9048 - val_loss: 1.6833 - val_acc: 0.6154\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.4318 - acc: 0.8286 - val_loss: 1.6831 - val_acc: 0.6154\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 114us/step - loss: 0.4505 - acc: 0.8762 - val_loss: 1.6837 - val_acc: 0.6154\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.4252 - acc: 0.8762 - val_loss: 1.6866 - val_acc: 0.6154\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4929 - acc: 0.8571 - val_loss: 1.6926 - val_acc: 0.6154\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4548 - acc: 0.8762 - val_loss: 1.6968 - val_acc: 0.6154\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4129 - acc: 0.9143 - val_loss: 1.7023 - val_acc: 0.6154\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4289 - acc: 0.8762 - val_loss: 1.7100 - val_acc: 0.6154\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.4362 - acc: 0.8667 - val_loss: 1.7204 - val_acc: 0.6154\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4240 - acc: 0.8667 - val_loss: 1.7339 - val_acc: 0.6154\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4735 - acc: 0.8381 - val_loss: 1.7491 - val_acc: 0.6154\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4450 - acc: 0.8667 - val_loss: 1.7631 - val_acc: 0.6154\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4541 - acc: 0.8571 - val_loss: 1.7768 - val_acc: 0.6154\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4174 - acc: 0.8857 - val_loss: 1.7882 - val_acc: 0.6154\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4420 - acc: 0.8571 - val_loss: 1.7985 - val_acc: 0.6154\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4440 - acc: 0.8857 - val_loss: 1.8074 - val_acc: 0.6154\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4599 - acc: 0.8762 - val_loss: 1.8134 - val_acc: 0.6154\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4832 - acc: 0.8000 - val_loss: 1.8165 - val_acc: 0.6154\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4917 - acc: 0.8476 - val_loss: 1.8132 - val_acc: 0.6154\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4432 - acc: 0.8381 - val_loss: 1.8117 - val_acc: 0.6154\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4733 - acc: 0.8286 - val_loss: 1.8102 - val_acc: 0.6154\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 108us/step - loss: 0.4301 - acc: 0.8857 - val_loss: 1.8071 - val_acc: 0.6154\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 121us/step - loss: 0.4510 - acc: 0.8571 - val_loss: 1.8077 - val_acc: 0.6154\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4411 - acc: 0.8857 - val_loss: 1.8084 - val_acc: 0.6154\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.4150 - acc: 0.8857 - val_loss: 1.8133 - val_acc: 0.6154\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.4645 - acc: 0.8857 - val_loss: 1.8179 - val_acc: 0.6154\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5014 - acc: 0.8190 - val_loss: 1.8215 - val_acc: 0.6154\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.4448 - acc: 0.8571 - val_loss: 1.8197 - val_acc: 0.6154\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.4154 - acc: 0.8667 - val_loss: 1.8178 - val_acc: 0.6154\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4829 - acc: 0.8095 - val_loss: 1.8186 - val_acc: 0.6154\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.4609 - acc: 0.8476 - val_loss: 1.8187 - val_acc: 0.6154\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4028 - acc: 0.8857 - val_loss: 1.8195 - val_acc: 0.6154\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.4715 - acc: 0.8381 - val_loss: 1.8189 - val_acc: 0.6154\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4529 - acc: 0.8952 - val_loss: 1.8206 - val_acc: 0.6154\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4591 - acc: 0.8667 - val_loss: 1.8243 - val_acc: 0.6154\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4243 - acc: 0.8571 - val_loss: 1.8298 - val_acc: 0.6154\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4289 - acc: 0.8857 - val_loss: 1.8384 - val_acc: 0.6154\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4293 - acc: 0.8762 - val_loss: 1.8469 - val_acc: 0.6154\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4050 - acc: 0.8667 - val_loss: 1.8555 - val_acc: 0.6154\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5254 - acc: 0.8095 - val_loss: 1.8626 - val_acc: 0.6154\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4528 - acc: 0.8667 - val_loss: 1.8672 - val_acc: 0.6154\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4363 - acc: 0.8762 - val_loss: 1.8698 - val_acc: 0.6154\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4390 - acc: 0.8571 - val_loss: 1.8694 - val_acc: 0.6154\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4659 - acc: 0.8857 - val_loss: 1.8689 - val_acc: 0.6154\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4778 - acc: 0.8476 - val_loss: 1.8676 - val_acc: 0.6154\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4082 - acc: 0.8667 - val_loss: 1.8677 - val_acc: 0.6154\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3824 - acc: 0.9048 - val_loss: 1.8659 - val_acc: 0.6154\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4678 - acc: 0.8762 - val_loss: 1.8658 - val_acc: 0.6154\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4538 - acc: 0.8190 - val_loss: 1.8671 - val_acc: 0.6154\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4789 - acc: 0.8571 - val_loss: 1.8726 - val_acc: 0.6154\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4928 - acc: 0.8286 - val_loss: 1.8793 - val_acc: 0.6154\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4315 - acc: 0.9048 - val_loss: 1.8875 - val_acc: 0.6154\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4452 - acc: 0.8762 - val_loss: 1.8939 - val_acc: 0.6154\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4811 - acc: 0.8381 - val_loss: 1.8988 - val_acc: 0.6154\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.4244 - acc: 0.9238 - val_loss: 1.9052 - val_acc: 0.6154\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4257 - acc: 0.8667 - val_loss: 1.9050 - val_acc: 0.6154\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4105 - acc: 0.8762 - val_loss: 1.9038 - val_acc: 0.6154\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3951 - acc: 0.9048 - val_loss: 1.9011 - val_acc: 0.6154\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4060 - acc: 0.9143 - val_loss: 1.8947 - val_acc: 0.6154\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4142 - acc: 0.8857 - val_loss: 1.8884 - val_acc: 0.6154\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4191 - acc: 0.8571 - val_loss: 1.8832 - val_acc: 0.6154\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4450 - acc: 0.9048 - val_loss: 1.8763 - val_acc: 0.6154\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4430 - acc: 0.8667 - val_loss: 1.8711 - val_acc: 0.6154\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3834 - acc: 0.9048 - val_loss: 1.8671 - val_acc: 0.6154\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4044 - acc: 0.8571 - val_loss: 1.8659 - val_acc: 0.6154\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4169 - acc: 0.8762 - val_loss: 1.8637 - val_acc: 0.6154\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.4398 - acc: 0.8476 - val_loss: 1.8661 - val_acc: 0.6154\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.4211 - acc: 0.8857 - val_loss: 1.8716 - val_acc: 0.6154\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 88us/step - loss: 0.4506 - acc: 0.8762 - val_loss: 1.8781 - val_acc: 0.6154\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.4048 - acc: 0.9048 - val_loss: 1.8872 - val_acc: 0.6154\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4175 - acc: 0.8571 - val_loss: 1.8997 - val_acc: 0.6154\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.4339 - acc: 0.8762 - val_loss: 1.9150 - val_acc: 0.6154\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4285 - acc: 0.8381 - val_loss: 1.9254 - val_acc: 0.6154\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4041 - acc: 0.9048 - val_loss: 1.9367 - val_acc: 0.6154\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3823 - acc: 0.8762 - val_loss: 1.9483 - val_acc: 0.6154\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3793 - acc: 0.9048 - val_loss: 1.9591 - val_acc: 0.6154\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4550 - acc: 0.8381 - val_loss: 1.9695 - val_acc: 0.6154\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3827 - acc: 0.9143 - val_loss: 1.9792 - val_acc: 0.6154\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3835 - acc: 0.8762 - val_loss: 1.9873 - val_acc: 0.6154\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 90us/step - loss: 0.4233 - acc: 0.8571 - val_loss: 1.9941 - val_acc: 0.6154\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4191 - acc: 0.8857 - val_loss: 2.0034 - val_acc: 0.6154\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4188 - acc: 0.8667 - val_loss: 2.0129 - val_acc: 0.6154\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3946 - acc: 0.9333 - val_loss: 2.0233 - val_acc: 0.6154\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3945 - acc: 0.8952 - val_loss: 2.0300 - val_acc: 0.6154\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.3983 - acc: 0.8857 - val_loss: 2.0344 - val_acc: 0.6154\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3983 - acc: 0.8762 - val_loss: 2.0361 - val_acc: 0.6154\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3569 - acc: 0.9238 - val_loss: 2.0362 - val_acc: 0.6154\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3522 - acc: 0.8857 - val_loss: 2.0338 - val_acc: 0.6154\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3863 - acc: 0.8381 - val_loss: 2.0316 - val_acc: 0.6154\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4101 - acc: 0.8762 - val_loss: 2.0282 - val_acc: 0.6154\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3651 - acc: 0.9048 - val_loss: 2.0230 - val_acc: 0.6154\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3901 - acc: 0.8857 - val_loss: 2.0222 - val_acc: 0.6154\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.4117 - acc: 0.8857 - val_loss: 2.0238 - val_acc: 0.6154\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4077 - acc: 0.8667 - val_loss: 2.0306 - val_acc: 0.6154\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.3965 - acc: 0.8667 - val_loss: 2.0349 - val_acc: 0.6154\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3599 - acc: 0.9143 - val_loss: 2.0396 - val_acc: 0.6154\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.3960 - acc: 0.9143 - val_loss: 2.0413 - val_acc: 0.6154\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.4030 - acc: 0.9143 - val_loss: 2.0466 - val_acc: 0.6154\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4294 - acc: 0.9143 - val_loss: 2.0531 - val_acc: 0.6154\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4134 - acc: 0.8476 - val_loss: 2.0600 - val_acc: 0.6154\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3137 - acc: 0.9333 - val_loss: 2.0640 - val_acc: 0.6154\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.4164 - acc: 0.8952 - val_loss: 2.0655 - val_acc: 0.6154\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.4062 - acc: 0.8952 - val_loss: 2.0622 - val_acc: 0.6154\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3857 - acc: 0.8952 - val_loss: 2.0568 - val_acc: 0.6154\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4351 - acc: 0.8381 - val_loss: 2.0521 - val_acc: 0.6154\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.3888 - acc: 0.9143 - val_loss: 2.0472 - val_acc: 0.6154\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3950 - acc: 0.9238 - val_loss: 2.0420 - val_acc: 0.6154\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3652 - acc: 0.9143 - val_loss: 2.0380 - val_acc: 0.6154\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4178 - acc: 0.8476 - val_loss: 2.0332 - val_acc: 0.6154\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4763 - acc: 0.8381 - val_loss: 2.0312 - val_acc: 0.6154\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3667 - acc: 0.9048 - val_loss: 2.0321 - val_acc: 0.6154\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 84us/step - loss: 0.3666 - acc: 0.9429 - val_loss: 2.0352 - val_acc: 0.6154\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 141us/step - loss: 0.3991 - acc: 0.8857 - val_loss: 2.0394 - val_acc: 0.6154\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3509 - acc: 0.9048 - val_loss: 2.0429 - val_acc: 0.6154\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3370 - acc: 0.9429 - val_loss: 2.0477 - val_acc: 0.6154\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3485 - acc: 0.9143 - val_loss: 2.0513 - val_acc: 0.6154\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3548 - acc: 0.9429 - val_loss: 2.0563 - val_acc: 0.6154\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3588 - acc: 0.9333 - val_loss: 2.0672 - val_acc: 0.6154\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3447 - acc: 0.9333 - val_loss: 2.0738 - val_acc: 0.6154\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3918 - acc: 0.9048 - val_loss: 2.0750 - val_acc: 0.6154\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.3527 - acc: 0.9238 - val_loss: 2.0782 - val_acc: 0.6154\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 88us/step - loss: 0.3153 - acc: 0.9524 - val_loss: 2.0799 - val_acc: 0.6154\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4356 - acc: 0.8571 - val_loss: 2.0829 - val_acc: 0.6154\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.3342 - acc: 0.9333 - val_loss: 2.0847 - val_acc: 0.6154\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3681 - acc: 0.8952 - val_loss: 2.0860 - val_acc: 0.6154\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.4280 - acc: 0.8762 - val_loss: 2.0931 - val_acc: 0.5385\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3428 - acc: 0.9524 - val_loss: 2.1000 - val_acc: 0.5385\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.4009 - acc: 0.8762 - val_loss: 2.1063 - val_acc: 0.5385\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3526 - acc: 0.9143 - val_loss: 2.1125 - val_acc: 0.5385\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3561 - acc: 0.9048 - val_loss: 2.1164 - val_acc: 0.5385\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3585 - acc: 0.9238 - val_loss: 2.1206 - val_acc: 0.5385\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3448 - acc: 0.9333 - val_loss: 2.1254 - val_acc: 0.5385\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4312 - acc: 0.8571 - val_loss: 2.1314 - val_acc: 0.5385\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3195 - acc: 0.9333 - val_loss: 2.1383 - val_acc: 0.5385\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3443 - acc: 0.9048 - val_loss: 2.1451 - val_acc: 0.5385\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3661 - acc: 0.9333 - val_loss: 2.1539 - val_acc: 0.5385\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3838 - acc: 0.9143 - val_loss: 2.1606 - val_acc: 0.5385\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3852 - acc: 0.9143 - val_loss: 2.1613 - val_acc: 0.5385\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3214 - acc: 0.8952 - val_loss: 2.1630 - val_acc: 0.5385\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4195 - acc: 0.8857 - val_loss: 2.1659 - val_acc: 0.5385\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3390 - acc: 0.9238 - val_loss: 2.1675 - val_acc: 0.5385\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3933 - acc: 0.8476 - val_loss: 2.1678 - val_acc: 0.5385\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3802 - acc: 0.9238 - val_loss: 2.1723 - val_acc: 0.5385\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4225 - acc: 0.8571 - val_loss: 2.1717 - val_acc: 0.5385\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3181 - acc: 0.9333 - val_loss: 2.1728 - val_acc: 0.5385\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3410 - acc: 0.9143 - val_loss: 2.1738 - val_acc: 0.5385\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3835 - acc: 0.9143 - val_loss: 2.1743 - val_acc: 0.5385\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3684 - acc: 0.9048 - val_loss: 2.1734 - val_acc: 0.5385\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3584 - acc: 0.9429 - val_loss: 2.1725 - val_acc: 0.5385\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3258 - acc: 0.9524 - val_loss: 2.1749 - val_acc: 0.5385\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3925 - acc: 0.9048 - val_loss: 2.1776 - val_acc: 0.5385\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3506 - acc: 0.9143 - val_loss: 2.1808 - val_acc: 0.5385\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3496 - acc: 0.9143 - val_loss: 2.1878 - val_acc: 0.5385\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4483 - acc: 0.8571 - val_loss: 2.1909 - val_acc: 0.5385\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3929 - acc: 0.9143 - val_loss: 2.2010 - val_acc: 0.5385\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.3584 - acc: 0.9048 - val_loss: 2.2045 - val_acc: 0.5385\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3809 - acc: 0.8857 - val_loss: 2.2066 - val_acc: 0.5385\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.4538 - acc: 0.8381 - val_loss: 2.2030 - val_acc: 0.5385\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3544 - acc: 0.8857 - val_loss: 2.2051 - val_acc: 0.5385\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3875 - acc: 0.9048 - val_loss: 2.2073 - val_acc: 0.5385\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3437 - acc: 0.8952 - val_loss: 2.2075 - val_acc: 0.5385\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3394 - acc: 0.9048 - val_loss: 2.2068 - val_acc: 0.5385\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3014 - acc: 0.9238 - val_loss: 2.2078 - val_acc: 0.5385\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.4050 - acc: 0.8857 - val_loss: 2.2094 - val_acc: 0.5385\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3848 - acc: 0.9143 - val_loss: 2.2161 - val_acc: 0.5385\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3661 - acc: 0.9143 - val_loss: 2.2250 - val_acc: 0.5385\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3626 - acc: 0.9333 - val_loss: 2.2373 - val_acc: 0.5385\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3795 - acc: 0.8857 - val_loss: 2.2477 - val_acc: 0.5385\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3699 - acc: 0.9048 - val_loss: 2.2548 - val_acc: 0.5385\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3162 - acc: 0.9524 - val_loss: 2.2614 - val_acc: 0.5385\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3895 - acc: 0.8952 - val_loss: 2.2687 - val_acc: 0.5385\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3981 - acc: 0.8667 - val_loss: 2.2744 - val_acc: 0.5385\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4251 - acc: 0.8381 - val_loss: 2.2767 - val_acc: 0.5385\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3586 - acc: 0.9048 - val_loss: 2.2760 - val_acc: 0.5385\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3362 - acc: 0.9143 - val_loss: 2.2719 - val_acc: 0.5385\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3594 - acc: 0.8952 - val_loss: 2.2717 - val_acc: 0.5385\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3854 - acc: 0.8952 - val_loss: 2.2680 - val_acc: 0.5385\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3598 - acc: 0.9238 - val_loss: 2.2641 - val_acc: 0.5385\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3588 - acc: 0.8952 - val_loss: 2.2599 - val_acc: 0.5385\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3418 - acc: 0.9238 - val_loss: 2.2555 - val_acc: 0.5385\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3683 - acc: 0.9048 - val_loss: 2.2582 - val_acc: 0.5385\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3597 - acc: 0.8952 - val_loss: 2.2604 - val_acc: 0.5385\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 87us/step - loss: 0.3694 - acc: 0.9048 - val_loss: 2.2638 - val_acc: 0.5385\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.3508 - acc: 0.9143 - val_loss: 2.2663 - val_acc: 0.5385\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3756 - acc: 0.9048 - val_loss: 2.2710 - val_acc: 0.6154\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 105us/step - loss: 0.3131 - acc: 0.9429 - val_loss: 2.2800 - val_acc: 0.6154\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.3748 - acc: 0.9143 - val_loss: 2.2871 - val_acc: 0.6154\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 97us/step - loss: 0.3493 - acc: 0.9048 - val_loss: 2.2925 - val_acc: 0.6154\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3698 - acc: 0.9048 - val_loss: 2.2969 - val_acc: 0.6154\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3819 - acc: 0.9238 - val_loss: 2.2979 - val_acc: 0.6154\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3353 - acc: 0.9048 - val_loss: 2.2973 - val_acc: 0.6154\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.3422 - acc: 0.9143 - val_loss: 2.2919 - val_acc: 0.6154\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3251 - acc: 0.8952 - val_loss: 2.2825 - val_acc: 0.6154\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3217 - acc: 0.9333 - val_loss: 2.2769 - val_acc: 0.6154\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3360 - acc: 0.9048 - val_loss: 2.2737 - val_acc: 0.6154\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3524 - acc: 0.8762 - val_loss: 2.2754 - val_acc: 0.5385\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3211 - acc: 0.9143 - val_loss: 2.2810 - val_acc: 0.5385\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3139 - acc: 0.9333 - val_loss: 2.2937 - val_acc: 0.5385\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3777 - acc: 0.8952 - val_loss: 2.3055 - val_acc: 0.5385\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3076 - acc: 0.9429 - val_loss: 2.3178 - val_acc: 0.5385\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3242 - acc: 0.9238 - val_loss: 2.3327 - val_acc: 0.6154\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3632 - acc: 0.8762 - val_loss: 2.3447 - val_acc: 0.6154\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3433 - acc: 0.8952 - val_loss: 2.3564 - val_acc: 0.6154\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.3567 - acc: 0.9238 - val_loss: 2.3625 - val_acc: 0.6154\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.3606 - acc: 0.9048 - val_loss: 2.3662 - val_acc: 0.6154\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2900 - acc: 0.9429 - val_loss: 2.3671 - val_acc: 0.6154\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3051 - acc: 0.9143 - val_loss: 2.3647 - val_acc: 0.6154\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3090 - acc: 0.9333 - val_loss: 2.3653 - val_acc: 0.6154\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3166 - acc: 0.9333 - val_loss: 2.3641 - val_acc: 0.6154\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3162 - acc: 0.8952 - val_loss: 2.3622 - val_acc: 0.6154\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3291 - acc: 0.9238 - val_loss: 2.3579 - val_acc: 0.6154\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3363 - acc: 0.9333 - val_loss: 2.3538 - val_acc: 0.6154\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3145 - acc: 0.9333 - val_loss: 2.3506 - val_acc: 0.6154\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3130 - acc: 0.9238 - val_loss: 2.3481 - val_acc: 0.6154\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3450 - acc: 0.8857 - val_loss: 2.3421 - val_acc: 0.5385\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3344 - acc: 0.9333 - val_loss: 2.3349 - val_acc: 0.5385\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3598 - acc: 0.9333 - val_loss: 2.3318 - val_acc: 0.5385\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3982 - acc: 0.8762 - val_loss: 2.3284 - val_acc: 0.5385\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3557 - acc: 0.9048 - val_loss: 2.3247 - val_acc: 0.5385\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3446 - acc: 0.9429 - val_loss: 2.3266 - val_acc: 0.5385\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3207 - acc: 0.9143 - val_loss: 2.3273 - val_acc: 0.5385\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.2422 - acc: 0.9714 - val_loss: 2.3297 - val_acc: 0.5385\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.3488 - acc: 0.9048 - val_loss: 2.3343 - val_acc: 0.5385\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3816 - acc: 0.9048 - val_loss: 2.3413 - val_acc: 0.5385\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3063 - acc: 0.9429 - val_loss: 2.3473 - val_acc: 0.5385\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2977 - acc: 0.9333 - val_loss: 2.3504 - val_acc: 0.5385\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3365 - acc: 0.9524 - val_loss: 2.3490 - val_acc: 0.5385\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3380 - acc: 0.9333 - val_loss: 2.3493 - val_acc: 0.5385\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3304 - acc: 0.9238 - val_loss: 2.3537 - val_acc: 0.5385\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4035 - acc: 0.8762 - val_loss: 2.3589 - val_acc: 0.5385\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3328 - acc: 0.9143 - val_loss: 2.3670 - val_acc: 0.5385\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3285 - acc: 0.9143 - val_loss: 2.3738 - val_acc: 0.5385\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3318 - acc: 0.9333 - val_loss: 2.3788 - val_acc: 0.5385\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3555 - acc: 0.9333 - val_loss: 2.3860 - val_acc: 0.5385\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3169 - acc: 0.9048 - val_loss: 2.3951 - val_acc: 0.5385\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3246 - acc: 0.9238 - val_loss: 2.4041 - val_acc: 0.5385\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3831 - acc: 0.8762 - val_loss: 2.4153 - val_acc: 0.5385\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3370 - acc: 0.8762 - val_loss: 2.4260 - val_acc: 0.5385\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3685 - acc: 0.8857 - val_loss: 2.4375 - val_acc: 0.5385\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3283 - acc: 0.9238 - val_loss: 2.4432 - val_acc: 0.5385\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2713 - acc: 0.9524 - val_loss: 2.4496 - val_acc: 0.5385\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3208 - acc: 0.9143 - val_loss: 2.4499 - val_acc: 0.5385\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3659 - acc: 0.9048 - val_loss: 2.4419 - val_acc: 0.5385\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3179 - acc: 0.9143 - val_loss: 2.4301 - val_acc: 0.5385\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3131 - acc: 0.9524 - val_loss: 2.4195 - val_acc: 0.5385\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3561 - acc: 0.9238 - val_loss: 2.4130 - val_acc: 0.5385\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3437 - acc: 0.9333 - val_loss: 2.4093 - val_acc: 0.5385\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3303 - acc: 0.9333 - val_loss: 2.4061 - val_acc: 0.5385\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2755 - acc: 0.9810 - val_loss: 2.4049 - val_acc: 0.5385\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3080 - acc: 0.9619 - val_loss: 2.4068 - val_acc: 0.5385\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2819 - acc: 0.9714 - val_loss: 2.4091 - val_acc: 0.5385\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3341 - acc: 0.9143 - val_loss: 2.4121 - val_acc: 0.5385\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3176 - acc: 0.9333 - val_loss: 2.4184 - val_acc: 0.5385\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2736 - acc: 0.9619 - val_loss: 2.4236 - val_acc: 0.5385\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3368 - acc: 0.9048 - val_loss: 2.4316 - val_acc: 0.5385\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2990 - acc: 0.9619 - val_loss: 2.4378 - val_acc: 0.5385\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3504 - acc: 0.9048 - val_loss: 2.4378 - val_acc: 0.5385\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2979 - acc: 0.9524 - val_loss: 2.4355 - val_acc: 0.5385\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3639 - acc: 0.9143 - val_loss: 2.4364 - val_acc: 0.5385\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2746 - acc: 0.9524 - val_loss: 2.4310 - val_acc: 0.5385\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3541 - acc: 0.8952 - val_loss: 2.4232 - val_acc: 0.5385\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3269 - acc: 0.9238 - val_loss: 2.4157 - val_acc: 0.5385\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3383 - acc: 0.8952 - val_loss: 2.4110 - val_acc: 0.5385\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3383 - acc: 0.9048 - val_loss: 2.4093 - val_acc: 0.5385\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3164 - acc: 0.9524 - val_loss: 2.4080 - val_acc: 0.5385\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3258 - acc: 0.9143 - val_loss: 2.4047 - val_acc: 0.5385\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3354 - acc: 0.9238 - val_loss: 2.3995 - val_acc: 0.5385\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2837 - acc: 0.9524 - val_loss: 2.3960 - val_acc: 0.5385\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3120 - acc: 0.9333 - val_loss: 2.3911 - val_acc: 0.5385\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3276 - acc: 0.9524 - val_loss: 2.3883 - val_acc: 0.5385\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3005 - acc: 0.9143 - val_loss: 2.3854 - val_acc: 0.5385\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2747 - acc: 0.9429 - val_loss: 2.3854 - val_acc: 0.5385\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3149 - acc: 0.9524 - val_loss: 2.3876 - val_acc: 0.5385\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4060 - acc: 0.8762 - val_loss: 2.3910 - val_acc: 0.5385\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2728 - acc: 0.9714 - val_loss: 2.3959 - val_acc: 0.5385\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3728 - acc: 0.8857 - val_loss: 2.4025 - val_acc: 0.5385\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2732 - acc: 0.9524 - val_loss: 2.4103 - val_acc: 0.5385\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2839 - acc: 0.9524 - val_loss: 2.4183 - val_acc: 0.5385\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3565 - acc: 0.9333 - val_loss: 2.4290 - val_acc: 0.5385\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3608 - acc: 0.9333 - val_loss: 2.4420 - val_acc: 0.5385\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3350 - acc: 0.9333 - val_loss: 2.4573 - val_acc: 0.5385\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2743 - acc: 0.9524 - val_loss: 2.4746 - val_acc: 0.5385\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2762 - acc: 0.9429 - val_loss: 2.4930 - val_acc: 0.5385\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2763 - acc: 0.9429 - val_loss: 2.5037 - val_acc: 0.5385\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2724 - acc: 0.9333 - val_loss: 2.5111 - val_acc: 0.5385\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3073 - acc: 0.9143 - val_loss: 2.5108 - val_acc: 0.5385\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2812 - acc: 0.9238 - val_loss: 2.5098 - val_acc: 0.5385\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2992 - acc: 0.9619 - val_loss: 2.5130 - val_acc: 0.5385\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3208 - acc: 0.9429 - val_loss: 2.5132 - val_acc: 0.5385\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3096 - acc: 0.9524 - val_loss: 2.5112 - val_acc: 0.5385\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2984 - acc: 0.9333 - val_loss: 2.5113 - val_acc: 0.5385\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2994 - acc: 0.9524 - val_loss: 2.5131 - val_acc: 0.5385\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3271 - acc: 0.9429 - val_loss: 2.5164 - val_acc: 0.5385\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3368 - acc: 0.9048 - val_loss: 2.5159 - val_acc: 0.5385\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2978 - acc: 0.9619 - val_loss: 2.5169 - val_acc: 0.5385\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.3204 - acc: 0.9429 - val_loss: 2.5162 - val_acc: 0.5385\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2858 - acc: 0.9429 - val_loss: 2.5118 - val_acc: 0.5385\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3882 - acc: 0.8857 - val_loss: 2.5055 - val_acc: 0.5385\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2828 - acc: 0.9429 - val_loss: 2.5036 - val_acc: 0.5385\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3020 - acc: 0.9238 - val_loss: 2.5030 - val_acc: 0.5385\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2479 - acc: 0.9810 - val_loss: 2.5027 - val_acc: 0.5385\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2676 - acc: 0.9524 - val_loss: 2.5048 - val_acc: 0.5385\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3384 - acc: 0.9238 - val_loss: 2.5030 - val_acc: 0.5385\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3463 - acc: 0.9048 - val_loss: 2.5021 - val_acc: 0.5385\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3184 - acc: 0.9048 - val_loss: 2.4986 - val_acc: 0.5385\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3132 - acc: 0.9524 - val_loss: 2.4959 - val_acc: 0.5385\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2704 - acc: 0.9333 - val_loss: 2.4958 - val_acc: 0.5385\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2938 - acc: 0.9429 - val_loss: 2.4966 - val_acc: 0.5385\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3156 - acc: 0.9143 - val_loss: 2.5003 - val_acc: 0.5385\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2701 - acc: 0.9524 - val_loss: 2.5060 - val_acc: 0.5385\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3019 - acc: 0.9524 - val_loss: 2.5118 - val_acc: 0.5385\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2547 - acc: 0.9619 - val_loss: 2.5186 - val_acc: 0.5385\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3303 - acc: 0.9238 - val_loss: 2.5265 - val_acc: 0.5385\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2794 - acc: 0.9333 - val_loss: 2.5315 - val_acc: 0.5385\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3641 - acc: 0.8952 - val_loss: 2.5263 - val_acc: 0.5385\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3212 - acc: 0.9143 - val_loss: 2.5208 - val_acc: 0.5385\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2911 - acc: 0.9333 - val_loss: 2.5129 - val_acc: 0.5385\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3033 - acc: 0.9429 - val_loss: 2.5082 - val_acc: 0.5385\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3051 - acc: 0.9238 - val_loss: 2.5067 - val_acc: 0.5385\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.3345 - acc: 0.9238 - val_loss: 2.5053 - val_acc: 0.5385\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 93us/step - loss: 0.2993 - acc: 0.9333 - val_loss: 2.5073 - val_acc: 0.5385\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.2771 - acc: 0.9619 - val_loss: 2.5125 - val_acc: 0.5385\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 84us/step - loss: 0.2467 - acc: 0.9524 - val_loss: 2.5174 - val_acc: 0.5385\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3433 - acc: 0.9143 - val_loss: 2.5252 - val_acc: 0.5385\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3406 - acc: 0.9429 - val_loss: 2.5351 - val_acc: 0.5385\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.2822 - acc: 0.9429 - val_loss: 2.5482 - val_acc: 0.5385\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3272 - acc: 0.9333 - val_loss: 2.5638 - val_acc: 0.5385\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2980 - acc: 0.9524 - val_loss: 2.5734 - val_acc: 0.5385\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2815 - acc: 0.9619 - val_loss: 2.5845 - val_acc: 0.5385\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3604 - acc: 0.8857 - val_loss: 2.5992 - val_acc: 0.5385\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.2942 - acc: 0.9524 - val_loss: 2.6112 - val_acc: 0.5385\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.3373 - acc: 0.8952 - val_loss: 2.6178 - val_acc: 0.5385\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3215 - acc: 0.9238 - val_loss: 2.6174 - val_acc: 0.5385\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3097 - acc: 0.9238 - val_loss: 2.6159 - val_acc: 0.5385\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2867 - acc: 0.9333 - val_loss: 2.6160 - val_acc: 0.5385\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3621 - acc: 0.9143 - val_loss: 2.6111 - val_acc: 0.5385\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3357 - acc: 0.9143 - val_loss: 2.6050 - val_acc: 0.5385\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2467 - acc: 0.9619 - val_loss: 2.5966 - val_acc: 0.5385\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3345 - acc: 0.9048 - val_loss: 2.5886 - val_acc: 0.5385\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2866 - acc: 0.9333 - val_loss: 2.5775 - val_acc: 0.5385\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2805 - acc: 0.9333 - val_loss: 2.5698 - val_acc: 0.5385\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3099 - acc: 0.9333 - val_loss: 2.5618 - val_acc: 0.5385\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3670 - acc: 0.9048 - val_loss: 2.5524 - val_acc: 0.5385\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2530 - acc: 0.9524 - val_loss: 2.5477 - val_acc: 0.5385\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2341 - acc: 0.9524 - val_loss: 2.5430 - val_acc: 0.5385\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 77us/step - loss: 0.3406 - acc: 0.8762 - val_loss: 2.5414 - val_acc: 0.5385\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2673 - acc: 0.9524 - val_loss: 2.5428 - val_acc: 0.5385\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2745 - acc: 0.9619 - val_loss: 2.5493 - val_acc: 0.5385\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2625 - acc: 0.9524 - val_loss: 2.5562 - val_acc: 0.5385\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2574 - acc: 0.9619 - val_loss: 2.5612 - val_acc: 0.5385\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2463 - acc: 0.9714 - val_loss: 2.5681 - val_acc: 0.5385\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3453 - acc: 0.9143 - val_loss: 2.5724 - val_acc: 0.5385\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2633 - acc: 0.9619 - val_loss: 2.5757 - val_acc: 0.5385\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3006 - acc: 0.9238 - val_loss: 2.5811 - val_acc: 0.5385\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2834 - acc: 0.9333 - val_loss: 2.5882 - val_acc: 0.5385\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2805 - acc: 0.9429 - val_loss: 2.5853 - val_acc: 0.5385\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3153 - acc: 0.9048 - val_loss: 2.5865 - val_acc: 0.5385\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.2342 - acc: 0.9810 - val_loss: 2.5838 - val_acc: 0.5385\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3029 - acc: 0.9429 - val_loss: 2.5794 - val_acc: 0.5385\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2720 - acc: 0.9524 - val_loss: 2.5761 - val_acc: 0.5385\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2620 - acc: 0.9143 - val_loss: 2.5720 - val_acc: 0.5385\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3012 - acc: 0.9238 - val_loss: 2.5647 - val_acc: 0.5385\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2611 - acc: 0.9429 - val_loss: 2.5633 - val_acc: 0.5385\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3196 - acc: 0.9143 - val_loss: 2.5607 - val_acc: 0.5385\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2413 - acc: 0.9524 - val_loss: 2.5601 - val_acc: 0.5385\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2707 - acc: 0.9619 - val_loss: 2.5554 - val_acc: 0.5385\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.3010 - acc: 0.9429 - val_loss: 2.5501 - val_acc: 0.5385\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3102 - acc: 0.9333 - val_loss: 2.5447 - val_acc: 0.5385\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2624 - acc: 0.9524 - val_loss: 2.5402 - val_acc: 0.5385\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.3349 - acc: 0.9238 - val_loss: 2.5373 - val_acc: 0.5385\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3022 - acc: 0.9238 - val_loss: 2.5401 - val_acc: 0.5385\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.2916 - acc: 0.9143 - val_loss: 2.5425 - val_acc: 0.5385\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.3548 - acc: 0.9048 - val_loss: 2.5485 - val_acc: 0.5385\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.2794 - acc: 0.9333 - val_loss: 2.5546 - val_acc: 0.5385\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2578 - acc: 0.9524 - val_loss: 2.5604 - val_acc: 0.5385\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2457 - acc: 0.9524 - val_loss: 2.5679 - val_acc: 0.5385\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2886 - acc: 0.9143 - val_loss: 2.5760 - val_acc: 0.5385\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2779 - acc: 0.9429 - val_loss: 2.5850 - val_acc: 0.5385\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2574 - acc: 0.9619 - val_loss: 2.5972 - val_acc: 0.5385\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3073 - acc: 0.9524 - val_loss: 2.6029 - val_acc: 0.5385\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2468 - acc: 0.9619 - val_loss: 2.6105 - val_acc: 0.5385\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2996 - acc: 0.9524 - val_loss: 2.6195 - val_acc: 0.5385\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3004 - acc: 0.9238 - val_loss: 2.6223 - val_acc: 0.5385\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 85us/step - loss: 0.2888 - acc: 0.9238 - val_loss: 2.6236 - val_acc: 0.5385\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3015 - acc: 0.9143 - val_loss: 2.6301 - val_acc: 0.5385\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2859 - acc: 0.9238 - val_loss: 2.6393 - val_acc: 0.5385\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2848 - acc: 0.9429 - val_loss: 2.6405 - val_acc: 0.5385\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2914 - acc: 0.9143 - val_loss: 2.6426 - val_acc: 0.5385\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2747 - acc: 0.9524 - val_loss: 2.6457 - val_acc: 0.5385\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2702 - acc: 0.9524 - val_loss: 2.6483 - val_acc: 0.5385\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2999 - acc: 0.9524 - val_loss: 2.6507 - val_acc: 0.5385\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3289 - acc: 0.9048 - val_loss: 2.6509 - val_acc: 0.5385\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2819 - acc: 0.9524 - val_loss: 2.6447 - val_acc: 0.5385\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3232 - acc: 0.9429 - val_loss: 2.6378 - val_acc: 0.5385\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2245 - acc: 0.9619 - val_loss: 2.6322 - val_acc: 0.5385\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3037 - acc: 0.9333 - val_loss: 2.6254 - val_acc: 0.5385\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2316 - acc: 0.9810 - val_loss: 2.6183 - val_acc: 0.5385\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3063 - acc: 0.9333 - val_loss: 2.6124 - val_acc: 0.5385\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2987 - acc: 0.9048 - val_loss: 2.6008 - val_acc: 0.5385\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2573 - acc: 0.9524 - val_loss: 2.5879 - val_acc: 0.5385\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2435 - acc: 0.9619 - val_loss: 2.5753 - val_acc: 0.5385\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2906 - acc: 0.9524 - val_loss: 2.5692 - val_acc: 0.5385\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2900 - acc: 0.9333 - val_loss: 2.5669 - val_acc: 0.5385\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2800 - acc: 0.9333 - val_loss: 2.5632 - val_acc: 0.5385\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3044 - acc: 0.9429 - val_loss: 2.5681 - val_acc: 0.5385\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3030 - acc: 0.9143 - val_loss: 2.5760 - val_acc: 0.5385\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3089 - acc: 0.9333 - val_loss: 2.5794 - val_acc: 0.5385\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2882 - acc: 0.9238 - val_loss: 2.5820 - val_acc: 0.5385\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2808 - acc: 0.9238 - val_loss: 2.5844 - val_acc: 0.5385\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2867 - acc: 0.9429 - val_loss: 2.5905 - val_acc: 0.5385\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2427 - acc: 0.9619 - val_loss: 2.5946 - val_acc: 0.5385\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2509 - acc: 0.9619 - val_loss: 2.5934 - val_acc: 0.5385\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2703 - acc: 0.9429 - val_loss: 2.5890 - val_acc: 0.5385\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2949 - acc: 0.9333 - val_loss: 2.5825 - val_acc: 0.5385\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3554 - acc: 0.9143 - val_loss: 2.5820 - val_acc: 0.5385\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3110 - acc: 0.9048 - val_loss: 2.5800 - val_acc: 0.5385\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 68us/step - loss: 0.2959 - acc: 0.9429 - val_loss: 2.5732 - val_acc: 0.5385\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2778 - acc: 0.9429 - val_loss: 2.5660 - val_acc: 0.5385\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2363 - acc: 0.9810 - val_loss: 2.5596 - val_acc: 0.5385\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.3138 - acc: 0.9143 - val_loss: 2.5532 - val_acc: 0.5385\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.3283 - acc: 0.9143 - val_loss: 2.5496 - val_acc: 0.5385\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 75us/step - loss: 0.2936 - acc: 0.9238 - val_loss: 2.5416 - val_acc: 0.5385\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3042 - acc: 0.9143 - val_loss: 2.5290 - val_acc: 0.5385\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2917 - acc: 0.9524 - val_loss: 2.5195 - val_acc: 0.5385\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3273 - acc: 0.8952 - val_loss: 2.5136 - val_acc: 0.5385\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2948 - acc: 0.9048 - val_loss: 2.5139 - val_acc: 0.5385\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2630 - acc: 0.9619 - val_loss: 2.5153 - val_acc: 0.5385\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2748 - acc: 0.9714 - val_loss: 2.5192 - val_acc: 0.5385\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3012 - acc: 0.9143 - val_loss: 2.5267 - val_acc: 0.5385\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2825 - acc: 0.9619 - val_loss: 2.5316 - val_acc: 0.5385\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2945 - acc: 0.9429 - val_loss: 2.5393 - val_acc: 0.5385\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2884 - acc: 0.9524 - val_loss: 2.5532 - val_acc: 0.5385\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.3079 - acc: 0.9238 - val_loss: 2.5718 - val_acc: 0.5385\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2610 - acc: 0.9524 - val_loss: 2.5931 - val_acc: 0.5385\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2190 - acc: 0.9810 - val_loss: 2.6160 - val_acc: 0.5385\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2752 - acc: 0.9524 - val_loss: 2.6361 - val_acc: 0.5385\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.2476 - acc: 0.9429 - val_loss: 2.6566 - val_acc: 0.5385\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2428 - acc: 0.9714 - val_loss: 2.6767 - val_acc: 0.5385\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.2815 - acc: 0.9619 - val_loss: 2.6931 - val_acc: 0.5385\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.2466 - acc: 0.9619 - val_loss: 2.7080 - val_acc: 0.5385\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2881 - acc: 0.9524 - val_loss: 2.7213 - val_acc: 0.5385\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2635 - acc: 0.9333 - val_loss: 2.7316 - val_acc: 0.5385\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2662 - acc: 0.9429 - val_loss: 2.7373 - val_acc: 0.5385\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2479 - acc: 0.9619 - val_loss: 2.7447 - val_acc: 0.5385\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2628 - acc: 0.9714 - val_loss: 2.7492 - val_acc: 0.5385\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2837 - acc: 0.9524 - val_loss: 2.7509 - val_acc: 0.5385\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2081 - acc: 0.9810 - val_loss: 2.7520 - val_acc: 0.5385\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2297 - acc: 0.9619 - val_loss: 2.7536 - val_acc: 0.5385\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2331 - acc: 0.9810 - val_loss: 2.7542 - val_acc: 0.5385\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2283 - acc: 0.9619 - val_loss: 2.7541 - val_acc: 0.5385\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.2852 - acc: 0.9524 - val_loss: 2.7515 - val_acc: 0.5385\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.2608 - acc: 0.9333 - val_loss: 2.7485 - val_acc: 0.5385\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2635 - acc: 0.9429 - val_loss: 2.7405 - val_acc: 0.5385\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2757 - acc: 0.9429 - val_loss: 2.7300 - val_acc: 0.5385\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2793 - acc: 0.9429 - val_loss: 2.7198 - val_acc: 0.5385\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2287 - acc: 0.9714 - val_loss: 2.7143 - val_acc: 0.5385\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3225 - acc: 0.9333 - val_loss: 2.7104 - val_acc: 0.5385\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2600 - acc: 0.9524 - val_loss: 2.7097 - val_acc: 0.5385\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2663 - acc: 0.9524 - val_loss: 2.7083 - val_acc: 0.5385\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2631 - acc: 0.9429 - val_loss: 2.7091 - val_acc: 0.5385\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2461 - acc: 0.9714 - val_loss: 2.7114 - val_acc: 0.5385\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3109 - acc: 0.9333 - val_loss: 2.7123 - val_acc: 0.5385\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3121 - acc: 0.9429 - val_loss: 2.7131 - val_acc: 0.5385\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3023 - acc: 0.9333 - val_loss: 2.7096 - val_acc: 0.5385\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2711 - acc: 0.9429 - val_loss: 2.7035 - val_acc: 0.5385\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2857 - acc: 0.9429 - val_loss: 2.7000 - val_acc: 0.5385\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2461 - acc: 0.9429 - val_loss: 2.6994 - val_acc: 0.5385\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2149 - acc: 0.9810 - val_loss: 2.7009 - val_acc: 0.5385\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3285 - acc: 0.8952 - val_loss: 2.7005 - val_acc: 0.5385\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2947 - acc: 0.9238 - val_loss: 2.7016 - val_acc: 0.5385\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2781 - acc: 0.9333 - val_loss: 2.7011 - val_acc: 0.5385\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3081 - acc: 0.9143 - val_loss: 2.7002 - val_acc: 0.5385\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2557 - acc: 0.9524 - val_loss: 2.6977 - val_acc: 0.5385\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3077 - acc: 0.9238 - val_loss: 2.6980 - val_acc: 0.5385\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3069 - acc: 0.9143 - val_loss: 2.7003 - val_acc: 0.5385\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2040 - acc: 0.9810 - val_loss: 2.7061 - val_acc: 0.5385\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2771 - acc: 0.9429 - val_loss: 2.7117 - val_acc: 0.5385\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2437 - acc: 0.9619 - val_loss: 2.7180 - val_acc: 0.5385\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2405 - acc: 0.9619 - val_loss: 2.7253 - val_acc: 0.5385\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2668 - acc: 0.9714 - val_loss: 2.7337 - val_acc: 0.5385\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2227 - acc: 0.9714 - val_loss: 2.7401 - val_acc: 0.5385\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2915 - acc: 0.9524 - val_loss: 2.7417 - val_acc: 0.5385\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2219 - acc: 0.9714 - val_loss: 2.7438 - val_acc: 0.5385\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2677 - acc: 0.9429 - val_loss: 2.7447 - val_acc: 0.5385\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.2648 - acc: 0.9524 - val_loss: 2.7467 - val_acc: 0.5385\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2639 - acc: 0.9524 - val_loss: 2.7496 - val_acc: 0.5385\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.3162 - acc: 0.8952 - val_loss: 2.7511 - val_acc: 0.5385\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2887 - acc: 0.9429 - val_loss: 2.7469 - val_acc: 0.5385\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3027 - acc: 0.8762 - val_loss: 2.7389 - val_acc: 0.5385\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.3166 - acc: 0.9143 - val_loss: 2.7320 - val_acc: 0.5385\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2792 - acc: 0.9429 - val_loss: 2.7273 - val_acc: 0.5385\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.2703 - acc: 0.9524 - val_loss: 2.7218 - val_acc: 0.5385\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.2753 - acc: 0.9429 - val_loss: 2.7121 - val_acc: 0.5385\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2620 - acc: 0.9619 - val_loss: 2.7046 - val_acc: 0.5385\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 86us/step - loss: 0.2555 - acc: 0.9333 - val_loss: 2.7025 - val_acc: 0.5385\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3731 - acc: 0.9143 - val_loss: 2.7029 - val_acc: 0.5385\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2891 - acc: 0.9333 - val_loss: 2.7053 - val_acc: 0.5385\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3159 - acc: 0.9333 - val_loss: 2.7098 - val_acc: 0.5385\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.2370 - acc: 0.9524 - val_loss: 2.7168 - val_acc: 0.5385\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2609 - acc: 0.9429 - val_loss: 2.7255 - val_acc: 0.5385\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2300 - acc: 0.9619 - val_loss: 2.7305 - val_acc: 0.5385\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2607 - acc: 0.9810 - val_loss: 2.7349 - val_acc: 0.5385\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2602 - acc: 0.9619 - val_loss: 2.7394 - val_acc: 0.5385\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.2722 - acc: 0.9524 - val_loss: 2.7445 - val_acc: 0.5385\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2940 - acc: 0.9143 - val_loss: 2.7523 - val_acc: 0.5385\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3089 - acc: 0.9238 - val_loss: 2.7641 - val_acc: 0.5385\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2975 - acc: 0.9333 - val_loss: 2.7692 - val_acc: 0.5385\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2402 - acc: 0.9905 - val_loss: 2.7733 - val_acc: 0.5385\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2252 - acc: 0.9619 - val_loss: 2.7752 - val_acc: 0.5385\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2366 - acc: 0.9619 - val_loss: 2.7760 - val_acc: 0.5385\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2794 - acc: 0.9524 - val_loss: 2.7712 - val_acc: 0.5385\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.2104 - acc: 0.9905 - val_loss: 2.7651 - val_acc: 0.5385\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2747 - acc: 0.9143 - val_loss: 2.7609 - val_acc: 0.5385\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2405 - acc: 0.9714 - val_loss: 2.7527 - val_acc: 0.5385\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2589 - acc: 0.9333 - val_loss: 2.7501 - val_acc: 0.5385\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3244 - acc: 0.9333 - val_loss: 2.7403 - val_acc: 0.5385\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2206 - acc: 0.9810 - val_loss: 2.7330 - val_acc: 0.5385\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 86us/step - loss: 0.3438 - acc: 0.9333 - val_loss: 2.7253 - val_acc: 0.5385\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2762 - acc: 0.9524 - val_loss: 2.7155 - val_acc: 0.5385\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2554 - acc: 0.9524 - val_loss: 2.7091 - val_acc: 0.5385\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2693 - acc: 0.9429 - val_loss: 2.7086 - val_acc: 0.5385\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2349 - acc: 0.9619 - val_loss: 2.7088 - val_acc: 0.5385\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2733 - acc: 0.9333 - val_loss: 2.7115 - val_acc: 0.5385\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 80us/step - loss: 0.3235 - acc: 0.9048 - val_loss: 2.7115 - val_acc: 0.5385\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2611 - acc: 0.9524 - val_loss: 2.7140 - val_acc: 0.5385\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2964 - acc: 0.9143 - val_loss: 2.7223 - val_acc: 0.5385\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2341 - acc: 0.9524 - val_loss: 2.7327 - val_acc: 0.5385\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2302 - acc: 0.9524 - val_loss: 2.7413 - val_acc: 0.5385\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2267 - acc: 0.9524 - val_loss: 2.7531 - val_acc: 0.5385\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2440 - acc: 0.9429 - val_loss: 2.7660 - val_acc: 0.5385\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3180 - acc: 0.8857 - val_loss: 2.7673 - val_acc: 0.5385\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2310 - acc: 0.9429 - val_loss: 2.7711 - val_acc: 0.5385\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3100 - acc: 0.9524 - val_loss: 2.7765 - val_acc: 0.5385\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2443 - acc: 0.9429 - val_loss: 2.7829 - val_acc: 0.5385\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2868 - acc: 0.9429 - val_loss: 2.7886 - val_acc: 0.5385\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2640 - acc: 0.9333 - val_loss: 2.7944 - val_acc: 0.5385\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.2880 - acc: 0.9238 - val_loss: 2.7952 - val_acc: 0.5385\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.2701 - acc: 0.9238 - val_loss: 2.7906 - val_acc: 0.5385\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2456 - acc: 0.9619 - val_loss: 2.7884 - val_acc: 0.5385\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2794 - acc: 0.9143 - val_loss: 2.7821 - val_acc: 0.5385\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2511 - acc: 0.9714 - val_loss: 2.7768 - val_acc: 0.5385\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2331 - acc: 0.9619 - val_loss: 2.7742 - val_acc: 0.5385\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.2541 - acc: 0.9619 - val_loss: 2.7764 - val_acc: 0.5385\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.2480 - acc: 0.9524 - val_loss: 2.7849 - val_acc: 0.5385\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2662 - acc: 0.9143 - val_loss: 2.7934 - val_acc: 0.5385\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2867 - acc: 0.9143 - val_loss: 2.8005 - val_acc: 0.5385\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2715 - acc: 0.9143 - val_loss: 2.7999 - val_acc: 0.5385\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2362 - acc: 0.9905 - val_loss: 2.7985 - val_acc: 0.5385\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3306 - acc: 0.8952 - val_loss: 2.7862 - val_acc: 0.5385\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.2252 - acc: 0.9714 - val_loss: 2.7734 - val_acc: 0.5385\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2825 - acc: 0.9524 - val_loss: 2.7590 - val_acc: 0.5385\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.2413 - acc: 0.9524 - val_loss: 2.7509 - val_acc: 0.5385\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2511 - acc: 0.9810 - val_loss: 2.7446 - val_acc: 0.5385\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3233 - acc: 0.9143 - val_loss: 2.7449 - val_acc: 0.5385\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2554 - acc: 0.9238 - val_loss: 2.7473 - val_acc: 0.5385\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2168 - acc: 0.9810 - val_loss: 2.7486 - val_acc: 0.5385\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2170 - acc: 0.9619 - val_loss: 2.7512 - val_acc: 0.5385\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3092 - acc: 0.9429 - val_loss: 2.7512 - val_acc: 0.5385\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2379 - acc: 0.9429 - val_loss: 2.7475 - val_acc: 0.5385\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.2340 - acc: 0.9524 - val_loss: 2.7420 - val_acc: 0.5385\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2688 - acc: 0.9238 - val_loss: 2.7359 - val_acc: 0.5385\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2797 - acc: 0.9429 - val_loss: 2.7324 - val_acc: 0.5385\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2362 - acc: 0.9714 - val_loss: 2.7288 - val_acc: 0.5385\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2000 - acc: 0.9905 - val_loss: 2.7300 - val_acc: 0.5385\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2272 - acc: 0.9619 - val_loss: 2.7344 - val_acc: 0.5385\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2322 - acc: 0.9810 - val_loss: 2.7372 - val_acc: 0.5385\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.2332 - acc: 0.9524 - val_loss: 2.7391 - val_acc: 0.5385\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.2537 - acc: 0.9524 - val_loss: 2.7437 - val_acc: 0.5385\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.2186 - acc: 0.9810 - val_loss: 2.7487 - val_acc: 0.5385\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2428 - acc: 0.9524 - val_loss: 2.7549 - val_acc: 0.5385\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.2177 - acc: 0.9714 - val_loss: 2.7611 - val_acc: 0.5385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uP80ULqrL5Y"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tFvJFmK7rL5m",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A87CoQRRrL5-",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5de62dc4-a760-4547-98d9-7c9c98079e7d",
        "id": "ND8HNb6mrL6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d8a892e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ni3MeCJxzLs55IHDOuTj3/1oSznFf9Km4AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJQ7YzU3rRI0"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2eadb653-9952-4361-f70f-4018d98f61ac",
        "id": "xJfPS8GgrRI_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff7d89ef6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 198
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAWOKQLDMIwC5/8DAkcGwuUf3u8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1mbIbgXbrVPG"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3b42c5fa-33d7-4012-b9ad-fec3e34b7dad",
        "id": "0rE0zqHzrVPR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=105, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "105/105 [==============================] - 3s 25ms/step - loss: 1.3937 - acc: 0.1905\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 1.4110 - acc: 0.1810\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.3383 - acc: 0.2571\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.3043 - acc: 0.2381\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.3153 - acc: 0.2286\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.3610 - acc: 0.2381\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2880 - acc: 0.1810\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.3193 - acc: 0.2286\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2666 - acc: 0.2381\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2252 - acc: 0.2952\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.2234 - acc: 0.3238\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1984 - acc: 0.3619\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2300 - acc: 0.3524\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.2067 - acc: 0.3810\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.1403 - acc: 0.3333\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1621 - acc: 0.3810\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1743 - acc: 0.3905\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1227 - acc: 0.4667\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.1019 - acc: 0.4571\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1252 - acc: 0.4667\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.1100 - acc: 0.5238\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0915 - acc: 0.4762\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.1084 - acc: 0.4762\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1050 - acc: 0.4857\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0968 - acc: 0.5143\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 1.0940 - acc: 0.4857\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0622 - acc: 0.5333\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0457 - acc: 0.4762\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0667 - acc: 0.5619\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0476 - acc: 0.5238\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0391 - acc: 0.5048\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0422 - acc: 0.4857\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0038 - acc: 0.5619\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0137 - acc: 0.5810\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0271 - acc: 0.5143\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.9952 - acc: 0.5810\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9775 - acc: 0.5905\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9867 - acc: 0.5524\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9978 - acc: 0.5333\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9850 - acc: 0.5524\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0007 - acc: 0.5238\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9849 - acc: 0.5714\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9770 - acc: 0.5333\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.9846 - acc: 0.5905\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.9231 - acc: 0.6286\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.9772 - acc: 0.6286\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9700 - acc: 0.5524\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9697 - acc: 0.5619\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.9433 - acc: 0.6000\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9482 - acc: 0.5905\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0039 - acc: 0.5238\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.9589 - acc: 0.5619\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9850 - acc: 0.5238\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9561 - acc: 0.6381\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9695 - acc: 0.6000\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.9591 - acc: 0.5714\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9592 - acc: 0.5810\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.9183 - acc: 0.6857\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9185 - acc: 0.6667\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9377 - acc: 0.6095\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9160 - acc: 0.6286\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.8839 - acc: 0.6571\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9107 - acc: 0.6095\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9032 - acc: 0.6000\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8975 - acc: 0.6476\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9169 - acc: 0.6381\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.9451 - acc: 0.5714\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.9131 - acc: 0.5905\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9219 - acc: 0.6381\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9005 - acc: 0.6286\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9056 - acc: 0.6381\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.9099 - acc: 0.6286\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.9113 - acc: 0.6286\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8849 - acc: 0.6476\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9197 - acc: 0.6381\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8924 - acc: 0.6667\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8921 - acc: 0.6571\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8889 - acc: 0.6381\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8970 - acc: 0.6286\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8806 - acc: 0.6286\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8833 - acc: 0.6667\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9167 - acc: 0.6000\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 164us/step - loss: 0.8883 - acc: 0.6095\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8831 - acc: 0.6190\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8809 - acc: 0.6286\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8579 - acc: 0.6762\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8746 - acc: 0.6095\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.9011 - acc: 0.6095\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8654 - acc: 0.6381\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8840 - acc: 0.6286\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8554 - acc: 0.6667\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8368 - acc: 0.7048\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.8601 - acc: 0.6571\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.8930 - acc: 0.6190\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8767 - acc: 0.6762\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8916 - acc: 0.6190\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8798 - acc: 0.6476\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8708 - acc: 0.6571\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8654 - acc: 0.6762\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8620 - acc: 0.6476\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8637 - acc: 0.6286\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8318 - acc: 0.6667\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8762 - acc: 0.6762\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8185 - acc: 0.7048\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8340 - acc: 0.7048\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8421 - acc: 0.6476\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.8477 - acc: 0.6857\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8383 - acc: 0.6571\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8530 - acc: 0.6667\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.8129 - acc: 0.6952\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8356 - acc: 0.6762\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.8471 - acc: 0.6952\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8344 - acc: 0.6571\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8355 - acc: 0.6667\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8443 - acc: 0.6571\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8416 - acc: 0.6857\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8074 - acc: 0.7048\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8425 - acc: 0.6667\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8138 - acc: 0.6857\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8200 - acc: 0.6571\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.8418 - acc: 0.6667\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8391 - acc: 0.6476\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8314 - acc: 0.7143\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7967 - acc: 0.7048\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7981 - acc: 0.7048\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8365 - acc: 0.6476\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8111 - acc: 0.6667\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8153 - acc: 0.6571\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8377 - acc: 0.6381\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.8188 - acc: 0.6667\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.8090 - acc: 0.7333\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8031 - acc: 0.6762\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.8067 - acc: 0.6667\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7981 - acc: 0.6952\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.8168 - acc: 0.6571\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.8258 - acc: 0.6762\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8166 - acc: 0.6762\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.7743 - acc: 0.7333\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7744 - acc: 0.7048\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.7991 - acc: 0.7238\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.8120 - acc: 0.6381\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7765 - acc: 0.7238\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7893 - acc: 0.7143\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7863 - acc: 0.6381\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.8006 - acc: 0.6667\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.7592 - acc: 0.7143\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7818 - acc: 0.6762\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.7868 - acc: 0.6857\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.7676 - acc: 0.6857\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.7504 - acc: 0.7143\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7969 - acc: 0.6952\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.7639 - acc: 0.7143\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8001 - acc: 0.6571\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7951 - acc: 0.6667\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7913 - acc: 0.7238\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7812 - acc: 0.6857\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8231 - acc: 0.6476\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7422 - acc: 0.7333\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7702 - acc: 0.6667\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7592 - acc: 0.6762\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7420 - acc: 0.7143\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7269 - acc: 0.7714\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7695 - acc: 0.6667\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7669 - acc: 0.7333\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7546 - acc: 0.7048\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7713 - acc: 0.6667\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.7573 - acc: 0.6952\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7701 - acc: 0.7238\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7329 - acc: 0.7143\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7711 - acc: 0.7238\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7547 - acc: 0.6857\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7535 - acc: 0.7048\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7643 - acc: 0.6857\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7505 - acc: 0.6952\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7354 - acc: 0.7524\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7263 - acc: 0.7143\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7716 - acc: 0.6381\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7174 - acc: 0.7048\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7159 - acc: 0.7238\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.7346 - acc: 0.7143\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7297 - acc: 0.7143\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7288 - acc: 0.7143\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7298 - acc: 0.7143\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7516 - acc: 0.7714\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6870 - acc: 0.7238\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.7613 - acc: 0.6952\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6978 - acc: 0.7429\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7369 - acc: 0.6667\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7168 - acc: 0.7048\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7670 - acc: 0.6762\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7620 - acc: 0.6762\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7233 - acc: 0.7238\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7024 - acc: 0.7810\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7226 - acc: 0.6857\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7392 - acc: 0.7143\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7229 - acc: 0.7143\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7348 - acc: 0.6762\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7374 - acc: 0.7143\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7492 - acc: 0.7048\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7019 - acc: 0.6952\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7199 - acc: 0.7048\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6964 - acc: 0.7143\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7337 - acc: 0.7048\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7473 - acc: 0.6857\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.7461 - acc: 0.6857\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7218 - acc: 0.7238\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7239 - acc: 0.7524\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7065 - acc: 0.7333\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.7237 - acc: 0.7238\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7258 - acc: 0.7238\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7144 - acc: 0.7619\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7182 - acc: 0.6857\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6942 - acc: 0.7333\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7236 - acc: 0.6857\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.6900 - acc: 0.7143\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.7060 - acc: 0.7143\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6999 - acc: 0.7238\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6915 - acc: 0.7048\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.6888 - acc: 0.7238\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6937 - acc: 0.7333\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.7502 - acc: 0.6476\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6903 - acc: 0.7429\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.6657 - acc: 0.7429\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6865 - acc: 0.7143\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.7035 - acc: 0.7048\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6866 - acc: 0.6952\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.6632 - acc: 0.7238\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 108us/step - loss: 0.6999 - acc: 0.6762\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.6837 - acc: 0.7143\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.7002 - acc: 0.7238\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6856 - acc: 0.7524\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6607 - acc: 0.7333\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6844 - acc: 0.6952\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6536 - acc: 0.7619\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6380 - acc: 0.7333\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6792 - acc: 0.7143\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6552 - acc: 0.7714\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6618 - acc: 0.7619\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 136us/step - loss: 0.6750 - acc: 0.7143\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6748 - acc: 0.6857\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6498 - acc: 0.7714\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6580 - acc: 0.7048\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6946 - acc: 0.7429\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 83us/step - loss: 0.6465 - acc: 0.7619\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6815 - acc: 0.7333\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6653 - acc: 0.7429\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 110us/step - loss: 0.6445 - acc: 0.7524\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6649 - acc: 0.7333\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6202 - acc: 0.7905\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.6587 - acc: 0.7333\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6323 - acc: 0.8000\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6880 - acc: 0.7524\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.6480 - acc: 0.7619\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6189 - acc: 0.7619\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6628 - acc: 0.7048\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6104 - acc: 0.7810\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6605 - acc: 0.7619\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6486 - acc: 0.7143\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6182 - acc: 0.7905\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6133 - acc: 0.7714\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6410 - acc: 0.7429\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6380 - acc: 0.7810\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6291 - acc: 0.7714\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.7031 - acc: 0.7048\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6810 - acc: 0.6857\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6209 - acc: 0.7714\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6352 - acc: 0.7143\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6085 - acc: 0.7619\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.6462 - acc: 0.7524\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6233 - acc: 0.7714\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6891 - acc: 0.7429\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.6082 - acc: 0.7714\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5983 - acc: 0.7810\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6324 - acc: 0.7524\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.6702 - acc: 0.7333\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6971 - acc: 0.7143\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6366 - acc: 0.7619\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6421 - acc: 0.7810\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.6061 - acc: 0.7619\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.6071 - acc: 0.7905\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6328 - acc: 0.7429\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 109us/step - loss: 0.6007 - acc: 0.7714\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.5822 - acc: 0.7524\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.6877 - acc: 0.7048\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6212 - acc: 0.7429\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.6150 - acc: 0.7905\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.6213 - acc: 0.7524\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5799 - acc: 0.7619\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5769 - acc: 0.7619\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5980 - acc: 0.7905\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.5831 - acc: 0.7619\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6825 - acc: 0.6857\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.6457 - acc: 0.7143\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.6066 - acc: 0.7524\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.6260 - acc: 0.7905\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6378 - acc: 0.7333\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.6368 - acc: 0.7238\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.6001 - acc: 0.7810\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.6120 - acc: 0.7714\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 74us/step - loss: 0.5727 - acc: 0.8000\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.6005 - acc: 0.8000\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.5702 - acc: 0.8095\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.6287 - acc: 0.7524\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.5972 - acc: 0.7714\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.6073 - acc: 0.7524\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5926 - acc: 0.7714\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5714 - acc: 0.7238\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.5869 - acc: 0.7810\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 66us/step - loss: 0.6113 - acc: 0.7810\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.6140 - acc: 0.7429\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.5803 - acc: 0.7619\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5706 - acc: 0.7810\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.6087 - acc: 0.7714\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.5813 - acc: 0.7810\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.5561 - acc: 0.7905\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5470 - acc: 0.7524\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5669 - acc: 0.7810\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 73us/step - loss: 0.5655 - acc: 0.8000\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.5719 - acc: 0.7429\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5305 - acc: 0.8190\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5607 - acc: 0.7810\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.6242 - acc: 0.7429\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5409 - acc: 0.8095\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 131us/step - loss: 0.5532 - acc: 0.7714\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5900 - acc: 0.7524\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.5708 - acc: 0.7714\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5650 - acc: 0.7810\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5533 - acc: 0.7619\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5497 - acc: 0.7810\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5423 - acc: 0.7905\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5761 - acc: 0.7810\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5994 - acc: 0.7619\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5869 - acc: 0.7714\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5826 - acc: 0.7524\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5619 - acc: 0.7810\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5462 - acc: 0.7810\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5880 - acc: 0.7619\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4954 - acc: 0.8000\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5612 - acc: 0.8190\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5274 - acc: 0.8095\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 70us/step - loss: 0.5572 - acc: 0.8000\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 81us/step - loss: 0.5916 - acc: 0.7524\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5793 - acc: 0.7714\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.5331 - acc: 0.8190\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 92us/step - loss: 0.5683 - acc: 0.7810\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5823 - acc: 0.7524\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 67us/step - loss: 0.5648 - acc: 0.8190\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5606 - acc: 0.7714\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5593 - acc: 0.7714\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.5455 - acc: 0.8095\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5024 - acc: 0.7905\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5504 - acc: 0.8000\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5124 - acc: 0.8190\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5426 - acc: 0.7810\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5371 - acc: 0.7810\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5563 - acc: 0.8000\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5406 - acc: 0.7810\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5252 - acc: 0.8190\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6024 - acc: 0.7810\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5729 - acc: 0.7905\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5388 - acc: 0.8190\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5742 - acc: 0.7714\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5632 - acc: 0.7429\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5894 - acc: 0.7238\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5630 - acc: 0.8095\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5484 - acc: 0.7714\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5091 - acc: 0.8476\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5136 - acc: 0.8095\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5206 - acc: 0.8476\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5600 - acc: 0.8000\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5480 - acc: 0.7905\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4730 - acc: 0.8190\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5260 - acc: 0.8190\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5064 - acc: 0.8190\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5475 - acc: 0.7905\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4981 - acc: 0.8095\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5318 - acc: 0.8000\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5693 - acc: 0.7810\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5723 - acc: 0.7905\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5717 - acc: 0.8000\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5135 - acc: 0.8190\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5572 - acc: 0.8095\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5282 - acc: 0.7905\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5126 - acc: 0.8381\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4881 - acc: 0.8095\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4776 - acc: 0.8476\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5178 - acc: 0.7905\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.5326 - acc: 0.8000\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5182 - acc: 0.8095\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4901 - acc: 0.8381\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4890 - acc: 0.8286\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4861 - acc: 0.8571\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4824 - acc: 0.8286\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5039 - acc: 0.7810\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5207 - acc: 0.8000\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5202 - acc: 0.8095\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4921 - acc: 0.8571\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5152 - acc: 0.7810\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5603 - acc: 0.7714\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5383 - acc: 0.8095\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6292 - acc: 0.7429\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5140 - acc: 0.8476\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5244 - acc: 0.7714\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4850 - acc: 0.8286\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4961 - acc: 0.8571\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5365 - acc: 0.8190\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5057 - acc: 0.8476\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5148 - acc: 0.8476\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5360 - acc: 0.8000\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4821 - acc: 0.8381\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5210 - acc: 0.8000\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4814 - acc: 0.8381\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5187 - acc: 0.8381\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5380 - acc: 0.7905\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4846 - acc: 0.8190\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4936 - acc: 0.8286\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.5070 - acc: 0.8000\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5516 - acc: 0.7810\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5179 - acc: 0.7905\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4860 - acc: 0.8381\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5674 - acc: 0.8000\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5396 - acc: 0.7810\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4614 - acc: 0.8571\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5257 - acc: 0.8190\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4681 - acc: 0.8286\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5512 - acc: 0.7619\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4854 - acc: 0.8381\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4858 - acc: 0.8000\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4860 - acc: 0.8000\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4807 - acc: 0.8571\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4906 - acc: 0.8000\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4780 - acc: 0.7905\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5096 - acc: 0.8476\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4743 - acc: 0.8190\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4636 - acc: 0.8190\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4974 - acc: 0.7905\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.5082 - acc: 0.8286\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5294 - acc: 0.7619\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4874 - acc: 0.8095\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4933 - acc: 0.8095\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4857 - acc: 0.8571\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5505 - acc: 0.7810\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4831 - acc: 0.8571\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5045 - acc: 0.8476\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5620 - acc: 0.7619\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4750 - acc: 0.8571\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4606 - acc: 0.8476\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4912 - acc: 0.8095\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4785 - acc: 0.8381\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5239 - acc: 0.8190\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.5097 - acc: 0.8190\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4812 - acc: 0.8381\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5310 - acc: 0.8571\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4752 - acc: 0.8286\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4298 - acc: 0.8762\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5146 - acc: 0.8095\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4754 - acc: 0.8286\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4081 - acc: 0.9048\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4854 - acc: 0.8571\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4772 - acc: 0.8381\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4651 - acc: 0.8952\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5254 - acc: 0.8190\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4216 - acc: 0.8857\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5029 - acc: 0.8000\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4360 - acc: 0.8667\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4753 - acc: 0.8381\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4594 - acc: 0.8190\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4676 - acc: 0.8381\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5211 - acc: 0.8190\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4348 - acc: 0.8952\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4550 - acc: 0.8667\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4293 - acc: 0.8667\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4544 - acc: 0.8381\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.5068 - acc: 0.8667\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5010 - acc: 0.8286\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.5277 - acc: 0.7714\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4518 - acc: 0.8762\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4479 - acc: 0.8667\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4102 - acc: 0.8381\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4426 - acc: 0.8857\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4721 - acc: 0.8286\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.4752 - acc: 0.8667\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4016 - acc: 0.8667\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4381 - acc: 0.8667\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4737 - acc: 0.8190\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4214 - acc: 0.9048\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.5498 - acc: 0.8000\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4226 - acc: 0.8667\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4171 - acc: 0.8857\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4739 - acc: 0.8762\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5061 - acc: 0.8000\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4919 - acc: 0.8571\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4796 - acc: 0.8381\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3873 - acc: 0.8952\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4389 - acc: 0.8571\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.4484 - acc: 0.8571\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4019 - acc: 0.9048\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4456 - acc: 0.8476\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4179 - acc: 0.8952\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3975 - acc: 0.8762\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4385 - acc: 0.8762\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4336 - acc: 0.8762\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4994 - acc: 0.7905\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3745 - acc: 0.9333\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3977 - acc: 0.9048\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4423 - acc: 0.8762\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4538 - acc: 0.8762\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3897 - acc: 0.8952\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4028 - acc: 0.8476\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3861 - acc: 0.8857\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4313 - acc: 0.8762\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4624 - acc: 0.8381\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3772 - acc: 0.8952\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.5134 - acc: 0.8286\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4695 - acc: 0.8762\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4434 - acc: 0.8476\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4539 - acc: 0.8000\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4144 - acc: 0.8571\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4762 - acc: 0.8286\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.4579 - acc: 0.8190\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4248 - acc: 0.8952\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4672 - acc: 0.8857\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4722 - acc: 0.8762\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4828 - acc: 0.8000\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4192 - acc: 0.8571\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4599 - acc: 0.8190\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3661 - acc: 0.9048\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.4184 - acc: 0.8952\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4072 - acc: 0.8667\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4245 - acc: 0.8571\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.4678 - acc: 0.8476\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4224 - acc: 0.9048\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4046 - acc: 0.8571\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4076 - acc: 0.8857\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4369 - acc: 0.8857\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3940 - acc: 0.8667\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4163 - acc: 0.8476\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3502 - acc: 0.8952\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3672 - acc: 0.8952\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4026 - acc: 0.9048\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3913 - acc: 0.9048\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4593 - acc: 0.8286\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3873 - acc: 0.9048\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4513 - acc: 0.8476\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3996 - acc: 0.9048\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3739 - acc: 0.8857\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4336 - acc: 0.8762\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4501 - acc: 0.8571\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3540 - acc: 0.9143\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4088 - acc: 0.8857\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3663 - acc: 0.9238\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4599 - acc: 0.8571\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4180 - acc: 0.8857\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4177 - acc: 0.8476\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4711 - acc: 0.8381\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3752 - acc: 0.9048\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4234 - acc: 0.8571\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3704 - acc: 0.9238\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4490 - acc: 0.8286\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4360 - acc: 0.8762\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3578 - acc: 0.9143\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 71us/step - loss: 0.4461 - acc: 0.8381\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3965 - acc: 0.8571\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.4586 - acc: 0.8190\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4281 - acc: 0.8667\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4172 - acc: 0.9143\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4162 - acc: 0.8762\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3813 - acc: 0.8952\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3790 - acc: 0.9143\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4441 - acc: 0.8857\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3928 - acc: 0.8762\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4202 - acc: 0.8952\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3211 - acc: 0.9524\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4011 - acc: 0.8857\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4116 - acc: 0.8952\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4072 - acc: 0.8857\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4103 - acc: 0.8762\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.4144 - acc: 0.8857\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4339 - acc: 0.8381\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4116 - acc: 0.8857\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4076 - acc: 0.8857\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3374 - acc: 0.8952\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4114 - acc: 0.8952\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3763 - acc: 0.9429\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4284 - acc: 0.8857\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3930 - acc: 0.9048\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4283 - acc: 0.8667\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4002 - acc: 0.8952\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4396 - acc: 0.8476\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3580 - acc: 0.9143\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3413 - acc: 0.9333\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4069 - acc: 0.8762\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3863 - acc: 0.9048\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3636 - acc: 0.9238\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 57us/step - loss: 0.3624 - acc: 0.9238\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3756 - acc: 0.9143\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3441 - acc: 0.9143\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.4243 - acc: 0.9048\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4019 - acc: 0.8476\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4213 - acc: 0.8762\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4197 - acc: 0.8762\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3770 - acc: 0.8952\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3680 - acc: 0.9048\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3686 - acc: 0.9238\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3992 - acc: 0.8857\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4400 - acc: 0.8762\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3825 - acc: 0.8857\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3245 - acc: 0.9619\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3952 - acc: 0.8667\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3555 - acc: 0.9048\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3488 - acc: 0.9143\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3560 - acc: 0.9143\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3519 - acc: 0.9238\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3646 - acc: 0.9048\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.4008 - acc: 0.8667\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4256 - acc: 0.8762\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4372 - acc: 0.8286\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3627 - acc: 0.9048\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3695 - acc: 0.9048\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3597 - acc: 0.8952\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3990 - acc: 0.9048\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3542 - acc: 0.8952\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3681 - acc: 0.8952\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.4121 - acc: 0.8667\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.4011 - acc: 0.8952\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3884 - acc: 0.9238\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3514 - acc: 0.8952\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3564 - acc: 0.9143\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3340 - acc: 0.9238\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3491 - acc: 0.8857\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3444 - acc: 0.8952\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3400 - acc: 0.9238\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3887 - acc: 0.8667\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3900 - acc: 0.9048\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3497 - acc: 0.9238\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3638 - acc: 0.9048\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3723 - acc: 0.8571\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3636 - acc: 0.9333\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3626 - acc: 0.9143\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3728 - acc: 0.8762\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3782 - acc: 0.9238\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3645 - acc: 0.8857\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.4445 - acc: 0.8667\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.4511 - acc: 0.8857\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3846 - acc: 0.9333\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3771 - acc: 0.8857\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3995 - acc: 0.8952\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3425 - acc: 0.8952\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3519 - acc: 0.9238\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4161 - acc: 0.8476\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3776 - acc: 0.9048\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3395 - acc: 0.9143\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3927 - acc: 0.8762\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3972 - acc: 0.8381\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.4097 - acc: 0.8667\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3954 - acc: 0.8667\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3211 - acc: 0.9619\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3614 - acc: 0.8762\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.4170 - acc: 0.8571\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3720 - acc: 0.8857\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4011 - acc: 0.8571\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3415 - acc: 0.9048\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3250 - acc: 0.9143\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3016 - acc: 0.9333\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3384 - acc: 0.9238\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3508 - acc: 0.9143\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4041 - acc: 0.8952\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3876 - acc: 0.8667\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3593 - acc: 0.9048\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3421 - acc: 0.9333\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4155 - acc: 0.8857\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3745 - acc: 0.9143\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3582 - acc: 0.8952\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3687 - acc: 0.9048\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3498 - acc: 0.9143\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3376 - acc: 0.9429\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.4174 - acc: 0.8857\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.4081 - acc: 0.8762\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3452 - acc: 0.9333\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3209 - acc: 0.9143\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3170 - acc: 0.9238\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3024 - acc: 0.9524\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3640 - acc: 0.9048\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3165 - acc: 0.9333\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3376 - acc: 0.9143\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3761 - acc: 0.8952\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3442 - acc: 0.9048\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.4584 - acc: 0.9238\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3818 - acc: 0.8857\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3093 - acc: 0.9524\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3184 - acc: 0.9238\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3437 - acc: 0.9048\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3521 - acc: 0.9048\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2882 - acc: 0.9714\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4291 - acc: 0.8476\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3566 - acc: 0.9143\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3377 - acc: 0.9143\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3698 - acc: 0.8952\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2954 - acc: 0.9143\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3197 - acc: 0.9143\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2871 - acc: 0.9333\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2806 - acc: 0.9619\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3126 - acc: 0.9333\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2936 - acc: 0.9333\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3742 - acc: 0.9143\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3663 - acc: 0.9048\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3087 - acc: 0.9238\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3344 - acc: 0.9429\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2963 - acc: 0.9524\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3995 - acc: 0.9048\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3300 - acc: 0.9143\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3349 - acc: 0.8857\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3319 - acc: 0.9238\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3100 - acc: 0.9333\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.4045 - acc: 0.8571\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2995 - acc: 0.9333\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3400 - acc: 0.9429\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3172 - acc: 0.9429\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3277 - acc: 0.9048\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3377 - acc: 0.9238\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3058 - acc: 0.9524\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2997 - acc: 0.9429\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3298 - acc: 0.9429\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3733 - acc: 0.9048\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3700 - acc: 0.8952\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3314 - acc: 0.9238\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3104 - acc: 0.9143\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3294 - acc: 0.9333\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3817 - acc: 0.8952\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3914 - acc: 0.8952\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3228 - acc: 0.9333\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3240 - acc: 0.9048\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3127 - acc: 0.9429\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3161 - acc: 0.9048\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3123 - acc: 0.9238\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3153 - acc: 0.9143\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3129 - acc: 0.9143\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3502 - acc: 0.8667\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2868 - acc: 0.9524\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3523 - acc: 0.8857\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3407 - acc: 0.9238\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3311 - acc: 0.9238\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3410 - acc: 0.9333\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3568 - acc: 0.9238\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3504 - acc: 0.8762\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3587 - acc: 0.9333\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3769 - acc: 0.9238\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3099 - acc: 0.9524\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3133 - acc: 0.9333\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3678 - acc: 0.9048\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2864 - acc: 0.9333\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2896 - acc: 0.9429\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3903 - acc: 0.9048\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2851 - acc: 0.9429\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3082 - acc: 0.9048\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3461 - acc: 0.9048\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3200 - acc: 0.9238\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3359 - acc: 0.9143\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3264 - acc: 0.9333\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2983 - acc: 0.9429\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3267 - acc: 0.9524\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2996 - acc: 0.9238\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.3666 - acc: 0.8667\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4223 - acc: 0.9143\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2862 - acc: 0.9429\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2958 - acc: 0.9333\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.3446 - acc: 0.9333\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3221 - acc: 0.8952\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3007 - acc: 0.9238\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.3186 - acc: 0.9429\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3486 - acc: 0.9143\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2811 - acc: 0.9429\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3619 - acc: 0.8857\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2931 - acc: 0.9429\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.3619 - acc: 0.9048\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.3024 - acc: 0.9238\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3093 - acc: 0.9333\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2788 - acc: 0.9429\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3064 - acc: 0.9238\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2678 - acc: 0.9619\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3292 - acc: 0.8952\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3468 - acc: 0.9238\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3353 - acc: 0.9238\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3012 - acc: 0.9238\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3243 - acc: 0.9333\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3304 - acc: 0.8952\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3782 - acc: 0.8762\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2678 - acc: 0.9333\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3010 - acc: 0.9143\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2659 - acc: 0.9619\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2936 - acc: 0.9429\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2897 - acc: 0.9429\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3110 - acc: 0.9238\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2843 - acc: 0.9333\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3317 - acc: 0.9143\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3197 - acc: 0.9143\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3111 - acc: 0.9333\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3524 - acc: 0.8857\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3328 - acc: 0.9238\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3213 - acc: 0.9048\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2380 - acc: 0.9714\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3274 - acc: 0.9048\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2990 - acc: 0.9333\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3002 - acc: 0.9333\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.2905 - acc: 0.9524\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3729 - acc: 0.8952\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3255 - acc: 0.9048\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3232 - acc: 0.9048\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3057 - acc: 0.9429\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.3216 - acc: 0.9143\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3002 - acc: 0.9238\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3030 - acc: 0.9238\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3089 - acc: 0.9524\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3469 - acc: 0.9143\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3144 - acc: 0.9238\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3255 - acc: 0.9143\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3117 - acc: 0.9238\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2567 - acc: 0.9524\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3124 - acc: 0.9238\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2572 - acc: 0.9619\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3172 - acc: 0.9429\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3155 - acc: 0.8952\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2933 - acc: 0.9429\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3064 - acc: 0.9143\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2614 - acc: 0.9714\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2842 - acc: 0.9429\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3219 - acc: 0.9429\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.3016 - acc: 0.9143\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3246 - acc: 0.9238\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3190 - acc: 0.9143\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3018 - acc: 0.9238\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2801 - acc: 0.9524\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2518 - acc: 0.9714\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3283 - acc: 0.9429\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3036 - acc: 0.9524\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2667 - acc: 0.9429\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3155 - acc: 0.9238\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3430 - acc: 0.8952\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3161 - acc: 0.9238\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3301 - acc: 0.8952\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2629 - acc: 0.9619\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2464 - acc: 0.9714\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2679 - acc: 0.9619\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2666 - acc: 0.9619\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3165 - acc: 0.9333\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2845 - acc: 0.9143\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3345 - acc: 0.9238\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2713 - acc: 0.9429\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2727 - acc: 0.9619\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2933 - acc: 0.9143\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2638 - acc: 0.9619\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3139 - acc: 0.9143\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2830 - acc: 0.9333\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2800 - acc: 0.9619\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2940 - acc: 0.9143\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3349 - acc: 0.9619\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2758 - acc: 0.9429\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3298 - acc: 0.9048\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3106 - acc: 0.9048\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2884 - acc: 0.9429\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2903 - acc: 0.9429\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2934 - acc: 0.9524\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3022 - acc: 0.9524\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3691 - acc: 0.8857\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2757 - acc: 0.9333\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2845 - acc: 0.9333\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3219 - acc: 0.9333\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2441 - acc: 0.9714\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2870 - acc: 0.9429\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3348 - acc: 0.9143\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2704 - acc: 0.9429\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3211 - acc: 0.9333\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2973 - acc: 0.9238\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3100 - acc: 0.9048\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2579 - acc: 0.9619\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3476 - acc: 0.9048\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2880 - acc: 0.9333\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2538 - acc: 0.9524\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2826 - acc: 0.9429\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2292 - acc: 0.9714\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.3137 - acc: 0.9048\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2789 - acc: 0.9619\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2585 - acc: 0.9714\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3563 - acc: 0.9143\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2816 - acc: 0.9619\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2794 - acc: 0.9524\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3106 - acc: 0.9429\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2951 - acc: 0.9429\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2624 - acc: 0.9714\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2765 - acc: 0.9524\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2588 - acc: 0.9619\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2923 - acc: 0.9238\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2850 - acc: 0.9429\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2480 - acc: 0.9524\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2871 - acc: 0.9619\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2391 - acc: 0.9714\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2386 - acc: 0.9524\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2861 - acc: 0.9524\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2535 - acc: 0.9714\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2579 - acc: 0.9429\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2405 - acc: 0.9714\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2597 - acc: 0.9429\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2847 - acc: 0.9524\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2970 - acc: 0.9429\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2932 - acc: 0.9048\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3279 - acc: 0.9143\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3312 - acc: 0.9048\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3120 - acc: 0.9619\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2328 - acc: 0.9619\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.3102 - acc: 0.9238\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3018 - acc: 0.9429\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2797 - acc: 0.9429\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3074 - acc: 0.9333\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2675 - acc: 0.9619\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3064 - acc: 0.9143\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2243 - acc: 0.9714\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2530 - acc: 0.9429\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2894 - acc: 0.9048\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2599 - acc: 0.9619\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2470 - acc: 0.9524\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2711 - acc: 0.9238\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2165 - acc: 0.9810\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2663 - acc: 0.9714\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2501 - acc: 0.9333\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2512 - acc: 0.9524\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3126 - acc: 0.9429\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2858 - acc: 0.9714\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2801 - acc: 0.9524\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2646 - acc: 0.9429\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2568 - acc: 0.9524\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2450 - acc: 0.9619\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2994 - acc: 0.9143\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2059 - acc: 0.9905\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2515 - acc: 0.9238\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2499 - acc: 0.9619\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2577 - acc: 0.9619\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2609 - acc: 0.9524\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2663 - acc: 0.9429\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2238 - acc: 0.9810\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3529 - acc: 0.8952\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 69us/step - loss: 0.3644 - acc: 0.8857\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2540 - acc: 0.9429\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3209 - acc: 0.9143\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2728 - acc: 0.9333\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2832 - acc: 0.9333\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2770 - acc: 0.9429\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2314 - acc: 0.9810\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2286 - acc: 0.9619\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2739 - acc: 0.9524\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2516 - acc: 0.9714\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3039 - acc: 0.9619\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2512 - acc: 0.9619\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3096 - acc: 0.9333\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2533 - acc: 0.9429\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2335 - acc: 0.9619\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2975 - acc: 0.9238\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2402 - acc: 0.9619\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2494 - acc: 0.9429\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3083 - acc: 0.9238\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3057 - acc: 0.9524\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2864 - acc: 0.9429\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2509 - acc: 0.9524\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2604 - acc: 0.9429\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.2255 - acc: 0.9810\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2244 - acc: 0.9619\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2286 - acc: 0.9619\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2938 - acc: 0.9429\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3292 - acc: 0.9143\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.3720 - acc: 0.8667\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2987 - acc: 0.9238\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2663 - acc: 0.9524\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2483 - acc: 0.9524\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3037 - acc: 0.9238\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3137 - acc: 0.9048\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2818 - acc: 0.9238\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3126 - acc: 0.9048\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2672 - acc: 0.9714\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2308 - acc: 0.9714\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2599 - acc: 0.9429\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2695 - acc: 0.9524\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2321 - acc: 0.9714\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2632 - acc: 0.9333\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2566 - acc: 0.9524\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2715 - acc: 0.9429\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2441 - acc: 0.9619\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2301 - acc: 0.9619\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2331 - acc: 0.9714\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2937 - acc: 0.9238\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2546 - acc: 0.9524\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.3295 - acc: 0.9143\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2731 - acc: 0.9619\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2780 - acc: 0.9333\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2737 - acc: 0.9333\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2695 - acc: 0.9238\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2373 - acc: 0.9619\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2809 - acc: 0.9429\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2419 - acc: 0.9524\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2392 - acc: 0.9524\n",
            "13/13 [==============================] - 1s 85ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff7576f8-bef3-4d03-d19d-4ff558c89c12",
        "id": "kzOpP3sorVPp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6a12ba6a-9f05-4702-f549-37f98e0cff61",
        "id": "FMu192LtrVP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTT5PC0arVQB"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HBt6_60xcak",
        "colab_type": "text"
      },
      "source": [
        "#NO FEATURES SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvsgRz43UmmK",
        "colab_type": "text"
      },
      "source": [
        "#ANOVA features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjP9p0KUUpSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features_ANOVA = ['HighGrayLevelEmphasis', 'Maximum', 'Range', 'LongRunHighGrayLevelEmphasis', 'HighGrayLevelRunEmphasis', 'GrayLevelVariance.2', 'SmallAreaHighGrayLevelEmphasis', 'ZoneEntropy']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_7dMtXy6V6w8"
      },
      "source": [
        "##Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "d5cbddd9-19fb-4759-cb36-7e9ea242327c",
        "id": "nki5nfvxV6xJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvgkbEreV6xg",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gDd3cyhNV6xs",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wILUlSjuV6x3",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0a-oybEuV6yC",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v-kJNv3BV6yM",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G2z7Ac0iV6yV",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)\n",
        "public_data = public_data[features_ANOVA]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QlokVHzmV6yg",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)\n",
        "PA_data = PA_data[features_ANOVA]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kv3TVnS2V6yq",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UCA1Sw5-V6yy",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vHrm81WnWBw6"
      },
      "source": [
        "##Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1ivMbx62WBxH",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2kkLT2_HWBxd",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-5dgeIzIWG82"
      },
      "source": [
        "##Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SU7Sy4FMWG9G",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xw2hHWeqWG9c",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P0sRZggHWNyc"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QiLDU2y9WNym",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uHFwSfD0U7uw"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G6ioznwNU7u0",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i8aHr8G9U7vA",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3dRnQTcOU7vJ",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZaD5AmHkU7vX",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hO1-Xv7YU7vg",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IPc7DXP4U7vr",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MLMlbEQaU7v6"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MELTMqXrU7v-",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FRTRwqxqU7wL",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ndEYgOxCU7wX",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.optimizers import Adagrad\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.optimizers import Adam\n",
        "from keras.optimizers import Adamax\n",
        "from keras.optimizers import Nadam\n",
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xnZAB8sIU7wh",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RRrMo1-aU7wo",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ej_rRHOVU7wy",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(30, activation='relu', input_shape=(8,), kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(30, activation='relu', kernel_regularizer=regularizers.l2(l=0.001)))\n",
        "  #model.add(layers.Dropout(0.1))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.9)\n",
        "  adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "  model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VLMaqzP4U7w-",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)\n",
        "#usandolo la loss non scende anche se non agisce, COME MAI????\n",
        "#non usandolo e non variando nient'altro la loss scende molto rapidamente"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "31cb4697-cd47-4658-8095-d5790108c1fb",
        "id": "X5W1J2uQU7xH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "one_hot_val_labels.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "73130765-80fd-42e0-c0f1-23991820e0bc",
        "id": "HyO5tYZ_U7xP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 1000\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand, one_hot_train_labels, validation_data=(val_data_stand, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=105)\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/1000\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 1.1297 - acc: 0.3810 - val_loss: 1.0111 - val_acc: 0.6154\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.1178 - acc: 0.4000 - val_loss: 1.0094 - val_acc: 0.6154\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.1049 - acc: 0.3905 - val_loss: 1.0083 - val_acc: 0.6154\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0922 - acc: 0.4286 - val_loss: 1.0081 - val_acc: 0.6154\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0802 - acc: 0.4381 - val_loss: 1.0087 - val_acc: 0.6923\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0689 - acc: 0.4571 - val_loss: 1.0095 - val_acc: 0.6154\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0584 - acc: 0.4762 - val_loss: 1.0112 - val_acc: 0.6923\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0488 - acc: 0.4857 - val_loss: 1.0135 - val_acc: 0.6923\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0400 - acc: 0.4762 - val_loss: 1.0164 - val_acc: 0.6154\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0321 - acc: 0.4762 - val_loss: 1.0198 - val_acc: 0.6154\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0248 - acc: 0.4857 - val_loss: 1.0236 - val_acc: 0.5385\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0181 - acc: 0.4857 - val_loss: 1.0279 - val_acc: 0.5385\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0120 - acc: 0.4762 - val_loss: 1.0325 - val_acc: 0.5385\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0066 - acc: 0.4762 - val_loss: 1.0372 - val_acc: 0.5385\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0016 - acc: 0.4857 - val_loss: 1.0418 - val_acc: 0.5385\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9972 - acc: 0.4857 - val_loss: 1.0459 - val_acc: 0.5385\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9930 - acc: 0.4952 - val_loss: 1.0489 - val_acc: 0.5385\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9893 - acc: 0.5143 - val_loss: 1.0520 - val_acc: 0.5385\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.9859 - acc: 0.5143 - val_loss: 1.0550 - val_acc: 0.5385\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9826 - acc: 0.5524 - val_loss: 1.0578 - val_acc: 0.5385\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.9796 - acc: 0.5524 - val_loss: 1.0603 - val_acc: 0.5385\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.9766 - acc: 0.5524 - val_loss: 1.0625 - val_acc: 0.5385\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.9738 - acc: 0.5619 - val_loss: 1.0646 - val_acc: 0.5385\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.9711 - acc: 0.5714 - val_loss: 1.0665 - val_acc: 0.5385\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.9685 - acc: 0.5714 - val_loss: 1.0682 - val_acc: 0.5385\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.9660 - acc: 0.5810 - val_loss: 1.0695 - val_acc: 0.5385\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9636 - acc: 0.5905 - val_loss: 1.0706 - val_acc: 0.5385\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.9612 - acc: 0.5905 - val_loss: 1.0714 - val_acc: 0.5385\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9590 - acc: 0.5905 - val_loss: 1.0721 - val_acc: 0.5385\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.9569 - acc: 0.5905 - val_loss: 1.0726 - val_acc: 0.5385\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9547 - acc: 0.5905 - val_loss: 1.0732 - val_acc: 0.5385\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9524 - acc: 0.5905 - val_loss: 1.0737 - val_acc: 0.5385\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9502 - acc: 0.5905 - val_loss: 1.0741 - val_acc: 0.5385\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.9480 - acc: 0.6000 - val_loss: 1.0744 - val_acc: 0.5385\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9458 - acc: 0.6190 - val_loss: 1.0747 - val_acc: 0.5385\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.9436 - acc: 0.6190 - val_loss: 1.0749 - val_acc: 0.5385\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9415 - acc: 0.6190 - val_loss: 1.0750 - val_acc: 0.5385\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9393 - acc: 0.6190 - val_loss: 1.0752 - val_acc: 0.5385\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.9372 - acc: 0.6286 - val_loss: 1.0753 - val_acc: 0.5385\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.9351 - acc: 0.6381 - val_loss: 1.0759 - val_acc: 0.5385\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 82us/step - loss: 0.9330 - acc: 0.6381 - val_loss: 1.0766 - val_acc: 0.5385\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.9310 - acc: 0.6476 - val_loss: 1.0772 - val_acc: 0.5385\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.9290 - acc: 0.6476 - val_loss: 1.0777 - val_acc: 0.5385\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.9270 - acc: 0.6476 - val_loss: 1.0780 - val_acc: 0.5385\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9251 - acc: 0.6476 - val_loss: 1.0784 - val_acc: 0.5385\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.9232 - acc: 0.6381 - val_loss: 1.0788 - val_acc: 0.5385\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.9214 - acc: 0.6476 - val_loss: 1.0794 - val_acc: 0.5385\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.9195 - acc: 0.6476 - val_loss: 1.0799 - val_acc: 0.5385\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 78us/step - loss: 0.9177 - acc: 0.6476 - val_loss: 1.0804 - val_acc: 0.5385\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 89us/step - loss: 0.9158 - acc: 0.6476 - val_loss: 1.0810 - val_acc: 0.5385\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 76us/step - loss: 0.9140 - acc: 0.6476 - val_loss: 1.0816 - val_acc: 0.5385\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.9123 - acc: 0.6476 - val_loss: 1.0823 - val_acc: 0.5385\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.9105 - acc: 0.6476 - val_loss: 1.0830 - val_acc: 0.5385\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 64us/step - loss: 0.9087 - acc: 0.6476 - val_loss: 1.0838 - val_acc: 0.5385\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 62us/step - loss: 0.9070 - acc: 0.6476 - val_loss: 1.0846 - val_acc: 0.5385\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.9053 - acc: 0.6476 - val_loss: 1.0853 - val_acc: 0.5385\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.9035 - acc: 0.6476 - val_loss: 1.0861 - val_acc: 0.5385\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.9018 - acc: 0.6476 - val_loss: 1.0869 - val_acc: 0.5385\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.9000 - acc: 0.6476 - val_loss: 1.0876 - val_acc: 0.5385\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8983 - acc: 0.6476 - val_loss: 1.0884 - val_acc: 0.5385\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.8966 - acc: 0.6476 - val_loss: 1.0890 - val_acc: 0.5385\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8948 - acc: 0.6381 - val_loss: 1.0899 - val_acc: 0.5385\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.8932 - acc: 0.6381 - val_loss: 1.0910 - val_acc: 0.5385\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.8917 - acc: 0.6381 - val_loss: 1.0923 - val_acc: 0.5385\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8901 - acc: 0.6381 - val_loss: 1.0939 - val_acc: 0.5385\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8885 - acc: 0.6381 - val_loss: 1.0956 - val_acc: 0.5385\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8869 - acc: 0.6381 - val_loss: 1.0974 - val_acc: 0.5385\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8854 - acc: 0.6381 - val_loss: 1.0992 - val_acc: 0.5385\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8838 - acc: 0.6381 - val_loss: 1.1008 - val_acc: 0.5385\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8822 - acc: 0.6381 - val_loss: 1.1024 - val_acc: 0.5385\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.8807 - acc: 0.6381 - val_loss: 1.1039 - val_acc: 0.5385\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8791 - acc: 0.6476 - val_loss: 1.1054 - val_acc: 0.5385\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8775 - acc: 0.6476 - val_loss: 1.1068 - val_acc: 0.5385\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8759 - acc: 0.6476 - val_loss: 1.1078 - val_acc: 0.5385\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8744 - acc: 0.6571 - val_loss: 1.1087 - val_acc: 0.5385\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8729 - acc: 0.6571 - val_loss: 1.1094 - val_acc: 0.5385\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8713 - acc: 0.6571 - val_loss: 1.1100 - val_acc: 0.5385\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8698 - acc: 0.6571 - val_loss: 1.1105 - val_acc: 0.5385\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8682 - acc: 0.6571 - val_loss: 1.1109 - val_acc: 0.5385\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8667 - acc: 0.6571 - val_loss: 1.1115 - val_acc: 0.5385\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8651 - acc: 0.6571 - val_loss: 1.1122 - val_acc: 0.5385\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.8635 - acc: 0.6476 - val_loss: 1.1129 - val_acc: 0.4615\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8620 - acc: 0.6476 - val_loss: 1.1139 - val_acc: 0.4615\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8604 - acc: 0.6476 - val_loss: 1.1151 - val_acc: 0.4615\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8588 - acc: 0.6476 - val_loss: 1.1165 - val_acc: 0.4615\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8573 - acc: 0.6476 - val_loss: 1.1178 - val_acc: 0.4615\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8558 - acc: 0.6476 - val_loss: 1.1190 - val_acc: 0.4615\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.8543 - acc: 0.6476 - val_loss: 1.1203 - val_acc: 0.4615\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8528 - acc: 0.6476 - val_loss: 1.1215 - val_acc: 0.4615\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8513 - acc: 0.6476 - val_loss: 1.1228 - val_acc: 0.4615\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.8498 - acc: 0.6476 - val_loss: 1.1241 - val_acc: 0.4615\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.8484 - acc: 0.6476 - val_loss: 1.1255 - val_acc: 0.4615\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8469 - acc: 0.6476 - val_loss: 1.1268 - val_acc: 0.4615\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8454 - acc: 0.6476 - val_loss: 1.1281 - val_acc: 0.4615\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8440 - acc: 0.6476 - val_loss: 1.1294 - val_acc: 0.4615\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.8425 - acc: 0.6476 - val_loss: 1.1306 - val_acc: 0.4615\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.8410 - acc: 0.6476 - val_loss: 1.1319 - val_acc: 0.4615\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.8395 - acc: 0.6476 - val_loss: 1.1333 - val_acc: 0.4615\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8380 - acc: 0.6476 - val_loss: 1.1346 - val_acc: 0.4615\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8366 - acc: 0.6476 - val_loss: 1.1359 - val_acc: 0.4615\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8351 - acc: 0.6476 - val_loss: 1.1371 - val_acc: 0.4615\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8337 - acc: 0.6476 - val_loss: 1.1383 - val_acc: 0.4615\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8323 - acc: 0.6476 - val_loss: 1.1395 - val_acc: 0.4615\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8308 - acc: 0.6476 - val_loss: 1.1406 - val_acc: 0.4615\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8294 - acc: 0.6476 - val_loss: 1.1417 - val_acc: 0.4615\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8279 - acc: 0.6476 - val_loss: 1.1426 - val_acc: 0.4615\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.8265 - acc: 0.6571 - val_loss: 1.1435 - val_acc: 0.4615\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8250 - acc: 0.6571 - val_loss: 1.1444 - val_acc: 0.4615\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.8235 - acc: 0.6571 - val_loss: 1.1451 - val_acc: 0.4615\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.8221 - acc: 0.6571 - val_loss: 1.1460 - val_acc: 0.4615\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8206 - acc: 0.6571 - val_loss: 1.1471 - val_acc: 0.4615\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8192 - acc: 0.6571 - val_loss: 1.1481 - val_acc: 0.4615\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 145us/step - loss: 0.8178 - acc: 0.6571 - val_loss: 1.1491 - val_acc: 0.4615\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.8164 - acc: 0.6667 - val_loss: 1.1500 - val_acc: 0.4615\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.8149 - acc: 0.6667 - val_loss: 1.1509 - val_acc: 0.4615\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 92us/step - loss: 0.8135 - acc: 0.6667 - val_loss: 1.1519 - val_acc: 0.4615\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 63us/step - loss: 0.8121 - acc: 0.6667 - val_loss: 1.1530 - val_acc: 0.4615\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.8106 - acc: 0.6667 - val_loss: 1.1546 - val_acc: 0.4615\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.8092 - acc: 0.6762 - val_loss: 1.1561 - val_acc: 0.4615\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.8078 - acc: 0.6762 - val_loss: 1.1575 - val_acc: 0.4615\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.8063 - acc: 0.6762 - val_loss: 1.1588 - val_acc: 0.4615\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.8049 - acc: 0.6762 - val_loss: 1.1599 - val_acc: 0.4615\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8034 - acc: 0.6762 - val_loss: 1.1610 - val_acc: 0.4615\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8020 - acc: 0.6762 - val_loss: 1.1618 - val_acc: 0.4615\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.8005 - acc: 0.6762 - val_loss: 1.1623 - val_acc: 0.4615\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7991 - acc: 0.6762 - val_loss: 1.1627 - val_acc: 0.4615\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.7976 - acc: 0.6762 - val_loss: 1.1632 - val_acc: 0.4615\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7961 - acc: 0.6762 - val_loss: 1.1638 - val_acc: 0.4615\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7946 - acc: 0.6762 - val_loss: 1.1642 - val_acc: 0.4615\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7932 - acc: 0.6762 - val_loss: 1.1644 - val_acc: 0.4615\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7917 - acc: 0.6762 - val_loss: 1.1646 - val_acc: 0.4615\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7902 - acc: 0.6762 - val_loss: 1.1651 - val_acc: 0.4615\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7887 - acc: 0.6762 - val_loss: 1.1653 - val_acc: 0.4615\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7872 - acc: 0.6762 - val_loss: 1.1656 - val_acc: 0.4615\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7856 - acc: 0.6762 - val_loss: 1.1660 - val_acc: 0.4615\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7842 - acc: 0.6762 - val_loss: 1.1661 - val_acc: 0.4615\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7827 - acc: 0.6667 - val_loss: 1.1664 - val_acc: 0.4615\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7811 - acc: 0.6762 - val_loss: 1.1668 - val_acc: 0.4615\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7796 - acc: 0.6762 - val_loss: 1.1671 - val_acc: 0.4615\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7782 - acc: 0.6762 - val_loss: 1.1677 - val_acc: 0.4615\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7767 - acc: 0.6762 - val_loss: 1.1684 - val_acc: 0.4615\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7752 - acc: 0.6762 - val_loss: 1.1690 - val_acc: 0.4615\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7738 - acc: 0.6762 - val_loss: 1.1702 - val_acc: 0.4615\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7723 - acc: 0.6762 - val_loss: 1.1717 - val_acc: 0.4615\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7708 - acc: 0.6762 - val_loss: 1.1730 - val_acc: 0.4615\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7692 - acc: 0.6857 - val_loss: 1.1741 - val_acc: 0.4615\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7677 - acc: 0.6857 - val_loss: 1.1749 - val_acc: 0.4615\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7662 - acc: 0.6952 - val_loss: 1.1755 - val_acc: 0.4615\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7646 - acc: 0.6952 - val_loss: 1.1761 - val_acc: 0.4615\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7631 - acc: 0.7048 - val_loss: 1.1768 - val_acc: 0.4615\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7615 - acc: 0.7048 - val_loss: 1.1776 - val_acc: 0.4615\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7600 - acc: 0.7048 - val_loss: 1.1782 - val_acc: 0.4615\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7584 - acc: 0.7048 - val_loss: 1.1785 - val_acc: 0.4615\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7568 - acc: 0.7048 - val_loss: 1.1784 - val_acc: 0.4615\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7552 - acc: 0.7048 - val_loss: 1.1782 - val_acc: 0.4615\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7536 - acc: 0.7048 - val_loss: 1.1779 - val_acc: 0.4615\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7521 - acc: 0.7048 - val_loss: 1.1782 - val_acc: 0.4615\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7505 - acc: 0.7048 - val_loss: 1.1791 - val_acc: 0.4615\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7489 - acc: 0.7048 - val_loss: 1.1804 - val_acc: 0.4615\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7473 - acc: 0.7048 - val_loss: 1.1819 - val_acc: 0.4615\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7456 - acc: 0.7048 - val_loss: 1.1827 - val_acc: 0.4615\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7440 - acc: 0.7048 - val_loss: 1.1830 - val_acc: 0.4615\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7424 - acc: 0.7048 - val_loss: 1.1831 - val_acc: 0.4615\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7408 - acc: 0.7048 - val_loss: 1.1836 - val_acc: 0.4615\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7392 - acc: 0.7238 - val_loss: 1.1843 - val_acc: 0.4615\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7375 - acc: 0.7238 - val_loss: 1.1846 - val_acc: 0.4615\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7359 - acc: 0.7238 - val_loss: 1.1848 - val_acc: 0.4615\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7343 - acc: 0.7238 - val_loss: 1.1852 - val_acc: 0.4615\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7327 - acc: 0.7238 - val_loss: 1.1858 - val_acc: 0.4615\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7311 - acc: 0.7238 - val_loss: 1.1866 - val_acc: 0.4615\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7295 - acc: 0.7238 - val_loss: 1.1870 - val_acc: 0.4615\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7280 - acc: 0.7238 - val_loss: 1.1871 - val_acc: 0.4615\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7264 - acc: 0.7238 - val_loss: 1.1868 - val_acc: 0.4615\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.7247 - acc: 0.7238 - val_loss: 1.1863 - val_acc: 0.4615\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.7231 - acc: 0.7143 - val_loss: 1.1864 - val_acc: 0.4615\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.7215 - acc: 0.7143 - val_loss: 1.1869 - val_acc: 0.4615\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7199 - acc: 0.7238 - val_loss: 1.1876 - val_acc: 0.4615\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7183 - acc: 0.7238 - val_loss: 1.1875 - val_acc: 0.4615\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7167 - acc: 0.7238 - val_loss: 1.1868 - val_acc: 0.4615\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7150 - acc: 0.7238 - val_loss: 1.1861 - val_acc: 0.4615\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.7134 - acc: 0.7238 - val_loss: 1.1857 - val_acc: 0.4615\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.7118 - acc: 0.7238 - val_loss: 1.1858 - val_acc: 0.4615\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7102 - acc: 0.7238 - val_loss: 1.1861 - val_acc: 0.4615\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7085 - acc: 0.7238 - val_loss: 1.1861 - val_acc: 0.4615\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7069 - acc: 0.7238 - val_loss: 1.1859 - val_acc: 0.4615\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.7052 - acc: 0.7238 - val_loss: 1.1857 - val_acc: 0.4615\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.7036 - acc: 0.7238 - val_loss: 1.1860 - val_acc: 0.4615\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.7019 - acc: 0.7333 - val_loss: 1.1871 - val_acc: 0.5385\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.7003 - acc: 0.7333 - val_loss: 1.1878 - val_acc: 0.5385\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.6987 - acc: 0.7333 - val_loss: 1.1879 - val_acc: 0.5385\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.6971 - acc: 0.7333 - val_loss: 1.1882 - val_acc: 0.5385\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6954 - acc: 0.7429 - val_loss: 1.1885 - val_acc: 0.5385\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6937 - acc: 0.7429 - val_loss: 1.1888 - val_acc: 0.5385\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.6921 - acc: 0.7429 - val_loss: 1.1890 - val_acc: 0.5385\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6904 - acc: 0.7524 - val_loss: 1.1891 - val_acc: 0.5385\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.6888 - acc: 0.7524 - val_loss: 1.1884 - val_acc: 0.5385\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6871 - acc: 0.7524 - val_loss: 1.1882 - val_acc: 0.5385\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.6853 - acc: 0.7524 - val_loss: 1.1881 - val_acc: 0.5385\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6836 - acc: 0.7524 - val_loss: 1.1879 - val_acc: 0.5385\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6819 - acc: 0.7524 - val_loss: 1.1876 - val_acc: 0.5385\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.6802 - acc: 0.7524 - val_loss: 1.1875 - val_acc: 0.5385\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.6785 - acc: 0.7524 - val_loss: 1.1876 - val_acc: 0.5385\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6767 - acc: 0.7524 - val_loss: 1.1877 - val_acc: 0.5385\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6750 - acc: 0.7524 - val_loss: 1.1879 - val_acc: 0.5385\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6732 - acc: 0.7524 - val_loss: 1.1877 - val_acc: 0.5385\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.6715 - acc: 0.7524 - val_loss: 1.1878 - val_acc: 0.5385\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.6698 - acc: 0.7524 - val_loss: 1.1884 - val_acc: 0.5385\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.6680 - acc: 0.7524 - val_loss: 1.1891 - val_acc: 0.5385\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6663 - acc: 0.7524 - val_loss: 1.1894 - val_acc: 0.5385\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6645 - acc: 0.7524 - val_loss: 1.1893 - val_acc: 0.5385\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.6627 - acc: 0.7524 - val_loss: 1.1895 - val_acc: 0.5385\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6610 - acc: 0.7524 - val_loss: 1.1895 - val_acc: 0.5385\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.6592 - acc: 0.7524 - val_loss: 1.1895 - val_acc: 0.5385\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6574 - acc: 0.7524 - val_loss: 1.1894 - val_acc: 0.5385\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6557 - acc: 0.7524 - val_loss: 1.1897 - val_acc: 0.5385\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.6539 - acc: 0.7524 - val_loss: 1.1902 - val_acc: 0.5385\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6521 - acc: 0.7524 - val_loss: 1.1910 - val_acc: 0.5385\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 58us/step - loss: 0.6503 - acc: 0.7619 - val_loss: 1.1916 - val_acc: 0.5385\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.6485 - acc: 0.7619 - val_loss: 1.1916 - val_acc: 0.5385\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6467 - acc: 0.7714 - val_loss: 1.1915 - val_acc: 0.5385\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6449 - acc: 0.7714 - val_loss: 1.1916 - val_acc: 0.5385\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6431 - acc: 0.7810 - val_loss: 1.1919 - val_acc: 0.5385\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.6413 - acc: 0.7810 - val_loss: 1.1931 - val_acc: 0.5385\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6395 - acc: 0.7810 - val_loss: 1.1941 - val_acc: 0.5385\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6377 - acc: 0.7810 - val_loss: 1.1946 - val_acc: 0.5385\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6359 - acc: 0.7905 - val_loss: 1.1945 - val_acc: 0.5385\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.6341 - acc: 0.7905 - val_loss: 1.1945 - val_acc: 0.5385\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6323 - acc: 0.7905 - val_loss: 1.1944 - val_acc: 0.5385\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6305 - acc: 0.7905 - val_loss: 1.1948 - val_acc: 0.5385\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6287 - acc: 0.7905 - val_loss: 1.1954 - val_acc: 0.5385\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6269 - acc: 0.7905 - val_loss: 1.1967 - val_acc: 0.5385\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6250 - acc: 0.7905 - val_loss: 1.1981 - val_acc: 0.5385\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.6232 - acc: 0.7905 - val_loss: 1.1990 - val_acc: 0.5385\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6214 - acc: 0.7905 - val_loss: 1.1985 - val_acc: 0.5385\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6196 - acc: 0.7905 - val_loss: 1.1975 - val_acc: 0.5385\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6177 - acc: 0.7905 - val_loss: 1.1967 - val_acc: 0.5385\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6159 - acc: 0.7905 - val_loss: 1.1969 - val_acc: 0.5385\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.6140 - acc: 0.7905 - val_loss: 1.1976 - val_acc: 0.5385\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.6122 - acc: 0.7905 - val_loss: 1.1984 - val_acc: 0.5385\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.6103 - acc: 0.7905 - val_loss: 1.1989 - val_acc: 0.5385\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.6084 - acc: 0.7905 - val_loss: 1.1988 - val_acc: 0.5385\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.6065 - acc: 0.8000 - val_loss: 1.1988 - val_acc: 0.5385\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.6047 - acc: 0.8000 - val_loss: 1.1990 - val_acc: 0.5385\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.6028 - acc: 0.8095 - val_loss: 1.1995 - val_acc: 0.5385\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.6009 - acc: 0.8095 - val_loss: 1.1996 - val_acc: 0.5385\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5991 - acc: 0.8190 - val_loss: 1.1999 - val_acc: 0.5385\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5972 - acc: 0.8190 - val_loss: 1.2011 - val_acc: 0.5385\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5954 - acc: 0.8190 - val_loss: 1.2032 - val_acc: 0.5385\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5936 - acc: 0.8190 - val_loss: 1.2049 - val_acc: 0.5385\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5917 - acc: 0.8190 - val_loss: 1.2058 - val_acc: 0.5385\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5898 - acc: 0.8190 - val_loss: 1.2057 - val_acc: 0.5385\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5880 - acc: 0.8190 - val_loss: 1.2057 - val_acc: 0.5385\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5861 - acc: 0.8190 - val_loss: 1.2058 - val_acc: 0.5385\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5843 - acc: 0.8190 - val_loss: 1.2062 - val_acc: 0.5385\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5825 - acc: 0.8190 - val_loss: 1.2068 - val_acc: 0.5385\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.5807 - acc: 0.8190 - val_loss: 1.2078 - val_acc: 0.5385\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.5789 - acc: 0.8190 - val_loss: 1.2092 - val_acc: 0.5385\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5771 - acc: 0.8190 - val_loss: 1.2102 - val_acc: 0.5385\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5753 - acc: 0.8190 - val_loss: 1.2105 - val_acc: 0.5385\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5734 - acc: 0.8190 - val_loss: 1.2108 - val_acc: 0.5385\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5716 - acc: 0.8190 - val_loss: 1.2112 - val_acc: 0.5385\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5698 - acc: 0.8286 - val_loss: 1.2120 - val_acc: 0.5385\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5681 - acc: 0.8286 - val_loss: 1.2133 - val_acc: 0.5385\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5663 - acc: 0.8286 - val_loss: 1.2137 - val_acc: 0.5385\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5646 - acc: 0.8286 - val_loss: 1.2126 - val_acc: 0.5385\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.5628 - acc: 0.8381 - val_loss: 1.2129 - val_acc: 0.5385\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5610 - acc: 0.8381 - val_loss: 1.2152 - val_acc: 0.5385\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5592 - acc: 0.8286 - val_loss: 1.2184 - val_acc: 0.5385\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5575 - acc: 0.8286 - val_loss: 1.2206 - val_acc: 0.5385\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5557 - acc: 0.8286 - val_loss: 1.2205 - val_acc: 0.5385\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5540 - acc: 0.8286 - val_loss: 1.2193 - val_acc: 0.5385\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5522 - acc: 0.8286 - val_loss: 1.2184 - val_acc: 0.5385\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5504 - acc: 0.8286 - val_loss: 1.2194 - val_acc: 0.5385\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5487 - acc: 0.8286 - val_loss: 1.2216 - val_acc: 0.5385\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5470 - acc: 0.8286 - val_loss: 1.2241 - val_acc: 0.5385\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5452 - acc: 0.8286 - val_loss: 1.2261 - val_acc: 0.5385\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5434 - acc: 0.8286 - val_loss: 1.2260 - val_acc: 0.5385\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5416 - acc: 0.8286 - val_loss: 1.2262 - val_acc: 0.5385\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5399 - acc: 0.8286 - val_loss: 1.2273 - val_acc: 0.5385\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5381 - acc: 0.8286 - val_loss: 1.2291 - val_acc: 0.5385\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5363 - acc: 0.8286 - val_loss: 1.2311 - val_acc: 0.5385\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5346 - acc: 0.8286 - val_loss: 1.2312 - val_acc: 0.5385\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5328 - acc: 0.8286 - val_loss: 1.2310 - val_acc: 0.5385\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5310 - acc: 0.8381 - val_loss: 1.2316 - val_acc: 0.5385\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5292 - acc: 0.8381 - val_loss: 1.2327 - val_acc: 0.5385\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5274 - acc: 0.8381 - val_loss: 1.2342 - val_acc: 0.5385\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5256 - acc: 0.8381 - val_loss: 1.2356 - val_acc: 0.5385\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5238 - acc: 0.8381 - val_loss: 1.2371 - val_acc: 0.5385\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5221 - acc: 0.8381 - val_loss: 1.2383 - val_acc: 0.5385\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5203 - acc: 0.8381 - val_loss: 1.2388 - val_acc: 0.5385\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5185 - acc: 0.8381 - val_loss: 1.2390 - val_acc: 0.5385\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5167 - acc: 0.8476 - val_loss: 1.2389 - val_acc: 0.5385\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5149 - acc: 0.8476 - val_loss: 1.2379 - val_acc: 0.5385\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5131 - acc: 0.8476 - val_loss: 1.2370 - val_acc: 0.5385\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5113 - acc: 0.8476 - val_loss: 1.2390 - val_acc: 0.5385\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5094 - acc: 0.8571 - val_loss: 1.2415 - val_acc: 0.5385\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5076 - acc: 0.8571 - val_loss: 1.2437 - val_acc: 0.5385\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5057 - acc: 0.8571 - val_loss: 1.2446 - val_acc: 0.5385\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5039 - acc: 0.8571 - val_loss: 1.2451 - val_acc: 0.5385\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5021 - acc: 0.8571 - val_loss: 1.2463 - val_acc: 0.5385\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5003 - acc: 0.8571 - val_loss: 1.2485 - val_acc: 0.5385\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4985 - acc: 0.8667 - val_loss: 1.2515 - val_acc: 0.5385\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4967 - acc: 0.8667 - val_loss: 1.2534 - val_acc: 0.5385\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4948 - acc: 0.8667 - val_loss: 1.2537 - val_acc: 0.5385\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4930 - acc: 0.8667 - val_loss: 1.2542 - val_acc: 0.5385\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4913 - acc: 0.8667 - val_loss: 1.2545 - val_acc: 0.5385\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4894 - acc: 0.8667 - val_loss: 1.2550 - val_acc: 0.6154\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4876 - acc: 0.8667 - val_loss: 1.2550 - val_acc: 0.6154\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.4858 - acc: 0.8667 - val_loss: 1.2553 - val_acc: 0.6154\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.4841 - acc: 0.8667 - val_loss: 1.2562 - val_acc: 0.6154\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4823 - acc: 0.8667 - val_loss: 1.2583 - val_acc: 0.6154\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4805 - acc: 0.8667 - val_loss: 1.2603 - val_acc: 0.6154\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4786 - acc: 0.8667 - val_loss: 1.2604 - val_acc: 0.6154\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4768 - acc: 0.8667 - val_loss: 1.2596 - val_acc: 0.6154\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4750 - acc: 0.8762 - val_loss: 1.2599 - val_acc: 0.6154\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.4731 - acc: 0.8762 - val_loss: 1.2624 - val_acc: 0.6154\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4713 - acc: 0.8762 - val_loss: 1.2649 - val_acc: 0.6154\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4695 - acc: 0.8857 - val_loss: 1.2660 - val_acc: 0.6154\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4677 - acc: 0.8857 - val_loss: 1.2672 - val_acc: 0.6154\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4658 - acc: 0.8857 - val_loss: 1.2685 - val_acc: 0.6154\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4640 - acc: 0.8857 - val_loss: 1.2691 - val_acc: 0.6154\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4622 - acc: 0.8952 - val_loss: 1.2697 - val_acc: 0.6154\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4604 - acc: 0.8952 - val_loss: 1.2700 - val_acc: 0.6154\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4586 - acc: 0.9048 - val_loss: 1.2711 - val_acc: 0.6154\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4568 - acc: 0.9048 - val_loss: 1.2724 - val_acc: 0.5385\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4550 - acc: 0.9048 - val_loss: 1.2743 - val_acc: 0.5385\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4531 - acc: 0.9048 - val_loss: 1.2756 - val_acc: 0.5385\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4514 - acc: 0.9048 - val_loss: 1.2765 - val_acc: 0.5385\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4496 - acc: 0.9048 - val_loss: 1.2771 - val_acc: 0.5385\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4479 - acc: 0.9048 - val_loss: 1.2783 - val_acc: 0.5385\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4461 - acc: 0.9048 - val_loss: 1.2798 - val_acc: 0.5385\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4444 - acc: 0.9048 - val_loss: 1.2818 - val_acc: 0.5385\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4426 - acc: 0.9048 - val_loss: 1.2836 - val_acc: 0.5385\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4408 - acc: 0.9048 - val_loss: 1.2859 - val_acc: 0.5385\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4391 - acc: 0.9048 - val_loss: 1.2880 - val_acc: 0.5385\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4373 - acc: 0.9048 - val_loss: 1.2889 - val_acc: 0.5385\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.4356 - acc: 0.9048 - val_loss: 1.2892 - val_acc: 0.5385\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4338 - acc: 0.9048 - val_loss: 1.2893 - val_acc: 0.5385\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.4321 - acc: 0.9048 - val_loss: 1.2904 - val_acc: 0.5385\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 111us/step - loss: 0.4304 - acc: 0.9048 - val_loss: 1.2920 - val_acc: 0.5385\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4286 - acc: 0.9048 - val_loss: 1.2936 - val_acc: 0.5385\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.4270 - acc: 0.9048 - val_loss: 1.2956 - val_acc: 0.5385\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4253 - acc: 0.9048 - val_loss: 1.2969 - val_acc: 0.5385\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4236 - acc: 0.9048 - val_loss: 1.2991 - val_acc: 0.5385\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4220 - acc: 0.9048 - val_loss: 1.3023 - val_acc: 0.5385\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.4203 - acc: 0.9048 - val_loss: 1.3061 - val_acc: 0.5385\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.4187 - acc: 0.9048 - val_loss: 1.3108 - val_acc: 0.5385\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4171 - acc: 0.9048 - val_loss: 1.3115 - val_acc: 0.5385\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4154 - acc: 0.9048 - val_loss: 1.3103 - val_acc: 0.5385\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.4138 - acc: 0.9048 - val_loss: 1.3100 - val_acc: 0.5385\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 79us/step - loss: 0.4122 - acc: 0.9143 - val_loss: 1.3130 - val_acc: 0.5385\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4106 - acc: 0.9143 - val_loss: 1.3184 - val_acc: 0.5385\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.4090 - acc: 0.9143 - val_loss: 1.3195 - val_acc: 0.5385\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.4074 - acc: 0.9143 - val_loss: 1.3205 - val_acc: 0.5385\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4058 - acc: 0.9143 - val_loss: 1.3226 - val_acc: 0.5385\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4043 - acc: 0.9143 - val_loss: 1.3255 - val_acc: 0.5385\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4027 - acc: 0.9143 - val_loss: 1.3276 - val_acc: 0.5385\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4011 - acc: 0.9143 - val_loss: 1.3290 - val_acc: 0.5385\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3996 - acc: 0.9143 - val_loss: 1.3314 - val_acc: 0.5385\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3980 - acc: 0.9143 - val_loss: 1.3355 - val_acc: 0.5385\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3965 - acc: 0.9238 - val_loss: 1.3398 - val_acc: 0.5385\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3950 - acc: 0.9238 - val_loss: 1.3407 - val_acc: 0.5385\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3935 - acc: 0.9238 - val_loss: 1.3399 - val_acc: 0.5385\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3919 - acc: 0.9333 - val_loss: 1.3397 - val_acc: 0.5385\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3904 - acc: 0.9333 - val_loss: 1.3420 - val_acc: 0.5385\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3888 - acc: 0.9333 - val_loss: 1.3459 - val_acc: 0.5385\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3873 - acc: 0.9333 - val_loss: 1.3499 - val_acc: 0.5385\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3858 - acc: 0.9333 - val_loss: 1.3514 - val_acc: 0.5385\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3843 - acc: 0.9333 - val_loss: 1.3489 - val_acc: 0.5385\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3828 - acc: 0.9429 - val_loss: 1.3483 - val_acc: 0.5385\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3813 - acc: 0.9429 - val_loss: 1.3502 - val_acc: 0.5385\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3798 - acc: 0.9429 - val_loss: 1.3543 - val_acc: 0.5385\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3784 - acc: 0.9429 - val_loss: 1.3596 - val_acc: 0.5385\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3769 - acc: 0.9429 - val_loss: 1.3638 - val_acc: 0.5385\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3754 - acc: 0.9429 - val_loss: 1.3683 - val_acc: 0.5385\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3739 - acc: 0.9429 - val_loss: 1.3726 - val_acc: 0.5385\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3725 - acc: 0.9429 - val_loss: 1.3740 - val_acc: 0.5385\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3712 - acc: 0.9429 - val_loss: 1.3740 - val_acc: 0.5385\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3697 - acc: 0.9429 - val_loss: 1.3738 - val_acc: 0.5385\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3683 - acc: 0.9429 - val_loss: 1.3756 - val_acc: 0.5385\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3670 - acc: 0.9429 - val_loss: 1.3788 - val_acc: 0.5385\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3656 - acc: 0.9429 - val_loss: 1.3818 - val_acc: 0.5385\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3642 - acc: 0.9429 - val_loss: 1.3834 - val_acc: 0.5385\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3629 - acc: 0.9429 - val_loss: 1.3859 - val_acc: 0.5385\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3615 - acc: 0.9429 - val_loss: 1.3869 - val_acc: 0.5385\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3601 - acc: 0.9429 - val_loss: 1.3864 - val_acc: 0.5385\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3588 - acc: 0.9429 - val_loss: 1.3854 - val_acc: 0.5385\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3576 - acc: 0.9429 - val_loss: 1.3878 - val_acc: 0.5385\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3563 - acc: 0.9429 - val_loss: 1.3894 - val_acc: 0.5385\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3551 - acc: 0.9429 - val_loss: 1.3901 - val_acc: 0.5385\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3538 - acc: 0.9429 - val_loss: 1.3921 - val_acc: 0.5385\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.3525 - acc: 0.9429 - val_loss: 1.3958 - val_acc: 0.5385\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3512 - acc: 0.9429 - val_loss: 1.4010 - val_acc: 0.5385\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3500 - acc: 0.9429 - val_loss: 1.4035 - val_acc: 0.5385\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.3487 - acc: 0.9429 - val_loss: 1.4024 - val_acc: 0.5385\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.3474 - acc: 0.9429 - val_loss: 1.4026 - val_acc: 0.5385\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3462 - acc: 0.9429 - val_loss: 1.4028 - val_acc: 0.5385\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3449 - acc: 0.9429 - val_loss: 1.4049 - val_acc: 0.5385\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3437 - acc: 0.9429 - val_loss: 1.4076 - val_acc: 0.5385\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3425 - acc: 0.9429 - val_loss: 1.4108 - val_acc: 0.5385\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3413 - acc: 0.9429 - val_loss: 1.4144 - val_acc: 0.5385\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3401 - acc: 0.9429 - val_loss: 1.4175 - val_acc: 0.5385\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3389 - acc: 0.9429 - val_loss: 1.4183 - val_acc: 0.5385\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3377 - acc: 0.9429 - val_loss: 1.4211 - val_acc: 0.5385\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.3365 - acc: 0.9429 - val_loss: 1.4234 - val_acc: 0.5385\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3353 - acc: 0.9429 - val_loss: 1.4235 - val_acc: 0.5385\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.3342 - acc: 0.9429 - val_loss: 1.4254 - val_acc: 0.5385\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3330 - acc: 0.9429 - val_loss: 1.4290 - val_acc: 0.5385\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3318 - acc: 0.9429 - val_loss: 1.4337 - val_acc: 0.5385\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3306 - acc: 0.9429 - val_loss: 1.4351 - val_acc: 0.5385\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3295 - acc: 0.9429 - val_loss: 1.4356 - val_acc: 0.5385\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3283 - acc: 0.9429 - val_loss: 1.4396 - val_acc: 0.5385\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3272 - acc: 0.9429 - val_loss: 1.4438 - val_acc: 0.5385\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3261 - acc: 0.9429 - val_loss: 1.4475 - val_acc: 0.5385\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3250 - acc: 0.9429 - val_loss: 1.4486 - val_acc: 0.5385\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3238 - acc: 0.9429 - val_loss: 1.4476 - val_acc: 0.4615\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.3227 - acc: 0.9429 - val_loss: 1.4461 - val_acc: 0.4615\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3216 - acc: 0.9429 - val_loss: 1.4489 - val_acc: 0.4615\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3205 - acc: 0.9429 - val_loss: 1.4532 - val_acc: 0.5385\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3194 - acc: 0.9429 - val_loss: 1.4547 - val_acc: 0.5385\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3183 - acc: 0.9429 - val_loss: 1.4571 - val_acc: 0.5385\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3173 - acc: 0.9429 - val_loss: 1.4593 - val_acc: 0.5385\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3162 - acc: 0.9429 - val_loss: 1.4609 - val_acc: 0.5385\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3151 - acc: 0.9429 - val_loss: 1.4619 - val_acc: 0.5385\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3141 - acc: 0.9429 - val_loss: 1.4615 - val_acc: 0.4615\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3131 - acc: 0.9619 - val_loss: 1.4634 - val_acc: 0.4615\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.3120 - acc: 0.9619 - val_loss: 1.4660 - val_acc: 0.4615\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3110 - acc: 0.9619 - val_loss: 1.4693 - val_acc: 0.4615\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3100 - acc: 0.9619 - val_loss: 1.4710 - val_acc: 0.4615\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3089 - acc: 0.9619 - val_loss: 1.4722 - val_acc: 0.4615\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3079 - acc: 0.9619 - val_loss: 1.4739 - val_acc: 0.4615\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.3070 - acc: 0.9619 - val_loss: 1.4761 - val_acc: 0.4615\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3060 - acc: 0.9619 - val_loss: 1.4761 - val_acc: 0.4615\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3049 - acc: 0.9619 - val_loss: 1.4764 - val_acc: 0.4615\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3040 - acc: 0.9619 - val_loss: 1.4774 - val_acc: 0.4615\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.3030 - acc: 0.9619 - val_loss: 1.4778 - val_acc: 0.4615\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.3020 - acc: 0.9619 - val_loss: 1.4781 - val_acc: 0.4615\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.3010 - acc: 0.9619 - val_loss: 1.4821 - val_acc: 0.4615\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3001 - acc: 0.9619 - val_loss: 1.4850 - val_acc: 0.4615\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2992 - acc: 0.9619 - val_loss: 1.4857 - val_acc: 0.4615\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2982 - acc: 0.9619 - val_loss: 1.4858 - val_acc: 0.4615\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2973 - acc: 0.9619 - val_loss: 1.4855 - val_acc: 0.4615\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2963 - acc: 0.9714 - val_loss: 1.4910 - val_acc: 0.4615\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2953 - acc: 0.9714 - val_loss: 1.4978 - val_acc: 0.4615\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2944 - acc: 0.9714 - val_loss: 1.4986 - val_acc: 0.4615\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2936 - acc: 0.9714 - val_loss: 1.4969 - val_acc: 0.4615\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2926 - acc: 0.9714 - val_loss: 1.4944 - val_acc: 0.4615\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2917 - acc: 0.9714 - val_loss: 1.4935 - val_acc: 0.4615\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2908 - acc: 0.9714 - val_loss: 1.4935 - val_acc: 0.4615\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 61us/step - loss: 0.2899 - acc: 0.9714 - val_loss: 1.4960 - val_acc: 0.4615\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2891 - acc: 0.9714 - val_loss: 1.4965 - val_acc: 0.4615\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2882 - acc: 0.9714 - val_loss: 1.5012 - val_acc: 0.4615\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2873 - acc: 0.9714 - val_loss: 1.5051 - val_acc: 0.4615\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2864 - acc: 0.9714 - val_loss: 1.5064 - val_acc: 0.4615\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2855 - acc: 0.9810 - val_loss: 1.5053 - val_acc: 0.4615\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2847 - acc: 0.9810 - val_loss: 1.5047 - val_acc: 0.4615\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2838 - acc: 0.9810 - val_loss: 1.5088 - val_acc: 0.4615\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2830 - acc: 0.9810 - val_loss: 1.5109 - val_acc: 0.4615\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2821 - acc: 0.9810 - val_loss: 1.5111 - val_acc: 0.4615\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2813 - acc: 0.9810 - val_loss: 1.5139 - val_acc: 0.4615\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2805 - acc: 0.9810 - val_loss: 1.5156 - val_acc: 0.4615\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2797 - acc: 0.9810 - val_loss: 1.5168 - val_acc: 0.4615\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2788 - acc: 0.9810 - val_loss: 1.5199 - val_acc: 0.4615\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2779 - acc: 0.9810 - val_loss: 1.5239 - val_acc: 0.4615\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2771 - acc: 0.9810 - val_loss: 1.5259 - val_acc: 0.4615\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2764 - acc: 0.9810 - val_loss: 1.5246 - val_acc: 0.4615\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2756 - acc: 0.9810 - val_loss: 1.5269 - val_acc: 0.4615\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2747 - acc: 0.9810 - val_loss: 1.5307 - val_acc: 0.4615\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2739 - acc: 0.9810 - val_loss: 1.5335 - val_acc: 0.4615\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2732 - acc: 0.9810 - val_loss: 1.5328 - val_acc: 0.4615\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2724 - acc: 0.9810 - val_loss: 1.5312 - val_acc: 0.4615\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2715 - acc: 0.9810 - val_loss: 1.5313 - val_acc: 0.4615\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2708 - acc: 0.9810 - val_loss: 1.5338 - val_acc: 0.4615\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2700 - acc: 0.9810 - val_loss: 1.5351 - val_acc: 0.4615\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2692 - acc: 0.9810 - val_loss: 1.5390 - val_acc: 0.4615\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2684 - acc: 0.9810 - val_loss: 1.5431 - val_acc: 0.4615\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2676 - acc: 0.9810 - val_loss: 1.5454 - val_acc: 0.4615\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2668 - acc: 0.9810 - val_loss: 1.5450 - val_acc: 0.4615\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2661 - acc: 0.9810 - val_loss: 1.5458 - val_acc: 0.4615\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2653 - acc: 0.9810 - val_loss: 1.5480 - val_acc: 0.4615\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2645 - acc: 0.9810 - val_loss: 1.5477 - val_acc: 0.4615\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2638 - acc: 0.9810 - val_loss: 1.5472 - val_acc: 0.4615\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2631 - acc: 0.9810 - val_loss: 1.5453 - val_acc: 0.4615\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2624 - acc: 0.9810 - val_loss: 1.5485 - val_acc: 0.4615\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2616 - acc: 0.9810 - val_loss: 1.5552 - val_acc: 0.4615\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2608 - acc: 0.9810 - val_loss: 1.5622 - val_acc: 0.4615\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2601 - acc: 0.9810 - val_loss: 1.5649 - val_acc: 0.4615\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2594 - acc: 0.9810 - val_loss: 1.5630 - val_acc: 0.4615\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2587 - acc: 0.9810 - val_loss: 1.5598 - val_acc: 0.4615\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2579 - acc: 0.9810 - val_loss: 1.5617 - val_acc: 0.4615\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2572 - acc: 0.9810 - val_loss: 1.5666 - val_acc: 0.4615\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2565 - acc: 0.9810 - val_loss: 1.5702 - val_acc: 0.4615\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.2558 - acc: 0.9810 - val_loss: 1.5733 - val_acc: 0.4615\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2551 - acc: 0.9810 - val_loss: 1.5733 - val_acc: 0.4615\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2544 - acc: 0.9810 - val_loss: 1.5732 - val_acc: 0.4615\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.2537 - acc: 0.9810 - val_loss: 1.5737 - val_acc: 0.4615\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2531 - acc: 0.9810 - val_loss: 1.5745 - val_acc: 0.4615\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2524 - acc: 0.9810 - val_loss: 1.5765 - val_acc: 0.4615\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2518 - acc: 0.9810 - val_loss: 1.5787 - val_acc: 0.4615\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2511 - acc: 0.9810 - val_loss: 1.5815 - val_acc: 0.4615\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2505 - acc: 0.9810 - val_loss: 1.5837 - val_acc: 0.4615\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2498 - acc: 0.9810 - val_loss: 1.5842 - val_acc: 0.4615\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2492 - acc: 0.9810 - val_loss: 1.5837 - val_acc: 0.4615\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2485 - acc: 0.9810 - val_loss: 1.5852 - val_acc: 0.4615\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2479 - acc: 0.9810 - val_loss: 1.5905 - val_acc: 0.4615\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2472 - acc: 0.9810 - val_loss: 1.5944 - val_acc: 0.4615\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2466 - acc: 0.9810 - val_loss: 1.5930 - val_acc: 0.4615\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2459 - acc: 0.9810 - val_loss: 1.5921 - val_acc: 0.4615\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2453 - acc: 0.9810 - val_loss: 1.5964 - val_acc: 0.4615\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2447 - acc: 0.9810 - val_loss: 1.6032 - val_acc: 0.4615\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2441 - acc: 0.9810 - val_loss: 1.6055 - val_acc: 0.4615\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2435 - acc: 0.9810 - val_loss: 1.6035 - val_acc: 0.4615\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.2429 - acc: 0.9810 - val_loss: 1.6035 - val_acc: 0.4615\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2423 - acc: 0.9810 - val_loss: 1.6087 - val_acc: 0.4615\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2417 - acc: 0.9810 - val_loss: 1.6126 - val_acc: 0.4615\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2411 - acc: 0.9810 - val_loss: 1.6141 - val_acc: 0.4615\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2405 - acc: 0.9810 - val_loss: 1.6148 - val_acc: 0.4615\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 60us/step - loss: 0.2400 - acc: 0.9810 - val_loss: 1.6199 - val_acc: 0.4615\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2394 - acc: 0.9810 - val_loss: 1.6253 - val_acc: 0.4615\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2388 - acc: 0.9810 - val_loss: 1.6253 - val_acc: 0.4615\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2382 - acc: 0.9810 - val_loss: 1.6243 - val_acc: 0.4615\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2377 - acc: 0.9810 - val_loss: 1.6268 - val_acc: 0.4615\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 104us/step - loss: 0.2371 - acc: 0.9810 - val_loss: 1.6312 - val_acc: 0.4615\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 53us/step - loss: 0.2366 - acc: 0.9810 - val_loss: 1.6341 - val_acc: 0.4615\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2360 - acc: 0.9810 - val_loss: 1.6354 - val_acc: 0.4615\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2354 - acc: 0.9810 - val_loss: 1.6358 - val_acc: 0.4615\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2349 - acc: 0.9810 - val_loss: 1.6365 - val_acc: 0.4615\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2344 - acc: 0.9810 - val_loss: 1.6396 - val_acc: 0.4615\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2338 - acc: 0.9810 - val_loss: 1.6421 - val_acc: 0.4615\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2333 - acc: 0.9810 - val_loss: 1.6416 - val_acc: 0.4615\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2328 - acc: 0.9810 - val_loss: 1.6449 - val_acc: 0.4615\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2322 - acc: 0.9810 - val_loss: 1.6524 - val_acc: 0.4615\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2317 - acc: 0.9810 - val_loss: 1.6564 - val_acc: 0.4615\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2312 - acc: 0.9810 - val_loss: 1.6546 - val_acc: 0.4615\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2307 - acc: 0.9810 - val_loss: 1.6522 - val_acc: 0.4615\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 55us/step - loss: 0.2302 - acc: 0.9810 - val_loss: 1.6546 - val_acc: 0.4615\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2297 - acc: 0.9810 - val_loss: 1.6583 - val_acc: 0.4615\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2292 - acc: 0.9810 - val_loss: 1.6612 - val_acc: 0.4615\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 47us/step - loss: 0.2286 - acc: 0.9810 - val_loss: 1.6610 - val_acc: 0.4615\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2281 - acc: 0.9810 - val_loss: 1.6646 - val_acc: 0.4615\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2277 - acc: 0.9810 - val_loss: 1.6676 - val_acc: 0.4615\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2271 - acc: 0.9810 - val_loss: 1.6688 - val_acc: 0.4615\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2267 - acc: 0.9810 - val_loss: 1.6705 - val_acc: 0.4615\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 65us/step - loss: 0.2262 - acc: 0.9810 - val_loss: 1.6741 - val_acc: 0.4615\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2257 - acc: 0.9810 - val_loss: 1.6755 - val_acc: 0.4615\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2252 - acc: 0.9810 - val_loss: 1.6753 - val_acc: 0.4615\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2247 - acc: 0.9810 - val_loss: 1.6757 - val_acc: 0.4615\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2242 - acc: 0.9810 - val_loss: 1.6814 - val_acc: 0.4615\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 91us/step - loss: 0.2237 - acc: 0.9810 - val_loss: 1.6849 - val_acc: 0.4615\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2233 - acc: 0.9810 - val_loss: 1.6858 - val_acc: 0.4615\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.2228 - acc: 0.9810 - val_loss: 1.6888 - val_acc: 0.4615\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2223 - acc: 0.9810 - val_loss: 1.6918 - val_acc: 0.4615\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2218 - acc: 0.9810 - val_loss: 1.6924 - val_acc: 0.4615\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2214 - acc: 0.9810 - val_loss: 1.6923 - val_acc: 0.4615\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2209 - acc: 0.9810 - val_loss: 1.6944 - val_acc: 0.4615\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2205 - acc: 0.9810 - val_loss: 1.6987 - val_acc: 0.4615\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2200 - acc: 0.9810 - val_loss: 1.6998 - val_acc: 0.4615\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2196 - acc: 0.9810 - val_loss: 1.6985 - val_acc: 0.4615\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 108us/step - loss: 0.2191 - acc: 0.9810 - val_loss: 1.7001 - val_acc: 0.4615\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.2187 - acc: 0.9810 - val_loss: 1.7028 - val_acc: 0.4615\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2182 - acc: 0.9810 - val_loss: 1.7044 - val_acc: 0.4615\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2178 - acc: 0.9810 - val_loss: 1.7048 - val_acc: 0.4615\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2174 - acc: 0.9810 - val_loss: 1.7041 - val_acc: 0.4615\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2169 - acc: 0.9810 - val_loss: 1.7086 - val_acc: 0.4615\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2165 - acc: 0.9810 - val_loss: 1.7174 - val_acc: 0.4615\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2161 - acc: 0.9810 - val_loss: 1.7201 - val_acc: 0.4615\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2157 - acc: 0.9810 - val_loss: 1.7160 - val_acc: 0.4615\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2152 - acc: 0.9810 - val_loss: 1.7134 - val_acc: 0.4615\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2148 - acc: 0.9810 - val_loss: 1.7173 - val_acc: 0.4615\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2144 - acc: 0.9810 - val_loss: 1.7235 - val_acc: 0.4615\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2140 - acc: 0.9810 - val_loss: 1.7263 - val_acc: 0.4615\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2136 - acc: 0.9810 - val_loss: 1.7261 - val_acc: 0.4615\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.2131 - acc: 0.9810 - val_loss: 1.7249 - val_acc: 0.4615\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2128 - acc: 0.9810 - val_loss: 1.7274 - val_acc: 0.4615\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2123 - acc: 0.9810 - val_loss: 1.7320 - val_acc: 0.4615\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2119 - acc: 0.9810 - val_loss: 1.7367 - val_acc: 0.4615\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2116 - acc: 0.9810 - val_loss: 1.7395 - val_acc: 0.4615\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2112 - acc: 0.9810 - val_loss: 1.7409 - val_acc: 0.4615\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2108 - acc: 0.9810 - val_loss: 1.7411 - val_acc: 0.4615\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.2104 - acc: 0.9810 - val_loss: 1.7409 - val_acc: 0.4615\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 72us/step - loss: 0.2100 - acc: 0.9810 - val_loss: 1.7425 - val_acc: 0.4615\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.2096 - acc: 0.9810 - val_loss: 1.7451 - val_acc: 0.4615\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2092 - acc: 0.9810 - val_loss: 1.7481 - val_acc: 0.4615\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.2088 - acc: 0.9810 - val_loss: 1.7518 - val_acc: 0.4615\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.2084 - acc: 0.9810 - val_loss: 1.7557 - val_acc: 0.4615\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2080 - acc: 0.9810 - val_loss: 1.7586 - val_acc: 0.4615\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2077 - acc: 0.9810 - val_loss: 1.7597 - val_acc: 0.4615\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2072 - acc: 0.9905 - val_loss: 1.7607 - val_acc: 0.4615\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2068 - acc: 0.9905 - val_loss: 1.7644 - val_acc: 0.4615\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2065 - acc: 0.9905 - val_loss: 1.7653 - val_acc: 0.4615\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2061 - acc: 0.9905 - val_loss: 1.7637 - val_acc: 0.4615\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.2058 - acc: 0.9905 - val_loss: 1.7642 - val_acc: 0.4615\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2054 - acc: 0.9905 - val_loss: 1.7686 - val_acc: 0.4615\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2050 - acc: 0.9905 - val_loss: 1.7735 - val_acc: 0.4615\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2046 - acc: 0.9905 - val_loss: 1.7759 - val_acc: 0.4615\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2042 - acc: 0.9905 - val_loss: 1.7754 - val_acc: 0.4615\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.2039 - acc: 0.9905 - val_loss: 1.7774 - val_acc: 0.4615\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2035 - acc: 0.9905 - val_loss: 1.7807 - val_acc: 0.4615\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.2032 - acc: 0.9905 - val_loss: 1.7828 - val_acc: 0.4615\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.2028 - acc: 0.9905 - val_loss: 1.7827 - val_acc: 0.4615\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2024 - acc: 0.9905 - val_loss: 1.7820 - val_acc: 0.4615\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2021 - acc: 0.9905 - val_loss: 1.7844 - val_acc: 0.4615\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2017 - acc: 0.9905 - val_loss: 1.7878 - val_acc: 0.4615\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.2014 - acc: 0.9905 - val_loss: 1.7878 - val_acc: 0.4615\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2010 - acc: 0.9905 - val_loss: 1.7875 - val_acc: 0.4615\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2007 - acc: 0.9905 - val_loss: 1.7881 - val_acc: 0.4615\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.2004 - acc: 0.9905 - val_loss: 1.7901 - val_acc: 0.4615\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.2000 - acc: 0.9905 - val_loss: 1.7923 - val_acc: 0.4615\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1996 - acc: 0.9905 - val_loss: 1.7950 - val_acc: 0.4615\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1993 - acc: 0.9905 - val_loss: 1.7958 - val_acc: 0.4615\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1990 - acc: 0.9905 - val_loss: 1.7987 - val_acc: 0.4615\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1987 - acc: 0.9905 - val_loss: 1.8006 - val_acc: 0.4615\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1983 - acc: 0.9905 - val_loss: 1.8003 - val_acc: 0.4615\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1980 - acc: 0.9905 - val_loss: 1.8012 - val_acc: 0.4615\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1977 - acc: 0.9905 - val_loss: 1.8053 - val_acc: 0.4615\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1974 - acc: 0.9905 - val_loss: 1.8058 - val_acc: 0.4615\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1971 - acc: 0.9905 - val_loss: 1.8035 - val_acc: 0.4615\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1967 - acc: 0.9905 - val_loss: 1.8045 - val_acc: 0.4615\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1964 - acc: 0.9905 - val_loss: 1.8110 - val_acc: 0.4615\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1961 - acc: 0.9905 - val_loss: 1.8143 - val_acc: 0.4615\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1958 - acc: 0.9905 - val_loss: 1.8144 - val_acc: 0.4615\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1955 - acc: 0.9905 - val_loss: 1.8164 - val_acc: 0.4615\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1952 - acc: 0.9905 - val_loss: 1.8185 - val_acc: 0.4615\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1949 - acc: 0.9905 - val_loss: 1.8217 - val_acc: 0.4615\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1946 - acc: 0.9905 - val_loss: 1.8231 - val_acc: 0.4615\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1943 - acc: 0.9905 - val_loss: 1.8214 - val_acc: 0.4615\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1940 - acc: 0.9905 - val_loss: 1.8191 - val_acc: 0.4615\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1937 - acc: 1.0000 - val_loss: 1.8207 - val_acc: 0.4615\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1934 - acc: 1.0000 - val_loss: 1.8249 - val_acc: 0.4615\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1931 - acc: 1.0000 - val_loss: 1.8306 - val_acc: 0.4615\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1928 - acc: 1.0000 - val_loss: 1.8311 - val_acc: 0.4615\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1926 - acc: 1.0000 - val_loss: 1.8288 - val_acc: 0.4615\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1923 - acc: 1.0000 - val_loss: 1.8301 - val_acc: 0.4615\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 96us/step - loss: 0.1920 - acc: 1.0000 - val_loss: 1.8316 - val_acc: 0.4615\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.1917 - acc: 1.0000 - val_loss: 1.8340 - val_acc: 0.4615\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.1914 - acc: 1.0000 - val_loss: 1.8385 - val_acc: 0.4615\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1911 - acc: 1.0000 - val_loss: 1.8415 - val_acc: 0.4615\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1909 - acc: 1.0000 - val_loss: 1.8394 - val_acc: 0.4615\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1906 - acc: 1.0000 - val_loss: 1.8405 - val_acc: 0.4615\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1903 - acc: 1.0000 - val_loss: 1.8446 - val_acc: 0.4615\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1901 - acc: 1.0000 - val_loss: 1.8474 - val_acc: 0.4615\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1898 - acc: 1.0000 - val_loss: 1.8465 - val_acc: 0.4615\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1895 - acc: 1.0000 - val_loss: 1.8469 - val_acc: 0.4615\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1892 - acc: 1.0000 - val_loss: 1.8469 - val_acc: 0.4615\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1890 - acc: 1.0000 - val_loss: 1.8481 - val_acc: 0.4615\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1887 - acc: 1.0000 - val_loss: 1.8506 - val_acc: 0.4615\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1884 - acc: 1.0000 - val_loss: 1.8538 - val_acc: 0.4615\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1881 - acc: 1.0000 - val_loss: 1.8579 - val_acc: 0.4615\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1879 - acc: 1.0000 - val_loss: 1.8594 - val_acc: 0.4615\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1876 - acc: 1.0000 - val_loss: 1.8585 - val_acc: 0.4615\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1873 - acc: 1.0000 - val_loss: 1.8585 - val_acc: 0.4615\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1871 - acc: 1.0000 - val_loss: 1.8615 - val_acc: 0.4615\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1868 - acc: 1.0000 - val_loss: 1.8689 - val_acc: 0.4615\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1866 - acc: 1.0000 - val_loss: 1.8708 - val_acc: 0.4615\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1863 - acc: 1.0000 - val_loss: 1.8690 - val_acc: 0.4615\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1860 - acc: 1.0000 - val_loss: 1.8719 - val_acc: 0.4615\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1858 - acc: 1.0000 - val_loss: 1.8763 - val_acc: 0.4615\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1855 - acc: 1.0000 - val_loss: 1.8784 - val_acc: 0.4615\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1853 - acc: 1.0000 - val_loss: 1.8768 - val_acc: 0.4615\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1850 - acc: 1.0000 - val_loss: 1.8767 - val_acc: 0.4615\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1848 - acc: 1.0000 - val_loss: 1.8797 - val_acc: 0.4615\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1845 - acc: 1.0000 - val_loss: 1.8852 - val_acc: 0.4615\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1842 - acc: 1.0000 - val_loss: 1.8860 - val_acc: 0.4615\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1840 - acc: 1.0000 - val_loss: 1.8852 - val_acc: 0.4615\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1838 - acc: 1.0000 - val_loss: 1.8865 - val_acc: 0.4615\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1835 - acc: 1.0000 - val_loss: 1.8888 - val_acc: 0.4615\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1833 - acc: 1.0000 - val_loss: 1.8916 - val_acc: 0.4615\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1830 - acc: 1.0000 - val_loss: 1.8934 - val_acc: 0.4615\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1828 - acc: 1.0000 - val_loss: 1.8943 - val_acc: 0.4615\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1825 - acc: 1.0000 - val_loss: 1.8972 - val_acc: 0.4615\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1823 - acc: 1.0000 - val_loss: 1.9016 - val_acc: 0.4615\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1820 - acc: 1.0000 - val_loss: 1.9026 - val_acc: 0.4615\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1818 - acc: 1.0000 - val_loss: 1.9005 - val_acc: 0.4615\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1815 - acc: 1.0000 - val_loss: 1.8997 - val_acc: 0.4615\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1813 - acc: 1.0000 - val_loss: 1.9030 - val_acc: 0.4615\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1811 - acc: 1.0000 - val_loss: 1.9062 - val_acc: 0.4615\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1808 - acc: 1.0000 - val_loss: 1.9068 - val_acc: 0.4615\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1806 - acc: 1.0000 - val_loss: 1.9107 - val_acc: 0.4615\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1804 - acc: 1.0000 - val_loss: 1.9136 - val_acc: 0.4615\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1801 - acc: 1.0000 - val_loss: 1.9151 - val_acc: 0.4615\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1799 - acc: 1.0000 - val_loss: 1.9153 - val_acc: 0.4615\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1797 - acc: 1.0000 - val_loss: 1.9181 - val_acc: 0.4615\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1795 - acc: 1.0000 - val_loss: 1.9223 - val_acc: 0.4615\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1792 - acc: 1.0000 - val_loss: 1.9243 - val_acc: 0.4615\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1790 - acc: 1.0000 - val_loss: 1.9257 - val_acc: 0.4615\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1788 - acc: 1.0000 - val_loss: 1.9257 - val_acc: 0.4615\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1785 - acc: 1.0000 - val_loss: 1.9272 - val_acc: 0.4615\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1783 - acc: 1.0000 - val_loss: 1.9320 - val_acc: 0.4615\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1781 - acc: 1.0000 - val_loss: 1.9377 - val_acc: 0.4615\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1779 - acc: 1.0000 - val_loss: 1.9380 - val_acc: 0.4615\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1777 - acc: 1.0000 - val_loss: 1.9377 - val_acc: 0.4615\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1774 - acc: 1.0000 - val_loss: 1.9416 - val_acc: 0.4615\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 59us/step - loss: 0.1772 - acc: 1.0000 - val_loss: 1.9444 - val_acc: 0.4615\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1770 - acc: 1.0000 - val_loss: 1.9452 - val_acc: 0.4615\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1768 - acc: 1.0000 - val_loss: 1.9442 - val_acc: 0.4615\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1766 - acc: 1.0000 - val_loss: 1.9459 - val_acc: 0.4615\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1763 - acc: 1.0000 - val_loss: 1.9515 - val_acc: 0.4615\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1761 - acc: 1.0000 - val_loss: 1.9584 - val_acc: 0.4615\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1759 - acc: 1.0000 - val_loss: 1.9643 - val_acc: 0.5385\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1757 - acc: 1.0000 - val_loss: 1.9640 - val_acc: 0.4615\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1755 - acc: 1.0000 - val_loss: 1.9603 - val_acc: 0.4615\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1753 - acc: 1.0000 - val_loss: 1.9635 - val_acc: 0.4615\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1751 - acc: 1.0000 - val_loss: 1.9697 - val_acc: 0.5385\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1748 - acc: 1.0000 - val_loss: 1.9741 - val_acc: 0.5385\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1746 - acc: 1.0000 - val_loss: 1.9739 - val_acc: 0.5385\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1745 - acc: 1.0000 - val_loss: 1.9740 - val_acc: 0.4615\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1742 - acc: 1.0000 - val_loss: 1.9765 - val_acc: 0.4615\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1740 - acc: 1.0000 - val_loss: 1.9833 - val_acc: 0.5385\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.1738 - acc: 1.0000 - val_loss: 1.9852 - val_acc: 0.5385\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1736 - acc: 1.0000 - val_loss: 1.9861 - val_acc: 0.5385\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1734 - acc: 1.0000 - val_loss: 1.9862 - val_acc: 0.5385\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1732 - acc: 1.0000 - val_loss: 1.9890 - val_acc: 0.5385\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1730 - acc: 1.0000 - val_loss: 1.9920 - val_acc: 0.4615\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1728 - acc: 1.0000 - val_loss: 1.9935 - val_acc: 0.5385\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1726 - acc: 1.0000 - val_loss: 1.9926 - val_acc: 0.4615\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1724 - acc: 1.0000 - val_loss: 1.9939 - val_acc: 0.4615\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1722 - acc: 1.0000 - val_loss: 1.9983 - val_acc: 0.5385\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1720 - acc: 1.0000 - val_loss: 1.9999 - val_acc: 0.5385\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1718 - acc: 1.0000 - val_loss: 2.0012 - val_acc: 0.5385\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1716 - acc: 1.0000 - val_loss: 2.0020 - val_acc: 0.5385\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1714 - acc: 1.0000 - val_loss: 2.0049 - val_acc: 0.5385\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1712 - acc: 1.0000 - val_loss: 2.0061 - val_acc: 0.5385\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1710 - acc: 1.0000 - val_loss: 2.0077 - val_acc: 0.5385\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1708 - acc: 1.0000 - val_loss: 2.0088 - val_acc: 0.4615\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1707 - acc: 1.0000 - val_loss: 2.0126 - val_acc: 0.4615\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1705 - acc: 1.0000 - val_loss: 2.0176 - val_acc: 0.5385\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1703 - acc: 1.0000 - val_loss: 2.0185 - val_acc: 0.4615\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1701 - acc: 1.0000 - val_loss: 2.0175 - val_acc: 0.4615\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.1699 - acc: 1.0000 - val_loss: 2.0169 - val_acc: 0.4615\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1697 - acc: 1.0000 - val_loss: 2.0178 - val_acc: 0.4615\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.1695 - acc: 1.0000 - val_loss: 2.0205 - val_acc: 0.5385\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1693 - acc: 1.0000 - val_loss: 2.0239 - val_acc: 0.5385\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1692 - acc: 1.0000 - val_loss: 2.0270 - val_acc: 0.5385\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1690 - acc: 1.0000 - val_loss: 2.0275 - val_acc: 0.5385\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1688 - acc: 1.0000 - val_loss: 2.0300 - val_acc: 0.5385\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1686 - acc: 1.0000 - val_loss: 2.0331 - val_acc: 0.4615\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1684 - acc: 1.0000 - val_loss: 2.0361 - val_acc: 0.4615\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1682 - acc: 1.0000 - val_loss: 2.0388 - val_acc: 0.4615\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1681 - acc: 1.0000 - val_loss: 2.0403 - val_acc: 0.4615\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1679 - acc: 1.0000 - val_loss: 2.0422 - val_acc: 0.4615\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1677 - acc: 1.0000 - val_loss: 2.0450 - val_acc: 0.4615\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1675 - acc: 1.0000 - val_loss: 2.0480 - val_acc: 0.5385\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1673 - acc: 1.0000 - val_loss: 2.0488 - val_acc: 0.4615\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1672 - acc: 1.0000 - val_loss: 2.0514 - val_acc: 0.5385\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1670 - acc: 1.0000 - val_loss: 2.0532 - val_acc: 0.4615\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1668 - acc: 1.0000 - val_loss: 2.0540 - val_acc: 0.4615\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.1667 - acc: 1.0000 - val_loss: 2.0556 - val_acc: 0.4615\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.1665 - acc: 1.0000 - val_loss: 2.0562 - val_acc: 0.4615\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1663 - acc: 1.0000 - val_loss: 2.0573 - val_acc: 0.5385\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1661 - acc: 1.0000 - val_loss: 2.0596 - val_acc: 0.5385\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1660 - acc: 1.0000 - val_loss: 2.0611 - val_acc: 0.5385\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1658 - acc: 1.0000 - val_loss: 2.0651 - val_acc: 0.4615\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1656 - acc: 1.0000 - val_loss: 2.0685 - val_acc: 0.4615\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1654 - acc: 1.0000 - val_loss: 2.0720 - val_acc: 0.4615\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.1652 - acc: 1.0000 - val_loss: 2.0728 - val_acc: 0.4615\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.1651 - acc: 1.0000 - val_loss: 2.0769 - val_acc: 0.4615\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1649 - acc: 1.0000 - val_loss: 2.0819 - val_acc: 0.5385\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1647 - acc: 1.0000 - val_loss: 2.0807 - val_acc: 0.5385\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1646 - acc: 1.0000 - val_loss: 2.0769 - val_acc: 0.4615\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1644 - acc: 1.0000 - val_loss: 2.0764 - val_acc: 0.4615\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1642 - acc: 1.0000 - val_loss: 2.0810 - val_acc: 0.4615\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1641 - acc: 1.0000 - val_loss: 2.0877 - val_acc: 0.5385\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1639 - acc: 1.0000 - val_loss: 2.0915 - val_acc: 0.5385\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1637 - acc: 1.0000 - val_loss: 2.0935 - val_acc: 0.4615\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1636 - acc: 1.0000 - val_loss: 2.0936 - val_acc: 0.4615\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1634 - acc: 1.0000 - val_loss: 2.0955 - val_acc: 0.4615\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1633 - acc: 1.0000 - val_loss: 2.0969 - val_acc: 0.4615\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1631 - acc: 1.0000 - val_loss: 2.0973 - val_acc: 0.4615\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1629 - acc: 1.0000 - val_loss: 2.0991 - val_acc: 0.4615\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1628 - acc: 1.0000 - val_loss: 2.1040 - val_acc: 0.4615\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1626 - acc: 1.0000 - val_loss: 2.1090 - val_acc: 0.5385\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1625 - acc: 1.0000 - val_loss: 2.1102 - val_acc: 0.4615\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1623 - acc: 1.0000 - val_loss: 2.1085 - val_acc: 0.4615\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1621 - acc: 1.0000 - val_loss: 2.1132 - val_acc: 0.4615\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1620 - acc: 1.0000 - val_loss: 2.1174 - val_acc: 0.4615\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1619 - acc: 1.0000 - val_loss: 2.1187 - val_acc: 0.5385\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1617 - acc: 1.0000 - val_loss: 2.1172 - val_acc: 0.4615\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1616 - acc: 1.0000 - val_loss: 2.1137 - val_acc: 0.4615\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1614 - acc: 1.0000 - val_loss: 2.1174 - val_acc: 0.4615\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1612 - acc: 1.0000 - val_loss: 2.1232 - val_acc: 0.4615\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1611 - acc: 1.0000 - val_loss: 2.1280 - val_acc: 0.4615\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1609 - acc: 1.0000 - val_loss: 2.1274 - val_acc: 0.4615\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1608 - acc: 1.0000 - val_loss: 2.1255 - val_acc: 0.4615\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1607 - acc: 1.0000 - val_loss: 2.1260 - val_acc: 0.4615\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1605 - acc: 1.0000 - val_loss: 2.1285 - val_acc: 0.4615\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1604 - acc: 1.0000 - val_loss: 2.1302 - val_acc: 0.4615\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1602 - acc: 1.0000 - val_loss: 2.1295 - val_acc: 0.4615\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1601 - acc: 1.0000 - val_loss: 2.1311 - val_acc: 0.4615\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.1600 - acc: 1.0000 - val_loss: 2.1374 - val_acc: 0.4615\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1598 - acc: 1.0000 - val_loss: 2.1431 - val_acc: 0.4615\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1597 - acc: 1.0000 - val_loss: 2.1422 - val_acc: 0.4615\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1595 - acc: 1.0000 - val_loss: 2.1409 - val_acc: 0.4615\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1594 - acc: 1.0000 - val_loss: 2.1447 - val_acc: 0.4615\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1593 - acc: 1.0000 - val_loss: 2.1479 - val_acc: 0.4615\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1591 - acc: 1.0000 - val_loss: 2.1497 - val_acc: 0.4615\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1590 - acc: 1.0000 - val_loss: 2.1505 - val_acc: 0.4615\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1588 - acc: 1.0000 - val_loss: 2.1507 - val_acc: 0.4615\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1587 - acc: 1.0000 - val_loss: 2.1500 - val_acc: 0.4615\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 56us/step - loss: 0.1586 - acc: 1.0000 - val_loss: 2.1537 - val_acc: 0.4615\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1584 - acc: 1.0000 - val_loss: 2.1603 - val_acc: 0.4615\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1583 - acc: 1.0000 - val_loss: 2.1614 - val_acc: 0.4615\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1582 - acc: 1.0000 - val_loss: 2.1606 - val_acc: 0.4615\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1580 - acc: 1.0000 - val_loss: 2.1620 - val_acc: 0.4615\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 54us/step - loss: 0.1579 - acc: 1.0000 - val_loss: 2.1652 - val_acc: 0.4615\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.1578 - acc: 1.0000 - val_loss: 2.1689 - val_acc: 0.4615\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1576 - acc: 1.0000 - val_loss: 2.1713 - val_acc: 0.4615\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1575 - acc: 1.0000 - val_loss: 2.1732 - val_acc: 0.4615\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1573 - acc: 1.0000 - val_loss: 2.1735 - val_acc: 0.4615\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1572 - acc: 1.0000 - val_loss: 2.1740 - val_acc: 0.4615\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1571 - acc: 1.0000 - val_loss: 2.1754 - val_acc: 0.4615\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1570 - acc: 1.0000 - val_loss: 2.1773 - val_acc: 0.4615\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1568 - acc: 1.0000 - val_loss: 2.1778 - val_acc: 0.4615\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1567 - acc: 1.0000 - val_loss: 2.1808 - val_acc: 0.4615\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1566 - acc: 1.0000 - val_loss: 2.1830 - val_acc: 0.4615\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1564 - acc: 1.0000 - val_loss: 2.1836 - val_acc: 0.4615\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1563 - acc: 1.0000 - val_loss: 2.1866 - val_acc: 0.4615\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1562 - acc: 1.0000 - val_loss: 2.1925 - val_acc: 0.4615\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1560 - acc: 1.0000 - val_loss: 2.1939 - val_acc: 0.4615\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1559 - acc: 1.0000 - val_loss: 2.1898 - val_acc: 0.4615\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1558 - acc: 1.0000 - val_loss: 2.1912 - val_acc: 0.4615\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1557 - acc: 1.0000 - val_loss: 2.1970 - val_acc: 0.4615\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1555 - acc: 1.0000 - val_loss: 2.2002 - val_acc: 0.4615\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1554 - acc: 1.0000 - val_loss: 2.1993 - val_acc: 0.4615\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1553 - acc: 1.0000 - val_loss: 2.1972 - val_acc: 0.4615\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1551 - acc: 1.0000 - val_loss: 2.1979 - val_acc: 0.4615\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1550 - acc: 1.0000 - val_loss: 2.1997 - val_acc: 0.4615\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1549 - acc: 1.0000 - val_loss: 2.2031 - val_acc: 0.4615\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1547 - acc: 1.0000 - val_loss: 2.2063 - val_acc: 0.4615\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1546 - acc: 1.0000 - val_loss: 2.2097 - val_acc: 0.4615\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1545 - acc: 1.0000 - val_loss: 2.2106 - val_acc: 0.4615\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1544 - acc: 1.0000 - val_loss: 2.2112 - val_acc: 0.4615\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1542 - acc: 1.0000 - val_loss: 2.2125 - val_acc: 0.4615\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1541 - acc: 1.0000 - val_loss: 2.2167 - val_acc: 0.4615\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1540 - acc: 1.0000 - val_loss: 2.2206 - val_acc: 0.4615\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1539 - acc: 1.0000 - val_loss: 2.2204 - val_acc: 0.4615\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1538 - acc: 1.0000 - val_loss: 2.2192 - val_acc: 0.4615\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1536 - acc: 1.0000 - val_loss: 2.2201 - val_acc: 0.4615\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1535 - acc: 1.0000 - val_loss: 2.2233 - val_acc: 0.4615\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1534 - acc: 1.0000 - val_loss: 2.2254 - val_acc: 0.4615\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1533 - acc: 1.0000 - val_loss: 2.2277 - val_acc: 0.4615\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1532 - acc: 1.0000 - val_loss: 2.2308 - val_acc: 0.4615\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1530 - acc: 1.0000 - val_loss: 2.2314 - val_acc: 0.4615\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1529 - acc: 1.0000 - val_loss: 2.2323 - val_acc: 0.4615\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1528 - acc: 1.0000 - val_loss: 2.2324 - val_acc: 0.4615\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1527 - acc: 1.0000 - val_loss: 2.2325 - val_acc: 0.4615\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1525 - acc: 1.0000 - val_loss: 2.2349 - val_acc: 0.4615\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1524 - acc: 1.0000 - val_loss: 2.2394 - val_acc: 0.4615\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1523 - acc: 1.0000 - val_loss: 2.2428 - val_acc: 0.4615\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1522 - acc: 1.0000 - val_loss: 2.2428 - val_acc: 0.4615\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1521 - acc: 1.0000 - val_loss: 2.2413 - val_acc: 0.4615\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1519 - acc: 1.0000 - val_loss: 2.2433 - val_acc: 0.4615\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1518 - acc: 1.0000 - val_loss: 2.2467 - val_acc: 0.4615\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1517 - acc: 1.0000 - val_loss: 2.2472 - val_acc: 0.4615\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1516 - acc: 1.0000 - val_loss: 2.2495 - val_acc: 0.4615\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1515 - acc: 1.0000 - val_loss: 2.2518 - val_acc: 0.4615\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1514 - acc: 1.0000 - val_loss: 2.2512 - val_acc: 0.4615\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1512 - acc: 1.0000 - val_loss: 2.2521 - val_acc: 0.4615\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1511 - acc: 1.0000 - val_loss: 2.2564 - val_acc: 0.4615\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1510 - acc: 1.0000 - val_loss: 2.2576 - val_acc: 0.4615\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1509 - acc: 1.0000 - val_loss: 2.2589 - val_acc: 0.4615\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1508 - acc: 1.0000 - val_loss: 2.2589 - val_acc: 0.4615\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1507 - acc: 1.0000 - val_loss: 2.2607 - val_acc: 0.4615\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1506 - acc: 1.0000 - val_loss: 2.2633 - val_acc: 0.4615\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1505 - acc: 1.0000 - val_loss: 2.2668 - val_acc: 0.4615\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1503 - acc: 1.0000 - val_loss: 2.2693 - val_acc: 0.4615\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1502 - acc: 1.0000 - val_loss: 2.2703 - val_acc: 0.4615\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1501 - acc: 1.0000 - val_loss: 2.2686 - val_acc: 0.4615\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1500 - acc: 1.0000 - val_loss: 2.2686 - val_acc: 0.4615\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.1499 - acc: 1.0000 - val_loss: 2.2696 - val_acc: 0.4615\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1498 - acc: 1.0000 - val_loss: 2.2743 - val_acc: 0.4615\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.1496 - acc: 1.0000 - val_loss: 2.2779 - val_acc: 0.4615\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1495 - acc: 1.0000 - val_loss: 2.2800 - val_acc: 0.4615\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1494 - acc: 1.0000 - val_loss: 2.2821 - val_acc: 0.4615\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.1493 - acc: 1.0000 - val_loss: 2.2809 - val_acc: 0.4615\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1492 - acc: 1.0000 - val_loss: 2.2804 - val_acc: 0.4615\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1491 - acc: 1.0000 - val_loss: 2.2825 - val_acc: 0.4615\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1490 - acc: 1.0000 - val_loss: 2.2860 - val_acc: 0.4615\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1489 - acc: 1.0000 - val_loss: 2.2894 - val_acc: 0.4615\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 46us/step - loss: 0.1488 - acc: 1.0000 - val_loss: 2.2902 - val_acc: 0.4615\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1487 - acc: 1.0000 - val_loss: 2.2885 - val_acc: 0.4615\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1486 - acc: 1.0000 - val_loss: 2.2885 - val_acc: 0.4615\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1484 - acc: 1.0000 - val_loss: 2.2916 - val_acc: 0.4615\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1483 - acc: 1.0000 - val_loss: 2.2966 - val_acc: 0.4615\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1482 - acc: 1.0000 - val_loss: 2.2973 - val_acc: 0.4615\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1481 - acc: 1.0000 - val_loss: 2.2957 - val_acc: 0.4615\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1480 - acc: 1.0000 - val_loss: 2.2992 - val_acc: 0.4615\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1479 - acc: 1.0000 - val_loss: 2.3037 - val_acc: 0.4615\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1478 - acc: 1.0000 - val_loss: 2.3064 - val_acc: 0.4615\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.1477 - acc: 1.0000 - val_loss: 2.3076 - val_acc: 0.4615\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1476 - acc: 1.0000 - val_loss: 2.3084 - val_acc: 0.4615\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1475 - acc: 1.0000 - val_loss: 2.3073 - val_acc: 0.4615\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1474 - acc: 1.0000 - val_loss: 2.3095 - val_acc: 0.4615\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1473 - acc: 1.0000 - val_loss: 2.3131 - val_acc: 0.4615\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1472 - acc: 1.0000 - val_loss: 2.3179 - val_acc: 0.4615\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1471 - acc: 1.0000 - val_loss: 2.3182 - val_acc: 0.4615\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1470 - acc: 1.0000 - val_loss: 2.3180 - val_acc: 0.4615\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1468 - acc: 1.0000 - val_loss: 2.3188 - val_acc: 0.4615\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1467 - acc: 1.0000 - val_loss: 2.3197 - val_acc: 0.4615\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1466 - acc: 1.0000 - val_loss: 2.3215 - val_acc: 0.4615\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1465 - acc: 1.0000 - val_loss: 2.3194 - val_acc: 0.4615\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1464 - acc: 1.0000 - val_loss: 2.3204 - val_acc: 0.4615\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1463 - acc: 1.0000 - val_loss: 2.3226 - val_acc: 0.4615\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1462 - acc: 1.0000 - val_loss: 2.3266 - val_acc: 0.4615\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1461 - acc: 1.0000 - val_loss: 2.3307 - val_acc: 0.4615\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1460 - acc: 1.0000 - val_loss: 2.3315 - val_acc: 0.4615\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1459 - acc: 1.0000 - val_loss: 2.3317 - val_acc: 0.4615\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1458 - acc: 1.0000 - val_loss: 2.3312 - val_acc: 0.4615\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1457 - acc: 1.0000 - val_loss: 2.3331 - val_acc: 0.4615\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1456 - acc: 1.0000 - val_loss: 2.3354 - val_acc: 0.4615\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1455 - acc: 1.0000 - val_loss: 2.3361 - val_acc: 0.4615\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1454 - acc: 1.0000 - val_loss: 2.3365 - val_acc: 0.4615\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1453 - acc: 1.0000 - val_loss: 2.3369 - val_acc: 0.4615\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1452 - acc: 1.0000 - val_loss: 2.3415 - val_acc: 0.4615\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1451 - acc: 1.0000 - val_loss: 2.3443 - val_acc: 0.4615\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1450 - acc: 1.0000 - val_loss: 2.3438 - val_acc: 0.4615\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1449 - acc: 1.0000 - val_loss: 2.3447 - val_acc: 0.4615\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1448 - acc: 1.0000 - val_loss: 2.3467 - val_acc: 0.4615\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1447 - acc: 1.0000 - val_loss: 2.3501 - val_acc: 0.4615\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1446 - acc: 1.0000 - val_loss: 2.3537 - val_acc: 0.4615\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1445 - acc: 1.0000 - val_loss: 2.3538 - val_acc: 0.4615\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1444 - acc: 1.0000 - val_loss: 2.3521 - val_acc: 0.4615\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1443 - acc: 1.0000 - val_loss: 2.3546 - val_acc: 0.4615\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1442 - acc: 1.0000 - val_loss: 2.3584 - val_acc: 0.4615\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1441 - acc: 1.0000 - val_loss: 2.3591 - val_acc: 0.4615\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1440 - acc: 1.0000 - val_loss: 2.3597 - val_acc: 0.4615\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1439 - acc: 1.0000 - val_loss: 2.3599 - val_acc: 0.4615\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1439 - acc: 1.0000 - val_loss: 2.3629 - val_acc: 0.4615\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1438 - acc: 1.0000 - val_loss: 2.3660 - val_acc: 0.4615\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1437 - acc: 1.0000 - val_loss: 2.3678 - val_acc: 0.4615\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1436 - acc: 1.0000 - val_loss: 2.3678 - val_acc: 0.4615\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1435 - acc: 1.0000 - val_loss: 2.3683 - val_acc: 0.4615\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1434 - acc: 1.0000 - val_loss: 2.3702 - val_acc: 0.4615\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1433 - acc: 1.0000 - val_loss: 2.3735 - val_acc: 0.4615\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1432 - acc: 1.0000 - val_loss: 2.3741 - val_acc: 0.4615\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1431 - acc: 1.0000 - val_loss: 2.3727 - val_acc: 0.4615\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1430 - acc: 1.0000 - val_loss: 2.3743 - val_acc: 0.4615\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1429 - acc: 1.0000 - val_loss: 2.3774 - val_acc: 0.4615\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1428 - acc: 1.0000 - val_loss: 2.3785 - val_acc: 0.4615\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 0.1427 - acc: 1.0000 - val_loss: 2.3773 - val_acc: 0.4615\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1426 - acc: 1.0000 - val_loss: 2.3742 - val_acc: 0.4615\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1425 - acc: 1.0000 - val_loss: 2.3759 - val_acc: 0.4615\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1424 - acc: 1.0000 - val_loss: 2.3829 - val_acc: 0.4615\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 45us/step - loss: 0.1423 - acc: 1.0000 - val_loss: 2.3858 - val_acc: 0.4615\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 88us/step - loss: 0.1422 - acc: 1.0000 - val_loss: 2.3836 - val_acc: 0.4615\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1421 - acc: 1.0000 - val_loss: 2.3828 - val_acc: 0.4615\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1420 - acc: 1.0000 - val_loss: 2.3848 - val_acc: 0.4615\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1419 - acc: 1.0000 - val_loss: 2.3905 - val_acc: 0.4615\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1418 - acc: 1.0000 - val_loss: 2.3904 - val_acc: 0.4615\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1417 - acc: 1.0000 - val_loss: 2.3875 - val_acc: 0.4615\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1416 - acc: 1.0000 - val_loss: 2.3871 - val_acc: 0.4615\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1415 - acc: 1.0000 - val_loss: 2.3916 - val_acc: 0.4615\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1414 - acc: 1.0000 - val_loss: 2.3976 - val_acc: 0.4615\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1413 - acc: 1.0000 - val_loss: 2.3985 - val_acc: 0.4615\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1412 - acc: 1.0000 - val_loss: 2.3959 - val_acc: 0.4615\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1411 - acc: 1.0000 - val_loss: 2.3947 - val_acc: 0.4615\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1410 - acc: 1.0000 - val_loss: 2.3982 - val_acc: 0.4615\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1409 - acc: 1.0000 - val_loss: 2.4000 - val_acc: 0.4615\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1408 - acc: 1.0000 - val_loss: 2.4024 - val_acc: 0.4615\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1407 - acc: 1.0000 - val_loss: 2.4043 - val_acc: 0.4615\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1406 - acc: 1.0000 - val_loss: 2.4026 - val_acc: 0.4615\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1405 - acc: 1.0000 - val_loss: 2.4026 - val_acc: 0.4615\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1404 - acc: 1.0000 - val_loss: 2.4041 - val_acc: 0.4615\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1403 - acc: 1.0000 - val_loss: 2.4058 - val_acc: 0.4615\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1402 - acc: 1.0000 - val_loss: 2.4062 - val_acc: 0.4615\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1401 - acc: 1.0000 - val_loss: 2.4078 - val_acc: 0.4615\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1400 - acc: 1.0000 - val_loss: 2.4106 - val_acc: 0.4615\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1399 - acc: 1.0000 - val_loss: 2.4100 - val_acc: 0.4615\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1398 - acc: 1.0000 - val_loss: 2.4110 - val_acc: 0.4615\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1397 - acc: 1.0000 - val_loss: 2.4143 - val_acc: 0.4615\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1396 - acc: 1.0000 - val_loss: 2.4136 - val_acc: 0.4615\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1395 - acc: 1.0000 - val_loss: 2.4142 - val_acc: 0.4615\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1394 - acc: 1.0000 - val_loss: 2.4166 - val_acc: 0.4615\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 0.1393 - acc: 1.0000 - val_loss: 2.4179 - val_acc: 0.4615\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1392 - acc: 1.0000 - val_loss: 2.4169 - val_acc: 0.4615\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1391 - acc: 1.0000 - val_loss: 2.4168 - val_acc: 0.4615\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1390 - acc: 1.0000 - val_loss: 2.4164 - val_acc: 0.4615\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1389 - acc: 1.0000 - val_loss: 2.4152 - val_acc: 0.4615\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1388 - acc: 1.0000 - val_loss: 2.4161 - val_acc: 0.4615\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1387 - acc: 1.0000 - val_loss: 2.4200 - val_acc: 0.4615\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1385 - acc: 1.0000 - val_loss: 2.4208 - val_acc: 0.4615\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1383 - acc: 1.0000 - val_loss: 2.4198 - val_acc: 0.4615\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1381 - acc: 1.0000 - val_loss: 2.4206 - val_acc: 0.4615\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1379 - acc: 1.0000 - val_loss: 2.4205 - val_acc: 0.3846\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1377 - acc: 1.0000 - val_loss: 2.4191 - val_acc: 0.3846\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1375 - acc: 1.0000 - val_loss: 2.4185 - val_acc: 0.3846\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1373 - acc: 1.0000 - val_loss: 2.4182 - val_acc: 0.3846\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1372 - acc: 1.0000 - val_loss: 2.4172 - val_acc: 0.3846\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1370 - acc: 1.0000 - val_loss: 2.4191 - val_acc: 0.3846\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1370 - acc: 1.0000 - val_loss: 2.4215 - val_acc: 0.3846\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1369 - acc: 1.0000 - val_loss: 2.4213 - val_acc: 0.3846\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1368 - acc: 1.0000 - val_loss: 2.4166 - val_acc: 0.3846\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1367 - acc: 1.0000 - val_loss: 2.4137 - val_acc: 0.3846\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 48us/step - loss: 0.1366 - acc: 1.0000 - val_loss: 2.4114 - val_acc: 0.3846\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1365 - acc: 1.0000 - val_loss: 2.4124 - val_acc: 0.3846\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1364 - acc: 1.0000 - val_loss: 2.4146 - val_acc: 0.4615\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1363 - acc: 1.0000 - val_loss: 2.4157 - val_acc: 0.4615\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1362 - acc: 1.0000 - val_loss: 2.4175 - val_acc: 0.4615\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1361 - acc: 1.0000 - val_loss: 2.4169 - val_acc: 0.4615\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1360 - acc: 1.0000 - val_loss: 2.4155 - val_acc: 0.4615\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1359 - acc: 1.0000 - val_loss: 2.4161 - val_acc: 0.4615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CJPxmYQAU7xb"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2lYQvVptU7xd",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q_N5mOtXU7xm",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0c67c294-dd32-4cf8-c9dc-06134827a9f2",
        "id": "o1MrHCGKU7xv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3c3dd27cc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntspk1gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o9Co5DNRU7x1"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "befd310a-3992-46b5-b15f-cc04f61d425a",
        "id": "MPQLuNRJU7x4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3c3dd0f550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rj5Ji0KCU7x_"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "fa967e85-b0ec-4000-db49-0b9216ded515",
        "id": "DNAj2xd1U7yC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand, one_hot_train_labels, epochs= num_epochs, batch_size=105, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 1.1656 - acc: 0.2952\n",
            "Epoch 2/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.1523 - acc: 0.3429\n",
            "Epoch 3/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.1395 - acc: 0.3714\n",
            "Epoch 4/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.1270 - acc: 0.3810\n",
            "Epoch 5/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.1152 - acc: 0.3810\n",
            "Epoch 6/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.1038 - acc: 0.4095\n",
            "Epoch 7/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0932 - acc: 0.4667\n",
            "Epoch 8/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0833 - acc: 0.4857\n",
            "Epoch 9/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 1.0740 - acc: 0.4952\n",
            "Epoch 10/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.0652 - acc: 0.5143\n",
            "Epoch 11/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0570 - acc: 0.5429\n",
            "Epoch 12/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.0492 - acc: 0.5524\n",
            "Epoch 13/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0418 - acc: 0.5714\n",
            "Epoch 14/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0349 - acc: 0.5714\n",
            "Epoch 15/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0285 - acc: 0.5810\n",
            "Epoch 16/1000\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0225 - acc: 0.5905\n",
            "Epoch 17/1000\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0168 - acc: 0.6000\n",
            "Epoch 18/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0114 - acc: 0.6000\n",
            "Epoch 19/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.0063 - acc: 0.6000\n",
            "Epoch 20/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0014 - acc: 0.6000\n",
            "Epoch 21/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.9969 - acc: 0.6000\n",
            "Epoch 22/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9925 - acc: 0.6000\n",
            "Epoch 23/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.9883 - acc: 0.6000\n",
            "Epoch 24/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.9842 - acc: 0.5905\n",
            "Epoch 25/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9803 - acc: 0.5905\n",
            "Epoch 26/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9765 - acc: 0.6000\n",
            "Epoch 27/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9728 - acc: 0.6000\n",
            "Epoch 28/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9694 - acc: 0.6000\n",
            "Epoch 29/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9662 - acc: 0.6095\n",
            "Epoch 30/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9631 - acc: 0.6095\n",
            "Epoch 31/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.9600 - acc: 0.6095\n",
            "Epoch 32/1000\n",
            "105/105 [==============================] - 0s 17us/step - loss: 0.9569 - acc: 0.6095\n",
            "Epoch 33/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9540 - acc: 0.6095\n",
            "Epoch 34/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9512 - acc: 0.6095\n",
            "Epoch 35/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9484 - acc: 0.6190\n",
            "Epoch 36/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9458 - acc: 0.6190\n",
            "Epoch 37/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.9432 - acc: 0.6190\n",
            "Epoch 38/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.9406 - acc: 0.6190\n",
            "Epoch 39/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9379 - acc: 0.6190\n",
            "Epoch 40/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.9353 - acc: 0.6190\n",
            "Epoch 41/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9328 - acc: 0.6190\n",
            "Epoch 42/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9302 - acc: 0.6190\n",
            "Epoch 43/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9276 - acc: 0.6190\n",
            "Epoch 44/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9251 - acc: 0.6286\n",
            "Epoch 45/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9226 - acc: 0.6286\n",
            "Epoch 46/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9201 - acc: 0.6286\n",
            "Epoch 47/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9177 - acc: 0.6286\n",
            "Epoch 48/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9153 - acc: 0.6286\n",
            "Epoch 49/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9130 - acc: 0.6286\n",
            "Epoch 50/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.9106 - acc: 0.6286\n",
            "Epoch 51/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.9083 - acc: 0.6286\n",
            "Epoch 52/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.9060 - acc: 0.6476\n",
            "Epoch 53/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.9037 - acc: 0.6476\n",
            "Epoch 54/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.9015 - acc: 0.6381\n",
            "Epoch 55/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8993 - acc: 0.6381\n",
            "Epoch 56/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.8972 - acc: 0.6286\n",
            "Epoch 57/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8952 - acc: 0.6190\n",
            "Epoch 58/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8932 - acc: 0.6190\n",
            "Epoch 59/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8912 - acc: 0.6190\n",
            "Epoch 60/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8891 - acc: 0.6190\n",
            "Epoch 61/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8871 - acc: 0.6190\n",
            "Epoch 62/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8851 - acc: 0.6190\n",
            "Epoch 63/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.8831 - acc: 0.6190\n",
            "Epoch 64/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8812 - acc: 0.6286\n",
            "Epoch 65/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8794 - acc: 0.6286\n",
            "Epoch 66/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.8776 - acc: 0.6286\n",
            "Epoch 67/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8758 - acc: 0.6286\n",
            "Epoch 68/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8741 - acc: 0.6286\n",
            "Epoch 69/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8723 - acc: 0.6286\n",
            "Epoch 70/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.8705 - acc: 0.6190\n",
            "Epoch 71/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.8688 - acc: 0.6190\n",
            "Epoch 72/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8671 - acc: 0.6190\n",
            "Epoch 73/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8654 - acc: 0.6190\n",
            "Epoch 74/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8637 - acc: 0.6190\n",
            "Epoch 75/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8621 - acc: 0.6190\n",
            "Epoch 76/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8604 - acc: 0.6190\n",
            "Epoch 77/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8588 - acc: 0.6190\n",
            "Epoch 78/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8573 - acc: 0.6190\n",
            "Epoch 79/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8557 - acc: 0.6190\n",
            "Epoch 80/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8541 - acc: 0.6286\n",
            "Epoch 81/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.8526 - acc: 0.6286\n",
            "Epoch 82/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8510 - acc: 0.6286\n",
            "Epoch 83/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8495 - acc: 0.6381\n",
            "Epoch 84/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8480 - acc: 0.6381\n",
            "Epoch 85/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8467 - acc: 0.6381\n",
            "Epoch 86/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.8453 - acc: 0.6381\n",
            "Epoch 87/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8439 - acc: 0.6381\n",
            "Epoch 88/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8425 - acc: 0.6381\n",
            "Epoch 89/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.8411 - acc: 0.6381\n",
            "Epoch 90/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.8397 - acc: 0.6381\n",
            "Epoch 91/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8384 - acc: 0.6381\n",
            "Epoch 92/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.8370 - acc: 0.6381\n",
            "Epoch 93/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8357 - acc: 0.6381\n",
            "Epoch 94/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.8343 - acc: 0.6381\n",
            "Epoch 95/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.8330 - acc: 0.6381\n",
            "Epoch 96/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8316 - acc: 0.6381\n",
            "Epoch 97/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8303 - acc: 0.6381\n",
            "Epoch 98/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8289 - acc: 0.6381\n",
            "Epoch 99/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8276 - acc: 0.6381\n",
            "Epoch 100/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8262 - acc: 0.6381\n",
            "Epoch 101/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.8249 - acc: 0.6476\n",
            "Epoch 102/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8235 - acc: 0.6476\n",
            "Epoch 103/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.8222 - acc: 0.6476\n",
            "Epoch 104/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.8208 - acc: 0.6476\n",
            "Epoch 105/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8195 - acc: 0.6476\n",
            "Epoch 106/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8181 - acc: 0.6476\n",
            "Epoch 107/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8168 - acc: 0.6476\n",
            "Epoch 108/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8155 - acc: 0.6476\n",
            "Epoch 109/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8141 - acc: 0.6476\n",
            "Epoch 110/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8127 - acc: 0.6476\n",
            "Epoch 111/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8114 - acc: 0.6571\n",
            "Epoch 112/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8099 - acc: 0.6571\n",
            "Epoch 113/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8085 - acc: 0.6571\n",
            "Epoch 114/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.8071 - acc: 0.6571\n",
            "Epoch 115/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.8057 - acc: 0.6571\n",
            "Epoch 116/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.8043 - acc: 0.6571\n",
            "Epoch 117/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.8029 - acc: 0.6571\n",
            "Epoch 118/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.8015 - acc: 0.6476\n",
            "Epoch 119/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.8000 - acc: 0.6476\n",
            "Epoch 120/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7986 - acc: 0.6476\n",
            "Epoch 121/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7972 - acc: 0.6476\n",
            "Epoch 122/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7958 - acc: 0.6476\n",
            "Epoch 123/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7943 - acc: 0.6476\n",
            "Epoch 124/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7928 - acc: 0.6476\n",
            "Epoch 125/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7913 - acc: 0.6476\n",
            "Epoch 126/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7898 - acc: 0.6571\n",
            "Epoch 127/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7883 - acc: 0.6571\n",
            "Epoch 128/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7869 - acc: 0.6571\n",
            "Epoch 129/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7854 - acc: 0.6667\n",
            "Epoch 130/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7840 - acc: 0.6667\n",
            "Epoch 131/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7826 - acc: 0.6667\n",
            "Epoch 132/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7812 - acc: 0.6667\n",
            "Epoch 133/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7797 - acc: 0.6667\n",
            "Epoch 134/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7783 - acc: 0.6667\n",
            "Epoch 135/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7769 - acc: 0.6762\n",
            "Epoch 136/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7754 - acc: 0.6762\n",
            "Epoch 137/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7740 - acc: 0.6762\n",
            "Epoch 138/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7726 - acc: 0.6762\n",
            "Epoch 139/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7712 - acc: 0.6762\n",
            "Epoch 140/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7697 - acc: 0.6762\n",
            "Epoch 141/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7683 - acc: 0.6762\n",
            "Epoch 142/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7669 - acc: 0.6857\n",
            "Epoch 143/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7655 - acc: 0.6857\n",
            "Epoch 144/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7641 - acc: 0.6857\n",
            "Epoch 145/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7627 - acc: 0.6857\n",
            "Epoch 146/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7614 - acc: 0.6857\n",
            "Epoch 147/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.7600 - acc: 0.6857\n",
            "Epoch 148/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7586 - acc: 0.6857\n",
            "Epoch 149/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.7572 - acc: 0.6857\n",
            "Epoch 150/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7559 - acc: 0.6857\n",
            "Epoch 151/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7545 - acc: 0.6857\n",
            "Epoch 152/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.7531 - acc: 0.6857\n",
            "Epoch 153/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7518 - acc: 0.6857\n",
            "Epoch 154/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7504 - acc: 0.6857\n",
            "Epoch 155/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7490 - acc: 0.6857\n",
            "Epoch 156/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7476 - acc: 0.6857\n",
            "Epoch 157/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7462 - acc: 0.6857\n",
            "Epoch 158/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7448 - acc: 0.6857\n",
            "Epoch 159/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7434 - acc: 0.6952\n",
            "Epoch 160/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7420 - acc: 0.6952\n",
            "Epoch 161/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7406 - acc: 0.7143\n",
            "Epoch 162/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7392 - acc: 0.7238\n",
            "Epoch 163/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7378 - acc: 0.7238\n",
            "Epoch 164/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7364 - acc: 0.7238\n",
            "Epoch 165/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7349 - acc: 0.7238\n",
            "Epoch 166/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7335 - acc: 0.7238\n",
            "Epoch 167/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7321 - acc: 0.7238\n",
            "Epoch 168/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7306 - acc: 0.7238\n",
            "Epoch 169/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.7292 - acc: 0.7238\n",
            "Epoch 170/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7278 - acc: 0.7238\n",
            "Epoch 171/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7264 - acc: 0.7238\n",
            "Epoch 172/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7250 - acc: 0.7238\n",
            "Epoch 173/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7236 - acc: 0.7238\n",
            "Epoch 174/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7221 - acc: 0.7238\n",
            "Epoch 175/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7207 - acc: 0.7238\n",
            "Epoch 176/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.7193 - acc: 0.7238\n",
            "Epoch 177/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.7179 - acc: 0.7238\n",
            "Epoch 178/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.7165 - acc: 0.7238\n",
            "Epoch 179/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7150 - acc: 0.7238\n",
            "Epoch 180/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7136 - acc: 0.7238\n",
            "Epoch 181/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7122 - acc: 0.7238\n",
            "Epoch 182/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7107 - acc: 0.7333\n",
            "Epoch 183/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.7092 - acc: 0.7333\n",
            "Epoch 184/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.7078 - acc: 0.7333\n",
            "Epoch 185/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.7064 - acc: 0.7238\n",
            "Epoch 186/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.7049 - acc: 0.7238\n",
            "Epoch 187/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.7035 - acc: 0.7238\n",
            "Epoch 188/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.7020 - acc: 0.7238\n",
            "Epoch 189/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.7005 - acc: 0.7238\n",
            "Epoch 190/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6991 - acc: 0.7238\n",
            "Epoch 191/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6976 - acc: 0.7238\n",
            "Epoch 192/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6961 - acc: 0.7238\n",
            "Epoch 193/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6946 - acc: 0.7238\n",
            "Epoch 194/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6931 - acc: 0.7238\n",
            "Epoch 195/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.6916 - acc: 0.7238\n",
            "Epoch 196/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6901 - acc: 0.7238\n",
            "Epoch 197/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6887 - acc: 0.7238\n",
            "Epoch 198/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6872 - acc: 0.7238\n",
            "Epoch 199/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6857 - acc: 0.7238\n",
            "Epoch 200/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6842 - acc: 0.7238\n",
            "Epoch 201/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.6826 - acc: 0.7238\n",
            "Epoch 202/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6812 - acc: 0.7238\n",
            "Epoch 203/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6797 - acc: 0.7238\n",
            "Epoch 204/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6782 - acc: 0.7238\n",
            "Epoch 205/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6767 - acc: 0.7238\n",
            "Epoch 206/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6753 - acc: 0.7238\n",
            "Epoch 207/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6738 - acc: 0.7238\n",
            "Epoch 208/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6723 - acc: 0.7238\n",
            "Epoch 209/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6708 - acc: 0.7238\n",
            "Epoch 210/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6693 - acc: 0.7238\n",
            "Epoch 211/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6678 - acc: 0.7238\n",
            "Epoch 212/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6663 - acc: 0.7238\n",
            "Epoch 213/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6648 - acc: 0.7238\n",
            "Epoch 214/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6633 - acc: 0.7333\n",
            "Epoch 215/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6618 - acc: 0.7333\n",
            "Epoch 216/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6603 - acc: 0.7333\n",
            "Epoch 217/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6588 - acc: 0.7333\n",
            "Epoch 218/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6573 - acc: 0.7333\n",
            "Epoch 219/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6557 - acc: 0.7429\n",
            "Epoch 220/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6542 - acc: 0.7429\n",
            "Epoch 221/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6526 - acc: 0.7429\n",
            "Epoch 222/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6511 - acc: 0.7429\n",
            "Epoch 223/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6495 - acc: 0.7429\n",
            "Epoch 224/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6480 - acc: 0.7429\n",
            "Epoch 225/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6464 - acc: 0.7429\n",
            "Epoch 226/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6448 - acc: 0.7429\n",
            "Epoch 227/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6432 - acc: 0.7429\n",
            "Epoch 228/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6416 - acc: 0.7429\n",
            "Epoch 229/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6400 - acc: 0.7429\n",
            "Epoch 230/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.6385 - acc: 0.7429\n",
            "Epoch 231/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6369 - acc: 0.7429\n",
            "Epoch 232/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6354 - acc: 0.7429\n",
            "Epoch 233/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6338 - acc: 0.7429\n",
            "Epoch 234/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6322 - acc: 0.7429\n",
            "Epoch 235/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6307 - acc: 0.7429\n",
            "Epoch 236/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.6291 - acc: 0.7429\n",
            "Epoch 237/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.6276 - acc: 0.7429\n",
            "Epoch 238/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.6260 - acc: 0.7429\n",
            "Epoch 239/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6244 - acc: 0.7429\n",
            "Epoch 240/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6229 - acc: 0.7429\n",
            "Epoch 241/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6213 - acc: 0.7429\n",
            "Epoch 242/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6197 - acc: 0.7429\n",
            "Epoch 243/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.6181 - acc: 0.7429\n",
            "Epoch 244/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6165 - acc: 0.7429\n",
            "Epoch 245/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6149 - acc: 0.7429\n",
            "Epoch 246/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6134 - acc: 0.7524\n",
            "Epoch 247/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6118 - acc: 0.7524\n",
            "Epoch 248/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.6103 - acc: 0.7619\n",
            "Epoch 249/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.6087 - acc: 0.7619\n",
            "Epoch 250/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.6071 - acc: 0.7619\n",
            "Epoch 251/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.6056 - acc: 0.7619\n",
            "Epoch 252/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.6040 - acc: 0.7619\n",
            "Epoch 253/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.6025 - acc: 0.7619\n",
            "Epoch 254/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.6010 - acc: 0.7619\n",
            "Epoch 255/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5994 - acc: 0.7714\n",
            "Epoch 256/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5979 - acc: 0.7714\n",
            "Epoch 257/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5963 - acc: 0.7714\n",
            "Epoch 258/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5948 - acc: 0.7714\n",
            "Epoch 259/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5933 - acc: 0.7714\n",
            "Epoch 260/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5917 - acc: 0.7714\n",
            "Epoch 261/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5902 - acc: 0.7714\n",
            "Epoch 262/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5886 - acc: 0.7714\n",
            "Epoch 263/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5871 - acc: 0.7714\n",
            "Epoch 264/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5855 - acc: 0.7714\n",
            "Epoch 265/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5840 - acc: 0.7714\n",
            "Epoch 266/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5825 - acc: 0.7810\n",
            "Epoch 267/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5809 - acc: 0.7810\n",
            "Epoch 268/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5794 - acc: 0.7810\n",
            "Epoch 269/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5778 - acc: 0.7810\n",
            "Epoch 270/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5763 - acc: 0.7905\n",
            "Epoch 271/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5748 - acc: 0.7905\n",
            "Epoch 272/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5732 - acc: 0.8000\n",
            "Epoch 273/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5716 - acc: 0.8000\n",
            "Epoch 274/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5701 - acc: 0.8000\n",
            "Epoch 275/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5686 - acc: 0.8000\n",
            "Epoch 276/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5671 - acc: 0.8000\n",
            "Epoch 277/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5656 - acc: 0.8000\n",
            "Epoch 278/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.5640 - acc: 0.8000\n",
            "Epoch 279/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.5625 - acc: 0.8000\n",
            "Epoch 280/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5609 - acc: 0.8095\n",
            "Epoch 281/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5594 - acc: 0.8095\n",
            "Epoch 282/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5579 - acc: 0.8190\n",
            "Epoch 283/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.5564 - acc: 0.8286\n",
            "Epoch 284/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.5548 - acc: 0.8286\n",
            "Epoch 285/1000\n",
            "105/105 [==============================] - 0s 49us/step - loss: 0.5533 - acc: 0.8286\n",
            "Epoch 286/1000\n",
            "105/105 [==============================] - 0s 52us/step - loss: 0.5518 - acc: 0.8286\n",
            "Epoch 287/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.5503 - acc: 0.8286\n",
            "Epoch 288/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.5488 - acc: 0.8286\n",
            "Epoch 289/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.5473 - acc: 0.8286\n",
            "Epoch 290/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.5457 - acc: 0.8381\n",
            "Epoch 291/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5442 - acc: 0.8476\n",
            "Epoch 292/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5427 - acc: 0.8476\n",
            "Epoch 293/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.5411 - acc: 0.8476\n",
            "Epoch 294/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5396 - acc: 0.8476\n",
            "Epoch 295/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5382 - acc: 0.8476\n",
            "Epoch 296/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.5366 - acc: 0.8476\n",
            "Epoch 297/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5350 - acc: 0.8476\n",
            "Epoch 298/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.5336 - acc: 0.8476\n",
            "Epoch 299/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.5320 - acc: 0.8476\n",
            "Epoch 300/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5305 - acc: 0.8476\n",
            "Epoch 301/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.5290 - acc: 0.8476\n",
            "Epoch 302/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.5274 - acc: 0.8476\n",
            "Epoch 303/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.5259 - acc: 0.8476\n",
            "Epoch 304/1000\n",
            "105/105 [==============================] - 0s 51us/step - loss: 0.5244 - acc: 0.8476\n",
            "Epoch 305/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.5229 - acc: 0.8476\n",
            "Epoch 306/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.5214 - acc: 0.8476\n",
            "Epoch 307/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5199 - acc: 0.8476\n",
            "Epoch 308/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5184 - acc: 0.8476\n",
            "Epoch 309/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.5168 - acc: 0.8476\n",
            "Epoch 310/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.5153 - acc: 0.8476\n",
            "Epoch 311/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5138 - acc: 0.8476\n",
            "Epoch 312/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5123 - acc: 0.8476\n",
            "Epoch 313/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5108 - acc: 0.8476\n",
            "Epoch 314/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5093 - acc: 0.8476\n",
            "Epoch 315/1000\n",
            "105/105 [==============================] - 0s 16us/step - loss: 0.5078 - acc: 0.8476\n",
            "Epoch 316/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5063 - acc: 0.8476\n",
            "Epoch 317/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5047 - acc: 0.8476\n",
            "Epoch 318/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.5033 - acc: 0.8571\n",
            "Epoch 319/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.5018 - acc: 0.8667\n",
            "Epoch 320/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.5002 - acc: 0.8667\n",
            "Epoch 321/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4988 - acc: 0.8667\n",
            "Epoch 322/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4973 - acc: 0.8762\n",
            "Epoch 323/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4958 - acc: 0.8667\n",
            "Epoch 324/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4943 - acc: 0.8762\n",
            "Epoch 325/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4928 - acc: 0.8762\n",
            "Epoch 326/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.4913 - acc: 0.8762\n",
            "Epoch 327/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.4898 - acc: 0.8762\n",
            "Epoch 328/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.4883 - acc: 0.8857\n",
            "Epoch 329/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4869 - acc: 0.8857\n",
            "Epoch 330/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4854 - acc: 0.8857\n",
            "Epoch 331/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4840 - acc: 0.8857\n",
            "Epoch 332/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4825 - acc: 0.8857\n",
            "Epoch 333/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4810 - acc: 0.8857\n",
            "Epoch 334/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4797 - acc: 0.8857\n",
            "Epoch 335/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.4782 - acc: 0.8857\n",
            "Epoch 336/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4767 - acc: 0.8857\n",
            "Epoch 337/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4753 - acc: 0.8857\n",
            "Epoch 338/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.4738 - acc: 0.8857\n",
            "Epoch 339/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4724 - acc: 0.8857\n",
            "Epoch 340/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4709 - acc: 0.8857\n",
            "Epoch 341/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4694 - acc: 0.8857\n",
            "Epoch 342/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.4680 - acc: 0.8857\n",
            "Epoch 343/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4665 - acc: 0.8857\n",
            "Epoch 344/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4650 - acc: 0.8857\n",
            "Epoch 345/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4636 - acc: 0.8857\n",
            "Epoch 346/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4621 - acc: 0.8857\n",
            "Epoch 347/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4606 - acc: 0.8952\n",
            "Epoch 348/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.4592 - acc: 0.8952\n",
            "Epoch 349/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4577 - acc: 0.8952\n",
            "Epoch 350/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4563 - acc: 0.8952\n",
            "Epoch 351/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4548 - acc: 0.8952\n",
            "Epoch 352/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4533 - acc: 0.8952\n",
            "Epoch 353/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4519 - acc: 0.8952\n",
            "Epoch 354/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4505 - acc: 0.8952\n",
            "Epoch 355/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4491 - acc: 0.8952\n",
            "Epoch 356/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4476 - acc: 0.8952\n",
            "Epoch 357/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4462 - acc: 0.8952\n",
            "Epoch 358/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4448 - acc: 0.8952\n",
            "Epoch 359/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4435 - acc: 0.8952\n",
            "Epoch 360/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4421 - acc: 0.8952\n",
            "Epoch 361/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4407 - acc: 0.8952\n",
            "Epoch 362/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4394 - acc: 0.8952\n",
            "Epoch 363/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4381 - acc: 0.8952\n",
            "Epoch 364/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4368 - acc: 0.8952\n",
            "Epoch 365/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4355 - acc: 0.8952\n",
            "Epoch 366/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.4341 - acc: 0.8952\n",
            "Epoch 367/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4327 - acc: 0.8952\n",
            "Epoch 368/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4314 - acc: 0.8952\n",
            "Epoch 369/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.4301 - acc: 0.8952\n",
            "Epoch 370/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.4288 - acc: 0.8952\n",
            "Epoch 371/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4275 - acc: 0.8952\n",
            "Epoch 372/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4262 - acc: 0.8952\n",
            "Epoch 373/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4248 - acc: 0.8952\n",
            "Epoch 374/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.4235 - acc: 0.8952\n",
            "Epoch 375/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.4223 - acc: 0.8952\n",
            "Epoch 376/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4209 - acc: 0.8952\n",
            "Epoch 377/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4196 - acc: 0.8952\n",
            "Epoch 378/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.4182 - acc: 0.8952\n",
            "Epoch 379/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4169 - acc: 0.8952\n",
            "Epoch 380/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.4156 - acc: 0.8952\n",
            "Epoch 381/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4143 - acc: 0.8952\n",
            "Epoch 382/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.4130 - acc: 0.8952\n",
            "Epoch 383/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4116 - acc: 0.8952\n",
            "Epoch 384/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.4103 - acc: 0.8952\n",
            "Epoch 385/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.4089 - acc: 0.8952\n",
            "Epoch 386/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.4076 - acc: 0.8952\n",
            "Epoch 387/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.4063 - acc: 0.8952\n",
            "Epoch 388/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.4050 - acc: 0.8952\n",
            "Epoch 389/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.4037 - acc: 0.8952\n",
            "Epoch 390/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.4025 - acc: 0.8952\n",
            "Epoch 391/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.4011 - acc: 0.8952\n",
            "Epoch 392/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.3999 - acc: 0.8952\n",
            "Epoch 393/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3986 - acc: 0.8952\n",
            "Epoch 394/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3974 - acc: 0.8952\n",
            "Epoch 395/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3962 - acc: 0.8952\n",
            "Epoch 396/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3949 - acc: 0.8952\n",
            "Epoch 397/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3937 - acc: 0.8952\n",
            "Epoch 398/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3925 - acc: 0.8952\n",
            "Epoch 399/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3912 - acc: 0.8952\n",
            "Epoch 400/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3901 - acc: 0.8952\n",
            "Epoch 401/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3889 - acc: 0.8952\n",
            "Epoch 402/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3875 - acc: 0.8952\n",
            "Epoch 403/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3864 - acc: 0.8952\n",
            "Epoch 404/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3852 - acc: 0.8952\n",
            "Epoch 405/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.3841 - acc: 0.8952\n",
            "Epoch 406/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3829 - acc: 0.8952\n",
            "Epoch 407/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.3816 - acc: 0.8952\n",
            "Epoch 408/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3806 - acc: 0.8952\n",
            "Epoch 409/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3794 - acc: 0.8952\n",
            "Epoch 410/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3782 - acc: 0.8952\n",
            "Epoch 411/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3769 - acc: 0.8952\n",
            "Epoch 412/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3759 - acc: 0.8952\n",
            "Epoch 413/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3748 - acc: 0.8952\n",
            "Epoch 414/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3736 - acc: 0.8952\n",
            "Epoch 415/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3724 - acc: 0.8952\n",
            "Epoch 416/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3713 - acc: 0.8952\n",
            "Epoch 417/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3702 - acc: 0.8952\n",
            "Epoch 418/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3690 - acc: 0.8952\n",
            "Epoch 419/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3679 - acc: 0.8952\n",
            "Epoch 420/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3668 - acc: 0.8952\n",
            "Epoch 421/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3656 - acc: 0.8952\n",
            "Epoch 422/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3645 - acc: 0.8952\n",
            "Epoch 423/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3634 - acc: 0.8952\n",
            "Epoch 424/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.3622 - acc: 0.8952\n",
            "Epoch 425/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3611 - acc: 0.8952\n",
            "Epoch 426/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3600 - acc: 0.8952\n",
            "Epoch 427/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3587 - acc: 0.8952\n",
            "Epoch 428/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3578 - acc: 0.8952\n",
            "Epoch 429/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.3566 - acc: 0.8952\n",
            "Epoch 430/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3555 - acc: 0.8952\n",
            "Epoch 431/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3544 - acc: 0.8952\n",
            "Epoch 432/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3534 - acc: 0.8952\n",
            "Epoch 433/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3524 - acc: 0.8952\n",
            "Epoch 434/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3514 - acc: 0.8952\n",
            "Epoch 435/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3503 - acc: 0.8952\n",
            "Epoch 436/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3492 - acc: 0.8952\n",
            "Epoch 437/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3482 - acc: 0.9048\n",
            "Epoch 438/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3472 - acc: 0.9048\n",
            "Epoch 439/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3462 - acc: 0.9048\n",
            "Epoch 440/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3451 - acc: 0.9048\n",
            "Epoch 441/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3440 - acc: 0.9048\n",
            "Epoch 442/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3431 - acc: 0.9048\n",
            "Epoch 443/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3420 - acc: 0.9048\n",
            "Epoch 444/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.3410 - acc: 0.9048\n",
            "Epoch 445/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3400 - acc: 0.9048\n",
            "Epoch 446/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3390 - acc: 0.9048\n",
            "Epoch 447/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3380 - acc: 0.9048\n",
            "Epoch 448/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.3370 - acc: 0.9048\n",
            "Epoch 449/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3360 - acc: 0.9048\n",
            "Epoch 450/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3350 - acc: 0.9048\n",
            "Epoch 451/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3340 - acc: 0.9048\n",
            "Epoch 452/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3331 - acc: 0.9048\n",
            "Epoch 453/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3321 - acc: 0.9048\n",
            "Epoch 454/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3312 - acc: 0.9048\n",
            "Epoch 455/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3303 - acc: 0.9048\n",
            "Epoch 456/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.3293 - acc: 0.9048\n",
            "Epoch 457/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3283 - acc: 0.9048\n",
            "Epoch 458/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3274 - acc: 0.9048\n",
            "Epoch 459/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3265 - acc: 0.9048\n",
            "Epoch 460/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3256 - acc: 0.9143\n",
            "Epoch 461/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3247 - acc: 0.9143\n",
            "Epoch 462/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.3238 - acc: 0.9143\n",
            "Epoch 463/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.3229 - acc: 0.9143\n",
            "Epoch 464/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3220 - acc: 0.9143\n",
            "Epoch 465/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3211 - acc: 0.9143\n",
            "Epoch 466/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3202 - acc: 0.9143\n",
            "Epoch 467/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3194 - acc: 0.9143\n",
            "Epoch 468/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.3184 - acc: 0.9143\n",
            "Epoch 469/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3175 - acc: 0.9143\n",
            "Epoch 470/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3166 - acc: 0.9143\n",
            "Epoch 471/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3157 - acc: 0.9143\n",
            "Epoch 472/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3148 - acc: 0.9143\n",
            "Epoch 473/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3138 - acc: 0.9143\n",
            "Epoch 474/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3130 - acc: 0.9143\n",
            "Epoch 475/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3121 - acc: 0.9333\n",
            "Epoch 476/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3112 - acc: 0.9333\n",
            "Epoch 477/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3103 - acc: 0.9333\n",
            "Epoch 478/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3094 - acc: 0.9333\n",
            "Epoch 479/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.3086 - acc: 0.9429\n",
            "Epoch 480/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.3077 - acc: 0.9333\n",
            "Epoch 481/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3068 - acc: 0.9333\n",
            "Epoch 482/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.3060 - acc: 0.9333\n",
            "Epoch 483/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.3052 - acc: 0.9429\n",
            "Epoch 484/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3043 - acc: 0.9429\n",
            "Epoch 485/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3034 - acc: 0.9524\n",
            "Epoch 486/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.3026 - acc: 0.9429\n",
            "Epoch 487/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.3018 - acc: 0.9429\n",
            "Epoch 488/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.3010 - acc: 0.9524\n",
            "Epoch 489/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.3002 - acc: 0.9524\n",
            "Epoch 490/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2993 - acc: 0.9524\n",
            "Epoch 491/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2985 - acc: 0.9429\n",
            "Epoch 492/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2977 - acc: 0.9524\n",
            "Epoch 493/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2968 - acc: 0.9524\n",
            "Epoch 494/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2960 - acc: 0.9524\n",
            "Epoch 495/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2952 - acc: 0.9524\n",
            "Epoch 496/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2944 - acc: 0.9429\n",
            "Epoch 497/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2936 - acc: 0.9524\n",
            "Epoch 498/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2928 - acc: 0.9524\n",
            "Epoch 499/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2921 - acc: 0.9524\n",
            "Epoch 500/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2913 - acc: 0.9524\n",
            "Epoch 501/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2905 - acc: 0.9524\n",
            "Epoch 502/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2898 - acc: 0.9524\n",
            "Epoch 503/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2890 - acc: 0.9524\n",
            "Epoch 504/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2883 - acc: 0.9619\n",
            "Epoch 505/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2876 - acc: 0.9619\n",
            "Epoch 506/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.2868 - acc: 0.9619\n",
            "Epoch 507/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2860 - acc: 0.9619\n",
            "Epoch 508/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2852 - acc: 0.9619\n",
            "Epoch 509/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2846 - acc: 0.9619\n",
            "Epoch 510/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2838 - acc: 0.9619\n",
            "Epoch 511/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2831 - acc: 0.9619\n",
            "Epoch 512/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2824 - acc: 0.9619\n",
            "Epoch 513/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2817 - acc: 0.9619\n",
            "Epoch 514/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2810 - acc: 0.9619\n",
            "Epoch 515/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2803 - acc: 0.9714\n",
            "Epoch 516/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2797 - acc: 0.9714\n",
            "Epoch 517/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2790 - acc: 0.9714\n",
            "Epoch 518/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2783 - acc: 0.9714\n",
            "Epoch 519/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2776 - acc: 0.9714\n",
            "Epoch 520/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2769 - acc: 0.9714\n",
            "Epoch 521/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2762 - acc: 0.9714\n",
            "Epoch 522/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2756 - acc: 0.9714\n",
            "Epoch 523/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2749 - acc: 0.9714\n",
            "Epoch 524/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2742 - acc: 0.9714\n",
            "Epoch 525/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2736 - acc: 0.9714\n",
            "Epoch 526/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2730 - acc: 0.9714\n",
            "Epoch 527/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2723 - acc: 0.9714\n",
            "Epoch 528/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2716 - acc: 0.9714\n",
            "Epoch 529/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2710 - acc: 0.9714\n",
            "Epoch 530/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2703 - acc: 0.9714\n",
            "Epoch 531/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2698 - acc: 0.9714\n",
            "Epoch 532/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2691 - acc: 0.9714\n",
            "Epoch 533/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2684 - acc: 0.9714\n",
            "Epoch 534/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2678 - acc: 0.9714\n",
            "Epoch 535/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2672 - acc: 0.9714\n",
            "Epoch 536/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2666 - acc: 0.9714\n",
            "Epoch 537/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2659 - acc: 0.9714\n",
            "Epoch 538/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2653 - acc: 0.9714\n",
            "Epoch 539/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2647 - acc: 0.9714\n",
            "Epoch 540/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2641 - acc: 0.9714\n",
            "Epoch 541/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2635 - acc: 0.9714\n",
            "Epoch 542/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2629 - acc: 0.9714\n",
            "Epoch 543/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2623 - acc: 0.9714\n",
            "Epoch 544/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2617 - acc: 0.9714\n",
            "Epoch 545/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2610 - acc: 0.9714\n",
            "Epoch 546/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2604 - acc: 0.9714\n",
            "Epoch 547/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2598 - acc: 0.9714\n",
            "Epoch 548/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2592 - acc: 0.9714\n",
            "Epoch 549/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2587 - acc: 0.9714\n",
            "Epoch 550/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2581 - acc: 0.9714\n",
            "Epoch 551/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2575 - acc: 0.9714\n",
            "Epoch 552/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2570 - acc: 0.9714\n",
            "Epoch 553/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2564 - acc: 0.9714\n",
            "Epoch 554/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2558 - acc: 0.9714\n",
            "Epoch 555/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2552 - acc: 0.9714\n",
            "Epoch 556/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2546 - acc: 0.9714\n",
            "Epoch 557/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2540 - acc: 0.9714\n",
            "Epoch 558/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2536 - acc: 0.9714\n",
            "Epoch 559/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2530 - acc: 0.9714\n",
            "Epoch 560/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2524 - acc: 0.9714\n",
            "Epoch 561/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2519 - acc: 0.9714\n",
            "Epoch 562/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2514 - acc: 0.9714\n",
            "Epoch 563/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2508 - acc: 0.9714\n",
            "Epoch 564/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2503 - acc: 0.9714\n",
            "Epoch 565/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2498 - acc: 0.9714\n",
            "Epoch 566/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2492 - acc: 0.9714\n",
            "Epoch 567/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2487 - acc: 0.9714\n",
            "Epoch 568/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.2481 - acc: 0.9714\n",
            "Epoch 569/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2476 - acc: 0.9714\n",
            "Epoch 570/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2471 - acc: 0.9714\n",
            "Epoch 571/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2466 - acc: 0.9810\n",
            "Epoch 572/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2460 - acc: 0.9905\n",
            "Epoch 573/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2456 - acc: 0.9810\n",
            "Epoch 574/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2450 - acc: 0.9810\n",
            "Epoch 575/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2445 - acc: 0.9905\n",
            "Epoch 576/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2440 - acc: 0.9905\n",
            "Epoch 577/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2435 - acc: 0.9905\n",
            "Epoch 578/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2430 - acc: 0.9905\n",
            "Epoch 579/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2425 - acc: 0.9905\n",
            "Epoch 580/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2420 - acc: 0.9905\n",
            "Epoch 581/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2415 - acc: 0.9905\n",
            "Epoch 582/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2411 - acc: 0.9905\n",
            "Epoch 583/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2406 - acc: 0.9905\n",
            "Epoch 584/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2401 - acc: 0.9905\n",
            "Epoch 585/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2397 - acc: 0.9810\n",
            "Epoch 586/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.2392 - acc: 0.9905\n",
            "Epoch 587/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2387 - acc: 0.9905\n",
            "Epoch 588/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2382 - acc: 0.9905\n",
            "Epoch 589/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2377 - acc: 0.9905\n",
            "Epoch 590/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2373 - acc: 0.9905\n",
            "Epoch 591/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2368 - acc: 0.9905\n",
            "Epoch 592/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2363 - acc: 0.9905\n",
            "Epoch 593/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2358 - acc: 0.9905\n",
            "Epoch 594/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2354 - acc: 0.9905\n",
            "Epoch 595/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2349 - acc: 0.9905\n",
            "Epoch 596/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2345 - acc: 0.9905\n",
            "Epoch 597/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2340 - acc: 0.9905\n",
            "Epoch 598/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2335 - acc: 0.9905\n",
            "Epoch 599/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2331 - acc: 0.9905\n",
            "Epoch 600/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2326 - acc: 0.9905\n",
            "Epoch 601/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2321 - acc: 0.9905\n",
            "Epoch 602/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2316 - acc: 0.9905\n",
            "Epoch 603/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2312 - acc: 0.9905\n",
            "Epoch 604/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2306 - acc: 0.9905\n",
            "Epoch 605/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2302 - acc: 0.9905\n",
            "Epoch 606/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2298 - acc: 0.9905\n",
            "Epoch 607/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2293 - acc: 0.9905\n",
            "Epoch 608/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2288 - acc: 0.9905\n",
            "Epoch 609/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2284 - acc: 0.9905\n",
            "Epoch 610/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2280 - acc: 0.9905\n",
            "Epoch 611/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2275 - acc: 0.9905\n",
            "Epoch 612/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2271 - acc: 0.9905\n",
            "Epoch 613/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2267 - acc: 0.9905\n",
            "Epoch 614/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2262 - acc: 0.9905\n",
            "Epoch 615/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2258 - acc: 0.9905\n",
            "Epoch 616/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2254 - acc: 0.9905\n",
            "Epoch 617/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2250 - acc: 0.9905\n",
            "Epoch 618/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2245 - acc: 0.9905\n",
            "Epoch 619/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2241 - acc: 0.9905\n",
            "Epoch 620/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2237 - acc: 0.9905\n",
            "Epoch 621/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2233 - acc: 0.9905\n",
            "Epoch 622/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2229 - acc: 0.9905\n",
            "Epoch 623/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2225 - acc: 0.9905\n",
            "Epoch 624/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2220 - acc: 0.9905\n",
            "Epoch 625/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2217 - acc: 0.9905\n",
            "Epoch 626/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2213 - acc: 0.9905\n",
            "Epoch 627/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2208 - acc: 0.9905\n",
            "Epoch 628/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2205 - acc: 0.9905\n",
            "Epoch 629/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.2201 - acc: 0.9905\n",
            "Epoch 630/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2197 - acc: 0.9905\n",
            "Epoch 631/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2193 - acc: 0.9905\n",
            "Epoch 632/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2189 - acc: 0.9905\n",
            "Epoch 633/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2185 - acc: 0.9905\n",
            "Epoch 634/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2182 - acc: 0.9905\n",
            "Epoch 635/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2178 - acc: 0.9905\n",
            "Epoch 636/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2174 - acc: 0.9905\n",
            "Epoch 637/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2170 - acc: 0.9905\n",
            "Epoch 638/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2166 - acc: 0.9905\n",
            "Epoch 639/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2163 - acc: 0.9905\n",
            "Epoch 640/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2159 - acc: 0.9905\n",
            "Epoch 641/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.2155 - acc: 0.9905\n",
            "Epoch 642/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2151 - acc: 0.9905\n",
            "Epoch 643/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2148 - acc: 0.9905\n",
            "Epoch 644/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2144 - acc: 0.9905\n",
            "Epoch 645/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2140 - acc: 0.9905\n",
            "Epoch 646/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2137 - acc: 0.9905\n",
            "Epoch 647/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2134 - acc: 0.9905\n",
            "Epoch 648/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.2130 - acc: 1.0000\n",
            "Epoch 649/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.2126 - acc: 0.9905\n",
            "Epoch 650/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2123 - acc: 0.9905\n",
            "Epoch 651/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2119 - acc: 1.0000\n",
            "Epoch 652/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2116 - acc: 1.0000\n",
            "Epoch 653/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2112 - acc: 1.0000\n",
            "Epoch 654/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.2109 - acc: 1.0000\n",
            "Epoch 655/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.2105 - acc: 1.0000\n",
            "Epoch 656/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2102 - acc: 1.0000\n",
            "Epoch 657/1000\n",
            "105/105 [==============================] - 0s 15us/step - loss: 0.2099 - acc: 1.0000\n",
            "Epoch 658/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2096 - acc: 1.0000\n",
            "Epoch 659/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2092 - acc: 1.0000\n",
            "Epoch 660/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.2089 - acc: 1.0000\n",
            "Epoch 661/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2085 - acc: 1.0000\n",
            "Epoch 662/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2082 - acc: 1.0000\n",
            "Epoch 663/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2079 - acc: 1.0000\n",
            "Epoch 664/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2076 - acc: 1.0000\n",
            "Epoch 665/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.2072 - acc: 1.0000\n",
            "Epoch 666/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2069 - acc: 1.0000\n",
            "Epoch 667/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2066 - acc: 1.0000\n",
            "Epoch 668/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2062 - acc: 1.0000\n",
            "Epoch 669/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2059 - acc: 1.0000\n",
            "Epoch 670/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2056 - acc: 1.0000\n",
            "Epoch 671/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2053 - acc: 1.0000\n",
            "Epoch 672/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.2049 - acc: 1.0000\n",
            "Epoch 673/1000\n",
            "105/105 [==============================] - 0s 18us/step - loss: 0.2045 - acc: 1.0000\n",
            "Epoch 674/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2042 - acc: 1.0000\n",
            "Epoch 675/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.2039 - acc: 1.0000\n",
            "Epoch 676/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.2036 - acc: 1.0000\n",
            "Epoch 677/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.2033 - acc: 1.0000\n",
            "Epoch 678/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2030 - acc: 1.0000\n",
            "Epoch 679/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2027 - acc: 1.0000\n",
            "Epoch 680/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2024 - acc: 1.0000\n",
            "Epoch 681/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2020 - acc: 1.0000\n",
            "Epoch 682/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.2017 - acc: 1.0000\n",
            "Epoch 683/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2014 - acc: 1.0000\n",
            "Epoch 684/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2011 - acc: 1.0000\n",
            "Epoch 685/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.2008 - acc: 1.0000\n",
            "Epoch 686/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2005 - acc: 1.0000\n",
            "Epoch 687/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.2002 - acc: 1.0000\n",
            "Epoch 688/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1999 - acc: 1.0000\n",
            "Epoch 689/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1996 - acc: 1.0000\n",
            "Epoch 690/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1993 - acc: 1.0000\n",
            "Epoch 691/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1991 - acc: 1.0000\n",
            "Epoch 692/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1988 - acc: 1.0000\n",
            "Epoch 693/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1985 - acc: 1.0000\n",
            "Epoch 694/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1982 - acc: 1.0000\n",
            "Epoch 695/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1979 - acc: 1.0000\n",
            "Epoch 696/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1976 - acc: 1.0000\n",
            "Epoch 697/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1974 - acc: 1.0000\n",
            "Epoch 698/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1971 - acc: 1.0000\n",
            "Epoch 699/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1968 - acc: 1.0000\n",
            "Epoch 700/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1966 - acc: 1.0000\n",
            "Epoch 701/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1963 - acc: 1.0000\n",
            "Epoch 702/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1959 - acc: 1.0000\n",
            "Epoch 703/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1957 - acc: 1.0000\n",
            "Epoch 704/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1955 - acc: 1.0000\n",
            "Epoch 705/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1952 - acc: 1.0000\n",
            "Epoch 706/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1949 - acc: 1.0000\n",
            "Epoch 707/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1946 - acc: 1.0000\n",
            "Epoch 708/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1943 - acc: 1.0000\n",
            "Epoch 709/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1941 - acc: 1.0000\n",
            "Epoch 710/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1938 - acc: 1.0000\n",
            "Epoch 711/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1935 - acc: 1.0000\n",
            "Epoch 712/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1932 - acc: 1.0000\n",
            "Epoch 713/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1930 - acc: 1.0000\n",
            "Epoch 714/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1927 - acc: 1.0000\n",
            "Epoch 715/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.1924 - acc: 1.0000\n",
            "Epoch 716/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1922 - acc: 1.0000\n",
            "Epoch 717/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1919 - acc: 1.0000\n",
            "Epoch 718/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1917 - acc: 1.0000\n",
            "Epoch 719/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1914 - acc: 1.0000\n",
            "Epoch 720/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1911 - acc: 1.0000\n",
            "Epoch 721/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1909 - acc: 1.0000\n",
            "Epoch 722/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1906 - acc: 1.0000\n",
            "Epoch 723/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1904 - acc: 1.0000\n",
            "Epoch 724/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1901 - acc: 1.0000\n",
            "Epoch 725/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1899 - acc: 1.0000\n",
            "Epoch 726/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1896 - acc: 1.0000\n",
            "Epoch 727/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1894 - acc: 1.0000\n",
            "Epoch 728/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1891 - acc: 1.0000\n",
            "Epoch 729/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1889 - acc: 1.0000\n",
            "Epoch 730/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1886 - acc: 1.0000\n",
            "Epoch 731/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1883 - acc: 1.0000\n",
            "Epoch 732/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1881 - acc: 1.0000\n",
            "Epoch 733/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1879 - acc: 1.0000\n",
            "Epoch 734/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1876 - acc: 1.0000\n",
            "Epoch 735/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1874 - acc: 1.0000\n",
            "Epoch 736/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1871 - acc: 1.0000\n",
            "Epoch 737/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1869 - acc: 1.0000\n",
            "Epoch 738/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1867 - acc: 1.0000\n",
            "Epoch 739/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1864 - acc: 1.0000\n",
            "Epoch 740/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1862 - acc: 1.0000\n",
            "Epoch 741/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1860 - acc: 1.0000\n",
            "Epoch 742/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1857 - acc: 1.0000\n",
            "Epoch 743/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1855 - acc: 1.0000\n",
            "Epoch 744/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1852 - acc: 1.0000\n",
            "Epoch 745/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1850 - acc: 1.0000\n",
            "Epoch 746/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1848 - acc: 1.0000\n",
            "Epoch 747/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1846 - acc: 1.0000\n",
            "Epoch 748/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1843 - acc: 1.0000\n",
            "Epoch 749/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1841 - acc: 1.0000\n",
            "Epoch 750/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1839 - acc: 1.0000\n",
            "Epoch 751/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1837 - acc: 1.0000\n",
            "Epoch 752/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1834 - acc: 1.0000\n",
            "Epoch 753/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1833 - acc: 1.0000\n",
            "Epoch 754/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1831 - acc: 1.0000\n",
            "Epoch 755/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1828 - acc: 1.0000\n",
            "Epoch 756/1000\n",
            "105/105 [==============================] - 0s 41us/step - loss: 0.1826 - acc: 1.0000\n",
            "Epoch 757/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1824 - acc: 1.0000\n",
            "Epoch 758/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1822 - acc: 1.0000\n",
            "Epoch 759/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1820 - acc: 1.0000\n",
            "Epoch 760/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1817 - acc: 1.0000\n",
            "Epoch 761/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1815 - acc: 1.0000\n",
            "Epoch 762/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1813 - acc: 1.0000\n",
            "Epoch 763/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1811 - acc: 1.0000\n",
            "Epoch 764/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1809 - acc: 1.0000\n",
            "Epoch 765/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1807 - acc: 1.0000\n",
            "Epoch 766/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1805 - acc: 1.0000\n",
            "Epoch 767/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1803 - acc: 1.0000\n",
            "Epoch 768/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1800 - acc: 1.0000\n",
            "Epoch 769/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.1798 - acc: 1.0000\n",
            "Epoch 770/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1796 - acc: 1.0000\n",
            "Epoch 771/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1794 - acc: 1.0000\n",
            "Epoch 772/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1792 - acc: 1.0000\n",
            "Epoch 773/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1790 - acc: 1.0000\n",
            "Epoch 774/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1788 - acc: 1.0000\n",
            "Epoch 775/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1786 - acc: 1.0000\n",
            "Epoch 776/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1784 - acc: 1.0000\n",
            "Epoch 777/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1781 - acc: 1.0000\n",
            "Epoch 778/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1780 - acc: 1.0000\n",
            "Epoch 779/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1777 - acc: 1.0000\n",
            "Epoch 780/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1775 - acc: 1.0000\n",
            "Epoch 781/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1773 - acc: 1.0000\n",
            "Epoch 782/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1771 - acc: 1.0000\n",
            "Epoch 783/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1769 - acc: 1.0000\n",
            "Epoch 784/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1767 - acc: 1.0000\n",
            "Epoch 785/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1765 - acc: 1.0000\n",
            "Epoch 786/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1763 - acc: 1.0000\n",
            "Epoch 787/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1761 - acc: 1.0000\n",
            "Epoch 788/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1759 - acc: 1.0000\n",
            "Epoch 789/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1756 - acc: 1.0000\n",
            "Epoch 790/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1755 - acc: 1.0000\n",
            "Epoch 791/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1753 - acc: 1.0000\n",
            "Epoch 792/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1751 - acc: 1.0000\n",
            "Epoch 793/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1748 - acc: 1.0000\n",
            "Epoch 794/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1746 - acc: 1.0000\n",
            "Epoch 795/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1745 - acc: 1.0000\n",
            "Epoch 796/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1743 - acc: 1.0000\n",
            "Epoch 797/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1741 - acc: 1.0000\n",
            "Epoch 798/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1739 - acc: 1.0000\n",
            "Epoch 799/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1737 - acc: 1.0000\n",
            "Epoch 800/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1735 - acc: 1.0000\n",
            "Epoch 801/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1733 - acc: 1.0000\n",
            "Epoch 802/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1731 - acc: 1.0000\n",
            "Epoch 803/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1729 - acc: 1.0000\n",
            "Epoch 804/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1727 - acc: 1.0000\n",
            "Epoch 805/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1725 - acc: 1.0000\n",
            "Epoch 806/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1723 - acc: 1.0000\n",
            "Epoch 807/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1722 - acc: 1.0000\n",
            "Epoch 808/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1720 - acc: 1.0000\n",
            "Epoch 809/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1718 - acc: 1.0000\n",
            "Epoch 810/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1716 - acc: 1.0000\n",
            "Epoch 811/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1714 - acc: 1.0000\n",
            "Epoch 812/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1712 - acc: 1.0000\n",
            "Epoch 813/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1710 - acc: 1.0000\n",
            "Epoch 814/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1709 - acc: 1.0000\n",
            "Epoch 815/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1707 - acc: 1.0000\n",
            "Epoch 816/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1705 - acc: 1.0000\n",
            "Epoch 817/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1703 - acc: 1.0000\n",
            "Epoch 818/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1701 - acc: 1.0000\n",
            "Epoch 819/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1700 - acc: 1.0000\n",
            "Epoch 820/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1698 - acc: 1.0000\n",
            "Epoch 821/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1696 - acc: 1.0000\n",
            "Epoch 822/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1694 - acc: 1.0000\n",
            "Epoch 823/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1693 - acc: 1.0000\n",
            "Epoch 824/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.1691 - acc: 1.0000\n",
            "Epoch 825/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1689 - acc: 1.0000\n",
            "Epoch 826/1000\n",
            "105/105 [==============================] - 0s 50us/step - loss: 0.1687 - acc: 1.0000\n",
            "Epoch 827/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1686 - acc: 1.0000\n",
            "Epoch 828/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1684 - acc: 1.0000\n",
            "Epoch 829/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1682 - acc: 1.0000\n",
            "Epoch 830/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1680 - acc: 1.0000\n",
            "Epoch 831/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1678 - acc: 1.0000\n",
            "Epoch 832/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.1677 - acc: 1.0000\n",
            "Epoch 833/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.1675 - acc: 1.0000\n",
            "Epoch 834/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1674 - acc: 1.0000\n",
            "Epoch 835/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1672 - acc: 1.0000\n",
            "Epoch 836/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1670 - acc: 1.0000\n",
            "Epoch 837/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1668 - acc: 1.0000\n",
            "Epoch 838/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1667 - acc: 1.0000\n",
            "Epoch 839/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1665 - acc: 1.0000\n",
            "Epoch 840/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1663 - acc: 1.0000\n",
            "Epoch 841/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1662 - acc: 1.0000\n",
            "Epoch 842/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1660 - acc: 1.0000\n",
            "Epoch 843/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1658 - acc: 1.0000\n",
            "Epoch 844/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1657 - acc: 1.0000\n",
            "Epoch 845/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1655 - acc: 1.0000\n",
            "Epoch 846/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1654 - acc: 1.0000\n",
            "Epoch 847/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1652 - acc: 1.0000\n",
            "Epoch 848/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1650 - acc: 1.0000\n",
            "Epoch 849/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1649 - acc: 1.0000\n",
            "Epoch 850/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1647 - acc: 1.0000\n",
            "Epoch 851/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1646 - acc: 1.0000\n",
            "Epoch 852/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1644 - acc: 1.0000\n",
            "Epoch 853/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1643 - acc: 1.0000\n",
            "Epoch 854/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1641 - acc: 1.0000\n",
            "Epoch 855/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1640 - acc: 1.0000\n",
            "Epoch 856/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1638 - acc: 1.0000\n",
            "Epoch 857/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1636 - acc: 1.0000\n",
            "Epoch 858/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1635 - acc: 1.0000\n",
            "Epoch 859/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1633 - acc: 1.0000\n",
            "Epoch 860/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1632 - acc: 1.0000\n",
            "Epoch 861/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1630 - acc: 1.0000\n",
            "Epoch 862/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1629 - acc: 1.0000\n",
            "Epoch 863/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1627 - acc: 1.0000\n",
            "Epoch 864/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1626 - acc: 1.0000\n",
            "Epoch 865/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1624 - acc: 1.0000\n",
            "Epoch 866/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1623 - acc: 1.0000\n",
            "Epoch 867/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1622 - acc: 1.0000\n",
            "Epoch 868/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1620 - acc: 1.0000\n",
            "Epoch 869/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1619 - acc: 1.0000\n",
            "Epoch 870/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1617 - acc: 1.0000\n",
            "Epoch 871/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1616 - acc: 1.0000\n",
            "Epoch 872/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1614 - acc: 1.0000\n",
            "Epoch 873/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1613 - acc: 1.0000\n",
            "Epoch 874/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1611 - acc: 1.0000\n",
            "Epoch 875/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1610 - acc: 1.0000\n",
            "Epoch 876/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1608 - acc: 1.0000\n",
            "Epoch 877/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1607 - acc: 1.0000\n",
            "Epoch 878/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1606 - acc: 1.0000\n",
            "Epoch 879/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1604 - acc: 1.0000\n",
            "Epoch 880/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1603 - acc: 1.0000\n",
            "Epoch 881/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1601 - acc: 1.0000\n",
            "Epoch 882/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1600 - acc: 1.0000\n",
            "Epoch 883/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1599 - acc: 1.0000\n",
            "Epoch 884/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1597 - acc: 1.0000\n",
            "Epoch 885/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1596 - acc: 1.0000\n",
            "Epoch 886/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1595 - acc: 1.0000\n",
            "Epoch 887/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1593 - acc: 1.0000\n",
            "Epoch 888/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1592 - acc: 1.0000\n",
            "Epoch 889/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1590 - acc: 1.0000\n",
            "Epoch 890/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1589 - acc: 1.0000\n",
            "Epoch 891/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1588 - acc: 1.0000\n",
            "Epoch 892/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1586 - acc: 1.0000\n",
            "Epoch 893/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1585 - acc: 1.0000\n",
            "Epoch 894/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.1584 - acc: 1.0000\n",
            "Epoch 895/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1582 - acc: 1.0000\n",
            "Epoch 896/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1581 - acc: 1.0000\n",
            "Epoch 897/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1579 - acc: 1.0000\n",
            "Epoch 898/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1578 - acc: 1.0000\n",
            "Epoch 899/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1577 - acc: 1.0000\n",
            "Epoch 900/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1575 - acc: 1.0000\n",
            "Epoch 901/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1574 - acc: 1.0000\n",
            "Epoch 902/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1573 - acc: 1.0000\n",
            "Epoch 903/1000\n",
            "105/105 [==============================] - 0s 39us/step - loss: 0.1571 - acc: 1.0000\n",
            "Epoch 904/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1570 - acc: 1.0000\n",
            "Epoch 905/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1569 - acc: 1.0000\n",
            "Epoch 906/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1567 - acc: 1.0000\n",
            "Epoch 907/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1566 - acc: 1.0000\n",
            "Epoch 908/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1565 - acc: 1.0000\n",
            "Epoch 909/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1564 - acc: 1.0000\n",
            "Epoch 910/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1562 - acc: 1.0000\n",
            "Epoch 911/1000\n",
            "105/105 [==============================] - 0s 35us/step - loss: 0.1561 - acc: 1.0000\n",
            "Epoch 912/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1560 - acc: 1.0000\n",
            "Epoch 913/1000\n",
            "105/105 [==============================] - 0s 40us/step - loss: 0.1558 - acc: 1.0000\n",
            "Epoch 914/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1557 - acc: 1.0000\n",
            "Epoch 915/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1556 - acc: 1.0000\n",
            "Epoch 916/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1554 - acc: 1.0000\n",
            "Epoch 917/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1553 - acc: 1.0000\n",
            "Epoch 918/1000\n",
            "105/105 [==============================] - 0s 37us/step - loss: 0.1552 - acc: 1.0000\n",
            "Epoch 919/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1551 - acc: 1.0000\n",
            "Epoch 920/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1549 - acc: 1.0000\n",
            "Epoch 921/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1548 - acc: 1.0000\n",
            "Epoch 922/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1546 - acc: 1.0000\n",
            "Epoch 923/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1545 - acc: 1.0000\n",
            "Epoch 924/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1544 - acc: 1.0000\n",
            "Epoch 925/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1542 - acc: 1.0000\n",
            "Epoch 926/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1541 - acc: 1.0000\n",
            "Epoch 927/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1540 - acc: 1.0000\n",
            "Epoch 928/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1538 - acc: 1.0000\n",
            "Epoch 929/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1537 - acc: 1.0000\n",
            "Epoch 930/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1536 - acc: 1.0000\n",
            "Epoch 931/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1535 - acc: 1.0000\n",
            "Epoch 932/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1533 - acc: 1.0000\n",
            "Epoch 933/1000\n",
            "105/105 [==============================] - 0s 25us/step - loss: 0.1532 - acc: 1.0000\n",
            "Epoch 934/1000\n",
            "105/105 [==============================] - 0s 38us/step - loss: 0.1531 - acc: 1.0000\n",
            "Epoch 935/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1529 - acc: 1.0000\n",
            "Epoch 936/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1528 - acc: 1.0000\n",
            "Epoch 937/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1527 - acc: 1.0000\n",
            "Epoch 938/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1526 - acc: 1.0000\n",
            "Epoch 939/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1525 - acc: 1.0000\n",
            "Epoch 940/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1523 - acc: 1.0000\n",
            "Epoch 941/1000\n",
            "105/105 [==============================] - 0s 33us/step - loss: 0.1522 - acc: 1.0000\n",
            "Epoch 942/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1521 - acc: 1.0000\n",
            "Epoch 943/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1520 - acc: 1.0000\n",
            "Epoch 944/1000\n",
            "105/105 [==============================] - 0s 26us/step - loss: 0.1518 - acc: 1.0000\n",
            "Epoch 945/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1517 - acc: 1.0000\n",
            "Epoch 946/1000\n",
            "105/105 [==============================] - 0s 20us/step - loss: 0.1515 - acc: 1.0000\n",
            "Epoch 947/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1514 - acc: 1.0000\n",
            "Epoch 948/1000\n",
            "105/105 [==============================] - 0s 24us/step - loss: 0.1512 - acc: 1.0000\n",
            "Epoch 949/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1511 - acc: 1.0000\n",
            "Epoch 950/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1510 - acc: 1.0000\n",
            "Epoch 951/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1508 - acc: 1.0000\n",
            "Epoch 952/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1507 - acc: 1.0000\n",
            "Epoch 953/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1505 - acc: 1.0000\n",
            "Epoch 954/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1504 - acc: 1.0000\n",
            "Epoch 955/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1502 - acc: 1.0000\n",
            "Epoch 956/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1501 - acc: 1.0000\n",
            "Epoch 957/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1499 - acc: 1.0000\n",
            "Epoch 958/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1498 - acc: 1.0000\n",
            "Epoch 959/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1496 - acc: 1.0000\n",
            "Epoch 960/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1495 - acc: 1.0000\n",
            "Epoch 961/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1493 - acc: 1.0000\n",
            "Epoch 962/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1492 - acc: 1.0000\n",
            "Epoch 963/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1491 - acc: 1.0000\n",
            "Epoch 964/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1489 - acc: 1.0000\n",
            "Epoch 965/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1488 - acc: 1.0000\n",
            "Epoch 966/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1487 - acc: 1.0000\n",
            "Epoch 967/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1485 - acc: 1.0000\n",
            "Epoch 968/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1484 - acc: 1.0000\n",
            "Epoch 969/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1483 - acc: 1.0000\n",
            "Epoch 970/1000\n",
            "105/105 [==============================] - 0s 34us/step - loss: 0.1482 - acc: 1.0000\n",
            "Epoch 971/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1480 - acc: 1.0000\n",
            "Epoch 972/1000\n",
            "105/105 [==============================] - 0s 43us/step - loss: 0.1479 - acc: 1.0000\n",
            "Epoch 973/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1478 - acc: 1.0000\n",
            "Epoch 974/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1476 - acc: 1.0000\n",
            "Epoch 975/1000\n",
            "105/105 [==============================] - 0s 21us/step - loss: 0.1475 - acc: 1.0000\n",
            "Epoch 976/1000\n",
            "105/105 [==============================] - 0s 19us/step - loss: 0.1474 - acc: 1.0000\n",
            "Epoch 977/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1473 - acc: 1.0000\n",
            "Epoch 978/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1472 - acc: 1.0000\n",
            "Epoch 979/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1470 - acc: 1.0000\n",
            "Epoch 980/1000\n",
            "105/105 [==============================] - 0s 27us/step - loss: 0.1469 - acc: 1.0000\n",
            "Epoch 981/1000\n",
            "105/105 [==============================] - 0s 23us/step - loss: 0.1468 - acc: 1.0000\n",
            "Epoch 982/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1467 - acc: 1.0000\n",
            "Epoch 983/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1466 - acc: 1.0000\n",
            "Epoch 984/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1464 - acc: 1.0000\n",
            "Epoch 985/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1463 - acc: 1.0000\n",
            "Epoch 986/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1462 - acc: 1.0000\n",
            "Epoch 987/1000\n",
            "105/105 [==============================] - 0s 36us/step - loss: 0.1461 - acc: 1.0000\n",
            "Epoch 988/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1460 - acc: 1.0000\n",
            "Epoch 989/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1458 - acc: 1.0000\n",
            "Epoch 990/1000\n",
            "105/105 [==============================] - 0s 22us/step - loss: 0.1457 - acc: 1.0000\n",
            "Epoch 991/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1456 - acc: 1.0000\n",
            "Epoch 992/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1455 - acc: 1.0000\n",
            "Epoch 993/1000\n",
            "105/105 [==============================] - 0s 28us/step - loss: 0.1454 - acc: 1.0000\n",
            "Epoch 994/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1453 - acc: 1.0000\n",
            "Epoch 995/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1452 - acc: 1.0000\n",
            "Epoch 996/1000\n",
            "105/105 [==============================] - 0s 31us/step - loss: 0.1450 - acc: 1.0000\n",
            "Epoch 997/1000\n",
            "105/105 [==============================] - 0s 32us/step - loss: 0.1449 - acc: 1.0000\n",
            "Epoch 998/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1448 - acc: 1.0000\n",
            "Epoch 999/1000\n",
            "105/105 [==============================] - 0s 29us/step - loss: 0.1447 - acc: 1.0000\n",
            "Epoch 1000/1000\n",
            "105/105 [==============================] - 0s 30us/step - loss: 0.1446 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 19ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ff7576f8-bef3-4d03-d19d-4ff558c89c12",
        "id": "on2ulb7qU7yI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "4083a02b-8d1f-40d7-d636-189f79ac8f70",
        "id": "DyeIFw0hU7yO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.38461539149284363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-9NMt5gU7yV"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}
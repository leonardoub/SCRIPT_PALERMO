{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_NO_kfold.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOPF/kl3j8IXjjGaHWcZz9F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Classification_NO_kfold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i4O_AtKVzh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcb00g1tWTqw",
        "colab_type": "code",
        "outputId": "15795f4f-4fa9-446d-ee53-4018636c9cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "# Seed value\n",
        "# Apparently you may use different seed values at each stage\n",
        "seed_value= 0\n",
        "\n",
        "# 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
        "import os\n",
        "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
        "\n",
        "# 2. Set `python` built-in pseudo-random generator at a fixed value\n",
        "import random\n",
        "random.seed(seed_value)\n",
        "\n",
        "# 3. Set `numpy` pseudo-random generator at a fixed value\n",
        "import numpy as np\n",
        "np.random.seed(seed_value)\n",
        "\n",
        "# 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
        "import tensorflow as tf\n",
        "tf.set_random_seed(seed_value)\n",
        "\n",
        "# 5. Configure a new global `tensorflow` session\n",
        "from keras import backend as K\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "K.set_session(sess)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DME-inQ4ke_",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hq45TSf3WcR",
        "colab_type": "code",
        "outputId": "1f5aa5c2-7316-4548-eb43-ee469293941a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I5MNxeW3j2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLxDyFPo3sU9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXU_B2k03uYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1YCrOMP3_4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWj1mwjV4Mzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKdS4Low4PHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6EsAdEt4RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PA_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDYyNd6_3s4",
        "colab_type": "text"
      },
      "source": [
        "#Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7I8R-jd_3Hd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bnO8hgZ__GF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_big, X_test, y_train_big, y_test = train_test_split(public_data, public_labels, test_size=13, stratify=public_labels, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMuRNXFjVEiK",
        "colab_type": "text"
      },
      "source": [
        "#Train Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ovpVx4a7VMkl",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S3Tq1lHxVMlu",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_train_big, y_train_big, test_size=13, stratify=y_train_big, random_state=2)                                                         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "#Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKRmr5Am-860",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_data_stand = scaler.fit_transform(X_train)\n",
        "val_data_stand = scaler.transform(X_val)\n",
        "test_data_stand = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "#LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "904e7e72-a592-4757-fa99-36d178ec2c6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, y_train).transform(train_data_stand)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "abd7da05-a949-49d6-a55a-465b83f6a0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0UwBbCjf13g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = lda.transform(val_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF5fsvoQwBqR",
        "colab_type": "text"
      },
      "source": [
        "##Z-score dopo LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYbsLEB_6nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler_2 = StandardScaler()\n",
        "train_data_stand_lda = scaler_2.fit_transform(train_data_stand_lda)\n",
        "val_data_stand_lda = scaler_2.transform(val_data_stand_lda)\n",
        "test_data_stand_lda = scaler_2.transform(test_data_stand_lda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw8_CwJZwBD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KW2c_RIXpWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data_stand_lda = val_data_stand_lda - mean\n",
        "val_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3wGNwiWXvbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in y_train]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTLYdHX-mYtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_labels_dec = [word_index[label] for label in y_val]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in y_test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_val_labels = to_categorical(val_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(8, activation='relu', input_shape=(2,), kernel_regularizer=regularizers.l2(l=0.1)))\n",
        "  model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.9)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTAd2LU_dEO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau\n",
        "red_lr = ReduceLROnPlateau('val_loss', patience=10, verbose=1, min_lr=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF_sSb3CnLM2",
        "colab_type": "code",
        "outputId": "bd626a40-194d-42ec-cc6c-8a91d3f7ecf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "one_hot_val_labels.shape"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Hc4rG203_u",
        "colab_type": "code",
        "outputId": "44bfedfd-b9db-4233-a45d-5fe7c6f840b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 500\n",
        "\n",
        "model = build_model()\n",
        "history = model.fit(train_data_stand_lda, one_hot_train_labels, validation_data=(val_data_stand_lda, one_hot_val_labels), \n",
        "                      epochs= num_epochs, batch_size=105, callbacks=[red_lr])\n",
        "  \n",
        "\n",
        "acc_history = history.history['acc']\n",
        "loss_history = history.history['loss']\n",
        "acc_val_history = history.history['val_acc']\n",
        "loss_val_history = history.history['val_loss']\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 105 samples, validate on 13 samples\n",
            "Epoch 1/500\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 1.3727 - acc: 0.9810 - val_loss: 1.4264 - val_acc: 0.3846\n",
            "Epoch 2/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.3723 - acc: 0.9810 - val_loss: 1.4260 - val_acc: 0.3846\n",
            "Epoch 3/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.3716 - acc: 0.9810 - val_loss: 1.4255 - val_acc: 0.3846\n",
            "Epoch 4/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.3707 - acc: 0.9810 - val_loss: 1.4249 - val_acc: 0.3846\n",
            "Epoch 5/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.3695 - acc: 0.9810 - val_loss: 1.4241 - val_acc: 0.3846\n",
            "Epoch 6/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.3680 - acc: 0.9810 - val_loss: 1.4233 - val_acc: 0.3846\n",
            "Epoch 7/500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.3664 - acc: 0.9810 - val_loss: 1.4223 - val_acc: 0.3077\n",
            "Epoch 8/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.3645 - acc: 1.0000 - val_loss: 1.4213 - val_acc: 0.3077\n",
            "Epoch 9/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.3625 - acc: 1.0000 - val_loss: 1.4203 - val_acc: 0.3077\n",
            "Epoch 10/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.3604 - acc: 1.0000 - val_loss: 1.4191 - val_acc: 0.3077\n",
            "Epoch 11/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.3581 - acc: 1.0000 - val_loss: 1.4180 - val_acc: 0.3077\n",
            "Epoch 12/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.3558 - acc: 1.0000 - val_loss: 1.4168 - val_acc: 0.3077\n",
            "Epoch 13/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.3533 - acc: 1.0000 - val_loss: 1.4156 - val_acc: 0.3077\n",
            "Epoch 14/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.3507 - acc: 1.0000 - val_loss: 1.4144 - val_acc: 0.3077\n",
            "Epoch 15/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.3481 - acc: 1.0000 - val_loss: 1.4132 - val_acc: 0.3077\n",
            "Epoch 16/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.3454 - acc: 1.0000 - val_loss: 1.4120 - val_acc: 0.3077\n",
            "Epoch 17/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.3426 - acc: 1.0000 - val_loss: 1.4107 - val_acc: 0.3077\n",
            "Epoch 18/500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.3398 - acc: 1.0000 - val_loss: 1.4094 - val_acc: 0.3077\n",
            "Epoch 19/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.3369 - acc: 1.0000 - val_loss: 1.4081 - val_acc: 0.3077\n",
            "Epoch 20/500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.3340 - acc: 1.0000 - val_loss: 1.4067 - val_acc: 0.3077\n",
            "Epoch 21/500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.3311 - acc: 1.0000 - val_loss: 1.4054 - val_acc: 0.3077\n",
            "Epoch 22/500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.3281 - acc: 1.0000 - val_loss: 1.4041 - val_acc: 0.3077\n",
            "Epoch 23/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.3252 - acc: 1.0000 - val_loss: 1.4029 - val_acc: 0.3077\n",
            "Epoch 24/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.3222 - acc: 1.0000 - val_loss: 1.4016 - val_acc: 0.3077\n",
            "Epoch 25/500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.3192 - acc: 1.0000 - val_loss: 1.4003 - val_acc: 0.3077\n",
            "Epoch 26/500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.3161 - acc: 1.0000 - val_loss: 1.3987 - val_acc: 0.3077\n",
            "Epoch 27/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.3131 - acc: 1.0000 - val_loss: 1.3971 - val_acc: 0.3077\n",
            "Epoch 28/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.3101 - acc: 1.0000 - val_loss: 1.3955 - val_acc: 0.3077\n",
            "Epoch 29/500\n",
            "105/105 [==============================] - 0s 16us/step - loss: 1.3071 - acc: 1.0000 - val_loss: 1.3939 - val_acc: 0.3077\n",
            "Epoch 30/500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.3040 - acc: 1.0000 - val_loss: 1.3925 - val_acc: 0.3077\n",
            "Epoch 31/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.3010 - acc: 1.0000 - val_loss: 1.3913 - val_acc: 0.3077\n",
            "Epoch 32/500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.2980 - acc: 1.0000 - val_loss: 1.3900 - val_acc: 0.3077\n",
            "Epoch 33/500\n",
            "105/105 [==============================] - 0s 23us/step - loss: 1.2950 - acc: 1.0000 - val_loss: 1.3889 - val_acc: 0.3077\n",
            "Epoch 34/500\n",
            "105/105 [==============================] - 0s 17us/step - loss: 1.2919 - acc: 1.0000 - val_loss: 1.3877 - val_acc: 0.3077\n",
            "Epoch 35/500\n",
            "105/105 [==============================] - 0s 18us/step - loss: 1.2889 - acc: 1.0000 - val_loss: 1.3866 - val_acc: 0.3077\n",
            "Epoch 36/500\n",
            "105/105 [==============================] - 0s 19us/step - loss: 1.2859 - acc: 1.0000 - val_loss: 1.3855 - val_acc: 0.3077\n",
            "Epoch 37/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.2829 - acc: 1.0000 - val_loss: 1.3844 - val_acc: 0.2308\n",
            "Epoch 38/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.2799 - acc: 1.0000 - val_loss: 1.3834 - val_acc: 0.2308\n",
            "Epoch 39/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.2770 - acc: 1.0000 - val_loss: 1.3824 - val_acc: 0.2308\n",
            "Epoch 40/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.2740 - acc: 1.0000 - val_loss: 1.3814 - val_acc: 0.2308\n",
            "Epoch 41/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.2710 - acc: 1.0000 - val_loss: 1.3805 - val_acc: 0.2308\n",
            "Epoch 42/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.2681 - acc: 1.0000 - val_loss: 1.3796 - val_acc: 0.2308\n",
            "Epoch 43/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.2651 - acc: 1.0000 - val_loss: 1.3787 - val_acc: 0.2308\n",
            "Epoch 44/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.2622 - acc: 1.0000 - val_loss: 1.3780 - val_acc: 0.2308\n",
            "Epoch 45/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.2593 - acc: 1.0000 - val_loss: 1.3772 - val_acc: 0.2308\n",
            "Epoch 46/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.2564 - acc: 1.0000 - val_loss: 1.3765 - val_acc: 0.2308\n",
            "Epoch 47/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.2535 - acc: 1.0000 - val_loss: 1.3759 - val_acc: 0.2308\n",
            "Epoch 48/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.2506 - acc: 1.0000 - val_loss: 1.3752 - val_acc: 0.2308\n",
            "Epoch 49/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2477 - acc: 1.0000 - val_loss: 1.3746 - val_acc: 0.2308\n",
            "Epoch 50/500\n",
            "105/105 [==============================] - 0s 107us/step - loss: 1.2449 - acc: 1.0000 - val_loss: 1.3740 - val_acc: 0.2308\n",
            "Epoch 51/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.2420 - acc: 1.0000 - val_loss: 1.3735 - val_acc: 0.2308\n",
            "Epoch 52/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.2392 - acc: 1.0000 - val_loss: 1.3730 - val_acc: 0.2308\n",
            "Epoch 53/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.2364 - acc: 1.0000 - val_loss: 1.3724 - val_acc: 0.2308\n",
            "Epoch 54/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.2336 - acc: 1.0000 - val_loss: 1.3720 - val_acc: 0.2308\n",
            "Epoch 55/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.2308 - acc: 1.0000 - val_loss: 1.3715 - val_acc: 0.3077\n",
            "Epoch 56/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.2280 - acc: 1.0000 - val_loss: 1.3711 - val_acc: 0.3077\n",
            "Epoch 57/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.2252 - acc: 1.0000 - val_loss: 1.3706 - val_acc: 0.3077\n",
            "Epoch 58/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.2224 - acc: 1.0000 - val_loss: 1.3702 - val_acc: 0.3846\n",
            "Epoch 59/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.2197 - acc: 1.0000 - val_loss: 1.3699 - val_acc: 0.3846\n",
            "Epoch 60/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.2170 - acc: 1.0000 - val_loss: 1.3695 - val_acc: 0.3846\n",
            "Epoch 61/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.2142 - acc: 1.0000 - val_loss: 1.3691 - val_acc: 0.3846\n",
            "Epoch 62/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.2115 - acc: 1.0000 - val_loss: 1.3688 - val_acc: 0.3846\n",
            "Epoch 63/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.2088 - acc: 1.0000 - val_loss: 1.3685 - val_acc: 0.3846\n",
            "Epoch 64/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.2061 - acc: 1.0000 - val_loss: 1.3682 - val_acc: 0.3077\n",
            "Epoch 65/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.2034 - acc: 1.0000 - val_loss: 1.3679 - val_acc: 0.3077\n",
            "Epoch 66/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.2008 - acc: 1.0000 - val_loss: 1.3677 - val_acc: 0.3077\n",
            "Epoch 67/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.1981 - acc: 1.0000 - val_loss: 1.3675 - val_acc: 0.3077\n",
            "Epoch 68/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.1955 - acc: 1.0000 - val_loss: 1.3672 - val_acc: 0.3077\n",
            "Epoch 69/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.1928 - acc: 1.0000 - val_loss: 1.3670 - val_acc: 0.3077\n",
            "Epoch 70/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1902 - acc: 1.0000 - val_loss: 1.3668 - val_acc: 0.3077\n",
            "Epoch 71/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1876 - acc: 1.0000 - val_loss: 1.3666 - val_acc: 0.3077\n",
            "Epoch 72/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.1849 - acc: 1.0000 - val_loss: 1.3663 - val_acc: 0.3077\n",
            "Epoch 73/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.1823 - acc: 1.0000 - val_loss: 1.3660 - val_acc: 0.3077\n",
            "Epoch 74/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.1797 - acc: 1.0000 - val_loss: 1.3657 - val_acc: 0.3077\n",
            "Epoch 75/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1771 - acc: 1.0000 - val_loss: 1.3654 - val_acc: 0.3077\n",
            "Epoch 76/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.1746 - acc: 1.0000 - val_loss: 1.3652 - val_acc: 0.3077\n",
            "Epoch 77/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1720 - acc: 1.0000 - val_loss: 1.3649 - val_acc: 0.3077\n",
            "Epoch 78/500\n",
            "105/105 [==============================] - 0s 99us/step - loss: 1.1694 - acc: 1.0000 - val_loss: 1.3647 - val_acc: 0.3077\n",
            "Epoch 79/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.1669 - acc: 1.0000 - val_loss: 1.3645 - val_acc: 0.3077\n",
            "Epoch 80/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.1643 - acc: 1.0000 - val_loss: 1.3644 - val_acc: 0.3077\n",
            "Epoch 81/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.1618 - acc: 1.0000 - val_loss: 1.3642 - val_acc: 0.3077\n",
            "Epoch 82/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1593 - acc: 1.0000 - val_loss: 1.3641 - val_acc: 0.3077\n",
            "Epoch 83/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.1568 - acc: 1.0000 - val_loss: 1.3642 - val_acc: 0.3077\n",
            "Epoch 84/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.1542 - acc: 1.0000 - val_loss: 1.3644 - val_acc: 0.3077\n",
            "Epoch 85/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1517 - acc: 1.0000 - val_loss: 1.3646 - val_acc: 0.3077\n",
            "Epoch 86/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1492 - acc: 1.0000 - val_loss: 1.3648 - val_acc: 0.3077\n",
            "Epoch 87/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.1468 - acc: 1.0000 - val_loss: 1.3651 - val_acc: 0.3077\n",
            "Epoch 88/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.1443 - acc: 1.0000 - val_loss: 1.3653 - val_acc: 0.3077\n",
            "Epoch 89/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.1418 - acc: 1.0000 - val_loss: 1.3656 - val_acc: 0.3077\n",
            "Epoch 90/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1393 - acc: 1.0000 - val_loss: 1.3659 - val_acc: 0.3077\n",
            "Epoch 91/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.1369 - acc: 1.0000 - val_loss: 1.3663 - val_acc: 0.3077\n",
            "Epoch 92/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.1344 - acc: 1.0000 - val_loss: 1.3666 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 93/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.1320 - acc: 1.0000 - val_loss: 1.3670 - val_acc: 0.3846\n",
            "Epoch 94/500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.1298 - acc: 1.0000 - val_loss: 1.3673 - val_acc: 0.3846\n",
            "Epoch 95/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.1278 - acc: 1.0000 - val_loss: 1.3676 - val_acc: 0.3846\n",
            "Epoch 96/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.1259 - acc: 1.0000 - val_loss: 1.3679 - val_acc: 0.3846\n",
            "Epoch 97/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.1242 - acc: 1.0000 - val_loss: 1.3681 - val_acc: 0.3846\n",
            "Epoch 98/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.1227 - acc: 1.0000 - val_loss: 1.3684 - val_acc: 0.3846\n",
            "Epoch 99/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.1213 - acc: 1.0000 - val_loss: 1.3686 - val_acc: 0.3846\n",
            "Epoch 100/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1200 - acc: 1.0000 - val_loss: 1.3688 - val_acc: 0.3846\n",
            "Epoch 101/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.1188 - acc: 1.0000 - val_loss: 1.3691 - val_acc: 0.3846\n",
            "Epoch 102/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.1177 - acc: 1.0000 - val_loss: 1.3693 - val_acc: 0.3846\n",
            "\n",
            "Epoch 00102: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
            "Epoch 103/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.1167 - acc: 1.0000 - val_loss: 1.3695 - val_acc: 0.3846\n",
            "Epoch 104/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.1158 - acc: 1.0000 - val_loss: 1.3696 - val_acc: 0.3846\n",
            "Epoch 105/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1150 - acc: 1.0000 - val_loss: 1.3698 - val_acc: 0.3846\n",
            "Epoch 106/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1142 - acc: 1.0000 - val_loss: 1.3700 - val_acc: 0.3846\n",
            "Epoch 107/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.1134 - acc: 1.0000 - val_loss: 1.3701 - val_acc: 0.3846\n",
            "Epoch 108/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1127 - acc: 1.0000 - val_loss: 1.3703 - val_acc: 0.3846\n",
            "Epoch 109/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1121 - acc: 1.0000 - val_loss: 1.3704 - val_acc: 0.3846\n",
            "Epoch 110/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.1115 - acc: 1.0000 - val_loss: 1.3705 - val_acc: 0.3846\n",
            "Epoch 111/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.1109 - acc: 1.0000 - val_loss: 1.3707 - val_acc: 0.3846\n",
            "Epoch 112/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.1104 - acc: 1.0000 - val_loss: 1.3708 - val_acc: 0.3846\n",
            "Epoch 113/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.1099 - acc: 1.0000 - val_loss: 1.3709 - val_acc: 0.3846\n",
            "Epoch 114/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1094 - acc: 1.0000 - val_loss: 1.3710 - val_acc: 0.3846\n",
            "Epoch 115/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.1090 - acc: 1.0000 - val_loss: 1.3711 - val_acc: 0.3846\n",
            "Epoch 116/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.1085 - acc: 1.0000 - val_loss: 1.3712 - val_acc: 0.3846\n",
            "Epoch 117/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.1081 - acc: 1.0000 - val_loss: 1.3713 - val_acc: 0.3846\n",
            "Epoch 118/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1077 - acc: 1.0000 - val_loss: 1.3714 - val_acc: 0.3846\n",
            "Epoch 119/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1074 - acc: 1.0000 - val_loss: 1.3715 - val_acc: 0.3846\n",
            "Epoch 120/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1070 - acc: 1.0000 - val_loss: 1.3716 - val_acc: 0.3846\n",
            "Epoch 121/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.1066 - acc: 1.0000 - val_loss: 1.3717 - val_acc: 0.3846\n",
            "Epoch 122/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.1063 - acc: 1.0000 - val_loss: 1.3718 - val_acc: 0.3846\n",
            "Epoch 123/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.1060 - acc: 1.0000 - val_loss: 1.3719 - val_acc: 0.3846\n",
            "Epoch 124/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.1057 - acc: 1.0000 - val_loss: 1.3719 - val_acc: 0.3846\n",
            "Epoch 125/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.1054 - acc: 1.0000 - val_loss: 1.3720 - val_acc: 0.3846\n",
            "Epoch 126/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.1050 - acc: 1.0000 - val_loss: 1.3721 - val_acc: 0.3846\n",
            "Epoch 127/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1047 - acc: 1.0000 - val_loss: 1.3722 - val_acc: 0.3846\n",
            "Epoch 128/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.1045 - acc: 1.0000 - val_loss: 1.3723 - val_acc: 0.3846\n",
            "Epoch 129/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.1042 - acc: 1.0000 - val_loss: 1.3723 - val_acc: 0.3846\n",
            "Epoch 130/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.1039 - acc: 1.0000 - val_loss: 1.3724 - val_acc: 0.3846\n",
            "Epoch 131/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1036 - acc: 1.0000 - val_loss: 1.3725 - val_acc: 0.3846\n",
            "Epoch 132/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.1033 - acc: 1.0000 - val_loss: 1.3726 - val_acc: 0.3846\n",
            "Epoch 133/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.1031 - acc: 1.0000 - val_loss: 1.3726 - val_acc: 0.3846\n",
            "Epoch 134/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.1028 - acc: 1.0000 - val_loss: 1.3727 - val_acc: 0.3846\n",
            "Epoch 135/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1025 - acc: 1.0000 - val_loss: 1.3728 - val_acc: 0.3846\n",
            "Epoch 136/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.1023 - acc: 1.0000 - val_loss: 1.3728 - val_acc: 0.3846\n",
            "Epoch 137/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.1020 - acc: 1.0000 - val_loss: 1.3729 - val_acc: 0.3846\n",
            "Epoch 138/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.1018 - acc: 1.0000 - val_loss: 1.3730 - val_acc: 0.3846\n",
            "Epoch 139/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.1015 - acc: 1.0000 - val_loss: 1.3730 - val_acc: 0.3846\n",
            "Epoch 140/500\n",
            "105/105 [==============================] - 0s 49us/step - loss: 1.1013 - acc: 1.0000 - val_loss: 1.3731 - val_acc: 0.3846\n",
            "Epoch 141/500\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.1010 - acc: 1.0000 - val_loss: 1.3732 - val_acc: 0.3846\n",
            "Epoch 142/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.1008 - acc: 1.0000 - val_loss: 1.3732 - val_acc: 0.3846\n",
            "Epoch 143/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.1005 - acc: 1.0000 - val_loss: 1.3733 - val_acc: 0.3846\n",
            "Epoch 144/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.1003 - acc: 1.0000 - val_loss: 1.3734 - val_acc: 0.3846\n",
            "Epoch 145/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.1000 - acc: 1.0000 - val_loss: 1.3734 - val_acc: 0.3846\n",
            "Epoch 146/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0998 - acc: 1.0000 - val_loss: 1.3735 - val_acc: 0.3846\n",
            "Epoch 147/500\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0995 - acc: 1.0000 - val_loss: 1.3736 - val_acc: 0.3846\n",
            "Epoch 148/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0993 - acc: 1.0000 - val_loss: 1.3736 - val_acc: 0.3846\n",
            "Epoch 149/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0991 - acc: 1.0000 - val_loss: 1.3737 - val_acc: 0.3846\n",
            "Epoch 150/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0988 - acc: 1.0000 - val_loss: 1.3738 - val_acc: 0.3846\n",
            "Epoch 151/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0986 - acc: 1.0000 - val_loss: 1.3739 - val_acc: 0.3846\n",
            "Epoch 152/500\n",
            "105/105 [==============================] - 0s 54us/step - loss: 1.0983 - acc: 1.0000 - val_loss: 1.3740 - val_acc: 0.3846\n",
            "Epoch 153/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0981 - acc: 1.0000 - val_loss: 1.3741 - val_acc: 0.3846\n",
            "Epoch 154/500\n",
            "105/105 [==============================] - 0s 60us/step - loss: 1.0979 - acc: 1.0000 - val_loss: 1.3742 - val_acc: 0.3846\n",
            "Epoch 155/500\n",
            "105/105 [==============================] - 0s 49us/step - loss: 1.0976 - acc: 1.0000 - val_loss: 1.3743 - val_acc: 0.3846\n",
            "Epoch 156/500\n",
            "105/105 [==============================] - 0s 53us/step - loss: 1.0974 - acc: 1.0000 - val_loss: 1.3743 - val_acc: 0.3846\n",
            "Epoch 157/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0971 - acc: 1.0000 - val_loss: 1.3744 - val_acc: 0.3846\n",
            "Epoch 158/500\n",
            "105/105 [==============================] - 0s 56us/step - loss: 1.0969 - acc: 1.0000 - val_loss: 1.3745 - val_acc: 0.3846\n",
            "Epoch 159/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0967 - acc: 1.0000 - val_loss: 1.3746 - val_acc: 0.3846\n",
            "Epoch 160/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0964 - acc: 1.0000 - val_loss: 1.3747 - val_acc: 0.3846\n",
            "Epoch 161/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0962 - acc: 1.0000 - val_loss: 1.3748 - val_acc: 0.3846\n",
            "Epoch 162/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0960 - acc: 1.0000 - val_loss: 1.3749 - val_acc: 0.3846\n",
            "Epoch 163/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0957 - acc: 1.0000 - val_loss: 1.3750 - val_acc: 0.3846\n",
            "Epoch 164/500\n",
            "105/105 [==============================] - 0s 52us/step - loss: 1.0955 - acc: 1.0000 - val_loss: 1.3751 - val_acc: 0.3846\n",
            "Epoch 165/500\n",
            "105/105 [==============================] - 0s 49us/step - loss: 1.0953 - acc: 1.0000 - val_loss: 1.3751 - val_acc: 0.3846\n",
            "Epoch 166/500\n",
            "105/105 [==============================] - 0s 55us/step - loss: 1.0950 - acc: 1.0000 - val_loss: 1.3752 - val_acc: 0.3846\n",
            "Epoch 167/500\n",
            "105/105 [==============================] - 0s 66us/step - loss: 1.0948 - acc: 1.0000 - val_loss: 1.3753 - val_acc: 0.3846\n",
            "Epoch 168/500\n",
            "105/105 [==============================] - 0s 59us/step - loss: 1.0945 - acc: 1.0000 - val_loss: 1.3754 - val_acc: 0.3846\n",
            "Epoch 169/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0943 - acc: 1.0000 - val_loss: 1.3755 - val_acc: 0.3846\n",
            "Epoch 170/500\n",
            "105/105 [==============================] - 0s 74us/step - loss: 1.0941 - acc: 1.0000 - val_loss: 1.3756 - val_acc: 0.3846\n",
            "Epoch 171/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0938 - acc: 1.0000 - val_loss: 1.3757 - val_acc: 0.3846\n",
            "Epoch 172/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0936 - acc: 1.0000 - val_loss: 1.3758 - val_acc: 0.3846\n",
            "Epoch 173/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0934 - acc: 1.0000 - val_loss: 1.3759 - val_acc: 0.3846\n",
            "Epoch 174/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0931 - acc: 1.0000 - val_loss: 1.3760 - val_acc: 0.3846\n",
            "Epoch 175/500\n",
            "105/105 [==============================] - 0s 24us/step - loss: 1.0929 - acc: 1.0000 - val_loss: 1.3761 - val_acc: 0.3846\n",
            "Epoch 176/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0927 - acc: 1.0000 - val_loss: 1.3761 - val_acc: 0.3846\n",
            "Epoch 177/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0924 - acc: 1.0000 - val_loss: 1.3762 - val_acc: 0.3846\n",
            "Epoch 178/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0922 - acc: 1.0000 - val_loss: 1.3763 - val_acc: 0.3846\n",
            "Epoch 179/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0920 - acc: 1.0000 - val_loss: 1.3764 - val_acc: 0.3846\n",
            "Epoch 180/500\n",
            "105/105 [==============================] - 0s 91us/step - loss: 1.0917 - acc: 1.0000 - val_loss: 1.3765 - val_acc: 0.3846\n",
            "Epoch 181/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0915 - acc: 1.0000 - val_loss: 1.3766 - val_acc: 0.3846\n",
            "Epoch 182/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0912 - acc: 1.0000 - val_loss: 1.3767 - val_acc: 0.3846\n",
            "Epoch 183/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0910 - acc: 1.0000 - val_loss: 1.3768 - val_acc: 0.3846\n",
            "Epoch 184/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0908 - acc: 1.0000 - val_loss: 1.3769 - val_acc: 0.3846\n",
            "Epoch 185/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0905 - acc: 1.0000 - val_loss: 1.3770 - val_acc: 0.3846\n",
            "Epoch 186/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0903 - acc: 1.0000 - val_loss: 1.3771 - val_acc: 0.3846\n",
            "Epoch 187/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0901 - acc: 1.0000 - val_loss: 1.3772 - val_acc: 0.3846\n",
            "Epoch 188/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0898 - acc: 1.0000 - val_loss: 1.3773 - val_acc: 0.3846\n",
            "Epoch 189/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0896 - acc: 1.0000 - val_loss: 1.3774 - val_acc: 0.3846\n",
            "Epoch 190/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0894 - acc: 1.0000 - val_loss: 1.3774 - val_acc: 0.3846\n",
            "Epoch 191/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0891 - acc: 1.0000 - val_loss: 1.3775 - val_acc: 0.3846\n",
            "Epoch 192/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0889 - acc: 1.0000 - val_loss: 1.3776 - val_acc: 0.3846\n",
            "Epoch 193/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0887 - acc: 1.0000 - val_loss: 1.3777 - val_acc: 0.3846\n",
            "Epoch 194/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0884 - acc: 1.0000 - val_loss: 1.3778 - val_acc: 0.3846\n",
            "Epoch 195/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0882 - acc: 1.0000 - val_loss: 1.3779 - val_acc: 0.3846\n",
            "Epoch 196/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0880 - acc: 1.0000 - val_loss: 1.3780 - val_acc: 0.3846\n",
            "Epoch 197/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0877 - acc: 1.0000 - val_loss: 1.3781 - val_acc: 0.3846\n",
            "Epoch 198/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0875 - acc: 1.0000 - val_loss: 1.3782 - val_acc: 0.3846\n",
            "Epoch 199/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0873 - acc: 1.0000 - val_loss: 1.3783 - val_acc: 0.3846\n",
            "Epoch 200/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0870 - acc: 1.0000 - val_loss: 1.3784 - val_acc: 0.3846\n",
            "Epoch 201/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0868 - acc: 1.0000 - val_loss: 1.3785 - val_acc: 0.3846\n",
            "Epoch 202/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0866 - acc: 1.0000 - val_loss: 1.3786 - val_acc: 0.3846\n",
            "Epoch 203/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0863 - acc: 1.0000 - val_loss: 1.3787 - val_acc: 0.3846\n",
            "Epoch 204/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0861 - acc: 1.0000 - val_loss: 1.3788 - val_acc: 0.3846\n",
            "Epoch 205/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0859 - acc: 1.0000 - val_loss: 1.3789 - val_acc: 0.3846\n",
            "Epoch 206/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0856 - acc: 1.0000 - val_loss: 1.3790 - val_acc: 0.3846\n",
            "Epoch 207/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0854 - acc: 1.0000 - val_loss: 1.3791 - val_acc: 0.3846\n",
            "Epoch 208/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0852 - acc: 1.0000 - val_loss: 1.3792 - val_acc: 0.3846\n",
            "Epoch 209/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0849 - acc: 1.0000 - val_loss: 1.3793 - val_acc: 0.3846\n",
            "Epoch 210/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0847 - acc: 1.0000 - val_loss: 1.3794 - val_acc: 0.3846\n",
            "Epoch 211/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0845 - acc: 1.0000 - val_loss: 1.3795 - val_acc: 0.3846\n",
            "Epoch 212/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0842 - acc: 1.0000 - val_loss: 1.3796 - val_acc: 0.3846\n",
            "Epoch 213/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0840 - acc: 1.0000 - val_loss: 1.3797 - val_acc: 0.3846\n",
            "Epoch 214/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0838 - acc: 1.0000 - val_loss: 1.3798 - val_acc: 0.3846\n",
            "Epoch 215/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0835 - acc: 1.0000 - val_loss: 1.3799 - val_acc: 0.3846\n",
            "Epoch 216/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0833 - acc: 1.0000 - val_loss: 1.3800 - val_acc: 0.3846\n",
            "Epoch 217/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0831 - acc: 1.0000 - val_loss: 1.3801 - val_acc: 0.3846\n",
            "Epoch 218/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0828 - acc: 1.0000 - val_loss: 1.3802 - val_acc: 0.3846\n",
            "Epoch 219/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0826 - acc: 1.0000 - val_loss: 1.3803 - val_acc: 0.3846\n",
            "Epoch 220/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0824 - acc: 1.0000 - val_loss: 1.3804 - val_acc: 0.3846\n",
            "Epoch 221/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0821 - acc: 1.0000 - val_loss: 1.3805 - val_acc: 0.3846\n",
            "Epoch 222/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0819 - acc: 1.0000 - val_loss: 1.3806 - val_acc: 0.3846\n",
            "Epoch 223/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0816 - acc: 1.0000 - val_loss: 1.3807 - val_acc: 0.3846\n",
            "Epoch 224/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0814 - acc: 1.0000 - val_loss: 1.3808 - val_acc: 0.3846\n",
            "Epoch 225/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0812 - acc: 1.0000 - val_loss: 1.3809 - val_acc: 0.3846\n",
            "Epoch 226/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0809 - acc: 1.0000 - val_loss: 1.3810 - val_acc: 0.3846\n",
            "Epoch 227/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0807 - acc: 1.0000 - val_loss: 1.3811 - val_acc: 0.3846\n",
            "Epoch 228/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0805 - acc: 1.0000 - val_loss: 1.3812 - val_acc: 0.3846\n",
            "Epoch 229/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0802 - acc: 1.0000 - val_loss: 1.3813 - val_acc: 0.3846\n",
            "Epoch 230/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0800 - acc: 1.0000 - val_loss: 1.3814 - val_acc: 0.3846\n",
            "Epoch 231/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0798 - acc: 1.0000 - val_loss: 1.3815 - val_acc: 0.3846\n",
            "Epoch 232/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0795 - acc: 1.0000 - val_loss: 1.3816 - val_acc: 0.3846\n",
            "Epoch 233/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0793 - acc: 1.0000 - val_loss: 1.3817 - val_acc: 0.3846\n",
            "Epoch 234/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0791 - acc: 1.0000 - val_loss: 1.3818 - val_acc: 0.3846\n",
            "Epoch 235/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0788 - acc: 1.0000 - val_loss: 1.3819 - val_acc: 0.3846\n",
            "Epoch 236/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0786 - acc: 1.0000 - val_loss: 1.3820 - val_acc: 0.3846\n",
            "Epoch 237/500\n",
            "105/105 [==============================] - 0s 55us/step - loss: 1.0784 - acc: 1.0000 - val_loss: 1.3821 - val_acc: 0.3846\n",
            "Epoch 238/500\n",
            "105/105 [==============================] - 0s 54us/step - loss: 1.0781 - acc: 1.0000 - val_loss: 1.3822 - val_acc: 0.3846\n",
            "Epoch 239/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0779 - acc: 1.0000 - val_loss: 1.3823 - val_acc: 0.3846\n",
            "Epoch 240/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0777 - acc: 1.0000 - val_loss: 1.3824 - val_acc: 0.3846\n",
            "Epoch 241/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0774 - acc: 1.0000 - val_loss: 1.3825 - val_acc: 0.3846\n",
            "Epoch 242/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0772 - acc: 1.0000 - val_loss: 1.3826 - val_acc: 0.3846\n",
            "Epoch 243/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0770 - acc: 1.0000 - val_loss: 1.3827 - val_acc: 0.3846\n",
            "Epoch 244/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0767 - acc: 1.0000 - val_loss: 1.3828 - val_acc: 0.3846\n",
            "Epoch 245/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0765 - acc: 1.0000 - val_loss: 1.3829 - val_acc: 0.3846\n",
            "Epoch 246/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0763 - acc: 1.0000 - val_loss: 1.3830 - val_acc: 0.3846\n",
            "Epoch 247/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0760 - acc: 1.0000 - val_loss: 1.3832 - val_acc: 0.3846\n",
            "Epoch 248/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0758 - acc: 1.0000 - val_loss: 1.3833 - val_acc: 0.3846\n",
            "Epoch 249/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0756 - acc: 1.0000 - val_loss: 1.3834 - val_acc: 0.3846\n",
            "Epoch 250/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0753 - acc: 1.0000 - val_loss: 1.3835 - val_acc: 0.3846\n",
            "Epoch 251/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0751 - acc: 1.0000 - val_loss: 1.3836 - val_acc: 0.3846\n",
            "Epoch 252/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0749 - acc: 1.0000 - val_loss: 1.3837 - val_acc: 0.3846\n",
            "Epoch 253/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0746 - acc: 1.0000 - val_loss: 1.3838 - val_acc: 0.3846\n",
            "Epoch 254/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0744 - acc: 1.0000 - val_loss: 1.3839 - val_acc: 0.3846\n",
            "Epoch 255/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0742 - acc: 1.0000 - val_loss: 1.3840 - val_acc: 0.3846\n",
            "Epoch 256/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0739 - acc: 1.0000 - val_loss: 1.3841 - val_acc: 0.3846\n",
            "Epoch 257/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0737 - acc: 1.0000 - val_loss: 1.3842 - val_acc: 0.3846\n",
            "Epoch 258/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0735 - acc: 1.0000 - val_loss: 1.3843 - val_acc: 0.3846\n",
            "Epoch 259/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0732 - acc: 1.0000 - val_loss: 1.3844 - val_acc: 0.3846\n",
            "Epoch 260/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0730 - acc: 1.0000 - val_loss: 1.3845 - val_acc: 0.3846\n",
            "Epoch 261/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0728 - acc: 1.0000 - val_loss: 1.3847 - val_acc: 0.3846\n",
            "Epoch 262/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0725 - acc: 1.0000 - val_loss: 1.3848 - val_acc: 0.3846\n",
            "Epoch 263/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0723 - acc: 1.0000 - val_loss: 1.3849 - val_acc: 0.3846\n",
            "Epoch 264/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0721 - acc: 1.0000 - val_loss: 1.3850 - val_acc: 0.3846\n",
            "Epoch 265/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0718 - acc: 1.0000 - val_loss: 1.3851 - val_acc: 0.3846\n",
            "Epoch 266/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0716 - acc: 1.0000 - val_loss: 1.3852 - val_acc: 0.3846\n",
            "Epoch 267/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0714 - acc: 1.0000 - val_loss: 1.3853 - val_acc: 0.3846\n",
            "Epoch 268/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0712 - acc: 1.0000 - val_loss: 1.3854 - val_acc: 0.3846\n",
            "Epoch 269/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0709 - acc: 1.0000 - val_loss: 1.3855 - val_acc: 0.3846\n",
            "Epoch 270/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0707 - acc: 1.0000 - val_loss: 1.3856 - val_acc: 0.3846\n",
            "Epoch 271/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0705 - acc: 1.0000 - val_loss: 1.3857 - val_acc: 0.3846\n",
            "Epoch 272/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0702 - acc: 1.0000 - val_loss: 1.3859 - val_acc: 0.3846\n",
            "Epoch 273/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0700 - acc: 1.0000 - val_loss: 1.3860 - val_acc: 0.3846\n",
            "Epoch 274/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0698 - acc: 1.0000 - val_loss: 1.3861 - val_acc: 0.3846\n",
            "Epoch 275/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0695 - acc: 1.0000 - val_loss: 1.3862 - val_acc: 0.3846\n",
            "Epoch 276/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 1.0000 - val_loss: 1.3863 - val_acc: 0.3846\n",
            "Epoch 277/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0691 - acc: 1.0000 - val_loss: 1.3864 - val_acc: 0.3846\n",
            "Epoch 278/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0688 - acc: 1.0000 - val_loss: 1.3865 - val_acc: 0.3846\n",
            "Epoch 279/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0686 - acc: 1.0000 - val_loss: 1.3866 - val_acc: 0.3846\n",
            "Epoch 280/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0684 - acc: 1.0000 - val_loss: 1.3867 - val_acc: 0.3846\n",
            "Epoch 281/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0681 - acc: 1.0000 - val_loss: 1.3869 - val_acc: 0.3846\n",
            "Epoch 282/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0679 - acc: 1.0000 - val_loss: 1.3870 - val_acc: 0.3846\n",
            "Epoch 283/500\n",
            "105/105 [==============================] - 0s 49us/step - loss: 1.0677 - acc: 1.0000 - val_loss: 1.3871 - val_acc: 0.3846\n",
            "Epoch 284/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0674 - acc: 1.0000 - val_loss: 1.3872 - val_acc: 0.3846\n",
            "Epoch 285/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0672 - acc: 1.0000 - val_loss: 1.3873 - val_acc: 0.3846\n",
            "Epoch 286/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0670 - acc: 1.0000 - val_loss: 1.3874 - val_acc: 0.3846\n",
            "Epoch 287/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0667 - acc: 1.0000 - val_loss: 1.3875 - val_acc: 0.3846\n",
            "Epoch 288/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0665 - acc: 1.0000 - val_loss: 1.3876 - val_acc: 0.3846\n",
            "Epoch 289/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0663 - acc: 1.0000 - val_loss: 1.3877 - val_acc: 0.3846\n",
            "Epoch 290/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0660 - acc: 1.0000 - val_loss: 1.3879 - val_acc: 0.3846\n",
            "Epoch 291/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0658 - acc: 1.0000 - val_loss: 1.3880 - val_acc: 0.3846\n",
            "Epoch 292/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0656 - acc: 1.0000 - val_loss: 1.3881 - val_acc: 0.3846\n",
            "Epoch 293/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0653 - acc: 1.0000 - val_loss: 1.3882 - val_acc: 0.3846\n",
            "Epoch 294/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0651 - acc: 1.0000 - val_loss: 1.3883 - val_acc: 0.3846\n",
            "Epoch 295/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0649 - acc: 1.0000 - val_loss: 1.3884 - val_acc: 0.3846\n",
            "Epoch 296/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0646 - acc: 1.0000 - val_loss: 1.3885 - val_acc: 0.3846\n",
            "Epoch 297/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0644 - acc: 1.0000 - val_loss: 1.3887 - val_acc: 0.3846\n",
            "Epoch 298/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0642 - acc: 1.0000 - val_loss: 1.3888 - val_acc: 0.3846\n",
            "Epoch 299/500\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0639 - acc: 1.0000 - val_loss: 1.3889 - val_acc: 0.3846\n",
            "Epoch 300/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0637 - acc: 1.0000 - val_loss: 1.3890 - val_acc: 0.3846\n",
            "Epoch 301/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0635 - acc: 1.0000 - val_loss: 1.3891 - val_acc: 0.3846\n",
            "Epoch 302/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0632 - acc: 1.0000 - val_loss: 1.3892 - val_acc: 0.3846\n",
            "Epoch 303/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0630 - acc: 1.0000 - val_loss: 1.3893 - val_acc: 0.3846\n",
            "Epoch 304/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0628 - acc: 1.0000 - val_loss: 1.3895 - val_acc: 0.3846\n",
            "Epoch 305/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0625 - acc: 1.0000 - val_loss: 1.3896 - val_acc: 0.3846\n",
            "Epoch 306/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0623 - acc: 1.0000 - val_loss: 1.3897 - val_acc: 0.3846\n",
            "Epoch 307/500\n",
            "105/105 [==============================] - 0s 20us/step - loss: 1.0621 - acc: 1.0000 - val_loss: 1.3898 - val_acc: 0.3846\n",
            "Epoch 308/500\n",
            "105/105 [==============================] - 0s 49us/step - loss: 1.0618 - acc: 1.0000 - val_loss: 1.3899 - val_acc: 0.3846\n",
            "Epoch 309/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0616 - acc: 1.0000 - val_loss: 1.3900 - val_acc: 0.3846\n",
            "Epoch 310/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0614 - acc: 1.0000 - val_loss: 1.3902 - val_acc: 0.3846\n",
            "Epoch 311/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0611 - acc: 1.0000 - val_loss: 1.3903 - val_acc: 0.3846\n",
            "Epoch 312/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0609 - acc: 1.0000 - val_loss: 1.3904 - val_acc: 0.3846\n",
            "Epoch 313/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0607 - acc: 1.0000 - val_loss: 1.3905 - val_acc: 0.3846\n",
            "Epoch 314/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0604 - acc: 1.0000 - val_loss: 1.3906 - val_acc: 0.3846\n",
            "Epoch 315/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0602 - acc: 1.0000 - val_loss: 1.3907 - val_acc: 0.3846\n",
            "Epoch 316/500\n",
            "105/105 [==============================] - 0s 60us/step - loss: 1.0600 - acc: 1.0000 - val_loss: 1.3909 - val_acc: 0.3846\n",
            "Epoch 317/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0597 - acc: 1.0000 - val_loss: 1.3910 - val_acc: 0.3846\n",
            "Epoch 318/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0595 - acc: 1.0000 - val_loss: 1.3911 - val_acc: 0.3846\n",
            "Epoch 319/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0593 - acc: 1.0000 - val_loss: 1.3912 - val_acc: 0.3846\n",
            "Epoch 320/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0591 - acc: 1.0000 - val_loss: 1.3913 - val_acc: 0.3846\n",
            "Epoch 321/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0588 - acc: 1.0000 - val_loss: 1.3915 - val_acc: 0.3846\n",
            "Epoch 322/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0586 - acc: 1.0000 - val_loss: 1.3916 - val_acc: 0.3846\n",
            "Epoch 323/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0584 - acc: 1.0000 - val_loss: 1.3917 - val_acc: 0.3846\n",
            "Epoch 324/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0581 - acc: 1.0000 - val_loss: 1.3918 - val_acc: 0.3846\n",
            "Epoch 325/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0579 - acc: 1.0000 - val_loss: 1.3919 - val_acc: 0.3846\n",
            "Epoch 326/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0577 - acc: 1.0000 - val_loss: 1.3921 - val_acc: 0.3846\n",
            "Epoch 327/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0574 - acc: 1.0000 - val_loss: 1.3922 - val_acc: 0.3846\n",
            "Epoch 328/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0572 - acc: 1.0000 - val_loss: 1.3923 - val_acc: 0.3846\n",
            "Epoch 329/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0570 - acc: 1.0000 - val_loss: 1.3924 - val_acc: 0.3846\n",
            "Epoch 330/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0567 - acc: 1.0000 - val_loss: 1.3925 - val_acc: 0.3846\n",
            "Epoch 331/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0565 - acc: 1.0000 - val_loss: 1.3927 - val_acc: 0.3846\n",
            "Epoch 332/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0563 - acc: 1.0000 - val_loss: 1.3928 - val_acc: 0.3846\n",
            "Epoch 333/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0560 - acc: 1.0000 - val_loss: 1.3929 - val_acc: 0.3846\n",
            "Epoch 334/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0558 - acc: 1.0000 - val_loss: 1.3930 - val_acc: 0.3846\n",
            "Epoch 335/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0556 - acc: 1.0000 - val_loss: 1.3931 - val_acc: 0.3846\n",
            "Epoch 336/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0553 - acc: 1.0000 - val_loss: 1.3932 - val_acc: 0.3846\n",
            "Epoch 337/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0551 - acc: 1.0000 - val_loss: 1.3934 - val_acc: 0.3846\n",
            "Epoch 338/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0549 - acc: 1.0000 - val_loss: 1.3935 - val_acc: 0.3846\n",
            "Epoch 339/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0546 - acc: 1.0000 - val_loss: 1.3936 - val_acc: 0.3846\n",
            "Epoch 340/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0544 - acc: 1.0000 - val_loss: 1.3937 - val_acc: 0.3846\n",
            "Epoch 341/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0542 - acc: 1.0000 - val_loss: 1.3938 - val_acc: 0.3846\n",
            "Epoch 342/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0539 - acc: 1.0000 - val_loss: 1.3940 - val_acc: 0.3846\n",
            "Epoch 343/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0537 - acc: 1.0000 - val_loss: 1.3941 - val_acc: 0.3846\n",
            "Epoch 344/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0535 - acc: 1.0000 - val_loss: 1.3942 - val_acc: 0.3846\n",
            "Epoch 345/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0532 - acc: 1.0000 - val_loss: 1.3943 - val_acc: 0.3846\n",
            "Epoch 346/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0530 - acc: 1.0000 - val_loss: 1.3944 - val_acc: 0.3846\n",
            "Epoch 347/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0528 - acc: 1.0000 - val_loss: 1.3946 - val_acc: 0.3846\n",
            "Epoch 348/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0525 - acc: 1.0000 - val_loss: 1.3947 - val_acc: 0.3846\n",
            "Epoch 349/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0523 - acc: 1.0000 - val_loss: 1.3948 - val_acc: 0.3846\n",
            "Epoch 350/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0521 - acc: 1.0000 - val_loss: 1.3949 - val_acc: 0.3846\n",
            "Epoch 351/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0519 - acc: 1.0000 - val_loss: 1.3950 - val_acc: 0.3846\n",
            "Epoch 352/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0516 - acc: 1.0000 - val_loss: 1.3952 - val_acc: 0.3846\n",
            "Epoch 353/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0514 - acc: 1.0000 - val_loss: 1.3953 - val_acc: 0.3846\n",
            "Epoch 354/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0512 - acc: 1.0000 - val_loss: 1.3954 - val_acc: 0.3846\n",
            "Epoch 355/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0509 - acc: 1.0000 - val_loss: 1.3955 - val_acc: 0.3846\n",
            "Epoch 356/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0507 - acc: 1.0000 - val_loss: 1.3957 - val_acc: 0.3846\n",
            "Epoch 357/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0505 - acc: 1.0000 - val_loss: 1.3958 - val_acc: 0.3846\n",
            "Epoch 358/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0502 - acc: 1.0000 - val_loss: 1.3959 - val_acc: 0.3846\n",
            "Epoch 359/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0500 - acc: 1.0000 - val_loss: 1.3960 - val_acc: 0.3846\n",
            "Epoch 360/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0498 - acc: 1.0000 - val_loss: 1.3961 - val_acc: 0.3846\n",
            "Epoch 361/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0495 - acc: 1.0000 - val_loss: 1.3963 - val_acc: 0.3846\n",
            "Epoch 362/500\n",
            "105/105 [==============================] - 0s 40us/step - loss: 1.0493 - acc: 1.0000 - val_loss: 1.3964 - val_acc: 0.3846\n",
            "Epoch 363/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0491 - acc: 1.0000 - val_loss: 1.3965 - val_acc: 0.3846\n",
            "Epoch 364/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0488 - acc: 1.0000 - val_loss: 1.3966 - val_acc: 0.3846\n",
            "Epoch 365/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0486 - acc: 1.0000 - val_loss: 1.3968 - val_acc: 0.3846\n",
            "Epoch 366/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0484 - acc: 1.0000 - val_loss: 1.3969 - val_acc: 0.3846\n",
            "Epoch 367/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0481 - acc: 1.0000 - val_loss: 1.3970 - val_acc: 0.3846\n",
            "Epoch 368/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0479 - acc: 1.0000 - val_loss: 1.3971 - val_acc: 0.3846\n",
            "Epoch 369/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0477 - acc: 1.0000 - val_loss: 1.3973 - val_acc: 0.3846\n",
            "Epoch 370/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0474 - acc: 1.0000 - val_loss: 1.3974 - val_acc: 0.3846\n",
            "Epoch 371/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0472 - acc: 1.0000 - val_loss: 1.3975 - val_acc: 0.3846\n",
            "Epoch 372/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0470 - acc: 1.0000 - val_loss: 1.3976 - val_acc: 0.3846\n",
            "Epoch 373/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0467 - acc: 1.0000 - val_loss: 1.3978 - val_acc: 0.3846\n",
            "Epoch 374/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0465 - acc: 1.0000 - val_loss: 1.3979 - val_acc: 0.3846\n",
            "Epoch 375/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0463 - acc: 1.0000 - val_loss: 1.3980 - val_acc: 0.3846\n",
            "Epoch 376/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0461 - acc: 1.0000 - val_loss: 1.3981 - val_acc: 0.3846\n",
            "Epoch 377/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.0458 - acc: 1.0000 - val_loss: 1.3983 - val_acc: 0.3846\n",
            "Epoch 378/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0456 - acc: 1.0000 - val_loss: 1.3984 - val_acc: 0.3846\n",
            "Epoch 379/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0454 - acc: 1.0000 - val_loss: 1.3985 - val_acc: 0.3846\n",
            "Epoch 380/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0451 - acc: 1.0000 - val_loss: 1.3986 - val_acc: 0.3846\n",
            "Epoch 381/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0449 - acc: 1.0000 - val_loss: 1.3988 - val_acc: 0.3846\n",
            "Epoch 382/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0447 - acc: 1.0000 - val_loss: 1.3989 - val_acc: 0.3846\n",
            "Epoch 383/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0444 - acc: 1.0000 - val_loss: 1.3990 - val_acc: 0.3846\n",
            "Epoch 384/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0442 - acc: 1.0000 - val_loss: 1.3991 - val_acc: 0.3846\n",
            "Epoch 385/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0440 - acc: 1.0000 - val_loss: 1.3993 - val_acc: 0.3846\n",
            "Epoch 386/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0437 - acc: 1.0000 - val_loss: 1.3994 - val_acc: 0.3846\n",
            "Epoch 387/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0435 - acc: 1.0000 - val_loss: 1.3995 - val_acc: 0.3846\n",
            "Epoch 388/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0433 - acc: 1.0000 - val_loss: 1.3997 - val_acc: 0.3846\n",
            "Epoch 389/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0430 - acc: 1.0000 - val_loss: 1.3998 - val_acc: 0.3846\n",
            "Epoch 390/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0428 - acc: 1.0000 - val_loss: 1.3999 - val_acc: 0.3846\n",
            "Epoch 391/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0426 - acc: 1.0000 - val_loss: 1.4000 - val_acc: 0.3846\n",
            "Epoch 392/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0423 - acc: 1.0000 - val_loss: 1.4002 - val_acc: 0.3846\n",
            "Epoch 393/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0421 - acc: 1.0000 - val_loss: 1.4003 - val_acc: 0.3846\n",
            "Epoch 394/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.0419 - acc: 1.0000 - val_loss: 1.4004 - val_acc: 0.3846\n",
            "Epoch 395/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0416 - acc: 1.0000 - val_loss: 1.4006 - val_acc: 0.3846\n",
            "Epoch 396/500\n",
            "105/105 [==============================] - 0s 22us/step - loss: 1.0414 - acc: 1.0000 - val_loss: 1.4007 - val_acc: 0.3846\n",
            "Epoch 397/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0412 - acc: 1.0000 - val_loss: 1.4008 - val_acc: 0.3846\n",
            "Epoch 398/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0409 - acc: 1.0000 - val_loss: 1.4010 - val_acc: 0.3846\n",
            "Epoch 399/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0407 - acc: 1.0000 - val_loss: 1.4011 - val_acc: 0.3846\n",
            "Epoch 400/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0405 - acc: 1.0000 - val_loss: 1.4012 - val_acc: 0.3846\n",
            "Epoch 401/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0403 - acc: 1.0000 - val_loss: 1.4013 - val_acc: 0.3846\n",
            "Epoch 402/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0400 - acc: 1.0000 - val_loss: 1.4015 - val_acc: 0.3846\n",
            "Epoch 403/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0398 - acc: 1.0000 - val_loss: 1.4016 - val_acc: 0.3846\n",
            "Epoch 404/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0396 - acc: 1.0000 - val_loss: 1.4017 - val_acc: 0.3846\n",
            "Epoch 405/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0393 - acc: 1.0000 - val_loss: 1.4019 - val_acc: 0.3846\n",
            "Epoch 406/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0391 - acc: 1.0000 - val_loss: 1.4020 - val_acc: 0.3846\n",
            "Epoch 407/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0389 - acc: 1.0000 - val_loss: 1.4021 - val_acc: 0.3846\n",
            "Epoch 408/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0386 - acc: 1.0000 - val_loss: 1.4023 - val_acc: 0.3846\n",
            "Epoch 409/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0384 - acc: 1.0000 - val_loss: 1.4024 - val_acc: 0.3846\n",
            "Epoch 410/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0382 - acc: 1.0000 - val_loss: 1.4025 - val_acc: 0.3846\n",
            "Epoch 411/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0379 - acc: 1.0000 - val_loss: 1.4027 - val_acc: 0.3846\n",
            "Epoch 412/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0377 - acc: 1.0000 - val_loss: 1.4028 - val_acc: 0.3846\n",
            "Epoch 413/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0375 - acc: 1.0000 - val_loss: 1.4029 - val_acc: 0.3846\n",
            "Epoch 414/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0372 - acc: 1.0000 - val_loss: 1.4031 - val_acc: 0.3846\n",
            "Epoch 415/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0370 - acc: 1.0000 - val_loss: 1.4032 - val_acc: 0.3846\n",
            "Epoch 416/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0368 - acc: 1.0000 - val_loss: 1.4033 - val_acc: 0.3846\n",
            "Epoch 417/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0365 - acc: 1.0000 - val_loss: 1.4035 - val_acc: 0.3846\n",
            "Epoch 418/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0363 - acc: 1.0000 - val_loss: 1.4036 - val_acc: 0.3846\n",
            "Epoch 419/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0361 - acc: 1.0000 - val_loss: 1.4037 - val_acc: 0.3846\n",
            "Epoch 420/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0359 - acc: 1.0000 - val_loss: 1.4039 - val_acc: 0.3846\n",
            "Epoch 421/500\n",
            "105/105 [==============================] - 0s 25us/step - loss: 1.0356 - acc: 1.0000 - val_loss: 1.4040 - val_acc: 0.3846\n",
            "Epoch 422/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0354 - acc: 1.0000 - val_loss: 1.4041 - val_acc: 0.3846\n",
            "Epoch 423/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0352 - acc: 1.0000 - val_loss: 1.4043 - val_acc: 0.3846\n",
            "Epoch 424/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0349 - acc: 1.0000 - val_loss: 1.4044 - val_acc: 0.3846\n",
            "Epoch 425/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0347 - acc: 1.0000 - val_loss: 1.4045 - val_acc: 0.3846\n",
            "Epoch 426/500\n",
            "105/105 [==============================] - 0s 52us/step - loss: 1.0345 - acc: 1.0000 - val_loss: 1.4047 - val_acc: 0.3846\n",
            "Epoch 427/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0342 - acc: 1.0000 - val_loss: 1.4048 - val_acc: 0.3846\n",
            "Epoch 428/500\n",
            "105/105 [==============================] - 0s 31us/step - loss: 1.0340 - acc: 1.0000 - val_loss: 1.4049 - val_acc: 0.3846\n",
            "Epoch 429/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0338 - acc: 1.0000 - val_loss: 1.4051 - val_acc: 0.3846\n",
            "Epoch 430/500\n",
            "105/105 [==============================] - 0s 60us/step - loss: 1.0335 - acc: 1.0000 - val_loss: 1.4052 - val_acc: 0.3846\n",
            "Epoch 431/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0333 - acc: 1.0000 - val_loss: 1.4053 - val_acc: 0.3846\n",
            "Epoch 432/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0331 - acc: 1.0000 - val_loss: 1.4055 - val_acc: 0.3846\n",
            "Epoch 433/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0328 - acc: 1.0000 - val_loss: 1.4056 - val_acc: 0.3846\n",
            "Epoch 434/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0326 - acc: 1.0000 - val_loss: 1.4057 - val_acc: 0.3846\n",
            "Epoch 435/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0324 - acc: 1.0000 - val_loss: 1.4059 - val_acc: 0.3846\n",
            "Epoch 436/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0321 - acc: 1.0000 - val_loss: 1.4060 - val_acc: 0.3846\n",
            "Epoch 437/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0319 - acc: 1.0000 - val_loss: 1.4062 - val_acc: 0.3846\n",
            "Epoch 438/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0317 - acc: 1.0000 - val_loss: 1.4063 - val_acc: 0.3846\n",
            "Epoch 439/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0314 - acc: 1.0000 - val_loss: 1.4064 - val_acc: 0.3846\n",
            "Epoch 440/500\n",
            "105/105 [==============================] - 0s 52us/step - loss: 1.0312 - acc: 1.0000 - val_loss: 1.4066 - val_acc: 0.3846\n",
            "Epoch 441/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0310 - acc: 1.0000 - val_loss: 1.4067 - val_acc: 0.3846\n",
            "Epoch 442/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0308 - acc: 1.0000 - val_loss: 1.4068 - val_acc: 0.3846\n",
            "Epoch 443/500\n",
            "105/105 [==============================] - 0s 56us/step - loss: 1.0305 - acc: 1.0000 - val_loss: 1.4070 - val_acc: 0.3846\n",
            "Epoch 444/500\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0303 - acc: 1.0000 - val_loss: 1.4071 - val_acc: 0.3846\n",
            "Epoch 445/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0301 - acc: 1.0000 - val_loss: 1.4073 - val_acc: 0.3846\n",
            "Epoch 446/500\n",
            "105/105 [==============================] - 0s 48us/step - loss: 1.0298 - acc: 1.0000 - val_loss: 1.4074 - val_acc: 0.3846\n",
            "Epoch 447/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0296 - acc: 1.0000 - val_loss: 1.4075 - val_acc: 0.3846\n",
            "Epoch 448/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0294 - acc: 1.0000 - val_loss: 1.4077 - val_acc: 0.3846\n",
            "Epoch 449/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0291 - acc: 1.0000 - val_loss: 1.4078 - val_acc: 0.3846\n",
            "Epoch 450/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0289 - acc: 1.0000 - val_loss: 1.4080 - val_acc: 0.3846\n",
            "Epoch 451/500\n",
            "105/105 [==============================] - 0s 45us/step - loss: 1.0287 - acc: 1.0000 - val_loss: 1.4081 - val_acc: 0.3846\n",
            "Epoch 452/500\n",
            "105/105 [==============================] - 0s 54us/step - loss: 1.0284 - acc: 1.0000 - val_loss: 1.4082 - val_acc: 0.3846\n",
            "Epoch 453/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0282 - acc: 1.0000 - val_loss: 1.4084 - val_acc: 0.3846\n",
            "Epoch 454/500\n",
            "105/105 [==============================] - 0s 58us/step - loss: 1.0280 - acc: 1.0000 - val_loss: 1.4085 - val_acc: 0.3846\n",
            "Epoch 455/500\n",
            "105/105 [==============================] - 0s 51us/step - loss: 1.0277 - acc: 1.0000 - val_loss: 1.4087 - val_acc: 0.3846\n",
            "Epoch 456/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0275 - acc: 1.0000 - val_loss: 1.4088 - val_acc: 0.3846\n",
            "Epoch 457/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0273 - acc: 1.0000 - val_loss: 1.4089 - val_acc: 0.3846\n",
            "Epoch 458/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0270 - acc: 1.0000 - val_loss: 1.4091 - val_acc: 0.3846\n",
            "Epoch 459/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0268 - acc: 1.0000 - val_loss: 1.4092 - val_acc: 0.3846\n",
            "Epoch 460/500\n",
            "105/105 [==============================] - 0s 50us/step - loss: 1.0266 - acc: 1.0000 - val_loss: 1.4094 - val_acc: 0.3846\n",
            "Epoch 461/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0264 - acc: 1.0000 - val_loss: 1.4095 - val_acc: 0.3846\n",
            "Epoch 462/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0261 - acc: 1.0000 - val_loss: 1.4097 - val_acc: 0.3846\n",
            "Epoch 463/500\n",
            "105/105 [==============================] - 0s 54us/step - loss: 1.0259 - acc: 1.0000 - val_loss: 1.4098 - val_acc: 0.3846\n",
            "Epoch 464/500\n",
            "105/105 [==============================] - 0s 44us/step - loss: 1.0257 - acc: 1.0000 - val_loss: 1.4099 - val_acc: 0.3846\n",
            "Epoch 465/500\n",
            "105/105 [==============================] - 0s 46us/step - loss: 1.0254 - acc: 1.0000 - val_loss: 1.4101 - val_acc: 0.3846\n",
            "Epoch 466/500\n",
            "105/105 [==============================] - 0s 47us/step - loss: 1.0252 - acc: 1.0000 - val_loss: 1.4102 - val_acc: 0.3846\n",
            "Epoch 467/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0250 - acc: 1.0000 - val_loss: 1.4104 - val_acc: 0.3846\n",
            "Epoch 468/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0247 - acc: 1.0000 - val_loss: 1.4105 - val_acc: 0.3846\n",
            "Epoch 469/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0245 - acc: 1.0000 - val_loss: 1.4107 - val_acc: 0.3846\n",
            "Epoch 470/500\n",
            "105/105 [==============================] - 0s 36us/step - loss: 1.0243 - acc: 1.0000 - val_loss: 1.4108 - val_acc: 0.3846\n",
            "Epoch 471/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0240 - acc: 1.0000 - val_loss: 1.4109 - val_acc: 0.3846\n",
            "Epoch 472/500\n",
            "105/105 [==============================] - 0s 76us/step - loss: 1.0238 - acc: 1.0000 - val_loss: 1.4111 - val_acc: 0.3846\n",
            "Epoch 473/500\n",
            "105/105 [==============================] - 0s 42us/step - loss: 1.0236 - acc: 1.0000 - val_loss: 1.4112 - val_acc: 0.3846\n",
            "Epoch 474/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0233 - acc: 1.0000 - val_loss: 1.4114 - val_acc: 0.3846\n",
            "Epoch 475/500\n",
            "105/105 [==============================] - 0s 43us/step - loss: 1.0231 - acc: 1.0000 - val_loss: 1.4115 - val_acc: 0.3846\n",
            "Epoch 476/500\n",
            "105/105 [==============================] - 0s 134us/step - loss: 1.0229 - acc: 1.0000 - val_loss: 1.4117 - val_acc: 0.3846\n",
            "Epoch 477/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0226 - acc: 1.0000 - val_loss: 1.4118 - val_acc: 0.3846\n",
            "Epoch 478/500\n",
            "105/105 [==============================] - 0s 29us/step - loss: 1.0224 - acc: 1.0000 - val_loss: 1.4120 - val_acc: 0.3846\n",
            "Epoch 479/500\n",
            "105/105 [==============================] - 0s 26us/step - loss: 1.0222 - acc: 1.0000 - val_loss: 1.4121 - val_acc: 0.3846\n",
            "Epoch 480/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0220 - acc: 1.0000 - val_loss: 1.4122 - val_acc: 0.3846\n",
            "Epoch 481/500\n",
            "105/105 [==============================] - 0s 38us/step - loss: 1.0217 - acc: 1.0000 - val_loss: 1.4124 - val_acc: 0.3846\n",
            "Epoch 482/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0215 - acc: 1.0000 - val_loss: 1.4125 - val_acc: 0.3846\n",
            "Epoch 483/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0213 - acc: 1.0000 - val_loss: 1.4127 - val_acc: 0.3846\n",
            "Epoch 484/500\n",
            "105/105 [==============================] - 0s 37us/step - loss: 1.0210 - acc: 1.0000 - val_loss: 1.4128 - val_acc: 0.3846\n",
            "Epoch 485/500\n",
            "105/105 [==============================] - 0s 33us/step - loss: 1.0208 - acc: 1.0000 - val_loss: 1.4130 - val_acc: 0.3846\n",
            "Epoch 486/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0206 - acc: 1.0000 - val_loss: 1.4131 - val_acc: 0.3846\n",
            "Epoch 487/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0203 - acc: 1.0000 - val_loss: 1.4133 - val_acc: 0.3846\n",
            "Epoch 488/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0201 - acc: 1.0000 - val_loss: 1.4134 - val_acc: 0.3846\n",
            "Epoch 489/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0199 - acc: 1.0000 - val_loss: 1.4136 - val_acc: 0.3846\n",
            "Epoch 490/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0196 - acc: 1.0000 - val_loss: 1.4137 - val_acc: 0.3846\n",
            "Epoch 491/500\n",
            "105/105 [==============================] - 0s 32us/step - loss: 1.0194 - acc: 1.0000 - val_loss: 1.4139 - val_acc: 0.3846\n",
            "Epoch 492/500\n",
            "105/105 [==============================] - 0s 35us/step - loss: 1.0192 - acc: 1.0000 - val_loss: 1.4140 - val_acc: 0.3846\n",
            "Epoch 493/500\n",
            "105/105 [==============================] - 0s 34us/step - loss: 1.0190 - acc: 1.0000 - val_loss: 1.4141 - val_acc: 0.3846\n",
            "Epoch 494/500\n",
            "105/105 [==============================] - 0s 27us/step - loss: 1.0187 - acc: 1.0000 - val_loss: 1.4143 - val_acc: 0.3846\n",
            "Epoch 495/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0185 - acc: 1.0000 - val_loss: 1.4144 - val_acc: 0.3846\n",
            "Epoch 496/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0183 - acc: 1.0000 - val_loss: 1.4146 - val_acc: 0.3846\n",
            "Epoch 497/500\n",
            "105/105 [==============================] - 0s 30us/step - loss: 1.0180 - acc: 1.0000 - val_loss: 1.4147 - val_acc: 0.3846\n",
            "Epoch 498/500\n",
            "105/105 [==============================] - 0s 39us/step - loss: 1.0178 - acc: 1.0000 - val_loss: 1.4149 - val_acc: 0.3846\n",
            "Epoch 499/500\n",
            "105/105 [==============================] - 0s 41us/step - loss: 1.0176 - acc: 1.0000 - val_loss: 1.4150 - val_acc: 0.3846\n",
            "Epoch 500/500\n",
            "105/105 [==============================] - 0s 28us/step - loss: 1.0173 - acc: 1.0000 - val_loss: 1.4152 - val_acc: 0.3846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a31efb57-ed55-432f-c2be-33d68ed6d1a1",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, loss_val_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29d0bb50f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYVmCMMYYE9b/A7E3RjblSz38AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7546ab93-c732-4792-e573-378b8b8715a7",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f29d0b2a198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c8a40eee-cdb9-47ae-c1af-99ea63785ce4",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=1, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/180\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 1.1976 - acc: 0.4476\n",
            "Epoch 2/180\n",
            "105/105 [==============================] - 0s 895us/step - loss: 1.0908 - acc: 0.5810\n",
            "Epoch 3/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 1.0153 - acc: 0.5810\n",
            "Epoch 4/180\n",
            "105/105 [==============================] - 0s 917us/step - loss: 0.9541 - acc: 0.5905\n",
            "Epoch 5/180\n",
            "105/105 [==============================] - 0s 787us/step - loss: 0.9022 - acc: 0.7714\n",
            "Epoch 6/180\n",
            "105/105 [==============================] - 0s 799us/step - loss: 0.8565 - acc: 0.9810\n",
            "Epoch 7/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.8157 - acc: 0.9905\n",
            "Epoch 8/180\n",
            "105/105 [==============================] - 0s 852us/step - loss: 0.7787 - acc: 0.9905\n",
            "Epoch 9/180\n",
            "105/105 [==============================] - 0s 817us/step - loss: 0.7448 - acc: 1.0000\n",
            "Epoch 10/180\n",
            "105/105 [==============================] - 0s 809us/step - loss: 0.7135 - acc: 1.0000\n",
            "Epoch 11/180\n",
            "105/105 [==============================] - 0s 803us/step - loss: 0.6844 - acc: 1.0000\n",
            "Epoch 12/180\n",
            "105/105 [==============================] - 0s 822us/step - loss: 0.6575 - acc: 1.0000\n",
            "Epoch 13/180\n",
            "105/105 [==============================] - 0s 841us/step - loss: 0.6323 - acc: 1.0000\n",
            "Epoch 14/180\n",
            "105/105 [==============================] - 0s 842us/step - loss: 0.6090 - acc: 1.0000\n",
            "Epoch 15/180\n",
            "105/105 [==============================] - 0s 806us/step - loss: 0.5872 - acc: 1.0000\n",
            "Epoch 16/180\n",
            "105/105 [==============================] - 0s 832us/step - loss: 0.5666 - acc: 1.0000\n",
            "Epoch 17/180\n",
            "105/105 [==============================] - 0s 807us/step - loss: 0.5471 - acc: 1.0000\n",
            "Epoch 18/180\n",
            "105/105 [==============================] - 0s 901us/step - loss: 0.5287 - acc: 1.0000\n",
            "Epoch 19/180\n",
            "105/105 [==============================] - 0s 870us/step - loss: 0.5111 - acc: 1.0000\n",
            "Epoch 20/180\n",
            "105/105 [==============================] - 0s 917us/step - loss: 0.4944 - acc: 1.0000\n",
            "Epoch 21/180\n",
            "105/105 [==============================] - 0s 909us/step - loss: 0.4783 - acc: 1.0000\n",
            "Epoch 22/180\n",
            "105/105 [==============================] - 0s 899us/step - loss: 0.4628 - acc: 1.0000\n",
            "Epoch 23/180\n",
            "105/105 [==============================] - 0s 913us/step - loss: 0.4479 - acc: 1.0000\n",
            "Epoch 24/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.4337 - acc: 1.0000\n",
            "Epoch 25/180\n",
            "105/105 [==============================] - 0s 871us/step - loss: 0.4205 - acc: 1.0000\n",
            "Epoch 26/180\n",
            "105/105 [==============================] - 0s 994us/step - loss: 0.4080 - acc: 1.0000\n",
            "Epoch 27/180\n",
            "105/105 [==============================] - 0s 876us/step - loss: 0.3960 - acc: 1.0000\n",
            "Epoch 28/180\n",
            "105/105 [==============================] - 0s 908us/step - loss: 0.3848 - acc: 1.0000\n",
            "Epoch 29/180\n",
            "105/105 [==============================] - 0s 864us/step - loss: 0.3742 - acc: 1.0000\n",
            "Epoch 30/180\n",
            "105/105 [==============================] - 0s 813us/step - loss: 0.3641 - acc: 1.0000\n",
            "Epoch 31/180\n",
            "105/105 [==============================] - 0s 844us/step - loss: 0.3544 - acc: 1.0000\n",
            "Epoch 32/180\n",
            "105/105 [==============================] - 0s 824us/step - loss: 0.3453 - acc: 1.0000\n",
            "Epoch 33/180\n",
            "105/105 [==============================] - 0s 812us/step - loss: 0.3367 - acc: 1.0000\n",
            "Epoch 34/180\n",
            "105/105 [==============================] - 0s 842us/step - loss: 0.3285 - acc: 1.0000\n",
            "Epoch 35/180\n",
            "105/105 [==============================] - 0s 828us/step - loss: 0.3208 - acc: 1.0000\n",
            "Epoch 36/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.3136 - acc: 1.0000\n",
            "Epoch 37/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.3070 - acc: 1.0000\n",
            "Epoch 38/180\n",
            "105/105 [==============================] - 0s 816us/step - loss: 0.3007 - acc: 1.0000\n",
            "Epoch 39/180\n",
            "105/105 [==============================] - 0s 853us/step - loss: 0.2948 - acc: 1.0000\n",
            "Epoch 40/180\n",
            "105/105 [==============================] - 0s 825us/step - loss: 0.2894 - acc: 1.0000\n",
            "Epoch 41/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.2842 - acc: 1.0000\n",
            "Epoch 42/180\n",
            "105/105 [==============================] - 0s 954us/step - loss: 0.2793 - acc: 1.0000\n",
            "Epoch 43/180\n",
            "105/105 [==============================] - 0s 818us/step - loss: 0.2748 - acc: 1.0000\n",
            "Epoch 44/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.2704 - acc: 1.0000\n",
            "Epoch 45/180\n",
            "105/105 [==============================] - 0s 828us/step - loss: 0.2663 - acc: 1.0000\n",
            "Epoch 46/180\n",
            "105/105 [==============================] - 0s 808us/step - loss: 0.2625 - acc: 1.0000\n",
            "Epoch 47/180\n",
            "105/105 [==============================] - 0s 908us/step - loss: 0.2587 - acc: 1.0000\n",
            "Epoch 48/180\n",
            "105/105 [==============================] - 0s 846us/step - loss: 0.2552 - acc: 1.0000\n",
            "Epoch 49/180\n",
            "105/105 [==============================] - 0s 826us/step - loss: 0.2519 - acc: 1.0000\n",
            "Epoch 50/180\n",
            "105/105 [==============================] - 0s 889us/step - loss: 0.2488 - acc: 1.0000\n",
            "Epoch 51/180\n",
            "105/105 [==============================] - 0s 838us/step - loss: 0.2458 - acc: 1.0000\n",
            "Epoch 52/180\n",
            "105/105 [==============================] - 0s 874us/step - loss: 0.2428 - acc: 1.0000\n",
            "Epoch 53/180\n",
            "105/105 [==============================] - 0s 858us/step - loss: 0.2401 - acc: 1.0000\n",
            "Epoch 54/180\n",
            "105/105 [==============================] - 0s 887us/step - loss: 0.2374 - acc: 1.0000\n",
            "Epoch 55/180\n",
            "105/105 [==============================] - 0s 867us/step - loss: 0.2349 - acc: 1.0000\n",
            "Epoch 56/180\n",
            "105/105 [==============================] - 0s 852us/step - loss: 0.2325 - acc: 1.0000\n",
            "Epoch 57/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.2302 - acc: 1.0000\n",
            "Epoch 58/180\n",
            "105/105 [==============================] - 0s 824us/step - loss: 0.2279 - acc: 1.0000\n",
            "Epoch 59/180\n",
            "105/105 [==============================] - 0s 916us/step - loss: 0.2257 - acc: 1.0000\n",
            "Epoch 60/180\n",
            "105/105 [==============================] - 0s 899us/step - loss: 0.2237 - acc: 1.0000\n",
            "Epoch 61/180\n",
            "105/105 [==============================] - 0s 886us/step - loss: 0.2216 - acc: 1.0000\n",
            "Epoch 62/180\n",
            "105/105 [==============================] - 0s 876us/step - loss: 0.2197 - acc: 1.0000\n",
            "Epoch 63/180\n",
            "105/105 [==============================] - 0s 862us/step - loss: 0.2178 - acc: 1.0000\n",
            "Epoch 64/180\n",
            "105/105 [==============================] - 0s 902us/step - loss: 0.2159 - acc: 1.0000\n",
            "Epoch 65/180\n",
            "105/105 [==============================] - 0s 993us/step - loss: 0.2142 - acc: 1.0000\n",
            "Epoch 66/180\n",
            "105/105 [==============================] - 0s 977us/step - loss: 0.2125 - acc: 1.0000\n",
            "Epoch 67/180\n",
            "105/105 [==============================] - 0s 860us/step - loss: 0.2108 - acc: 1.0000\n",
            "Epoch 68/180\n",
            "105/105 [==============================] - 0s 952us/step - loss: 0.2092 - acc: 1.0000\n",
            "Epoch 69/180\n",
            "105/105 [==============================] - 0s 935us/step - loss: 0.2076 - acc: 1.0000\n",
            "Epoch 70/180\n",
            "105/105 [==============================] - 0s 833us/step - loss: 0.2061 - acc: 1.0000\n",
            "Epoch 71/180\n",
            "105/105 [==============================] - 0s 878us/step - loss: 0.2046 - acc: 1.0000\n",
            "Epoch 72/180\n",
            "105/105 [==============================] - 0s 876us/step - loss: 0.2032 - acc: 1.0000\n",
            "Epoch 73/180\n",
            "105/105 [==============================] - 0s 838us/step - loss: 0.2018 - acc: 1.0000\n",
            "Epoch 74/180\n",
            "105/105 [==============================] - 0s 813us/step - loss: 0.2005 - acc: 1.0000\n",
            "Epoch 75/180\n",
            "105/105 [==============================] - 0s 827us/step - loss: 0.1992 - acc: 1.0000\n",
            "Epoch 76/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.1978 - acc: 1.0000\n",
            "Epoch 77/180\n",
            "105/105 [==============================] - 0s 842us/step - loss: 0.1966 - acc: 1.0000\n",
            "Epoch 78/180\n",
            "105/105 [==============================] - 0s 836us/step - loss: 0.1953 - acc: 1.0000\n",
            "Epoch 79/180\n",
            "105/105 [==============================] - 0s 802us/step - loss: 0.1942 - acc: 1.0000\n",
            "Epoch 80/180\n",
            "105/105 [==============================] - 0s 858us/step - loss: 0.1930 - acc: 1.0000\n",
            "Epoch 81/180\n",
            "105/105 [==============================] - 0s 805us/step - loss: 0.1919 - acc: 1.0000\n",
            "Epoch 82/180\n",
            "105/105 [==============================] - 0s 781us/step - loss: 0.1907 - acc: 1.0000\n",
            "Epoch 83/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.1896 - acc: 1.0000\n",
            "Epoch 84/180\n",
            "105/105 [==============================] - 0s 778us/step - loss: 0.1885 - acc: 1.0000\n",
            "Epoch 85/180\n",
            "105/105 [==============================] - 0s 837us/step - loss: 0.1874 - acc: 1.0000\n",
            "Epoch 86/180\n",
            "105/105 [==============================] - 0s 808us/step - loss: 0.1864 - acc: 1.0000\n",
            "Epoch 87/180\n",
            "105/105 [==============================] - 0s 798us/step - loss: 0.1854 - acc: 1.0000\n",
            "Epoch 88/180\n",
            "105/105 [==============================] - 0s 810us/step - loss: 0.1844 - acc: 1.0000\n",
            "Epoch 89/180\n",
            "105/105 [==============================] - 0s 804us/step - loss: 0.1834 - acc: 1.0000\n",
            "Epoch 90/180\n",
            "105/105 [==============================] - 0s 802us/step - loss: 0.1825 - acc: 1.0000\n",
            "Epoch 91/180\n",
            "105/105 [==============================] - 0s 827us/step - loss: 0.1816 - acc: 1.0000\n",
            "Epoch 92/180\n",
            "105/105 [==============================] - 0s 937us/step - loss: 0.1806 - acc: 1.0000\n",
            "Epoch 93/180\n",
            "105/105 [==============================] - 0s 952us/step - loss: 0.1797 - acc: 1.0000\n",
            "Epoch 94/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.1788 - acc: 1.0000\n",
            "Epoch 95/180\n",
            "105/105 [==============================] - 0s 845us/step - loss: 0.1780 - acc: 1.0000\n",
            "Epoch 96/180\n",
            "105/105 [==============================] - 0s 944us/step - loss: 0.1771 - acc: 1.0000\n",
            "Epoch 97/180\n",
            "105/105 [==============================] - 0s 800us/step - loss: 0.1763 - acc: 1.0000\n",
            "Epoch 98/180\n",
            "105/105 [==============================] - 0s 866us/step - loss: 0.1755 - acc: 1.0000\n",
            "Epoch 99/180\n",
            "105/105 [==============================] - 0s 863us/step - loss: 0.1747 - acc: 1.0000\n",
            "Epoch 100/180\n",
            "105/105 [==============================] - 0s 819us/step - loss: 0.1739 - acc: 1.0000\n",
            "Epoch 101/180\n",
            "105/105 [==============================] - 0s 848us/step - loss: 0.1731 - acc: 1.0000\n",
            "Epoch 102/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.1723 - acc: 1.0000\n",
            "Epoch 103/180\n",
            "105/105 [==============================] - 0s 849us/step - loss: 0.1716 - acc: 1.0000\n",
            "Epoch 104/180\n",
            "105/105 [==============================] - 0s 837us/step - loss: 0.1708 - acc: 1.0000\n",
            "Epoch 105/180\n",
            "105/105 [==============================] - 0s 833us/step - loss: 0.1701 - acc: 1.0000\n",
            "Epoch 106/180\n",
            "105/105 [==============================] - 0s 893us/step - loss: 0.1694 - acc: 1.0000\n",
            "Epoch 107/180\n",
            "105/105 [==============================] - 0s 856us/step - loss: 0.1687 - acc: 1.0000\n",
            "Epoch 108/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.1680 - acc: 1.0000\n",
            "Epoch 109/180\n",
            "105/105 [==============================] - 0s 881us/step - loss: 0.1673 - acc: 1.0000\n",
            "Epoch 110/180\n",
            "105/105 [==============================] - 0s 827us/step - loss: 0.1666 - acc: 1.0000\n",
            "Epoch 111/180\n",
            "105/105 [==============================] - 0s 904us/step - loss: 0.1659 - acc: 1.0000\n",
            "Epoch 112/180\n",
            "105/105 [==============================] - 0s 848us/step - loss: 0.1653 - acc: 1.0000\n",
            "Epoch 113/180\n",
            "105/105 [==============================] - 0s 847us/step - loss: 0.1646 - acc: 1.0000\n",
            "Epoch 114/180\n",
            "105/105 [==============================] - 0s 877us/step - loss: 0.1640 - acc: 1.0000\n",
            "Epoch 115/180\n",
            "105/105 [==============================] - 0s 864us/step - loss: 0.1634 - acc: 1.0000\n",
            "Epoch 116/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.1627 - acc: 1.0000\n",
            "Epoch 117/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.1621 - acc: 1.0000\n",
            "Epoch 118/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.1615 - acc: 1.0000\n",
            "Epoch 119/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.1609 - acc: 1.0000\n",
            "Epoch 120/180\n",
            "105/105 [==============================] - 0s 939us/step - loss: 0.1603 - acc: 1.0000\n",
            "Epoch 121/180\n",
            "105/105 [==============================] - 0s 895us/step - loss: 0.1597 - acc: 1.0000\n",
            "Epoch 122/180\n",
            "105/105 [==============================] - 0s 947us/step - loss: 0.1592 - acc: 1.0000\n",
            "Epoch 123/180\n",
            "105/105 [==============================] - 0s 834us/step - loss: 0.1586 - acc: 1.0000\n",
            "Epoch 124/180\n",
            "105/105 [==============================] - 0s 838us/step - loss: 0.1581 - acc: 1.0000\n",
            "Epoch 125/180\n",
            "105/105 [==============================] - 0s 868us/step - loss: 0.1575 - acc: 1.0000\n",
            "Epoch 126/180\n",
            "105/105 [==============================] - 0s 869us/step - loss: 0.1569 - acc: 1.0000\n",
            "Epoch 127/180\n",
            "105/105 [==============================] - 0s 861us/step - loss: 0.1564 - acc: 1.0000\n",
            "Epoch 128/180\n",
            "105/105 [==============================] - 0s 966us/step - loss: 0.1559 - acc: 1.0000\n",
            "Epoch 129/180\n",
            "105/105 [==============================] - 0s 923us/step - loss: 0.1553 - acc: 1.0000\n",
            "Epoch 130/180\n",
            "105/105 [==============================] - 0s 955us/step - loss: 0.1548 - acc: 1.0000\n",
            "Epoch 131/180\n",
            "105/105 [==============================] - 0s 890us/step - loss: 0.1543 - acc: 1.0000\n",
            "Epoch 132/180\n",
            "105/105 [==============================] - 0s 938us/step - loss: 0.1538 - acc: 1.0000\n",
            "Epoch 133/180\n",
            "105/105 [==============================] - 0s 909us/step - loss: 0.1533 - acc: 1.0000\n",
            "Epoch 134/180\n",
            "105/105 [==============================] - 0s 907us/step - loss: 0.1528 - acc: 1.0000\n",
            "Epoch 135/180\n",
            "105/105 [==============================] - 0s 949us/step - loss: 0.1523 - acc: 1.0000\n",
            "Epoch 136/180\n",
            "105/105 [==============================] - 0s 902us/step - loss: 0.1518 - acc: 1.0000\n",
            "Epoch 137/180\n",
            "105/105 [==============================] - 0s 918us/step - loss: 0.1513 - acc: 1.0000\n",
            "Epoch 138/180\n",
            "105/105 [==============================] - 0s 924us/step - loss: 0.1509 - acc: 1.0000\n",
            "Epoch 139/180\n",
            "105/105 [==============================] - 0s 852us/step - loss: 0.1504 - acc: 1.0000\n",
            "Epoch 140/180\n",
            "105/105 [==============================] - 0s 858us/step - loss: 0.1499 - acc: 1.0000\n",
            "Epoch 141/180\n",
            "105/105 [==============================] - 0s 896us/step - loss: 0.1495 - acc: 1.0000\n",
            "Epoch 142/180\n",
            "105/105 [==============================] - 0s 853us/step - loss: 0.1490 - acc: 1.0000\n",
            "Epoch 143/180\n",
            "105/105 [==============================] - 0s 930us/step - loss: 0.1485 - acc: 1.0000\n",
            "Epoch 144/180\n",
            "105/105 [==============================] - 0s 872us/step - loss: 0.1481 - acc: 1.0000\n",
            "Epoch 145/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.1476 - acc: 1.0000\n",
            "Epoch 146/180\n",
            "105/105 [==============================] - 0s 868us/step - loss: 0.1472 - acc: 1.0000\n",
            "Epoch 147/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.1468 - acc: 1.0000\n",
            "Epoch 148/180\n",
            "105/105 [==============================] - 0s 903us/step - loss: 0.1463 - acc: 1.0000\n",
            "Epoch 149/180\n",
            "105/105 [==============================] - 0s 874us/step - loss: 0.1459 - acc: 1.0000\n",
            "Epoch 150/180\n",
            "105/105 [==============================] - 0s 931us/step - loss: 0.1455 - acc: 1.0000\n",
            "Epoch 151/180\n",
            "105/105 [==============================] - 0s 821us/step - loss: 0.1451 - acc: 1.0000\n",
            "Epoch 152/180\n",
            "105/105 [==============================] - 0s 867us/step - loss: 0.1447 - acc: 1.0000\n",
            "Epoch 153/180\n",
            "105/105 [==============================] - 0s 867us/step - loss: 0.1443 - acc: 1.0000\n",
            "Epoch 154/180\n",
            "105/105 [==============================] - 0s 865us/step - loss: 0.1439 - acc: 1.0000\n",
            "Epoch 155/180\n",
            "105/105 [==============================] - 0s 901us/step - loss: 0.1435 - acc: 1.0000\n",
            "Epoch 156/180\n",
            "105/105 [==============================] - 0s 905us/step - loss: 0.1431 - acc: 1.0000\n",
            "Epoch 157/180\n",
            "105/105 [==============================] - 0s 903us/step - loss: 0.1427 - acc: 1.0000\n",
            "Epoch 158/180\n",
            "105/105 [==============================] - 0s 933us/step - loss: 0.1423 - acc: 1.0000\n",
            "Epoch 159/180\n",
            "105/105 [==============================] - 0s 932us/step - loss: 0.1419 - acc: 1.0000\n",
            "Epoch 160/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1415 - acc: 1.0000\n",
            "Epoch 161/180\n",
            "105/105 [==============================] - 0s 861us/step - loss: 0.1412 - acc: 1.0000\n",
            "Epoch 162/180\n",
            "105/105 [==============================] - 0s 937us/step - loss: 0.1408 - acc: 1.0000\n",
            "Epoch 163/180\n",
            "105/105 [==============================] - 0s 906us/step - loss: 0.1405 - acc: 1.0000\n",
            "Epoch 164/180\n",
            "105/105 [==============================] - 0s 918us/step - loss: 0.1401 - acc: 1.0000\n",
            "Epoch 165/180\n",
            "105/105 [==============================] - 0s 987us/step - loss: 0.1397 - acc: 1.0000\n",
            "Epoch 166/180\n",
            "105/105 [==============================] - 0s 982us/step - loss: 0.1393 - acc: 1.0000\n",
            "Epoch 167/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1390 - acc: 1.0000\n",
            "Epoch 168/180\n",
            "105/105 [==============================] - 0s 927us/step - loss: 0.1386 - acc: 1.0000\n",
            "Epoch 169/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1383 - acc: 1.0000\n",
            "Epoch 170/180\n",
            "105/105 [==============================] - 0s 904us/step - loss: 0.1379 - acc: 1.0000\n",
            "Epoch 171/180\n",
            "105/105 [==============================] - 0s 927us/step - loss: 0.1376 - acc: 1.0000\n",
            "Epoch 172/180\n",
            "105/105 [==============================] - 0s 992us/step - loss: 0.1372 - acc: 1.0000\n",
            "Epoch 173/180\n",
            "105/105 [==============================] - 0s 969us/step - loss: 0.1369 - acc: 1.0000\n",
            "Epoch 174/180\n",
            "105/105 [==============================] - 0s 963us/step - loss: 0.1366 - acc: 1.0000\n",
            "Epoch 175/180\n",
            "105/105 [==============================] - 0s 926us/step - loss: 0.1363 - acc: 1.0000\n",
            "Epoch 176/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1359 - acc: 1.0000\n",
            "Epoch 177/180\n",
            "105/105 [==============================] - 0s 898us/step - loss: 0.1356 - acc: 1.0000\n",
            "Epoch 178/180\n",
            "105/105 [==============================] - 0s 915us/step - loss: 0.1352 - acc: 1.0000\n",
            "Epoch 179/180\n",
            "105/105 [==============================] - 0s 911us/step - loss: 0.1349 - acc: 1.0000\n",
            "Epoch 180/180\n",
            "105/105 [==============================] - 0s 1ms/step - loss: 0.1346 - acc: 1.0000\n",
            "13/13 [==============================] - 0s 9ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1e0fd245-3e13-4025-f307-48f228c13a5a",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "515455ab-de01-4627-c667-5239e9544c4d",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4615384638309479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyg3qdGpxYeh",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HBt6_60xcak",
        "colab_type": "text"
      },
      "source": [
        "#NO FEATURES SELECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    }
  ]
}
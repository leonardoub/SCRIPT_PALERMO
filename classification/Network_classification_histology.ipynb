{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "d5922592-1382-4116-f0cf-fd69d761c40a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "a7160c56-a09d-43f4-b80e-70ca1ecc411f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "bb48e8d5-96ba-417e-ab9c-ea3cd65c1128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRJJ4WxMgyIt"
      },
      "source": [
        "##Z score dei dati dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNbcqlgbgyI5",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_pca.mean(axis=0)\n",
        "std = train_data_stand_pca.std(axis=0)\n",
        "train_data_stand_pca = train_data_stand_pca - mean\n",
        "train_data_stand_pca /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZA9GJO6gyJO",
        "colab": {}
      },
      "source": [
        "test_data_stand_pca = test_data_stand_pca - mean\n",
        "test_data_stand_pca /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(20, activation='relu', input_shape=(7,)))\n",
        "  model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.005, momentum=0.9)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "11726d83-8ad8-4654-8a96-c83b160f53fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data, train_labels_dec)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "b308cd1c-8613-475f-cad4-9f88411a5965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "0c908bc0-aaf1-4be7-ccbc-602fe3892734",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVSoMnogHVqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "00b2e27c-0f7e-4c04-d7d3-2eadbf93ad67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['accuracy']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_accuracy']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/300\n",
            "87/87 [==============================] - 0s 941us/step - loss: 1.1931 - accuracy: 0.3218 - val_loss: 1.0887 - val_accuracy: 0.3409\n",
            "Epoch 2/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 1.0674 - accuracy: 0.3793 - val_loss: 1.0456 - val_accuracy: 0.3409\n",
            "Epoch 3/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9963 - accuracy: 0.4598 - val_loss: 1.0363 - val_accuracy: 0.3409\n",
            "Epoch 4/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.9689 - accuracy: 0.4713 - val_loss: 1.0357 - val_accuracy: 0.4318\n",
            "Epoch 5/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9479 - accuracy: 0.4943 - val_loss: 1.0330 - val_accuracy: 0.4091\n",
            "Epoch 6/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.9388 - accuracy: 0.5402 - val_loss: 1.0308 - val_accuracy: 0.3636\n",
            "Epoch 7/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.9271 - accuracy: 0.5517 - val_loss: 1.0315 - val_accuracy: 0.3636\n",
            "Epoch 8/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.9188 - accuracy: 0.5517 - val_loss: 1.0310 - val_accuracy: 0.4091\n",
            "Epoch 9/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9124 - accuracy: 0.5747 - val_loss: 1.0292 - val_accuracy: 0.3864\n",
            "Epoch 10/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9047 - accuracy: 0.5862 - val_loss: 1.0302 - val_accuracy: 0.4545\n",
            "Epoch 11/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8988 - accuracy: 0.5862 - val_loss: 1.0309 - val_accuracy: 0.4318\n",
            "Epoch 12/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8891 - accuracy: 0.6092 - val_loss: 1.0330 - val_accuracy: 0.5000\n",
            "Epoch 13/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8833 - accuracy: 0.6092 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
            "Epoch 14/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8729 - accuracy: 0.6092 - val_loss: 1.0362 - val_accuracy: 0.4773\n",
            "Epoch 15/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8676 - accuracy: 0.6092 - val_loss: 1.0379 - val_accuracy: 0.4773\n",
            "Epoch 16/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8575 - accuracy: 0.6207 - val_loss: 1.0426 - val_accuracy: 0.4773\n",
            "Epoch 17/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8517 - accuracy: 0.6207 - val_loss: 1.0496 - val_accuracy: 0.4545\n",
            "Epoch 18/300\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8454 - accuracy: 0.6092 - val_loss: 1.0533 - val_accuracy: 0.4773\n",
            "Epoch 19/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8330 - accuracy: 0.6437 - val_loss: 1.0587 - val_accuracy: 0.4545\n",
            "Epoch 20/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.8310 - accuracy: 0.6437 - val_loss: 1.0674 - val_accuracy: 0.4545\n",
            "Epoch 21/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8194 - accuracy: 0.6437 - val_loss: 1.0715 - val_accuracy: 0.4545\n",
            "Epoch 22/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8124 - accuracy: 0.6437 - val_loss: 1.0793 - val_accuracy: 0.4773\n",
            "Epoch 23/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8044 - accuracy: 0.6437 - val_loss: 1.0858 - val_accuracy: 0.4773\n",
            "Epoch 24/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.7959 - accuracy: 0.6437 - val_loss: 1.0943 - val_accuracy: 0.4773\n",
            "Epoch 25/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.7909 - accuracy: 0.6437 - val_loss: 1.1005 - val_accuracy: 0.4773\n",
            "Epoch 26/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7856 - accuracy: 0.6552 - val_loss: 1.1124 - val_accuracy: 0.4773\n",
            "Epoch 27/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.7818 - accuracy: 0.6437 - val_loss: 1.1238 - val_accuracy: 0.4773\n",
            "Epoch 28/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7692 - accuracy: 0.6552 - val_loss: 1.1312 - val_accuracy: 0.4773\n",
            "Epoch 29/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.7636 - accuracy: 0.6782 - val_loss: 1.1459 - val_accuracy: 0.4545\n",
            "Epoch 30/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.7559 - accuracy: 0.6667 - val_loss: 1.1536 - val_accuracy: 0.4773\n",
            "Epoch 31/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7538 - accuracy: 0.6552 - val_loss: 1.1625 - val_accuracy: 0.4773\n",
            "Epoch 32/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.7456 - accuracy: 0.6782 - val_loss: 1.1722 - val_accuracy: 0.4545\n",
            "Epoch 33/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7380 - accuracy: 0.6782 - val_loss: 1.1867 - val_accuracy: 0.4545\n",
            "Epoch 34/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7315 - accuracy: 0.6897 - val_loss: 1.1941 - val_accuracy: 0.4545\n",
            "Epoch 35/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.7279 - accuracy: 0.6782 - val_loss: 1.2025 - val_accuracy: 0.4545\n",
            "Epoch 36/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7222 - accuracy: 0.6782 - val_loss: 1.2138 - val_accuracy: 0.4545\n",
            "Epoch 37/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7172 - accuracy: 0.7011 - val_loss: 1.2281 - val_accuracy: 0.4545\n",
            "Epoch 38/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.7171 - accuracy: 0.7011 - val_loss: 1.2343 - val_accuracy: 0.4318\n",
            "Epoch 39/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.7100 - accuracy: 0.7011 - val_loss: 1.2519 - val_accuracy: 0.4773\n",
            "Epoch 40/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.7016 - accuracy: 0.7011 - val_loss: 1.2602 - val_accuracy: 0.4545\n",
            "Epoch 41/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6930 - accuracy: 0.7011 - val_loss: 1.2640 - val_accuracy: 0.4545\n",
            "Epoch 42/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6896 - accuracy: 0.7126 - val_loss: 1.2799 - val_accuracy: 0.4773\n",
            "Epoch 43/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6867 - accuracy: 0.7011 - val_loss: 1.2963 - val_accuracy: 0.4773\n",
            "Epoch 44/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6786 - accuracy: 0.7126 - val_loss: 1.3022 - val_accuracy: 0.4318\n",
            "Epoch 45/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.6718 - accuracy: 0.7241 - val_loss: 1.3095 - val_accuracy: 0.4318\n",
            "Epoch 46/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6709 - accuracy: 0.7241 - val_loss: 1.3208 - val_accuracy: 0.4091\n",
            "Epoch 47/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.6616 - accuracy: 0.7126 - val_loss: 1.3340 - val_accuracy: 0.4545\n",
            "Epoch 48/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.6652 - accuracy: 0.7126 - val_loss: 1.3324 - val_accuracy: 0.4318\n",
            "Epoch 49/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6531 - accuracy: 0.7241 - val_loss: 1.3513 - val_accuracy: 0.4318\n",
            "Epoch 50/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6515 - accuracy: 0.7241 - val_loss: 1.3635 - val_accuracy: 0.4318\n",
            "Epoch 51/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.6456 - accuracy: 0.7126 - val_loss: 1.3711 - val_accuracy: 0.4091\n",
            "Epoch 52/300\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.6361 - accuracy: 0.7241 - val_loss: 1.3830 - val_accuracy: 0.4318\n",
            "Epoch 53/300\n",
            "87/87 [==============================] - 0s 335us/step - loss: 0.6352 - accuracy: 0.7241 - val_loss: 1.4017 - val_accuracy: 0.4318\n",
            "Epoch 54/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.6279 - accuracy: 0.7241 - val_loss: 1.4000 - val_accuracy: 0.4545\n",
            "Epoch 55/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6244 - accuracy: 0.7241 - val_loss: 1.4176 - val_accuracy: 0.4091\n",
            "Epoch 56/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6190 - accuracy: 0.7356 - val_loss: 1.4282 - val_accuracy: 0.4545\n",
            "Epoch 57/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6249 - accuracy: 0.7241 - val_loss: 1.4403 - val_accuracy: 0.4318\n",
            "Epoch 58/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.6133 - accuracy: 0.7241 - val_loss: 1.4469 - val_accuracy: 0.4091\n",
            "Epoch 59/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.6184 - accuracy: 0.7471 - val_loss: 1.4715 - val_accuracy: 0.4318\n",
            "Epoch 60/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.6018 - accuracy: 0.7241 - val_loss: 1.4671 - val_accuracy: 0.4318\n",
            "Epoch 61/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.6107 - accuracy: 0.7126 - val_loss: 1.4947 - val_accuracy: 0.4091\n",
            "Epoch 62/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.5919 - accuracy: 0.7471 - val_loss: 1.4879 - val_accuracy: 0.4318\n",
            "Epoch 63/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.6065 - accuracy: 0.7126 - val_loss: 1.4975 - val_accuracy: 0.4318\n",
            "Epoch 64/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.6029 - accuracy: 0.7241 - val_loss: 1.5005 - val_accuracy: 0.4091\n",
            "Epoch 65/300\n",
            "87/87 [==============================] - 0s 308us/step - loss: 0.5833 - accuracy: 0.7356 - val_loss: 1.5104 - val_accuracy: 0.4545\n",
            "Epoch 66/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.5738 - accuracy: 0.7356 - val_loss: 1.5183 - val_accuracy: 0.4318\n",
            "Epoch 67/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.5651 - accuracy: 0.7356 - val_loss: 1.5378 - val_accuracy: 0.4545\n",
            "Epoch 68/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5640 - accuracy: 0.7471 - val_loss: 1.5441 - val_accuracy: 0.4545\n",
            "Epoch 69/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5529 - accuracy: 0.7701 - val_loss: 1.5689 - val_accuracy: 0.4318\n",
            "Epoch 70/300\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.5508 - accuracy: 0.7586 - val_loss: 1.5548 - val_accuracy: 0.4545\n",
            "Epoch 71/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.5454 - accuracy: 0.7356 - val_loss: 1.5821 - val_accuracy: 0.4773\n",
            "Epoch 72/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5480 - accuracy: 0.7586 - val_loss: 1.5821 - val_accuracy: 0.4773\n",
            "Epoch 73/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5422 - accuracy: 0.7701 - val_loss: 1.6016 - val_accuracy: 0.4545\n",
            "Epoch 74/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.5320 - accuracy: 0.7586 - val_loss: 1.5933 - val_accuracy: 0.4773\n",
            "Epoch 75/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.5193 - accuracy: 0.7701 - val_loss: 1.6115 - val_accuracy: 0.4773\n",
            "Epoch 76/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.5137 - accuracy: 0.7701 - val_loss: 1.6188 - val_accuracy: 0.4773\n",
            "Epoch 77/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5051 - accuracy: 0.8046 - val_loss: 1.6357 - val_accuracy: 0.5000\n",
            "Epoch 78/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4994 - accuracy: 0.7931 - val_loss: 1.6642 - val_accuracy: 0.4773\n",
            "Epoch 79/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.5044 - accuracy: 0.7816 - val_loss: 1.6723 - val_accuracy: 0.5000\n",
            "Epoch 80/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4819 - accuracy: 0.7816 - val_loss: 1.6739 - val_accuracy: 0.5000\n",
            "Epoch 81/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4886 - accuracy: 0.8046 - val_loss: 1.6700 - val_accuracy: 0.5227\n",
            "Epoch 82/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.4751 - accuracy: 0.8161 - val_loss: 1.6775 - val_accuracy: 0.5000\n",
            "Epoch 83/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4763 - accuracy: 0.8276 - val_loss: 1.7027 - val_accuracy: 0.5455\n",
            "Epoch 84/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.4526 - accuracy: 0.8276 - val_loss: 1.7250 - val_accuracy: 0.5000\n",
            "Epoch 85/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.4712 - accuracy: 0.8046 - val_loss: 1.7559 - val_accuracy: 0.5455\n",
            "Epoch 86/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4616 - accuracy: 0.8161 - val_loss: 1.7661 - val_accuracy: 0.4545\n",
            "Epoch 87/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.4493 - accuracy: 0.8276 - val_loss: 1.7515 - val_accuracy: 0.5000\n",
            "Epoch 88/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4468 - accuracy: 0.8391 - val_loss: 1.7778 - val_accuracy: 0.5000\n",
            "Epoch 89/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.4330 - accuracy: 0.8506 - val_loss: 1.7835 - val_accuracy: 0.5227\n",
            "Epoch 90/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4291 - accuracy: 0.8391 - val_loss: 1.7865 - val_accuracy: 0.5227\n",
            "Epoch 91/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4215 - accuracy: 0.8621 - val_loss: 1.8119 - val_accuracy: 0.5000\n",
            "Epoch 92/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4223 - accuracy: 0.8621 - val_loss: 1.8451 - val_accuracy: 0.4773\n",
            "Epoch 93/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4039 - accuracy: 0.8621 - val_loss: 1.8638 - val_accuracy: 0.5227\n",
            "Epoch 94/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.4071 - accuracy: 0.8621 - val_loss: 1.8728 - val_accuracy: 0.5227\n",
            "Epoch 95/300\n",
            "87/87 [==============================] - 0s 304us/step - loss: 0.3942 - accuracy: 0.8966 - val_loss: 1.8934 - val_accuracy: 0.5000\n",
            "Epoch 96/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.4012 - accuracy: 0.8851 - val_loss: 1.9223 - val_accuracy: 0.5000\n",
            "Epoch 97/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3886 - accuracy: 0.8851 - val_loss: 1.9147 - val_accuracy: 0.5000\n",
            "Epoch 98/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3836 - accuracy: 0.8736 - val_loss: 1.9970 - val_accuracy: 0.4773\n",
            "Epoch 99/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3729 - accuracy: 0.8966 - val_loss: 2.0033 - val_accuracy: 0.5000\n",
            "Epoch 100/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.3753 - accuracy: 0.8736 - val_loss: 2.0055 - val_accuracy: 0.5000\n",
            "Epoch 101/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.3725 - accuracy: 0.8851 - val_loss: 2.0154 - val_accuracy: 0.5000\n",
            "Epoch 102/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.3648 - accuracy: 0.8851 - val_loss: 2.0473 - val_accuracy: 0.5000\n",
            "Epoch 103/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3558 - accuracy: 0.9080 - val_loss: 2.1016 - val_accuracy: 0.5000\n",
            "Epoch 104/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3462 - accuracy: 0.9080 - val_loss: 2.1011 - val_accuracy: 0.5000\n",
            "Epoch 105/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3473 - accuracy: 0.9310 - val_loss: 2.1354 - val_accuracy: 0.5227\n",
            "Epoch 106/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3346 - accuracy: 0.9080 - val_loss: 2.1750 - val_accuracy: 0.5000\n",
            "Epoch 107/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3365 - accuracy: 0.9195 - val_loss: 2.2014 - val_accuracy: 0.5000\n",
            "Epoch 108/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3297 - accuracy: 0.9195 - val_loss: 2.2066 - val_accuracy: 0.5000\n",
            "Epoch 109/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.3342 - accuracy: 0.9310 - val_loss: 2.2089 - val_accuracy: 0.5000\n",
            "Epoch 110/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.3303 - accuracy: 0.8851 - val_loss: 2.2660 - val_accuracy: 0.5227\n",
            "Epoch 111/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.3257 - accuracy: 0.9080 - val_loss: 2.2431 - val_accuracy: 0.5227\n",
            "Epoch 112/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3161 - accuracy: 0.9080 - val_loss: 2.2828 - val_accuracy: 0.5000\n",
            "Epoch 113/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3219 - accuracy: 0.9195 - val_loss: 2.3599 - val_accuracy: 0.5000\n",
            "Epoch 114/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3144 - accuracy: 0.9195 - val_loss: 2.3153 - val_accuracy: 0.5227\n",
            "Epoch 115/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.2998 - accuracy: 0.9425 - val_loss: 2.3549 - val_accuracy: 0.4773\n",
            "Epoch 116/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.2944 - accuracy: 0.9310 - val_loss: 2.3762 - val_accuracy: 0.5000\n",
            "Epoch 117/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.3033 - accuracy: 0.9080 - val_loss: 2.3945 - val_accuracy: 0.5227\n",
            "Epoch 118/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.2850 - accuracy: 0.9310 - val_loss: 2.3775 - val_accuracy: 0.5227\n",
            "Epoch 119/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2791 - accuracy: 0.9195 - val_loss: 2.4605 - val_accuracy: 0.5227\n",
            "Epoch 120/300\n",
            "87/87 [==============================] - 0s 322us/step - loss: 0.3014 - accuracy: 0.9195 - val_loss: 2.5307 - val_accuracy: 0.5000\n",
            "Epoch 121/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2719 - accuracy: 0.9195 - val_loss: 2.4780 - val_accuracy: 0.5227\n",
            "Epoch 122/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.2860 - accuracy: 0.9080 - val_loss: 2.5106 - val_accuracy: 0.5227\n",
            "Epoch 123/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2780 - accuracy: 0.9425 - val_loss: 2.5125 - val_accuracy: 0.5227\n",
            "Epoch 124/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.2644 - accuracy: 0.9310 - val_loss: 2.5753 - val_accuracy: 0.5000\n",
            "Epoch 125/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2581 - accuracy: 0.9310 - val_loss: 2.5929 - val_accuracy: 0.5227\n",
            "Epoch 126/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2531 - accuracy: 0.9425 - val_loss: 2.5855 - val_accuracy: 0.5227\n",
            "Epoch 127/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.2517 - accuracy: 0.9425 - val_loss: 2.5671 - val_accuracy: 0.4773\n",
            "Epoch 128/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.2534 - accuracy: 0.9425 - val_loss: 2.6557 - val_accuracy: 0.4773\n",
            "Epoch 129/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.2491 - accuracy: 0.9195 - val_loss: 2.6947 - val_accuracy: 0.5227\n",
            "Epoch 130/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.2663 - accuracy: 0.9195 - val_loss: 2.6698 - val_accuracy: 0.5227\n",
            "Epoch 131/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.2349 - accuracy: 0.9425 - val_loss: 2.7285 - val_accuracy: 0.5000\n",
            "Epoch 132/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.2371 - accuracy: 0.9425 - val_loss: 2.7162 - val_accuracy: 0.5227\n",
            "Epoch 133/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2315 - accuracy: 0.9310 - val_loss: 2.7659 - val_accuracy: 0.5000\n",
            "Epoch 134/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2317 - accuracy: 0.9310 - val_loss: 2.8001 - val_accuracy: 0.5000\n",
            "Epoch 135/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2216 - accuracy: 0.9540 - val_loss: 2.7829 - val_accuracy: 0.5000\n",
            "Epoch 136/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.2235 - accuracy: 0.9540 - val_loss: 2.8459 - val_accuracy: 0.5000\n",
            "Epoch 137/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2180 - accuracy: 0.9425 - val_loss: 2.8445 - val_accuracy: 0.5227\n",
            "Epoch 138/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.2160 - accuracy: 0.9540 - val_loss: 2.8627 - val_accuracy: 0.4773\n",
            "Epoch 139/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.2124 - accuracy: 0.9540 - val_loss: 2.9234 - val_accuracy: 0.5000\n",
            "Epoch 140/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.2247 - accuracy: 0.9310 - val_loss: 2.9527 - val_accuracy: 0.5000\n",
            "Epoch 141/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.2247 - accuracy: 0.9540 - val_loss: 2.9050 - val_accuracy: 0.4773\n",
            "Epoch 142/300\n",
            "87/87 [==============================] - 0s 286us/step - loss: 0.2055 - accuracy: 0.9540 - val_loss: 2.9721 - val_accuracy: 0.5000\n",
            "Epoch 143/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.2006 - accuracy: 0.9540 - val_loss: 2.9985 - val_accuracy: 0.5000\n",
            "Epoch 144/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.2045 - accuracy: 0.9540 - val_loss: 3.0627 - val_accuracy: 0.5000\n",
            "Epoch 145/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1997 - accuracy: 0.9655 - val_loss: 3.0735 - val_accuracy: 0.5000\n",
            "Epoch 146/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.1896 - accuracy: 0.9770 - val_loss: 3.1166 - val_accuracy: 0.5000\n",
            "Epoch 147/300\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.1900 - accuracy: 0.9770 - val_loss: 3.0886 - val_accuracy: 0.5000\n",
            "Epoch 148/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.1927 - accuracy: 0.9655 - val_loss: 3.1109 - val_accuracy: 0.5000\n",
            "Epoch 149/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1893 - accuracy: 0.9540 - val_loss: 3.1666 - val_accuracy: 0.4773\n",
            "Epoch 150/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1875 - accuracy: 0.9540 - val_loss: 3.1823 - val_accuracy: 0.5000\n",
            "Epoch 151/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.1918 - accuracy: 0.9655 - val_loss: 3.2233 - val_accuracy: 0.5000\n",
            "Epoch 152/300\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.1866 - accuracy: 0.9655 - val_loss: 3.2061 - val_accuracy: 0.5000\n",
            "Epoch 153/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1786 - accuracy: 0.9770 - val_loss: 3.2433 - val_accuracy: 0.5000\n",
            "Epoch 154/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.1809 - accuracy: 0.9770 - val_loss: 3.2641 - val_accuracy: 0.5000\n",
            "Epoch 155/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1714 - accuracy: 0.9770 - val_loss: 3.2761 - val_accuracy: 0.5000\n",
            "Epoch 156/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1723 - accuracy: 0.9770 - val_loss: 3.3298 - val_accuracy: 0.5000\n",
            "Epoch 157/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1614 - accuracy: 0.9770 - val_loss: 3.2988 - val_accuracy: 0.5000\n",
            "Epoch 158/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.1634 - accuracy: 0.9655 - val_loss: 3.3210 - val_accuracy: 0.5000\n",
            "Epoch 159/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1615 - accuracy: 0.9770 - val_loss: 3.3698 - val_accuracy: 0.5000\n",
            "Epoch 160/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1652 - accuracy: 0.9655 - val_loss: 3.3762 - val_accuracy: 0.4773\n",
            "Epoch 161/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.1553 - accuracy: 0.9770 - val_loss: 3.4538 - val_accuracy: 0.5000\n",
            "Epoch 162/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1625 - accuracy: 0.9540 - val_loss: 3.4443 - val_accuracy: 0.4773\n",
            "Epoch 163/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.1627 - accuracy: 0.9655 - val_loss: 3.4623 - val_accuracy: 0.5000\n",
            "Epoch 164/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1472 - accuracy: 0.9770 - val_loss: 3.4626 - val_accuracy: 0.4773\n",
            "Epoch 165/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1583 - accuracy: 0.9655 - val_loss: 3.4973 - val_accuracy: 0.4773\n",
            "Epoch 166/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1494 - accuracy: 0.9770 - val_loss: 3.5359 - val_accuracy: 0.4773\n",
            "Epoch 167/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1544 - accuracy: 0.9655 - val_loss: 3.5303 - val_accuracy: 0.5000\n",
            "Epoch 168/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1477 - accuracy: 0.9770 - val_loss: 3.5622 - val_accuracy: 0.4545\n",
            "Epoch 169/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1453 - accuracy: 0.9770 - val_loss: 3.6132 - val_accuracy: 0.4773\n",
            "Epoch 170/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.1451 - accuracy: 0.9770 - val_loss: 3.5824 - val_accuracy: 0.4545\n",
            "Epoch 171/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.1429 - accuracy: 0.9655 - val_loss: 3.6400 - val_accuracy: 0.4773\n",
            "Epoch 172/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1351 - accuracy: 0.9770 - val_loss: 3.6410 - val_accuracy: 0.4545\n",
            "Epoch 173/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1523 - accuracy: 0.9655 - val_loss: 3.6629 - val_accuracy: 0.4773\n",
            "Epoch 174/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.1510 - accuracy: 0.9655 - val_loss: 3.6813 - val_accuracy: 0.4545\n",
            "Epoch 175/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.1481 - accuracy: 0.9655 - val_loss: 3.7335 - val_accuracy: 0.4545\n",
            "Epoch 176/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1300 - accuracy: 0.9655 - val_loss: 3.7304 - val_accuracy: 0.4773\n",
            "Epoch 177/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.1363 - accuracy: 0.9655 - val_loss: 3.7959 - val_accuracy: 0.4773\n",
            "Epoch 178/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.1371 - accuracy: 0.9770 - val_loss: 3.8522 - val_accuracy: 0.4773\n",
            "Epoch 179/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.1228 - accuracy: 0.9770 - val_loss: 3.8073 - val_accuracy: 0.4545\n",
            "Epoch 180/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1335 - accuracy: 0.9655 - val_loss: 3.8369 - val_accuracy: 0.4545\n",
            "Epoch 181/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.1343 - accuracy: 0.9655 - val_loss: 3.8573 - val_accuracy: 0.4545\n",
            "Epoch 182/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1287 - accuracy: 0.9655 - val_loss: 3.8660 - val_accuracy: 0.4545\n",
            "Epoch 183/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.1125 - accuracy: 0.9770 - val_loss: 3.8795 - val_accuracy: 0.4773\n",
            "Epoch 184/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1136 - accuracy: 0.9770 - val_loss: 3.9053 - val_accuracy: 0.4545\n",
            "Epoch 185/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1097 - accuracy: 0.9770 - val_loss: 3.9416 - val_accuracy: 0.4545\n",
            "Epoch 186/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.1080 - accuracy: 0.9770 - val_loss: 3.9553 - val_accuracy: 0.4545\n",
            "Epoch 187/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.1102 - accuracy: 0.9770 - val_loss: 3.9429 - val_accuracy: 0.4545\n",
            "Epoch 188/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.1083 - accuracy: 0.9770 - val_loss: 3.9984 - val_accuracy: 0.4545\n",
            "Epoch 189/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.1061 - accuracy: 0.9770 - val_loss: 4.0038 - val_accuracy: 0.4545\n",
            "Epoch 190/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.1044 - accuracy: 0.9770 - val_loss: 4.0036 - val_accuracy: 0.4545\n",
            "Epoch 191/300\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.1019 - accuracy: 0.9770 - val_loss: 4.0654 - val_accuracy: 0.4545\n",
            "Epoch 192/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.1062 - accuracy: 0.9770 - val_loss: 4.0905 - val_accuracy: 0.4545\n",
            "Epoch 193/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0992 - accuracy: 0.9770 - val_loss: 4.0718 - val_accuracy: 0.4545\n",
            "Epoch 194/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0985 - accuracy: 0.9770 - val_loss: 4.1097 - val_accuracy: 0.4545\n",
            "Epoch 195/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0956 - accuracy: 0.9770 - val_loss: 4.1076 - val_accuracy: 0.4545\n",
            "Epoch 196/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0956 - accuracy: 0.9770 - val_loss: 4.1366 - val_accuracy: 0.4545\n",
            "Epoch 197/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.0950 - accuracy: 0.9770 - val_loss: 4.1769 - val_accuracy: 0.4773\n",
            "Epoch 198/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0960 - accuracy: 0.9770 - val_loss: 4.1993 - val_accuracy: 0.4545\n",
            "Epoch 199/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0899 - accuracy: 0.9770 - val_loss: 4.1894 - val_accuracy: 0.4545\n",
            "Epoch 200/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0919 - accuracy: 0.9770 - val_loss: 4.2331 - val_accuracy: 0.4545\n",
            "Epoch 201/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0887 - accuracy: 0.9770 - val_loss: 4.2551 - val_accuracy: 0.4545\n",
            "Epoch 202/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0882 - accuracy: 0.9770 - val_loss: 4.2676 - val_accuracy: 0.4545\n",
            "Epoch 203/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0898 - accuracy: 0.9770 - val_loss: 4.2898 - val_accuracy: 0.4545\n",
            "Epoch 204/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0999 - accuracy: 0.9655 - val_loss: 4.3442 - val_accuracy: 0.4545\n",
            "Epoch 205/300\n",
            "87/87 [==============================] - 0s 322us/step - loss: 0.0924 - accuracy: 0.9885 - val_loss: 4.4018 - val_accuracy: 0.4545\n",
            "Epoch 206/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.0865 - accuracy: 0.9770 - val_loss: 4.3920 - val_accuracy: 0.4545\n",
            "Epoch 207/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0860 - accuracy: 0.9770 - val_loss: 4.3629 - val_accuracy: 0.4545\n",
            "Epoch 208/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.0808 - accuracy: 0.9770 - val_loss: 4.4108 - val_accuracy: 0.4545\n",
            "Epoch 209/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0813 - accuracy: 0.9770 - val_loss: 4.4528 - val_accuracy: 0.4545\n",
            "Epoch 210/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0825 - accuracy: 0.9770 - val_loss: 4.4401 - val_accuracy: 0.4545\n",
            "Epoch 211/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.0794 - accuracy: 0.9885 - val_loss: 4.4169 - val_accuracy: 0.4545\n",
            "Epoch 212/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.0769 - accuracy: 0.9770 - val_loss: 4.4720 - val_accuracy: 0.4545\n",
            "Epoch 213/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.0774 - accuracy: 0.9770 - val_loss: 4.4984 - val_accuracy: 0.4545\n",
            "Epoch 214/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.0750 - accuracy: 0.9770 - val_loss: 4.4952 - val_accuracy: 0.4545\n",
            "Epoch 215/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0736 - accuracy: 0.9770 - val_loss: 4.5299 - val_accuracy: 0.4773\n",
            "Epoch 216/300\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.0727 - accuracy: 0.9770 - val_loss: 4.5554 - val_accuracy: 0.4773\n",
            "Epoch 217/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0730 - accuracy: 0.9885 - val_loss: 4.5648 - val_accuracy: 0.4545\n",
            "Epoch 218/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0705 - accuracy: 0.9770 - val_loss: 4.5640 - val_accuracy: 0.4545\n",
            "Epoch 219/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.0699 - accuracy: 0.9885 - val_loss: 4.5923 - val_accuracy: 0.4545\n",
            "Epoch 220/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0698 - accuracy: 0.9885 - val_loss: 4.6150 - val_accuracy: 0.4773\n",
            "Epoch 221/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0668 - accuracy: 0.9885 - val_loss: 4.6324 - val_accuracy: 0.4545\n",
            "Epoch 222/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0669 - accuracy: 0.9770 - val_loss: 4.6650 - val_accuracy: 0.4545\n",
            "Epoch 223/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.0655 - accuracy: 0.9770 - val_loss: 4.6655 - val_accuracy: 0.4545\n",
            "Epoch 224/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.0651 - accuracy: 0.9770 - val_loss: 4.6799 - val_accuracy: 0.4545\n",
            "Epoch 225/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0652 - accuracy: 0.9885 - val_loss: 4.6945 - val_accuracy: 0.4545\n",
            "Epoch 226/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0634 - accuracy: 0.9885 - val_loss: 4.7117 - val_accuracy: 0.4545\n",
            "Epoch 227/300\n",
            "87/87 [==============================] - 0s 300us/step - loss: 0.0644 - accuracy: 0.9770 - val_loss: 4.7219 - val_accuracy: 0.4545\n",
            "Epoch 228/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0615 - accuracy: 0.9885 - val_loss: 4.7472 - val_accuracy: 0.4545\n",
            "Epoch 229/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0616 - accuracy: 0.9885 - val_loss: 4.7740 - val_accuracy: 0.4545\n",
            "Epoch 230/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.0607 - accuracy: 0.9885 - val_loss: 4.8062 - val_accuracy: 0.4545\n",
            "Epoch 231/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0608 - accuracy: 0.9885 - val_loss: 4.7935 - val_accuracy: 0.4545\n",
            "Epoch 232/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0595 - accuracy: 0.9885 - val_loss: 4.7981 - val_accuracy: 0.4545\n",
            "Epoch 233/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.0588 - accuracy: 0.9885 - val_loss: 4.8197 - val_accuracy: 0.4545\n",
            "Epoch 234/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0593 - accuracy: 0.9885 - val_loss: 4.8508 - val_accuracy: 0.4545\n",
            "Epoch 235/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0585 - accuracy: 0.9770 - val_loss: 4.8719 - val_accuracy: 0.4545\n",
            "Epoch 236/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0578 - accuracy: 0.9885 - val_loss: 4.8843 - val_accuracy: 0.4545\n",
            "Epoch 237/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.0560 - accuracy: 0.9885 - val_loss: 4.8969 - val_accuracy: 0.4545\n",
            "Epoch 238/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0564 - accuracy: 0.9885 - val_loss: 4.9206 - val_accuracy: 0.4545\n",
            "Epoch 239/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.0553 - accuracy: 0.9885 - val_loss: 4.9383 - val_accuracy: 0.4545\n",
            "Epoch 240/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0572 - accuracy: 0.9885 - val_loss: 4.9508 - val_accuracy: 0.4545\n",
            "Epoch 241/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0566 - accuracy: 0.9885 - val_loss: 4.9572 - val_accuracy: 0.4545\n",
            "Epoch 242/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0535 - accuracy: 0.9885 - val_loss: 4.9801 - val_accuracy: 0.4545\n",
            "Epoch 243/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0526 - accuracy: 0.9885 - val_loss: 5.0024 - val_accuracy: 0.4545\n",
            "Epoch 244/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0525 - accuracy: 0.9885 - val_loss: 5.0121 - val_accuracy: 0.4545\n",
            "Epoch 245/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.0537 - accuracy: 0.9885 - val_loss: 5.0498 - val_accuracy: 0.4545\n",
            "Epoch 246/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.0519 - accuracy: 0.9885 - val_loss: 5.0424 - val_accuracy: 0.4545\n",
            "Epoch 247/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0510 - accuracy: 0.9885 - val_loss: 5.0403 - val_accuracy: 0.4545\n",
            "Epoch 248/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0501 - accuracy: 0.9885 - val_loss: 5.0545 - val_accuracy: 0.4545\n",
            "Epoch 249/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0505 - accuracy: 0.9885 - val_loss: 5.0746 - val_accuracy: 0.4545\n",
            "Epoch 250/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.0498 - accuracy: 0.9885 - val_loss: 5.1155 - val_accuracy: 0.4545\n",
            "Epoch 251/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.0505 - accuracy: 0.9885 - val_loss: 5.1211 - val_accuracy: 0.4545\n",
            "Epoch 252/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0478 - accuracy: 0.9885 - val_loss: 5.1181 - val_accuracy: 0.4545\n",
            "Epoch 253/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.0476 - accuracy: 0.9885 - val_loss: 5.1486 - val_accuracy: 0.4545\n",
            "Epoch 254/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0473 - accuracy: 0.9885 - val_loss: 5.1530 - val_accuracy: 0.4545\n",
            "Epoch 255/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0469 - accuracy: 0.9885 - val_loss: 5.1823 - val_accuracy: 0.4545\n",
            "Epoch 256/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0481 - accuracy: 0.9885 - val_loss: 5.1944 - val_accuracy: 0.4545\n",
            "Epoch 257/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0453 - accuracy: 0.9885 - val_loss: 5.1964 - val_accuracy: 0.4545\n",
            "Epoch 258/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0475 - accuracy: 0.9885 - val_loss: 5.2044 - val_accuracy: 0.4545\n",
            "Epoch 259/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.0452 - accuracy: 0.9885 - val_loss: 5.2151 - val_accuracy: 0.4545\n",
            "Epoch 260/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0450 - accuracy: 0.9885 - val_loss: 5.2504 - val_accuracy: 0.4545\n",
            "Epoch 261/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0439 - accuracy: 0.9885 - val_loss: 5.2651 - val_accuracy: 0.4545\n",
            "Epoch 262/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.0439 - accuracy: 0.9885 - val_loss: 5.2795 - val_accuracy: 0.4545\n",
            "Epoch 263/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0437 - accuracy: 0.9885 - val_loss: 5.2988 - val_accuracy: 0.4545\n",
            "Epoch 264/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0430 - accuracy: 0.9885 - val_loss: 5.3197 - val_accuracy: 0.4545\n",
            "Epoch 265/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.0422 - accuracy: 0.9885 - val_loss: 5.3222 - val_accuracy: 0.4545\n",
            "Epoch 266/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.0421 - accuracy: 0.9885 - val_loss: 5.3434 - val_accuracy: 0.4545\n",
            "Epoch 267/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0423 - accuracy: 0.9885 - val_loss: 5.3465 - val_accuracy: 0.4545\n",
            "Epoch 268/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.0426 - accuracy: 0.9885 - val_loss: 5.3621 - val_accuracy: 0.4545\n",
            "Epoch 269/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0427 - accuracy: 0.9885 - val_loss: 5.3773 - val_accuracy: 0.4545\n",
            "Epoch 270/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0402 - accuracy: 0.9885 - val_loss: 5.3954 - val_accuracy: 0.4545\n",
            "Epoch 271/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0411 - accuracy: 0.9885 - val_loss: 5.4221 - val_accuracy: 0.4545\n",
            "Epoch 272/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 5.4304 - val_accuracy: 0.4545\n",
            "Epoch 273/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0399 - accuracy: 0.9885 - val_loss: 5.4255 - val_accuracy: 0.4545\n",
            "Epoch 274/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0394 - accuracy: 0.9885 - val_loss: 5.4508 - val_accuracy: 0.4545\n",
            "Epoch 275/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0388 - accuracy: 0.9885 - val_loss: 5.4618 - val_accuracy: 0.4545\n",
            "Epoch 276/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 5.4679 - val_accuracy: 0.4545\n",
            "Epoch 277/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 5.4870 - val_accuracy: 0.4545\n",
            "Epoch 278/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 5.5006 - val_accuracy: 0.4545\n",
            "Epoch 279/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.0374 - accuracy: 1.0000 - val_loss: 5.5158 - val_accuracy: 0.4545\n",
            "Epoch 280/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 5.5202 - val_accuracy: 0.4545\n",
            "Epoch 281/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0364 - accuracy: 1.0000 - val_loss: 5.5369 - val_accuracy: 0.4545\n",
            "Epoch 282/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 5.5468 - val_accuracy: 0.4545\n",
            "Epoch 283/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0357 - accuracy: 1.0000 - val_loss: 5.5676 - val_accuracy: 0.4545\n",
            "Epoch 284/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 5.5766 - val_accuracy: 0.4545\n",
            "Epoch 285/300\n",
            "87/87 [==============================] - 0s 336us/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 5.5983 - val_accuracy: 0.4545\n",
            "Epoch 286/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 5.6035 - val_accuracy: 0.4545\n",
            "Epoch 287/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 5.6167 - val_accuracy: 0.4545\n",
            "Epoch 288/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0351 - accuracy: 1.0000 - val_loss: 5.6254 - val_accuracy: 0.4545\n",
            "Epoch 289/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 5.6378 - val_accuracy: 0.4545\n",
            "Epoch 290/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 5.6586 - val_accuracy: 0.4545\n",
            "Epoch 291/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0336 - accuracy: 1.0000 - val_loss: 5.6650 - val_accuracy: 0.4545\n",
            "Epoch 292/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0338 - accuracy: 1.0000 - val_loss: 5.6843 - val_accuracy: 0.4545\n",
            "Epoch 293/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0332 - accuracy: 1.0000 - val_loss: 5.7086 - val_accuracy: 0.4545\n",
            "Epoch 294/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 5.7046 - val_accuracy: 0.4545\n",
            "Epoch 295/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 5.7204 - val_accuracy: 0.4545\n",
            "Epoch 296/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.0321 - accuracy: 1.0000 - val_loss: 5.7216 - val_accuracy: 0.4545\n",
            "Epoch 297/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.0319 - accuracy: 1.0000 - val_loss: 5.7490 - val_accuracy: 0.4545\n",
            "Epoch 298/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 5.7560 - val_accuracy: 0.4545\n",
            "Epoch 299/300\n",
            "87/87 [==============================] - 0s 352us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 5.7684 - val_accuracy: 0.4545\n",
            "Epoch 300/300\n",
            "87/87 [==============================] - 0s 358us/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 5.7760 - val_accuracy: 0.4545\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/300\n",
            "87/87 [==============================] - 0s 964us/step - loss: 1.1505 - accuracy: 0.3103 - val_loss: 1.0668 - val_accuracy: 0.4091\n",
            "Epoch 2/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.0547 - accuracy: 0.4023 - val_loss: 1.0277 - val_accuracy: 0.4091\n",
            "Epoch 3/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.9962 - accuracy: 0.4598 - val_loss: 1.0017 - val_accuracy: 0.4091\n",
            "Epoch 4/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.9717 - accuracy: 0.5057 - val_loss: 0.9794 - val_accuracy: 0.4545\n",
            "Epoch 5/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.9431 - accuracy: 0.5517 - val_loss: 0.9700 - val_accuracy: 0.4091\n",
            "Epoch 6/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9307 - accuracy: 0.5287 - val_loss: 0.9618 - val_accuracy: 0.5000\n",
            "Epoch 7/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9137 - accuracy: 0.5747 - val_loss: 0.9520 - val_accuracy: 0.5227\n",
            "Epoch 8/300\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.9061 - accuracy: 0.5632 - val_loss: 0.9444 - val_accuracy: 0.5227\n",
            "Epoch 9/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8953 - accuracy: 0.5517 - val_loss: 0.9531 - val_accuracy: 0.5227\n",
            "Epoch 10/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8861 - accuracy: 0.5517 - val_loss: 0.9467 - val_accuracy: 0.5455\n",
            "Epoch 11/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8781 - accuracy: 0.5517 - val_loss: 0.9467 - val_accuracy: 0.5455\n",
            "Epoch 12/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8692 - accuracy: 0.5402 - val_loss: 0.9442 - val_accuracy: 0.5455\n",
            "Epoch 13/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8617 - accuracy: 0.5632 - val_loss: 0.9473 - val_accuracy: 0.5000\n",
            "Epoch 14/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8537 - accuracy: 0.5862 - val_loss: 0.9413 - val_accuracy: 0.5455\n",
            "Epoch 15/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8426 - accuracy: 0.5862 - val_loss: 0.9511 - val_accuracy: 0.4773\n",
            "Epoch 16/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8416 - accuracy: 0.5747 - val_loss: 0.9610 - val_accuracy: 0.5000\n",
            "Epoch 17/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8254 - accuracy: 0.5862 - val_loss: 0.9501 - val_accuracy: 0.5000\n",
            "Epoch 18/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8215 - accuracy: 0.6092 - val_loss: 0.9535 - val_accuracy: 0.5000\n",
            "Epoch 19/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8133 - accuracy: 0.6322 - val_loss: 0.9688 - val_accuracy: 0.4773\n",
            "Epoch 20/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8107 - accuracy: 0.6322 - val_loss: 0.9505 - val_accuracy: 0.5000\n",
            "Epoch 21/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7995 - accuracy: 0.6552 - val_loss: 0.9681 - val_accuracy: 0.5000\n",
            "Epoch 22/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7927 - accuracy: 0.6322 - val_loss: 0.9852 - val_accuracy: 0.4545\n",
            "Epoch 23/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7837 - accuracy: 0.6437 - val_loss: 0.9680 - val_accuracy: 0.5227\n",
            "Epoch 24/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.7809 - accuracy: 0.6552 - val_loss: 0.9646 - val_accuracy: 0.5000\n",
            "Epoch 25/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7703 - accuracy: 0.6667 - val_loss: 0.9738 - val_accuracy: 0.5000\n",
            "Epoch 26/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.7594 - accuracy: 0.6552 - val_loss: 0.9818 - val_accuracy: 0.4773\n",
            "Epoch 27/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.7514 - accuracy: 0.6552 - val_loss: 0.9868 - val_accuracy: 0.4773\n",
            "Epoch 28/300\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.7456 - accuracy: 0.6897 - val_loss: 0.9759 - val_accuracy: 0.4773\n",
            "Epoch 29/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.7340 - accuracy: 0.6897 - val_loss: 0.9858 - val_accuracy: 0.5000\n",
            "Epoch 30/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7254 - accuracy: 0.6897 - val_loss: 0.9868 - val_accuracy: 0.5000\n",
            "Epoch 31/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7276 - accuracy: 0.6897 - val_loss: 0.9810 - val_accuracy: 0.5000\n",
            "Epoch 32/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.7140 - accuracy: 0.6782 - val_loss: 1.0142 - val_accuracy: 0.5000\n",
            "Epoch 33/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.7089 - accuracy: 0.7126 - val_loss: 0.9943 - val_accuracy: 0.5000\n",
            "Epoch 34/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.6986 - accuracy: 0.7011 - val_loss: 0.9796 - val_accuracy: 0.5000\n",
            "Epoch 35/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.6837 - accuracy: 0.7241 - val_loss: 0.9983 - val_accuracy: 0.5000\n",
            "Epoch 36/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.6766 - accuracy: 0.7126 - val_loss: 1.0080 - val_accuracy: 0.5000\n",
            "Epoch 37/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.6695 - accuracy: 0.7126 - val_loss: 1.0019 - val_accuracy: 0.5000\n",
            "Epoch 38/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.6594 - accuracy: 0.7356 - val_loss: 0.9869 - val_accuracy: 0.5000\n",
            "Epoch 39/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.6615 - accuracy: 0.7126 - val_loss: 1.0210 - val_accuracy: 0.5000\n",
            "Epoch 40/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6443 - accuracy: 0.7126 - val_loss: 0.9842 - val_accuracy: 0.5227\n",
            "Epoch 41/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6355 - accuracy: 0.7471 - val_loss: 1.0002 - val_accuracy: 0.5000\n",
            "Epoch 42/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.6253 - accuracy: 0.7356 - val_loss: 1.0232 - val_accuracy: 0.5227\n",
            "Epoch 43/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.6187 - accuracy: 0.7471 - val_loss: 0.9980 - val_accuracy: 0.5000\n",
            "Epoch 44/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.6105 - accuracy: 0.7471 - val_loss: 1.0147 - val_accuracy: 0.5000\n",
            "Epoch 45/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6030 - accuracy: 0.7701 - val_loss: 0.9878 - val_accuracy: 0.5227\n",
            "Epoch 46/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5950 - accuracy: 0.7816 - val_loss: 1.0351 - val_accuracy: 0.5000\n",
            "Epoch 47/300\n",
            "87/87 [==============================] - 0s 328us/step - loss: 0.5902 - accuracy: 0.7471 - val_loss: 1.0598 - val_accuracy: 0.5227\n",
            "Epoch 48/300\n",
            "87/87 [==============================] - 0s 345us/step - loss: 0.5737 - accuracy: 0.7931 - val_loss: 1.0069 - val_accuracy: 0.5227\n",
            "Epoch 49/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.5814 - accuracy: 0.7931 - val_loss: 1.0048 - val_accuracy: 0.5227\n",
            "Epoch 50/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.5595 - accuracy: 0.7931 - val_loss: 1.0390 - val_accuracy: 0.5227\n",
            "Epoch 51/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5550 - accuracy: 0.7701 - val_loss: 1.0124 - val_accuracy: 0.5227\n",
            "Epoch 52/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.5400 - accuracy: 0.8161 - val_loss: 1.0603 - val_accuracy: 0.5000\n",
            "Epoch 53/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.5467 - accuracy: 0.7931 - val_loss: 1.0269 - val_accuracy: 0.5227\n",
            "Epoch 54/300\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5295 - accuracy: 0.8161 - val_loss: 1.0740 - val_accuracy: 0.5000\n",
            "Epoch 55/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.5344 - accuracy: 0.8276 - val_loss: 1.0077 - val_accuracy: 0.5000\n",
            "Epoch 56/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.5263 - accuracy: 0.8046 - val_loss: 1.0979 - val_accuracy: 0.5000\n",
            "Epoch 57/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4973 - accuracy: 0.8276 - val_loss: 1.0633 - val_accuracy: 0.5227\n",
            "Epoch 58/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4959 - accuracy: 0.8276 - val_loss: 1.0398 - val_accuracy: 0.5000\n",
            "Epoch 59/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4870 - accuracy: 0.8391 - val_loss: 1.0877 - val_accuracy: 0.5000\n",
            "Epoch 60/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.4792 - accuracy: 0.8276 - val_loss: 1.0734 - val_accuracy: 0.5227\n",
            "Epoch 61/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4711 - accuracy: 0.8391 - val_loss: 1.0913 - val_accuracy: 0.5000\n",
            "Epoch 62/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.4660 - accuracy: 0.8506 - val_loss: 1.0776 - val_accuracy: 0.4773\n",
            "Epoch 63/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4549 - accuracy: 0.8506 - val_loss: 1.0931 - val_accuracy: 0.5000\n",
            "Epoch 64/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.4482 - accuracy: 0.8391 - val_loss: 1.1079 - val_accuracy: 0.5000\n",
            "Epoch 65/300\n",
            "87/87 [==============================] - 0s 332us/step - loss: 0.4426 - accuracy: 0.8391 - val_loss: 1.1101 - val_accuracy: 0.4773\n",
            "Epoch 66/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4399 - accuracy: 0.8966 - val_loss: 1.1432 - val_accuracy: 0.4773\n",
            "Epoch 67/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4280 - accuracy: 0.8736 - val_loss: 1.1197 - val_accuracy: 0.5000\n",
            "Epoch 68/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4341 - accuracy: 0.8391 - val_loss: 1.0941 - val_accuracy: 0.4773\n",
            "Epoch 69/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4222 - accuracy: 0.8736 - val_loss: 1.1629 - val_accuracy: 0.4545\n",
            "Epoch 70/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.4108 - accuracy: 0.8851 - val_loss: 1.1616 - val_accuracy: 0.4545\n",
            "Epoch 71/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.3979 - accuracy: 0.8736 - val_loss: 1.1518 - val_accuracy: 0.4545\n",
            "Epoch 72/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.3986 - accuracy: 0.8506 - val_loss: 1.1429 - val_accuracy: 0.4545\n",
            "Epoch 73/300\n",
            "87/87 [==============================] - 0s 189us/step - loss: 0.3909 - accuracy: 0.8621 - val_loss: 1.1833 - val_accuracy: 0.4773\n",
            "Epoch 74/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3858 - accuracy: 0.8506 - val_loss: 1.1593 - val_accuracy: 0.4545\n",
            "Epoch 75/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3794 - accuracy: 0.8966 - val_loss: 1.1714 - val_accuracy: 0.4545\n",
            "Epoch 76/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3650 - accuracy: 0.8736 - val_loss: 1.1860 - val_accuracy: 0.4545\n",
            "Epoch 77/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.3617 - accuracy: 0.9195 - val_loss: 1.2000 - val_accuracy: 0.4545\n",
            "Epoch 78/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.3616 - accuracy: 0.8966 - val_loss: 1.1987 - val_accuracy: 0.4318\n",
            "Epoch 79/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.3676 - accuracy: 0.9080 - val_loss: 1.1697 - val_accuracy: 0.4545\n",
            "Epoch 80/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.3350 - accuracy: 0.9080 - val_loss: 1.2855 - val_accuracy: 0.4545\n",
            "Epoch 81/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.3490 - accuracy: 0.8966 - val_loss: 1.1999 - val_accuracy: 0.4545\n",
            "Epoch 82/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.3292 - accuracy: 0.8966 - val_loss: 1.2334 - val_accuracy: 0.4545\n",
            "Epoch 83/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3320 - accuracy: 0.9310 - val_loss: 1.2499 - val_accuracy: 0.4318\n",
            "Epoch 84/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.3475 - accuracy: 0.8851 - val_loss: 1.2461 - val_accuracy: 0.4318\n",
            "Epoch 85/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.3229 - accuracy: 0.9195 - val_loss: 1.2639 - val_accuracy: 0.4318\n",
            "Epoch 86/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.3152 - accuracy: 0.8966 - val_loss: 1.2758 - val_accuracy: 0.4318\n",
            "Epoch 87/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.3168 - accuracy: 0.8851 - val_loss: 1.3090 - val_accuracy: 0.4318\n",
            "Epoch 88/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3012 - accuracy: 0.9310 - val_loss: 1.3009 - val_accuracy: 0.4318\n",
            "Epoch 89/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3022 - accuracy: 0.9195 - val_loss: 1.2648 - val_accuracy: 0.4318\n",
            "Epoch 90/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.2897 - accuracy: 0.9425 - val_loss: 1.3175 - val_accuracy: 0.4318\n",
            "Epoch 91/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.2839 - accuracy: 0.9195 - val_loss: 1.3312 - val_accuracy: 0.4318\n",
            "Epoch 92/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.2845 - accuracy: 0.9310 - val_loss: 1.3631 - val_accuracy: 0.4318\n",
            "Epoch 93/300\n",
            "87/87 [==============================] - 0s 309us/step - loss: 0.2876 - accuracy: 0.9080 - val_loss: 1.3217 - val_accuracy: 0.4318\n",
            "Epoch 94/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.2713 - accuracy: 0.9310 - val_loss: 1.3964 - val_accuracy: 0.3864\n",
            "Epoch 95/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.2694 - accuracy: 0.9080 - val_loss: 1.3068 - val_accuracy: 0.4318\n",
            "Epoch 96/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.2714 - accuracy: 0.9425 - val_loss: 1.3367 - val_accuracy: 0.4318\n",
            "Epoch 97/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.2630 - accuracy: 0.9425 - val_loss: 1.3563 - val_accuracy: 0.4318\n",
            "Epoch 98/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.2545 - accuracy: 0.9425 - val_loss: 1.3973 - val_accuracy: 0.4318\n",
            "Epoch 99/300\n",
            "87/87 [==============================] - 0s 301us/step - loss: 0.2487 - accuracy: 0.9425 - val_loss: 1.3931 - val_accuracy: 0.4091\n",
            "Epoch 100/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.2494 - accuracy: 0.9540 - val_loss: 1.4026 - val_accuracy: 0.4091\n",
            "Epoch 101/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.2350 - accuracy: 0.9540 - val_loss: 1.4558 - val_accuracy: 0.4091\n",
            "Epoch 102/300\n",
            "87/87 [==============================] - 0s 354us/step - loss: 0.2360 - accuracy: 0.9425 - val_loss: 1.3986 - val_accuracy: 0.4091\n",
            "Epoch 103/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.2321 - accuracy: 0.9540 - val_loss: 1.4408 - val_accuracy: 0.4091\n",
            "Epoch 104/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.2256 - accuracy: 0.9540 - val_loss: 1.4427 - val_accuracy: 0.4545\n",
            "Epoch 105/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.2244 - accuracy: 0.9540 - val_loss: 1.3869 - val_accuracy: 0.4091\n",
            "Epoch 106/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.2161 - accuracy: 0.9655 - val_loss: 1.4882 - val_accuracy: 0.4091\n",
            "Epoch 107/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.2167 - accuracy: 0.9655 - val_loss: 1.4817 - val_accuracy: 0.4091\n",
            "Epoch 108/300\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.2074 - accuracy: 0.9655 - val_loss: 1.5032 - val_accuracy: 0.4091\n",
            "Epoch 109/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.2110 - accuracy: 0.9425 - val_loss: 1.4835 - val_accuracy: 0.4318\n",
            "Epoch 110/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.2049 - accuracy: 0.9655 - val_loss: 1.5239 - val_accuracy: 0.4318\n",
            "Epoch 111/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.2048 - accuracy: 0.9655 - val_loss: 1.4703 - val_accuracy: 0.4091\n",
            "Epoch 112/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.2015 - accuracy: 0.9655 - val_loss: 1.5778 - val_accuracy: 0.3864\n",
            "Epoch 113/300\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.2025 - accuracy: 0.9655 - val_loss: 1.4598 - val_accuracy: 0.4318\n",
            "Epoch 114/300\n",
            "87/87 [==============================] - 0s 341us/step - loss: 0.2036 - accuracy: 0.9655 - val_loss: 1.5442 - val_accuracy: 0.4545\n",
            "Epoch 115/300\n",
            "87/87 [==============================] - 0s 309us/step - loss: 0.1876 - accuracy: 0.9770 - val_loss: 1.5561 - val_accuracy: 0.4091\n",
            "Epoch 116/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.1774 - accuracy: 0.9655 - val_loss: 1.5747 - val_accuracy: 0.4318\n",
            "Epoch 117/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.1884 - accuracy: 0.9770 - val_loss: 1.4990 - val_accuracy: 0.4318\n",
            "Epoch 118/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1848 - accuracy: 0.9655 - val_loss: 1.6224 - val_accuracy: 0.4773\n",
            "Epoch 119/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1833 - accuracy: 0.9770 - val_loss: 1.5572 - val_accuracy: 0.4318\n",
            "Epoch 120/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.1717 - accuracy: 0.9770 - val_loss: 1.6560 - val_accuracy: 0.4318\n",
            "Epoch 121/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.1704 - accuracy: 0.9885 - val_loss: 1.6077 - val_accuracy: 0.4318\n",
            "Epoch 122/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.1621 - accuracy: 0.9885 - val_loss: 1.6875 - val_accuracy: 0.4545\n",
            "Epoch 123/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1645 - accuracy: 0.9770 - val_loss: 1.6608 - val_accuracy: 0.4091\n",
            "Epoch 124/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1583 - accuracy: 0.9770 - val_loss: 1.6357 - val_accuracy: 0.4318\n",
            "Epoch 125/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.1524 - accuracy: 0.9885 - val_loss: 1.6999 - val_accuracy: 0.4545\n",
            "Epoch 126/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1523 - accuracy: 0.9885 - val_loss: 1.6676 - val_accuracy: 0.4318\n",
            "Epoch 127/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.1520 - accuracy: 0.9770 - val_loss: 1.7095 - val_accuracy: 0.4091\n",
            "Epoch 128/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1430 - accuracy: 0.9885 - val_loss: 1.7098 - val_accuracy: 0.4318\n",
            "Epoch 129/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.1447 - accuracy: 0.9885 - val_loss: 1.6554 - val_accuracy: 0.4545\n",
            "Epoch 130/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1482 - accuracy: 0.9885 - val_loss: 1.7289 - val_accuracy: 0.4545\n",
            "Epoch 131/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.1394 - accuracy: 0.9885 - val_loss: 1.7279 - val_accuracy: 0.4091\n",
            "Epoch 132/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.1465 - accuracy: 0.9885 - val_loss: 1.7585 - val_accuracy: 0.4091\n",
            "Epoch 133/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.1453 - accuracy: 0.9770 - val_loss: 1.7573 - val_accuracy: 0.4773\n",
            "Epoch 134/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.1464 - accuracy: 0.9770 - val_loss: 1.7987 - val_accuracy: 0.4773\n",
            "Epoch 135/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.1503 - accuracy: 0.9885 - val_loss: 1.7603 - val_accuracy: 0.4318\n",
            "Epoch 136/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1382 - accuracy: 0.9655 - val_loss: 1.9545 - val_accuracy: 0.4318\n",
            "Epoch 137/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.1411 - accuracy: 0.9885 - val_loss: 1.7344 - val_accuracy: 0.4773\n",
            "Epoch 138/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1240 - accuracy: 0.9770 - val_loss: 1.9153 - val_accuracy: 0.4773\n",
            "Epoch 139/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.1325 - accuracy: 0.9770 - val_loss: 1.7803 - val_accuracy: 0.5227\n",
            "Epoch 140/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.1231 - accuracy: 0.9885 - val_loss: 1.9141 - val_accuracy: 0.4091\n",
            "Epoch 141/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.1251 - accuracy: 0.9770 - val_loss: 1.8482 - val_accuracy: 0.4091\n",
            "Epoch 142/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.1150 - accuracy: 0.9885 - val_loss: 1.8956 - val_accuracy: 0.4773\n",
            "Epoch 143/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.1116 - accuracy: 0.9885 - val_loss: 1.8796 - val_accuracy: 0.4318\n",
            "Epoch 144/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.1114 - accuracy: 0.9885 - val_loss: 1.8973 - val_accuracy: 0.4545\n",
            "Epoch 145/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.1135 - accuracy: 0.9885 - val_loss: 1.8922 - val_accuracy: 0.4545\n",
            "Epoch 146/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.1153 - accuracy: 0.9885 - val_loss: 1.9819 - val_accuracy: 0.4545\n",
            "Epoch 147/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.1057 - accuracy: 0.9885 - val_loss: 1.9473 - val_accuracy: 0.4773\n",
            "Epoch 148/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.1007 - accuracy: 0.9885 - val_loss: 1.9152 - val_accuracy: 0.4773\n",
            "Epoch 149/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.1002 - accuracy: 0.9885 - val_loss: 1.9756 - val_accuracy: 0.4773\n",
            "Epoch 150/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.0999 - accuracy: 0.9885 - val_loss: 1.9534 - val_accuracy: 0.4773\n",
            "Epoch 151/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.1003 - accuracy: 0.9885 - val_loss: 1.9986 - val_accuracy: 0.4773\n",
            "Epoch 152/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.0984 - accuracy: 0.9885 - val_loss: 2.0160 - val_accuracy: 0.4545\n",
            "Epoch 153/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0950 - accuracy: 0.9885 - val_loss: 1.9381 - val_accuracy: 0.5000\n",
            "Epoch 154/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0957 - accuracy: 0.9885 - val_loss: 1.9977 - val_accuracy: 0.4773\n",
            "Epoch 155/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0920 - accuracy: 0.9885 - val_loss: 2.0266 - val_accuracy: 0.4773\n",
            "Epoch 156/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.0869 - accuracy: 0.9885 - val_loss: 2.0239 - val_accuracy: 0.4773\n",
            "Epoch 157/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0873 - accuracy: 0.9885 - val_loss: 2.0340 - val_accuracy: 0.4773\n",
            "Epoch 158/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0847 - accuracy: 0.9885 - val_loss: 2.0625 - val_accuracy: 0.4773\n",
            "Epoch 159/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.0878 - accuracy: 0.9885 - val_loss: 2.0382 - val_accuracy: 0.4773\n",
            "Epoch 160/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0832 - accuracy: 0.9885 - val_loss: 2.1009 - val_accuracy: 0.4773\n",
            "Epoch 161/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0823 - accuracy: 0.9885 - val_loss: 2.0844 - val_accuracy: 0.4773\n",
            "Epoch 162/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.0813 - accuracy: 0.9885 - val_loss: 2.0970 - val_accuracy: 0.4773\n",
            "Epoch 163/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0792 - accuracy: 0.9885 - val_loss: 2.1034 - val_accuracy: 0.4773\n",
            "Epoch 164/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0792 - accuracy: 0.9885 - val_loss: 2.1113 - val_accuracy: 0.4773\n",
            "Epoch 165/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.0780 - accuracy: 0.9885 - val_loss: 2.1108 - val_accuracy: 0.4773\n",
            "Epoch 166/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0771 - accuracy: 0.9885 - val_loss: 2.1511 - val_accuracy: 0.4773\n",
            "Epoch 167/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.0753 - accuracy: 0.9885 - val_loss: 2.1424 - val_accuracy: 0.4773\n",
            "Epoch 168/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0746 - accuracy: 0.9885 - val_loss: 2.1414 - val_accuracy: 0.4773\n",
            "Epoch 169/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.0754 - accuracy: 0.9885 - val_loss: 2.1810 - val_accuracy: 0.4773\n",
            "Epoch 170/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0763 - accuracy: 0.9885 - val_loss: 2.1557 - val_accuracy: 0.4773\n",
            "Epoch 171/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0751 - accuracy: 0.9885 - val_loss: 2.2267 - val_accuracy: 0.4773\n",
            "Epoch 172/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.0759 - accuracy: 0.9885 - val_loss: 2.1469 - val_accuracy: 0.5000\n",
            "Epoch 173/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0777 - accuracy: 0.9885 - val_loss: 2.2068 - val_accuracy: 0.4773\n",
            "Epoch 174/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0809 - accuracy: 0.9885 - val_loss: 2.2969 - val_accuracy: 0.4773\n",
            "Epoch 175/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0762 - accuracy: 0.9885 - val_loss: 2.2460 - val_accuracy: 0.4773\n",
            "Epoch 176/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0708 - accuracy: 0.9885 - val_loss: 2.2276 - val_accuracy: 0.4773\n",
            "Epoch 177/300\n",
            "87/87 [==============================] - 0s 357us/step - loss: 0.0658 - accuracy: 0.9885 - val_loss: 2.2546 - val_accuracy: 0.4773\n",
            "Epoch 178/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.0655 - accuracy: 0.9885 - val_loss: 2.2316 - val_accuracy: 0.4773\n",
            "Epoch 179/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0659 - accuracy: 0.9885 - val_loss: 2.3022 - val_accuracy: 0.4773\n",
            "Epoch 180/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.0629 - accuracy: 0.9885 - val_loss: 2.2410 - val_accuracy: 0.5000\n",
            "Epoch 181/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0638 - accuracy: 0.9885 - val_loss: 2.3241 - val_accuracy: 0.4773\n",
            "Epoch 182/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0624 - accuracy: 0.9885 - val_loss: 2.3073 - val_accuracy: 0.4773\n",
            "Epoch 183/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.0634 - accuracy: 0.9885 - val_loss: 2.3258 - val_accuracy: 0.4773\n",
            "Epoch 184/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.0626 - accuracy: 0.9885 - val_loss: 2.3529 - val_accuracy: 0.4773\n",
            "Epoch 185/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0599 - accuracy: 0.9885 - val_loss: 2.2982 - val_accuracy: 0.4773\n",
            "Epoch 186/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0584 - accuracy: 0.9885 - val_loss: 2.3240 - val_accuracy: 0.4773\n",
            "Epoch 187/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.0568 - accuracy: 0.9885 - val_loss: 2.3656 - val_accuracy: 0.4773\n",
            "Epoch 188/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0570 - accuracy: 0.9885 - val_loss: 2.3649 - val_accuracy: 0.4773\n",
            "Epoch 189/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0573 - accuracy: 0.9885 - val_loss: 2.3604 - val_accuracy: 0.4773\n",
            "Epoch 190/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0574 - accuracy: 0.9885 - val_loss: 2.4261 - val_accuracy: 0.4545\n",
            "Epoch 191/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0537 - accuracy: 0.9885 - val_loss: 2.3771 - val_accuracy: 0.4773\n",
            "Epoch 192/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0541 - accuracy: 0.9885 - val_loss: 2.4000 - val_accuracy: 0.4545\n",
            "Epoch 193/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0530 - accuracy: 0.9885 - val_loss: 2.4027 - val_accuracy: 0.4545\n",
            "Epoch 194/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0526 - accuracy: 0.9885 - val_loss: 2.4012 - val_accuracy: 0.4545\n",
            "Epoch 195/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.0525 - accuracy: 0.9885 - val_loss: 2.4547 - val_accuracy: 0.4545\n",
            "Epoch 196/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.0512 - accuracy: 0.9885 - val_loss: 2.4361 - val_accuracy: 0.4545\n",
            "Epoch 197/300\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.0518 - accuracy: 0.9885 - val_loss: 2.4540 - val_accuracy: 0.4773\n",
            "Epoch 198/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.0560 - accuracy: 0.9885 - val_loss: 2.5094 - val_accuracy: 0.4545\n",
            "Epoch 199/300\n",
            "87/87 [==============================] - 0s 337us/step - loss: 0.0548 - accuracy: 0.9885 - val_loss: 2.4032 - val_accuracy: 0.5227\n",
            "Epoch 200/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0483 - accuracy: 0.9885 - val_loss: 2.5740 - val_accuracy: 0.4545\n",
            "Epoch 201/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0512 - accuracy: 0.9885 - val_loss: 2.4828 - val_accuracy: 0.4545\n",
            "Epoch 202/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0497 - accuracy: 0.9885 - val_loss: 2.5065 - val_accuracy: 0.4773\n",
            "Epoch 203/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.0513 - accuracy: 0.9885 - val_loss: 2.5891 - val_accuracy: 0.4545\n",
            "Epoch 204/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.0488 - accuracy: 0.9885 - val_loss: 2.4983 - val_accuracy: 0.4773\n",
            "Epoch 205/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0482 - accuracy: 0.9885 - val_loss: 2.5667 - val_accuracy: 0.4545\n",
            "Epoch 206/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.0463 - accuracy: 0.9885 - val_loss: 2.5564 - val_accuracy: 0.4545\n",
            "Epoch 207/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 2.5627 - val_accuracy: 0.4773\n",
            "Epoch 208/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.0449 - accuracy: 0.9885 - val_loss: 2.5773 - val_accuracy: 0.4545\n",
            "Epoch 209/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.0446 - accuracy: 0.9885 - val_loss: 2.5798 - val_accuracy: 0.4773\n",
            "Epoch 210/300\n",
            "87/87 [==============================] - 0s 346us/step - loss: 0.0433 - accuracy: 0.9885 - val_loss: 2.5826 - val_accuracy: 0.4773\n",
            "Epoch 211/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.0445 - accuracy: 0.9885 - val_loss: 2.6214 - val_accuracy: 0.4773\n",
            "Epoch 212/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 2.6087 - val_accuracy: 0.4773\n",
            "Epoch 213/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0432 - accuracy: 0.9885 - val_loss: 2.6337 - val_accuracy: 0.4545\n",
            "Epoch 214/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.0423 - accuracy: 0.9885 - val_loss: 2.6370 - val_accuracy: 0.4773\n",
            "Epoch 215/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.0419 - accuracy: 0.9885 - val_loss: 2.6478 - val_accuracy: 0.4773\n",
            "Epoch 216/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 2.6341 - val_accuracy: 0.4773\n",
            "Epoch 217/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0407 - accuracy: 0.9885 - val_loss: 2.6600 - val_accuracy: 0.4773\n",
            "Epoch 218/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0404 - accuracy: 0.9885 - val_loss: 2.7035 - val_accuracy: 0.4773\n",
            "Epoch 219/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 2.6833 - val_accuracy: 0.4773\n",
            "Epoch 220/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0396 - accuracy: 1.0000 - val_loss: 2.7107 - val_accuracy: 0.4773\n",
            "Epoch 221/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0388 - accuracy: 0.9885 - val_loss: 2.6959 - val_accuracy: 0.4773\n",
            "Epoch 222/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.0385 - accuracy: 0.9885 - val_loss: 2.7289 - val_accuracy: 0.4773\n",
            "Epoch 223/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.0385 - accuracy: 1.0000 - val_loss: 2.7378 - val_accuracy: 0.4773\n",
            "Epoch 224/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.0381 - accuracy: 0.9885 - val_loss: 2.7488 - val_accuracy: 0.4773\n",
            "Epoch 225/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 2.7553 - val_accuracy: 0.4773\n",
            "Epoch 226/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.0373 - accuracy: 1.0000 - val_loss: 2.7539 - val_accuracy: 0.4773\n",
            "Epoch 227/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 2.7778 - val_accuracy: 0.4773\n",
            "Epoch 228/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0368 - accuracy: 1.0000 - val_loss: 2.7558 - val_accuracy: 0.4773\n",
            "Epoch 229/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 2.8186 - val_accuracy: 0.4773\n",
            "Epoch 230/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0356 - accuracy: 1.0000 - val_loss: 2.7762 - val_accuracy: 0.4773\n",
            "Epoch 231/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 2.8096 - val_accuracy: 0.4773\n",
            "Epoch 232/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 2.8282 - val_accuracy: 0.4773\n",
            "Epoch 233/300\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.0358 - accuracy: 1.0000 - val_loss: 2.8333 - val_accuracy: 0.4773\n",
            "Epoch 234/300\n",
            "87/87 [==============================] - 0s 316us/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 2.8551 - val_accuracy: 0.4773\n",
            "Epoch 235/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.0346 - accuracy: 1.0000 - val_loss: 2.8444 - val_accuracy: 0.4773\n",
            "Epoch 236/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 2.8794 - val_accuracy: 0.4773\n",
            "Epoch 237/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 2.8294 - val_accuracy: 0.4773\n",
            "Epoch 238/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0334 - accuracy: 1.0000 - val_loss: 2.9085 - val_accuracy: 0.4773\n",
            "Epoch 239/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.0331 - accuracy: 1.0000 - val_loss: 2.8931 - val_accuracy: 0.4773\n",
            "Epoch 240/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.0323 - accuracy: 1.0000 - val_loss: 2.8632 - val_accuracy: 0.4773\n",
            "Epoch 241/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 2.8997 - val_accuracy: 0.4773\n",
            "Epoch 242/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.0318 - accuracy: 1.0000 - val_loss: 2.9053 - val_accuracy: 0.4773\n",
            "Epoch 243/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0313 - accuracy: 1.0000 - val_loss: 2.9113 - val_accuracy: 0.4773\n",
            "Epoch 244/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.0315 - accuracy: 1.0000 - val_loss: 2.9097 - val_accuracy: 0.4773\n",
            "Epoch 245/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 2.9518 - val_accuracy: 0.4773\n",
            "Epoch 246/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 2.9247 - val_accuracy: 0.4773\n",
            "Epoch 247/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.0310 - accuracy: 1.0000 - val_loss: 2.9508 - val_accuracy: 0.4773\n",
            "Epoch 248/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 2.9542 - val_accuracy: 0.4773\n",
            "Epoch 249/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.0298 - accuracy: 1.0000 - val_loss: 2.9506 - val_accuracy: 0.4773\n",
            "Epoch 250/300\n",
            "87/87 [==============================] - 0s 301us/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 2.9570 - val_accuracy: 0.4773\n",
            "Epoch 251/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 2.9804 - val_accuracy: 0.4773\n",
            "Epoch 252/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 2.9774 - val_accuracy: 0.4773\n",
            "Epoch 253/300\n",
            "87/87 [==============================] - 0s 305us/step - loss: 0.0286 - accuracy: 1.0000 - val_loss: 2.9960 - val_accuracy: 0.4773\n",
            "Epoch 254/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.0289 - accuracy: 1.0000 - val_loss: 2.9937 - val_accuracy: 0.4773\n",
            "Epoch 255/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.0290 - accuracy: 1.0000 - val_loss: 2.9894 - val_accuracy: 0.4773\n",
            "Epoch 256/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0287 - accuracy: 1.0000 - val_loss: 3.0244 - val_accuracy: 0.4773\n",
            "Epoch 257/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0281 - accuracy: 1.0000 - val_loss: 3.0028 - val_accuracy: 0.4773\n",
            "Epoch 258/300\n",
            "87/87 [==============================] - 0s 294us/step - loss: 0.0282 - accuracy: 1.0000 - val_loss: 3.0339 - val_accuracy: 0.4773\n",
            "Epoch 259/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 3.0584 - val_accuracy: 0.4773\n",
            "Epoch 260/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 3.0452 - val_accuracy: 0.4773\n",
            "Epoch 261/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.0274 - accuracy: 1.0000 - val_loss: 3.0501 - val_accuracy: 0.4773\n",
            "Epoch 262/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.0478 - val_accuracy: 0.4773\n",
            "Epoch 263/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 3.0646 - val_accuracy: 0.4773\n",
            "Epoch 264/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.0900 - val_accuracy: 0.4773\n",
            "Epoch 265/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0265 - accuracy: 1.0000 - val_loss: 3.0514 - val_accuracy: 0.4545\n",
            "Epoch 266/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 3.0598 - val_accuracy: 0.4545\n",
            "Epoch 267/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0262 - accuracy: 1.0000 - val_loss: 3.0759 - val_accuracy: 0.4773\n",
            "Epoch 268/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 3.0940 - val_accuracy: 0.4773\n",
            "Epoch 269/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0255 - accuracy: 1.0000 - val_loss: 3.1134 - val_accuracy: 0.4773\n",
            "Epoch 270/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.1169 - val_accuracy: 0.4773\n",
            "Epoch 271/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.0953 - val_accuracy: 0.4773\n",
            "Epoch 272/300\n",
            "87/87 [==============================] - 0s 345us/step - loss: 0.0251 - accuracy: 1.0000 - val_loss: 3.1164 - val_accuracy: 0.4773\n",
            "Epoch 273/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.0247 - accuracy: 1.0000 - val_loss: 3.1204 - val_accuracy: 0.4773\n",
            "Epoch 274/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 3.1289 - val_accuracy: 0.4773\n",
            "Epoch 275/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0244 - accuracy: 1.0000 - val_loss: 3.1617 - val_accuracy: 0.4773\n",
            "Epoch 276/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 3.1320 - val_accuracy: 0.4545\n",
            "Epoch 277/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.0246 - accuracy: 1.0000 - val_loss: 3.1361 - val_accuracy: 0.4545\n",
            "Epoch 278/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 3.1657 - val_accuracy: 0.4773\n",
            "Epoch 279/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 3.1722 - val_accuracy: 0.4773\n",
            "Epoch 280/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 3.1392 - val_accuracy: 0.4545\n",
            "Epoch 281/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0239 - accuracy: 1.0000 - val_loss: 3.1960 - val_accuracy: 0.4773\n",
            "Epoch 282/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0236 - accuracy: 1.0000 - val_loss: 3.1804 - val_accuracy: 0.4773\n",
            "Epoch 283/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.1781 - val_accuracy: 0.4773\n",
            "Epoch 284/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.1922 - val_accuracy: 0.4773\n",
            "Epoch 285/300\n",
            "87/87 [==============================] - 0s 371us/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 3.2124 - val_accuracy: 0.4773\n",
            "Epoch 286/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.2156 - val_accuracy: 0.4773\n",
            "Epoch 287/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0226 - accuracy: 1.0000 - val_loss: 3.2114 - val_accuracy: 0.4773\n",
            "Epoch 288/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 3.1981 - val_accuracy: 0.4545\n",
            "Epoch 289/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.0229 - accuracy: 1.0000 - val_loss: 3.2086 - val_accuracy: 0.4545\n",
            "Epoch 290/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 3.2290 - val_accuracy: 0.4773\n",
            "Epoch 291/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.0224 - accuracy: 1.0000 - val_loss: 3.2207 - val_accuracy: 0.4773\n",
            "Epoch 292/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 3.2381 - val_accuracy: 0.4545\n",
            "Epoch 293/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.2499 - val_accuracy: 0.4773\n",
            "Epoch 294/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.2541 - val_accuracy: 0.4773\n",
            "Epoch 295/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 3.2452 - val_accuracy: 0.4545\n",
            "Epoch 296/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 3.2627 - val_accuracy: 0.4545\n",
            "Epoch 297/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 3.2552 - val_accuracy: 0.4545\n",
            "Epoch 298/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.0222 - accuracy: 1.0000 - val_loss: 3.2505 - val_accuracy: 0.4773\n",
            "Epoch 299/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.0214 - accuracy: 1.0000 - val_loss: 3.2866 - val_accuracy: 0.4545\n",
            "Epoch 300/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 3.2761 - val_accuracy: 0.4545\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/300\n",
            "88/88 [==============================] - 0s 967us/step - loss: 1.0662 - accuracy: 0.4318 - val_loss: 1.0866 - val_accuracy: 0.5116\n",
            "Epoch 2/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0130 - accuracy: 0.4432 - val_loss: 1.0446 - val_accuracy: 0.4884\n",
            "Epoch 3/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.9735 - accuracy: 0.4886 - val_loss: 1.0264 - val_accuracy: 0.4884\n",
            "Epoch 4/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9435 - accuracy: 0.4773 - val_loss: 1.0157 - val_accuracy: 0.5349\n",
            "Epoch 5/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9248 - accuracy: 0.4886 - val_loss: 1.0146 - val_accuracy: 0.5581\n",
            "Epoch 6/300\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.9101 - accuracy: 0.4886 - val_loss: 1.0136 - val_accuracy: 0.5581\n",
            "Epoch 7/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8995 - accuracy: 0.5114 - val_loss: 1.0085 - val_accuracy: 0.5116\n",
            "Epoch 8/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8910 - accuracy: 0.5455 - val_loss: 1.0090 - val_accuracy: 0.5116\n",
            "Epoch 9/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8787 - accuracy: 0.5682 - val_loss: 1.0108 - val_accuracy: 0.5116\n",
            "Epoch 10/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8729 - accuracy: 0.5795 - val_loss: 1.0127 - val_accuracy: 0.5116\n",
            "Epoch 11/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8625 - accuracy: 0.5795 - val_loss: 1.0133 - val_accuracy: 0.5349\n",
            "Epoch 12/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8562 - accuracy: 0.5795 - val_loss: 1.0136 - val_accuracy: 0.5116\n",
            "Epoch 13/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8516 - accuracy: 0.5682 - val_loss: 1.0215 - val_accuracy: 0.5116\n",
            "Epoch 14/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8441 - accuracy: 0.5909 - val_loss: 1.0149 - val_accuracy: 0.5349\n",
            "Epoch 15/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8358 - accuracy: 0.5795 - val_loss: 1.0188 - val_accuracy: 0.5349\n",
            "Epoch 16/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8347 - accuracy: 0.5795 - val_loss: 1.0173 - val_accuracy: 0.5116\n",
            "Epoch 17/300\n",
            "88/88 [==============================] - 0s 197us/step - loss: 0.8220 - accuracy: 0.6023 - val_loss: 1.0213 - val_accuracy: 0.5349\n",
            "Epoch 18/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.8147 - accuracy: 0.5795 - val_loss: 1.0263 - val_accuracy: 0.5116\n",
            "Epoch 19/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8080 - accuracy: 0.6136 - val_loss: 1.0267 - val_accuracy: 0.5116\n",
            "Epoch 20/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8004 - accuracy: 0.6364 - val_loss: 1.0282 - val_accuracy: 0.5116\n",
            "Epoch 21/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.7913 - accuracy: 0.6591 - val_loss: 1.0349 - val_accuracy: 0.4884\n",
            "Epoch 22/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.7837 - accuracy: 0.6591 - val_loss: 1.0384 - val_accuracy: 0.4884\n",
            "Epoch 23/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7749 - accuracy: 0.6705 - val_loss: 1.0422 - val_accuracy: 0.4884\n",
            "Epoch 24/300\n",
            "88/88 [==============================] - 0s 301us/step - loss: 0.7677 - accuracy: 0.6705 - val_loss: 1.0460 - val_accuracy: 0.4884\n",
            "Epoch 25/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.7624 - accuracy: 0.6705 - val_loss: 1.0484 - val_accuracy: 0.4884\n",
            "Epoch 26/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.7502 - accuracy: 0.6818 - val_loss: 1.0539 - val_accuracy: 0.4884\n",
            "Epoch 27/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7397 - accuracy: 0.6818 - val_loss: 1.0561 - val_accuracy: 0.4884\n",
            "Epoch 28/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7328 - accuracy: 0.6932 - val_loss: 1.0619 - val_accuracy: 0.5116\n",
            "Epoch 29/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7237 - accuracy: 0.7045 - val_loss: 1.0663 - val_accuracy: 0.5116\n",
            "Epoch 30/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.7148 - accuracy: 0.6932 - val_loss: 1.0679 - val_accuracy: 0.5349\n",
            "Epoch 31/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.7042 - accuracy: 0.7159 - val_loss: 1.0677 - val_accuracy: 0.5349\n",
            "Epoch 32/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.7013 - accuracy: 0.7045 - val_loss: 1.0676 - val_accuracy: 0.5349\n",
            "Epoch 33/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.6901 - accuracy: 0.7273 - val_loss: 1.0754 - val_accuracy: 0.5116\n",
            "Epoch 34/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.6831 - accuracy: 0.7273 - val_loss: 1.0779 - val_accuracy: 0.5349\n",
            "Epoch 35/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.6744 - accuracy: 0.7273 - val_loss: 1.0844 - val_accuracy: 0.5349\n",
            "Epoch 36/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6655 - accuracy: 0.7273 - val_loss: 1.0855 - val_accuracy: 0.5349\n",
            "Epoch 37/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.6575 - accuracy: 0.7500 - val_loss: 1.0898 - val_accuracy: 0.5349\n",
            "Epoch 38/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.6568 - accuracy: 0.7500 - val_loss: 1.1023 - val_accuracy: 0.4884\n",
            "Epoch 39/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6460 - accuracy: 0.7500 - val_loss: 1.0955 - val_accuracy: 0.5349\n",
            "Epoch 40/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.6398 - accuracy: 0.7273 - val_loss: 1.0978 - val_accuracy: 0.5349\n",
            "Epoch 41/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.6305 - accuracy: 0.7386 - val_loss: 1.1067 - val_accuracy: 0.5116\n",
            "Epoch 42/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.6225 - accuracy: 0.7386 - val_loss: 1.1056 - val_accuracy: 0.5116\n",
            "Epoch 43/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.6133 - accuracy: 0.7386 - val_loss: 1.1086 - val_accuracy: 0.5116\n",
            "Epoch 44/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.6091 - accuracy: 0.7500 - val_loss: 1.1142 - val_accuracy: 0.5116\n",
            "Epoch 45/300\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.6004 - accuracy: 0.7500 - val_loss: 1.1128 - val_accuracy: 0.5349\n",
            "Epoch 46/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5943 - accuracy: 0.7614 - val_loss: 1.1175 - val_accuracy: 0.5116\n",
            "Epoch 47/300\n",
            "88/88 [==============================] - 0s 199us/step - loss: 0.5893 - accuracy: 0.7386 - val_loss: 1.1231 - val_accuracy: 0.5349\n",
            "Epoch 48/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.5807 - accuracy: 0.7386 - val_loss: 1.1318 - val_accuracy: 0.5349\n",
            "Epoch 49/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.5778 - accuracy: 0.7727 - val_loss: 1.1437 - val_accuracy: 0.5581\n",
            "Epoch 50/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.5757 - accuracy: 0.7614 - val_loss: 1.1357 - val_accuracy: 0.5349\n",
            "Epoch 51/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.5696 - accuracy: 0.7614 - val_loss: 1.1408 - val_accuracy: 0.5349\n",
            "Epoch 52/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.5569 - accuracy: 0.7727 - val_loss: 1.1535 - val_accuracy: 0.5581\n",
            "Epoch 53/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.5562 - accuracy: 0.7841 - val_loss: 1.1624 - val_accuracy: 0.5349\n",
            "Epoch 54/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5504 - accuracy: 0.7614 - val_loss: 1.1570 - val_accuracy: 0.5581\n",
            "Epoch 55/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.5413 - accuracy: 0.7727 - val_loss: 1.1606 - val_accuracy: 0.5581\n",
            "Epoch 56/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.5350 - accuracy: 0.7955 - val_loss: 1.1603 - val_accuracy: 0.5581\n",
            "Epoch 57/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.5298 - accuracy: 0.7727 - val_loss: 1.1699 - val_accuracy: 0.5581\n",
            "Epoch 58/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.5265 - accuracy: 0.7727 - val_loss: 1.1625 - val_accuracy: 0.5814\n",
            "Epoch 59/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5181 - accuracy: 0.7841 - val_loss: 1.1752 - val_accuracy: 0.5814\n",
            "Epoch 60/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5167 - accuracy: 0.7841 - val_loss: 1.1793 - val_accuracy: 0.6047\n",
            "Epoch 61/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5044 - accuracy: 0.7955 - val_loss: 1.1909 - val_accuracy: 0.5814\n",
            "Epoch 62/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5012 - accuracy: 0.7841 - val_loss: 1.1925 - val_accuracy: 0.6047\n",
            "Epoch 63/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.4991 - accuracy: 0.7955 - val_loss: 1.1949 - val_accuracy: 0.6047\n",
            "Epoch 64/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4900 - accuracy: 0.7841 - val_loss: 1.1937 - val_accuracy: 0.6047\n",
            "Epoch 65/300\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.4845 - accuracy: 0.8068 - val_loss: 1.2115 - val_accuracy: 0.6047\n",
            "Epoch 66/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.4852 - accuracy: 0.8295 - val_loss: 1.2135 - val_accuracy: 0.5814\n",
            "Epoch 67/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.4719 - accuracy: 0.8295 - val_loss: 1.2312 - val_accuracy: 0.6047\n",
            "Epoch 68/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4667 - accuracy: 0.8295 - val_loss: 1.2267 - val_accuracy: 0.6047\n",
            "Epoch 69/300\n",
            "88/88 [==============================] - 0s 323us/step - loss: 0.4688 - accuracy: 0.8182 - val_loss: 1.2211 - val_accuracy: 0.5814\n",
            "Epoch 70/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.4575 - accuracy: 0.8295 - val_loss: 1.2424 - val_accuracy: 0.5814\n",
            "Epoch 71/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.4523 - accuracy: 0.8182 - val_loss: 1.2448 - val_accuracy: 0.6047\n",
            "Epoch 72/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.4497 - accuracy: 0.8182 - val_loss: 1.2423 - val_accuracy: 0.5814\n",
            "Epoch 73/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.4392 - accuracy: 0.8295 - val_loss: 1.2572 - val_accuracy: 0.5814\n",
            "Epoch 74/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.4422 - accuracy: 0.8295 - val_loss: 1.2576 - val_accuracy: 0.5814\n",
            "Epoch 75/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.4292 - accuracy: 0.8295 - val_loss: 1.2717 - val_accuracy: 0.5814\n",
            "Epoch 76/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.4250 - accuracy: 0.8182 - val_loss: 1.2796 - val_accuracy: 0.6047\n",
            "Epoch 77/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.4183 - accuracy: 0.8295 - val_loss: 1.2747 - val_accuracy: 0.5814\n",
            "Epoch 78/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.4120 - accuracy: 0.8409 - val_loss: 1.2875 - val_accuracy: 0.5814\n",
            "Epoch 79/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.4084 - accuracy: 0.8295 - val_loss: 1.2913 - val_accuracy: 0.5814\n",
            "Epoch 80/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4125 - accuracy: 0.8409 - val_loss: 1.2995 - val_accuracy: 0.5814\n",
            "Epoch 81/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.4096 - accuracy: 0.8409 - val_loss: 1.2989 - val_accuracy: 0.5581\n",
            "Epoch 82/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3907 - accuracy: 0.8523 - val_loss: 1.3245 - val_accuracy: 0.5814\n",
            "Epoch 83/300\n",
            "88/88 [==============================] - 0s 301us/step - loss: 0.3933 - accuracy: 0.8409 - val_loss: 1.3187 - val_accuracy: 0.5581\n",
            "Epoch 84/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.3824 - accuracy: 0.8523 - val_loss: 1.3294 - val_accuracy: 0.5814\n",
            "Epoch 85/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.3821 - accuracy: 0.8409 - val_loss: 1.3252 - val_accuracy: 0.5814\n",
            "Epoch 86/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.3771 - accuracy: 0.8523 - val_loss: 1.3390 - val_accuracy: 0.5581\n",
            "Epoch 87/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.3900 - accuracy: 0.8409 - val_loss: 1.3561 - val_accuracy: 0.5581\n",
            "Epoch 88/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.3624 - accuracy: 0.8523 - val_loss: 1.3478 - val_accuracy: 0.5116\n",
            "Epoch 89/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3635 - accuracy: 0.8523 - val_loss: 1.3920 - val_accuracy: 0.5581\n",
            "Epoch 90/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.3708 - accuracy: 0.8409 - val_loss: 1.3756 - val_accuracy: 0.5349\n",
            "Epoch 91/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.3476 - accuracy: 0.8636 - val_loss: 1.4030 - val_accuracy: 0.5814\n",
            "Epoch 92/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.3447 - accuracy: 0.8750 - val_loss: 1.4122 - val_accuracy: 0.5116\n",
            "Epoch 93/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.3341 - accuracy: 0.8523 - val_loss: 1.3973 - val_accuracy: 0.5116\n",
            "Epoch 94/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3371 - accuracy: 0.8523 - val_loss: 1.4220 - val_accuracy: 0.5349\n",
            "Epoch 95/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.3395 - accuracy: 0.8636 - val_loss: 1.4266 - val_accuracy: 0.5116\n",
            "Epoch 96/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3420 - accuracy: 0.8636 - val_loss: 1.4348 - val_accuracy: 0.4884\n",
            "Epoch 97/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.3264 - accuracy: 0.8864 - val_loss: 1.4392 - val_accuracy: 0.5814\n",
            "Epoch 98/300\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.3203 - accuracy: 0.8523 - val_loss: 1.4597 - val_accuracy: 0.4884\n",
            "Epoch 99/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.3081 - accuracy: 0.8750 - val_loss: 1.4684 - val_accuracy: 0.5349\n",
            "Epoch 100/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3133 - accuracy: 0.8750 - val_loss: 1.4696 - val_accuracy: 0.5116\n",
            "Epoch 101/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.2999 - accuracy: 0.8750 - val_loss: 1.4888 - val_accuracy: 0.5349\n",
            "Epoch 102/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.3040 - accuracy: 0.8750 - val_loss: 1.4924 - val_accuracy: 0.4884\n",
            "Epoch 103/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.2955 - accuracy: 0.8864 - val_loss: 1.5035 - val_accuracy: 0.4884\n",
            "Epoch 104/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.2903 - accuracy: 0.8977 - val_loss: 1.5362 - val_accuracy: 0.5116\n",
            "Epoch 105/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.2875 - accuracy: 0.8977 - val_loss: 1.5378 - val_accuracy: 0.4884\n",
            "Epoch 106/300\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.2810 - accuracy: 0.8750 - val_loss: 1.5597 - val_accuracy: 0.5116\n",
            "Epoch 107/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.2820 - accuracy: 0.9091 - val_loss: 1.5717 - val_accuracy: 0.4884\n",
            "Epoch 108/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.2683 - accuracy: 0.9091 - val_loss: 1.5689 - val_accuracy: 0.5116\n",
            "Epoch 109/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.2789 - accuracy: 0.8750 - val_loss: 1.5917 - val_accuracy: 0.5116\n",
            "Epoch 110/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.2716 - accuracy: 0.9205 - val_loss: 1.6327 - val_accuracy: 0.4419\n",
            "Epoch 111/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.2668 - accuracy: 0.9318 - val_loss: 1.6550 - val_accuracy: 0.4651\n",
            "Epoch 112/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2905 - accuracy: 0.9091 - val_loss: 1.6284 - val_accuracy: 0.5116\n",
            "Epoch 113/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.2558 - accuracy: 0.9432 - val_loss: 1.6778 - val_accuracy: 0.4651\n",
            "Epoch 114/300\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.2457 - accuracy: 0.9545 - val_loss: 1.6811 - val_accuracy: 0.4884\n",
            "Epoch 115/300\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.2497 - accuracy: 0.9318 - val_loss: 1.6906 - val_accuracy: 0.4884\n",
            "Epoch 116/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2367 - accuracy: 0.9432 - val_loss: 1.7013 - val_accuracy: 0.5349\n",
            "Epoch 117/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.2334 - accuracy: 0.9318 - val_loss: 1.7006 - val_accuracy: 0.4884\n",
            "Epoch 118/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.2320 - accuracy: 0.9432 - val_loss: 1.7247 - val_accuracy: 0.4884\n",
            "Epoch 119/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.2234 - accuracy: 0.9773 - val_loss: 1.7456 - val_accuracy: 0.5116\n",
            "Epoch 120/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.2292 - accuracy: 0.9545 - val_loss: 1.7737 - val_accuracy: 0.4651\n",
            "Epoch 121/300\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.2281 - accuracy: 0.9659 - val_loss: 1.7634 - val_accuracy: 0.4884\n",
            "Epoch 122/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.2225 - accuracy: 0.9545 - val_loss: 1.7874 - val_accuracy: 0.4884\n",
            "Epoch 123/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.2097 - accuracy: 0.9545 - val_loss: 1.7876 - val_accuracy: 0.4651\n",
            "Epoch 124/300\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.2109 - accuracy: 0.9659 - val_loss: 1.8165 - val_accuracy: 0.5116\n",
            "Epoch 125/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.2049 - accuracy: 0.9659 - val_loss: 1.8158 - val_accuracy: 0.4884\n",
            "Epoch 126/300\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.2009 - accuracy: 0.9773 - val_loss: 1.8631 - val_accuracy: 0.4884\n",
            "Epoch 127/300\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.2011 - accuracy: 0.9659 - val_loss: 1.8349 - val_accuracy: 0.4884\n",
            "Epoch 128/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.2018 - accuracy: 0.9659 - val_loss: 1.8639 - val_accuracy: 0.4884\n",
            "Epoch 129/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.1942 - accuracy: 0.9773 - val_loss: 1.8989 - val_accuracy: 0.4884\n",
            "Epoch 130/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.1883 - accuracy: 0.9773 - val_loss: 1.9010 - val_accuracy: 0.4884\n",
            "Epoch 131/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1875 - accuracy: 0.9773 - val_loss: 1.9322 - val_accuracy: 0.4884\n",
            "Epoch 132/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.1768 - accuracy: 0.9886 - val_loss: 1.9380 - val_accuracy: 0.4884\n",
            "Epoch 133/300\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.1741 - accuracy: 0.9773 - val_loss: 1.9558 - val_accuracy: 0.4884\n",
            "Epoch 134/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.1722 - accuracy: 0.9886 - val_loss: 1.9920 - val_accuracy: 0.4884\n",
            "Epoch 135/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1685 - accuracy: 0.9886 - val_loss: 1.9809 - val_accuracy: 0.4884\n",
            "Epoch 136/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.1658 - accuracy: 0.9886 - val_loss: 1.9994 - val_accuracy: 0.4884\n",
            "Epoch 137/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.1655 - accuracy: 0.9886 - val_loss: 2.0350 - val_accuracy: 0.4884\n",
            "Epoch 138/300\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.1588 - accuracy: 0.9886 - val_loss: 2.0383 - val_accuracy: 0.4884\n",
            "Epoch 139/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1576 - accuracy: 0.9886 - val_loss: 2.0661 - val_accuracy: 0.4884\n",
            "Epoch 140/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.1546 - accuracy: 0.9886 - val_loss: 2.0492 - val_accuracy: 0.4884\n",
            "Epoch 141/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.1502 - accuracy: 0.9886 - val_loss: 2.0881 - val_accuracy: 0.4884\n",
            "Epoch 142/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.1507 - accuracy: 0.9886 - val_loss: 2.0902 - val_accuracy: 0.4884\n",
            "Epoch 143/300\n",
            "88/88 [==============================] - 0s 330us/step - loss: 0.1460 - accuracy: 0.9886 - val_loss: 2.1112 - val_accuracy: 0.4884\n",
            "Epoch 144/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.1423 - accuracy: 0.9886 - val_loss: 2.1272 - val_accuracy: 0.4884\n",
            "Epoch 145/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.1410 - accuracy: 0.9886 - val_loss: 2.1607 - val_accuracy: 0.4884\n",
            "Epoch 146/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.1375 - accuracy: 0.9886 - val_loss: 2.1784 - val_accuracy: 0.4884\n",
            "Epoch 147/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.1376 - accuracy: 0.9886 - val_loss: 2.1748 - val_accuracy: 0.4884\n",
            "Epoch 148/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1369 - accuracy: 0.9773 - val_loss: 2.2064 - val_accuracy: 0.4884\n",
            "Epoch 149/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.1319 - accuracy: 0.9886 - val_loss: 2.2234 - val_accuracy: 0.4884\n",
            "Epoch 150/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1328 - accuracy: 0.9886 - val_loss: 2.2297 - val_accuracy: 0.4884\n",
            "Epoch 151/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.1303 - accuracy: 0.9886 - val_loss: 2.2923 - val_accuracy: 0.4884\n",
            "Epoch 152/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.1221 - accuracy: 0.9886 - val_loss: 2.2707 - val_accuracy: 0.4884\n",
            "Epoch 153/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.1234 - accuracy: 0.9886 - val_loss: 2.2812 - val_accuracy: 0.4884\n",
            "Epoch 154/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1262 - accuracy: 0.9886 - val_loss: 2.3203 - val_accuracy: 0.4884\n",
            "Epoch 155/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.1386 - accuracy: 0.9773 - val_loss: 2.3166 - val_accuracy: 0.4884\n",
            "Epoch 156/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.1269 - accuracy: 0.9886 - val_loss: 2.3369 - val_accuracy: 0.4651\n",
            "Epoch 157/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.1152 - accuracy: 0.9886 - val_loss: 2.3395 - val_accuracy: 0.5116\n",
            "Epoch 158/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.1101 - accuracy: 0.9886 - val_loss: 2.3566 - val_accuracy: 0.4651\n",
            "Epoch 159/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.1116 - accuracy: 1.0000 - val_loss: 2.3604 - val_accuracy: 0.5116\n",
            "Epoch 160/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.1072 - accuracy: 0.9886 - val_loss: 2.4033 - val_accuracy: 0.4884\n",
            "Epoch 161/300\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.1076 - accuracy: 1.0000 - val_loss: 2.4024 - val_accuracy: 0.4651\n",
            "Epoch 162/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.1063 - accuracy: 1.0000 - val_loss: 2.4001 - val_accuracy: 0.4884\n",
            "Epoch 163/300\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.1013 - accuracy: 1.0000 - val_loss: 2.4282 - val_accuracy: 0.4651\n",
            "Epoch 164/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.1018 - accuracy: 0.9886 - val_loss: 2.4513 - val_accuracy: 0.4651\n",
            "Epoch 165/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.1005 - accuracy: 1.0000 - val_loss: 2.4675 - val_accuracy: 0.4651\n",
            "Epoch 166/300\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.0957 - accuracy: 1.0000 - val_loss: 2.4707 - val_accuracy: 0.4651\n",
            "Epoch 167/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.0949 - accuracy: 1.0000 - val_loss: 2.4853 - val_accuracy: 0.4419\n",
            "Epoch 168/300\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.0959 - accuracy: 1.0000 - val_loss: 2.4769 - val_accuracy: 0.4651\n",
            "Epoch 169/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.0926 - accuracy: 1.0000 - val_loss: 2.5178 - val_accuracy: 0.4651\n",
            "Epoch 170/300\n",
            "88/88 [==============================] - 0s 315us/step - loss: 0.0903 - accuracy: 1.0000 - val_loss: 2.5252 - val_accuracy: 0.4651\n",
            "Epoch 171/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.0874 - accuracy: 1.0000 - val_loss: 2.5448 - val_accuracy: 0.4419\n",
            "Epoch 172/300\n",
            "88/88 [==============================] - 0s 288us/step - loss: 0.0885 - accuracy: 1.0000 - val_loss: 2.5425 - val_accuracy: 0.4651\n",
            "Epoch 173/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0852 - accuracy: 1.0000 - val_loss: 2.5456 - val_accuracy: 0.4651\n",
            "Epoch 174/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.0851 - accuracy: 1.0000 - val_loss: 2.5944 - val_accuracy: 0.4651\n",
            "Epoch 175/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.0850 - accuracy: 1.0000 - val_loss: 2.5937 - val_accuracy: 0.4651\n",
            "Epoch 176/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.0817 - accuracy: 1.0000 - val_loss: 2.6160 - val_accuracy: 0.4651\n",
            "Epoch 177/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.0820 - accuracy: 1.0000 - val_loss: 2.6187 - val_accuracy: 0.4651\n",
            "Epoch 178/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.0812 - accuracy: 1.0000 - val_loss: 2.6159 - val_accuracy: 0.4651\n",
            "Epoch 179/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.0771 - accuracy: 1.0000 - val_loss: 2.6458 - val_accuracy: 0.4651\n",
            "Epoch 180/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.0809 - accuracy: 1.0000 - val_loss: 2.6579 - val_accuracy: 0.4651\n",
            "Epoch 181/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.0780 - accuracy: 1.0000 - val_loss: 2.6412 - val_accuracy: 0.4651\n",
            "Epoch 182/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.0744 - accuracy: 1.0000 - val_loss: 2.7072 - val_accuracy: 0.4419\n",
            "Epoch 183/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0716 - accuracy: 1.0000 - val_loss: 2.6876 - val_accuracy: 0.4651\n",
            "Epoch 184/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.0749 - accuracy: 1.0000 - val_loss: 2.6775 - val_accuracy: 0.4651\n",
            "Epoch 185/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.0711 - accuracy: 1.0000 - val_loss: 2.7157 - val_accuracy: 0.4651\n",
            "Epoch 186/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0695 - accuracy: 1.0000 - val_loss: 2.7219 - val_accuracy: 0.4651\n",
            "Epoch 187/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0682 - accuracy: 1.0000 - val_loss: 2.7299 - val_accuracy: 0.4651\n",
            "Epoch 188/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0683 - accuracy: 1.0000 - val_loss: 2.7406 - val_accuracy: 0.4651\n",
            "Epoch 189/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0646 - accuracy: 1.0000 - val_loss: 2.7464 - val_accuracy: 0.4651\n",
            "Epoch 190/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0635 - accuracy: 1.0000 - val_loss: 2.7822 - val_accuracy: 0.4651\n",
            "Epoch 191/300\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.0636 - accuracy: 1.0000 - val_loss: 2.7599 - val_accuracy: 0.4651\n",
            "Epoch 192/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.0617 - accuracy: 1.0000 - val_loss: 2.7982 - val_accuracy: 0.4651\n",
            "Epoch 193/300\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.0597 - accuracy: 1.0000 - val_loss: 2.8064 - val_accuracy: 0.4651\n",
            "Epoch 194/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.0588 - accuracy: 1.0000 - val_loss: 2.7940 - val_accuracy: 0.4651\n",
            "Epoch 195/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0587 - accuracy: 1.0000 - val_loss: 2.8273 - val_accuracy: 0.4651\n",
            "Epoch 196/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0577 - accuracy: 1.0000 - val_loss: 2.8128 - val_accuracy: 0.4651\n",
            "Epoch 197/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0557 - accuracy: 1.0000 - val_loss: 2.8391 - val_accuracy: 0.4651\n",
            "Epoch 198/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.0554 - accuracy: 1.0000 - val_loss: 2.8661 - val_accuracy: 0.4651\n",
            "Epoch 199/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0535 - accuracy: 1.0000 - val_loss: 2.8514 - val_accuracy: 0.4651\n",
            "Epoch 200/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.0521 - accuracy: 1.0000 - val_loss: 2.8812 - val_accuracy: 0.4651\n",
            "Epoch 201/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.0528 - accuracy: 1.0000 - val_loss: 2.8774 - val_accuracy: 0.4651\n",
            "Epoch 202/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0508 - accuracy: 1.0000 - val_loss: 2.8980 - val_accuracy: 0.4651\n",
            "Epoch 203/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.0505 - accuracy: 1.0000 - val_loss: 2.9046 - val_accuracy: 0.4651\n",
            "Epoch 204/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 2.9043 - val_accuracy: 0.4651\n",
            "Epoch 205/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.0492 - accuracy: 1.0000 - val_loss: 2.9260 - val_accuracy: 0.4651\n",
            "Epoch 206/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.0475 - accuracy: 1.0000 - val_loss: 2.9279 - val_accuracy: 0.4651\n",
            "Epoch 207/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0471 - accuracy: 1.0000 - val_loss: 2.9498 - val_accuracy: 0.4651\n",
            "Epoch 208/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.0452 - accuracy: 1.0000 - val_loss: 2.9464 - val_accuracy: 0.4651\n",
            "Epoch 209/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.0457 - accuracy: 1.0000 - val_loss: 2.9555 - val_accuracy: 0.4651\n",
            "Epoch 210/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.0447 - accuracy: 1.0000 - val_loss: 2.9612 - val_accuracy: 0.4651\n",
            "Epoch 211/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.0440 - accuracy: 1.0000 - val_loss: 2.9784 - val_accuracy: 0.4651\n",
            "Epoch 212/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.0435 - accuracy: 1.0000 - val_loss: 2.9942 - val_accuracy: 0.4651\n",
            "Epoch 213/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.0422 - accuracy: 1.0000 - val_loss: 2.9916 - val_accuracy: 0.4651\n",
            "Epoch 214/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.0429 - accuracy: 1.0000 - val_loss: 3.0060 - val_accuracy: 0.4651\n",
            "Epoch 215/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.0413 - accuracy: 1.0000 - val_loss: 3.0285 - val_accuracy: 0.4651\n",
            "Epoch 216/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 3.0157 - val_accuracy: 0.4651\n",
            "Epoch 217/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0401 - accuracy: 1.0000 - val_loss: 3.0130 - val_accuracy: 0.4651\n",
            "Epoch 218/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.0391 - accuracy: 1.0000 - val_loss: 3.0381 - val_accuracy: 0.4651\n",
            "Epoch 219/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.0383 - accuracy: 1.0000 - val_loss: 3.0453 - val_accuracy: 0.4651\n",
            "Epoch 220/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.0386 - accuracy: 1.0000 - val_loss: 3.0694 - val_accuracy: 0.4651\n",
            "Epoch 221/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.0378 - accuracy: 1.0000 - val_loss: 3.0632 - val_accuracy: 0.4651\n",
            "Epoch 222/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.0375 - accuracy: 1.0000 - val_loss: 3.0505 - val_accuracy: 0.4884\n",
            "Epoch 223/300\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.0363 - accuracy: 1.0000 - val_loss: 3.0863 - val_accuracy: 0.4651\n",
            "Epoch 224/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.0354 - accuracy: 1.0000 - val_loss: 3.0901 - val_accuracy: 0.4651\n",
            "Epoch 225/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.0350 - accuracy: 1.0000 - val_loss: 3.0954 - val_accuracy: 0.4884\n",
            "Epoch 226/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0347 - accuracy: 1.0000 - val_loss: 3.1148 - val_accuracy: 0.4884\n",
            "Epoch 227/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.0348 - accuracy: 1.0000 - val_loss: 3.1195 - val_accuracy: 0.4884\n",
            "Epoch 228/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0333 - accuracy: 1.0000 - val_loss: 3.1175 - val_accuracy: 0.4884\n",
            "Epoch 229/300\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 3.1275 - val_accuracy: 0.4884\n",
            "Epoch 230/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 3.1397 - val_accuracy: 0.4884\n",
            "Epoch 231/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 3.1401 - val_accuracy: 0.4884\n",
            "Epoch 232/300\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.0320 - accuracy: 1.0000 - val_loss: 3.1535 - val_accuracy: 0.4884\n",
            "Epoch 233/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.0317 - accuracy: 1.0000 - val_loss: 3.1644 - val_accuracy: 0.4884\n",
            "Epoch 234/300\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.0304 - accuracy: 1.0000 - val_loss: 3.1750 - val_accuracy: 0.4884\n",
            "Epoch 235/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0300 - accuracy: 1.0000 - val_loss: 3.1771 - val_accuracy: 0.4884\n",
            "Epoch 236/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.0299 - accuracy: 1.0000 - val_loss: 3.1932 - val_accuracy: 0.4884\n",
            "Epoch 237/300\n",
            "88/88 [==============================] - 0s 304us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 3.1963 - val_accuracy: 0.4884\n",
            "Epoch 238/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 3.2061 - val_accuracy: 0.4884\n",
            "Epoch 239/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 3.1973 - val_accuracy: 0.4884\n",
            "Epoch 240/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0280 - accuracy: 1.0000 - val_loss: 3.2169 - val_accuracy: 0.4884\n",
            "Epoch 241/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 3.2394 - val_accuracy: 0.4884\n",
            "Epoch 242/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0279 - accuracy: 1.0000 - val_loss: 3.2336 - val_accuracy: 0.4884\n",
            "Epoch 243/300\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.0269 - accuracy: 1.0000 - val_loss: 3.2410 - val_accuracy: 0.4884\n",
            "Epoch 244/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.0264 - accuracy: 1.0000 - val_loss: 3.2552 - val_accuracy: 0.4884\n",
            "Epoch 245/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0256 - accuracy: 1.0000 - val_loss: 3.2541 - val_accuracy: 0.4884\n",
            "Epoch 246/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.0261 - accuracy: 1.0000 - val_loss: 3.2655 - val_accuracy: 0.4884\n",
            "Epoch 247/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.0260 - accuracy: 1.0000 - val_loss: 3.2506 - val_accuracy: 0.4884\n",
            "Epoch 248/300\n",
            "88/88 [==============================] - 0s 298us/step - loss: 0.0253 - accuracy: 1.0000 - val_loss: 3.2765 - val_accuracy: 0.4884\n",
            "Epoch 249/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0252 - accuracy: 1.0000 - val_loss: 3.2944 - val_accuracy: 0.4884\n",
            "Epoch 250/300\n",
            "88/88 [==============================] - 0s 316us/step - loss: 0.0249 - accuracy: 1.0000 - val_loss: 3.2810 - val_accuracy: 0.4884\n",
            "Epoch 251/300\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.0242 - accuracy: 1.0000 - val_loss: 3.3043 - val_accuracy: 0.4884\n",
            "Epoch 252/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 3.2994 - val_accuracy: 0.4884\n",
            "Epoch 253/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0234 - accuracy: 1.0000 - val_loss: 3.3082 - val_accuracy: 0.4884\n",
            "Epoch 254/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.0231 - accuracy: 1.0000 - val_loss: 3.3156 - val_accuracy: 0.4884\n",
            "Epoch 255/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 3.3224 - val_accuracy: 0.4884\n",
            "Epoch 256/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.0225 - accuracy: 1.0000 - val_loss: 3.3359 - val_accuracy: 0.4884\n",
            "Epoch 257/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 3.3347 - val_accuracy: 0.4884\n",
            "Epoch 258/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.0223 - accuracy: 1.0000 - val_loss: 3.3283 - val_accuracy: 0.4884\n",
            "Epoch 259/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.0216 - accuracy: 1.0000 - val_loss: 3.3599 - val_accuracy: 0.4884\n",
            "Epoch 260/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 3.3568 - val_accuracy: 0.4884\n",
            "Epoch 261/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0210 - accuracy: 1.0000 - val_loss: 3.3516 - val_accuracy: 0.4884\n",
            "Epoch 262/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 3.3630 - val_accuracy: 0.4884\n",
            "Epoch 263/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.0204 - accuracy: 1.0000 - val_loss: 3.3759 - val_accuracy: 0.4884\n",
            "Epoch 264/300\n",
            "88/88 [==============================] - 0s 317us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 3.3774 - val_accuracy: 0.4884\n",
            "Epoch 265/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 3.3774 - val_accuracy: 0.4884\n",
            "Epoch 266/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 3.3941 - val_accuracy: 0.4884\n",
            "Epoch 267/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 3.3953 - val_accuracy: 0.4884\n",
            "Epoch 268/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 3.4060 - val_accuracy: 0.4884\n",
            "Epoch 269/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0192 - accuracy: 1.0000 - val_loss: 3.4090 - val_accuracy: 0.4884\n",
            "Epoch 270/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0189 - accuracy: 1.0000 - val_loss: 3.4159 - val_accuracy: 0.4884\n",
            "Epoch 271/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 3.4232 - val_accuracy: 0.4884\n",
            "Epoch 272/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 3.4188 - val_accuracy: 0.4884\n",
            "Epoch 273/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0184 - accuracy: 1.0000 - val_loss: 3.4309 - val_accuracy: 0.4884\n",
            "Epoch 274/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.0187 - accuracy: 1.0000 - val_loss: 3.4456 - val_accuracy: 0.4884\n",
            "Epoch 275/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 3.4352 - val_accuracy: 0.4884\n",
            "Epoch 276/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.0177 - accuracy: 1.0000 - val_loss: 3.4576 - val_accuracy: 0.4884\n",
            "Epoch 277/300\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 3.4616 - val_accuracy: 0.4884\n",
            "Epoch 278/300\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 3.4587 - val_accuracy: 0.4884\n",
            "Epoch 279/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.0172 - accuracy: 1.0000 - val_loss: 3.4577 - val_accuracy: 0.4884\n",
            "Epoch 280/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 3.4701 - val_accuracy: 0.4884\n",
            "Epoch 281/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.0169 - accuracy: 1.0000 - val_loss: 3.4809 - val_accuracy: 0.4884\n",
            "Epoch 282/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.0167 - accuracy: 1.0000 - val_loss: 3.4821 - val_accuracy: 0.4884\n",
            "Epoch 283/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.0164 - accuracy: 1.0000 - val_loss: 3.4842 - val_accuracy: 0.4884\n",
            "Epoch 284/300\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 3.5064 - val_accuracy: 0.4884\n",
            "Epoch 285/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.0160 - accuracy: 1.0000 - val_loss: 3.4935 - val_accuracy: 0.4884\n",
            "Epoch 286/300\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 3.5021 - val_accuracy: 0.4884\n",
            "Epoch 287/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 3.5176 - val_accuracy: 0.4884\n",
            "Epoch 288/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 3.5161 - val_accuracy: 0.4884\n",
            "Epoch 289/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 3.5214 - val_accuracy: 0.4884\n",
            "Epoch 290/300\n",
            "88/88 [==============================] - 0s 288us/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 3.5217 - val_accuracy: 0.4884\n",
            "Epoch 291/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.0152 - accuracy: 1.0000 - val_loss: 3.5352 - val_accuracy: 0.4884\n",
            "Epoch 292/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.5295 - val_accuracy: 0.4884\n",
            "Epoch 293/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 3.5363 - val_accuracy: 0.5116\n",
            "Epoch 294/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 3.5531 - val_accuracy: 0.4884\n",
            "Epoch 295/300\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.5534 - val_accuracy: 0.4884\n",
            "Epoch 296/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.0143 - accuracy: 1.0000 - val_loss: 3.5572 - val_accuracy: 0.4884\n",
            "Epoch 297/300\n",
            "88/88 [==============================] - 0s 286us/step - loss: 0.0144 - accuracy: 1.0000 - val_loss: 3.5672 - val_accuracy: 0.5116\n",
            "Epoch 298/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.0141 - accuracy: 1.0000 - val_loss: 3.5578 - val_accuracy: 0.4884\n",
            "Epoch 299/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 3.5701 - val_accuracy: 0.5116\n",
            "Epoch 300/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 3.5756 - val_accuracy: 0.4884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkYxTUsmLOQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data, train_labels_dec):\n",
        "  \n",
        "  train_data_np = \n",
        "\n",
        "  partial_train_data = np.array([train_data[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "  #Z-score\n",
        "  scaler = StandardScaler()\n",
        "  partial_train_data_stand = scaler.fit_transform(partial_train_data)\n",
        "  val_data_stand = scaler.transform(val_data)\n",
        "\n",
        "  #PCA\n",
        "  pca = PCA(n_components=7, svd_solver='full')\n",
        "  pca.fit(partial_train_data_stand)\n",
        "  partial_train_data_stand_pca = pca.transform(partial_train_data_stand)\n",
        "  val_data_stand_pca = pca.transform(val_data_stand)\n",
        "\n",
        "  #Z-score after PCA\n",
        "  scaler_2 = StandardScaler()\n",
        "  partial_train_data_stand_pca_stand = scaler_2.fit_transform(partial_train_data_stand_pca)\n",
        "  val_data_stand_pca_stand = scaler_2.transform(val_data_stand_pca)\n",
        "\n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data_stand_pca_stand, one_hot_partial_train_targets, validation_data=(val_data_stand_pca_stand, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=88)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1xS2-BcJraQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "be1256fc-76bf-49dd-ce81-61483874b366"
      },
      "source": [
        "train_index"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  1,   2,   4,   5,   8,  10,  11,  12,  13,  14,  17,  20,  21,\n",
              "        22,  23,  24,  25,  26,  27,  29,  30,  31,  33,  34,  37,  38,\n",
              "        39,  40,  41,  42,  43,  46,  47,  48,  49,  50,  55,  58,  60,\n",
              "        61,  62,  63,  64,  65,  67,  69,  70,  71,  73,  75,  76,  77,\n",
              "        79,  81,  82,  83,  84,  85,  87,  88,  89,  91,  92,  94,  96,\n",
              "        97,  98,  99, 100, 101, 103, 106, 107, 108, 110, 115, 116, 117,\n",
              "       118, 119, 121, 122, 124, 126, 127, 129, 130])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "9186b836-7e3b-4d77-a67b-ae1097e62e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "51edaaba-4158-460c-8bcf-b0f4feb889f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "acaf3151-8673-4743-dadb-ee89b96dd7a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "6da816c5-be44-452a-c69a-ace68c9765a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8a162cd978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEWCAYAAABsY4yMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvHPOxaj/B4eMQ0YNQVm+AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "1b75e3f0-5d99-4482-831b-127892721259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f8a162b8710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "8bec803c-0ddc-48bd-f0a1-f57a8ef4879f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.2654 - acc: 0.3130\n",
            "Epoch 2/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.2582 - acc: 0.3206\n",
            "Epoch 3/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.2512 - acc: 0.3282\n",
            "Epoch 4/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.2441 - acc: 0.3511\n",
            "Epoch 5/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2377 - acc: 0.3588\n",
            "Epoch 6/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.2312 - acc: 0.3817\n",
            "Epoch 7/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.2248 - acc: 0.3817\n",
            "Epoch 8/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2190 - acc: 0.3893\n",
            "Epoch 9/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.2133 - acc: 0.3893\n",
            "Epoch 10/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.2078 - acc: 0.3893\n",
            "Epoch 11/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.2026 - acc: 0.3969\n",
            "Epoch 12/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1976 - acc: 0.3969\n",
            "Epoch 13/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.1924 - acc: 0.3969\n",
            "Epoch 14/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1874 - acc: 0.4046\n",
            "Epoch 15/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1828 - acc: 0.4198\n",
            "Epoch 16/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.1780 - acc: 0.4198\n",
            "Epoch 17/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.1734 - acc: 0.4275\n",
            "Epoch 18/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.1692 - acc: 0.4351\n",
            "Epoch 19/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1649 - acc: 0.4198\n",
            "Epoch 20/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.1608 - acc: 0.4198\n",
            "Epoch 21/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.1572 - acc: 0.4275\n",
            "Epoch 22/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.1535 - acc: 0.4275\n",
            "Epoch 23/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.1498 - acc: 0.4275\n",
            "Epoch 24/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1461 - acc: 0.4351\n",
            "Epoch 25/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1426 - acc: 0.4351\n",
            "Epoch 26/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.1394 - acc: 0.4351\n",
            "Epoch 27/1000\n",
            "131/131 [==============================] - 0s 225us/step - loss: 1.1360 - acc: 0.4351\n",
            "Epoch 28/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.1329 - acc: 0.4504\n",
            "Epoch 29/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1299 - acc: 0.4504\n",
            "Epoch 30/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 1.1269 - acc: 0.4504\n",
            "Epoch 31/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 1.1237 - acc: 0.4504\n",
            "Epoch 32/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.1209 - acc: 0.4504\n",
            "Epoch 33/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 1.1182 - acc: 0.4580\n",
            "Epoch 34/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.1154 - acc: 0.4580\n",
            "Epoch 35/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.1123 - acc: 0.4580\n",
            "Epoch 36/1000\n",
            "131/131 [==============================] - 0s 208us/step - loss: 1.1099 - acc: 0.4580\n",
            "Epoch 37/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.1074 - acc: 0.4656\n",
            "Epoch 38/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1051 - acc: 0.4656\n",
            "Epoch 39/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1026 - acc: 0.4733\n",
            "Epoch 40/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.1002 - acc: 0.4733\n",
            "Epoch 41/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.0979 - acc: 0.4809\n",
            "Epoch 42/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0960 - acc: 0.4809\n",
            "Epoch 43/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0938 - acc: 0.4809\n",
            "Epoch 44/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0914 - acc: 0.4809\n",
            "Epoch 45/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0893 - acc: 0.4809\n",
            "Epoch 46/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0873 - acc: 0.4809\n",
            "Epoch 47/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0852 - acc: 0.4809\n",
            "Epoch 48/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0832 - acc: 0.4885\n",
            "Epoch 49/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0815 - acc: 0.4962\n",
            "Epoch 50/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 1.0797 - acc: 0.4962\n",
            "Epoch 51/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 1.0779 - acc: 0.4962\n",
            "Epoch 52/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0761 - acc: 0.4962\n",
            "Epoch 53/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 1.0746 - acc: 0.4962\n",
            "Epoch 54/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0726 - acc: 0.4962\n",
            "Epoch 55/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0708 - acc: 0.4962\n",
            "Epoch 56/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0691 - acc: 0.5038\n",
            "Epoch 57/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0675 - acc: 0.5038\n",
            "Epoch 58/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0657 - acc: 0.5038\n",
            "Epoch 59/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0642 - acc: 0.5038\n",
            "Epoch 60/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.0625 - acc: 0.4962\n",
            "Epoch 61/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0610 - acc: 0.4962\n",
            "Epoch 62/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0594 - acc: 0.4962\n",
            "Epoch 63/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0579 - acc: 0.4962\n",
            "Epoch 64/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0564 - acc: 0.4962\n",
            "Epoch 65/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0551 - acc: 0.4962\n",
            "Epoch 66/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 1.0535 - acc: 0.4962\n",
            "Epoch 67/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0521 - acc: 0.4962\n",
            "Epoch 68/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.0506 - acc: 0.4962\n",
            "Epoch 69/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.0491 - acc: 0.4962\n",
            "Epoch 70/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0478 - acc: 0.4962\n",
            "Epoch 71/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0464 - acc: 0.4962\n",
            "Epoch 72/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 1.0452 - acc: 0.4962\n",
            "Epoch 73/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0438 - acc: 0.4962\n",
            "Epoch 74/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0426 - acc: 0.4962\n",
            "Epoch 75/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.0413 - acc: 0.4962\n",
            "Epoch 76/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0401 - acc: 0.4962\n",
            "Epoch 77/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 1.0388 - acc: 0.4962\n",
            "Epoch 78/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0375 - acc: 0.4962\n",
            "Epoch 79/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.0365 - acc: 0.4962\n",
            "Epoch 80/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.0355 - acc: 0.4962\n",
            "Epoch 81/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 1.0343 - acc: 0.4962\n",
            "Epoch 82/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0332 - acc: 0.4962\n",
            "Epoch 83/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0322 - acc: 0.4885\n",
            "Epoch 84/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0311 - acc: 0.4885\n",
            "Epoch 85/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0302 - acc: 0.4885\n",
            "Epoch 86/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 1.0291 - acc: 0.4885\n",
            "Epoch 87/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0281 - acc: 0.4885\n",
            "Epoch 88/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0271 - acc: 0.4885\n",
            "Epoch 89/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0262 - acc: 0.4885\n",
            "Epoch 90/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0250 - acc: 0.4885\n",
            "Epoch 91/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.0240 - acc: 0.4809\n",
            "Epoch 92/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0232 - acc: 0.4733\n",
            "Epoch 93/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0222 - acc: 0.4733\n",
            "Epoch 94/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0213 - acc: 0.4733\n",
            "Epoch 95/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0203 - acc: 0.4809\n",
            "Epoch 96/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0195 - acc: 0.4733\n",
            "Epoch 97/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0186 - acc: 0.4733\n",
            "Epoch 98/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0176 - acc: 0.4809\n",
            "Epoch 99/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 1.0168 - acc: 0.4809\n",
            "Epoch 100/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0161 - acc: 0.4809\n",
            "Epoch 101/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 1.0151 - acc: 0.4809\n",
            "Epoch 102/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0143 - acc: 0.4809\n",
            "Epoch 103/1000\n",
            "131/131 [==============================] - 0s 196us/step - loss: 1.0136 - acc: 0.4809\n",
            "Epoch 104/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0128 - acc: 0.4809\n",
            "Epoch 105/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0121 - acc: 0.4809\n",
            "Epoch 106/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0115 - acc: 0.4809\n",
            "Epoch 107/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0107 - acc: 0.4809\n",
            "Epoch 108/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.0101 - acc: 0.4809\n",
            "Epoch 109/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0094 - acc: 0.4809\n",
            "Epoch 110/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0087 - acc: 0.4885\n",
            "Epoch 111/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.0081 - acc: 0.4885\n",
            "Epoch 112/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0074 - acc: 0.4885\n",
            "Epoch 113/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0068 - acc: 0.4885\n",
            "Epoch 114/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0061 - acc: 0.4885\n",
            "Epoch 115/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0055 - acc: 0.4885\n",
            "Epoch 116/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0049 - acc: 0.4885\n",
            "Epoch 117/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0043 - acc: 0.4885\n",
            "Epoch 118/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.0037 - acc: 0.4885\n",
            "Epoch 119/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.0031 - acc: 0.4885\n",
            "Epoch 120/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.0024 - acc: 0.4885\n",
            "Epoch 121/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0020 - acc: 0.4885\n",
            "Epoch 122/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.0013 - acc: 0.4885\n",
            "Epoch 123/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0008 - acc: 0.4885\n",
            "Epoch 124/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0002 - acc: 0.4885\n",
            "Epoch 125/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9995 - acc: 0.4885\n",
            "Epoch 126/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9989 - acc: 0.4885\n",
            "Epoch 127/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9983 - acc: 0.4885\n",
            "Epoch 128/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9978 - acc: 0.4885\n",
            "Epoch 129/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9972 - acc: 0.4885\n",
            "Epoch 130/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9966 - acc: 0.4885\n",
            "Epoch 131/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9960 - acc: 0.4885\n",
            "Epoch 132/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9954 - acc: 0.4885\n",
            "Epoch 133/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9948 - acc: 0.4885\n",
            "Epoch 134/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9943 - acc: 0.4885\n",
            "Epoch 135/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9937 - acc: 0.4885\n",
            "Epoch 136/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9930 - acc: 0.4962\n",
            "Epoch 137/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9923 - acc: 0.4962\n",
            "Epoch 138/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9918 - acc: 0.4962\n",
            "Epoch 139/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9913 - acc: 0.4962\n",
            "Epoch 140/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9907 - acc: 0.4962\n",
            "Epoch 141/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9900 - acc: 0.5038\n",
            "Epoch 142/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9894 - acc: 0.5038\n",
            "Epoch 143/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9890 - acc: 0.5038\n",
            "Epoch 144/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9884 - acc: 0.5115\n",
            "Epoch 145/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9880 - acc: 0.5038\n",
            "Epoch 146/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9874 - acc: 0.5115\n",
            "Epoch 147/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9869 - acc: 0.5115\n",
            "Epoch 148/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9864 - acc: 0.5115\n",
            "Epoch 149/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9859 - acc: 0.5115\n",
            "Epoch 150/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9854 - acc: 0.5191\n",
            "Epoch 151/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9849 - acc: 0.5191\n",
            "Epoch 152/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9845 - acc: 0.5191\n",
            "Epoch 153/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9840 - acc: 0.5191\n",
            "Epoch 154/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9836 - acc: 0.5191\n",
            "Epoch 155/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9831 - acc: 0.5191\n",
            "Epoch 156/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9826 - acc: 0.5191\n",
            "Epoch 157/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9821 - acc: 0.5267\n",
            "Epoch 158/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9817 - acc: 0.5420\n",
            "Epoch 159/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9812 - acc: 0.5420\n",
            "Epoch 160/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9807 - acc: 0.5420\n",
            "Epoch 161/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9802 - acc: 0.5344\n",
            "Epoch 162/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9797 - acc: 0.5420\n",
            "Epoch 163/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9793 - acc: 0.5420\n",
            "Epoch 164/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9788 - acc: 0.5420\n",
            "Epoch 165/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9783 - acc: 0.5420\n",
            "Epoch 166/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9778 - acc: 0.5420\n",
            "Epoch 167/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9773 - acc: 0.5420\n",
            "Epoch 168/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9768 - acc: 0.5420\n",
            "Epoch 169/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9765 - acc: 0.5420\n",
            "Epoch 170/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9760 - acc: 0.5420\n",
            "Epoch 171/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9755 - acc: 0.5420\n",
            "Epoch 172/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9750 - acc: 0.5420\n",
            "Epoch 173/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9746 - acc: 0.5420\n",
            "Epoch 174/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9743 - acc: 0.5420\n",
            "Epoch 175/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9738 - acc: 0.5420\n",
            "Epoch 176/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9733 - acc: 0.5420\n",
            "Epoch 177/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9727 - acc: 0.5420\n",
            "Epoch 178/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9725 - acc: 0.5420\n",
            "Epoch 179/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9719 - acc: 0.5344\n",
            "Epoch 180/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9715 - acc: 0.5344\n",
            "Epoch 181/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9711 - acc: 0.5344\n",
            "Epoch 182/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9707 - acc: 0.5344\n",
            "Epoch 183/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9702 - acc: 0.5344\n",
            "Epoch 184/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9699 - acc: 0.5344\n",
            "Epoch 185/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9693 - acc: 0.5344\n",
            "Epoch 186/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.9690 - acc: 0.5344\n",
            "Epoch 187/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9686 - acc: 0.5344\n",
            "Epoch 188/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9682 - acc: 0.5344\n",
            "Epoch 189/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9679 - acc: 0.5344\n",
            "Epoch 190/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9676 - acc: 0.5344\n",
            "Epoch 191/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9670 - acc: 0.5344\n",
            "Epoch 192/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9666 - acc: 0.5344\n",
            "Epoch 193/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9662 - acc: 0.5344\n",
            "Epoch 194/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9658 - acc: 0.5344\n",
            "Epoch 195/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9655 - acc: 0.5344\n",
            "Epoch 196/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9651 - acc: 0.5344\n",
            "Epoch 197/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9648 - acc: 0.5344\n",
            "Epoch 198/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9645 - acc: 0.5344\n",
            "Epoch 199/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9641 - acc: 0.5344\n",
            "Epoch 200/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9638 - acc: 0.5344\n",
            "Epoch 201/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9635 - acc: 0.5344\n",
            "Epoch 202/1000\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.9631 - acc: 0.5344\n",
            "Epoch 203/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9628 - acc: 0.5344\n",
            "Epoch 204/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9625 - acc: 0.5344\n",
            "Epoch 205/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9621 - acc: 0.5344\n",
            "Epoch 206/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9619 - acc: 0.5344\n",
            "Epoch 207/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9617 - acc: 0.5344\n",
            "Epoch 208/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9613 - acc: 0.5344\n",
            "Epoch 209/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9610 - acc: 0.5344\n",
            "Epoch 210/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9607 - acc: 0.5344\n",
            "Epoch 211/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9603 - acc: 0.5344\n",
            "Epoch 212/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9600 - acc: 0.5344\n",
            "Epoch 213/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9598 - acc: 0.5344\n",
            "Epoch 214/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9595 - acc: 0.5344\n",
            "Epoch 215/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9591 - acc: 0.5344\n",
            "Epoch 216/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9589 - acc: 0.5344\n",
            "Epoch 217/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9585 - acc: 0.5420\n",
            "Epoch 218/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9584 - acc: 0.5344\n",
            "Epoch 219/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9580 - acc: 0.5420\n",
            "Epoch 220/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9577 - acc: 0.5420\n",
            "Epoch 221/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9575 - acc: 0.5420\n",
            "Epoch 222/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9571 - acc: 0.5496\n",
            "Epoch 223/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9568 - acc: 0.5420\n",
            "Epoch 224/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9565 - acc: 0.5496\n",
            "Epoch 225/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9563 - acc: 0.5496\n",
            "Epoch 226/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9561 - acc: 0.5496\n",
            "Epoch 227/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9557 - acc: 0.5496\n",
            "Epoch 228/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9554 - acc: 0.5496\n",
            "Epoch 229/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9551 - acc: 0.5496\n",
            "Epoch 230/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9548 - acc: 0.5496\n",
            "Epoch 231/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9546 - acc: 0.5496\n",
            "Epoch 232/1000\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.9544 - acc: 0.5496\n",
            "Epoch 233/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9541 - acc: 0.5496\n",
            "Epoch 234/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9540 - acc: 0.5344\n",
            "Epoch 235/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9536 - acc: 0.5420\n",
            "Epoch 236/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9535 - acc: 0.5344\n",
            "Epoch 237/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9532 - acc: 0.5344\n",
            "Epoch 238/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9529 - acc: 0.5420\n",
            "Epoch 239/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9526 - acc: 0.5420\n",
            "Epoch 240/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9523 - acc: 0.5420\n",
            "Epoch 241/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9522 - acc: 0.5420\n",
            "Epoch 242/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9519 - acc: 0.5420\n",
            "Epoch 243/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9517 - acc: 0.5420\n",
            "Epoch 244/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9514 - acc: 0.5420\n",
            "Epoch 245/1000\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.9512 - acc: 0.5344\n",
            "Epoch 246/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9509 - acc: 0.5344\n",
            "Epoch 247/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9506 - acc: 0.5344\n",
            "Epoch 248/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9505 - acc: 0.5344\n",
            "Epoch 249/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9501 - acc: 0.5420\n",
            "Epoch 250/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9499 - acc: 0.5420\n",
            "Epoch 251/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9496 - acc: 0.5344\n",
            "Epoch 252/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9494 - acc: 0.5344\n",
            "Epoch 253/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9492 - acc: 0.5344\n",
            "Epoch 254/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9489 - acc: 0.5420\n",
            "Epoch 255/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9488 - acc: 0.5420\n",
            "Epoch 256/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9485 - acc: 0.5420\n",
            "Epoch 257/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9483 - acc: 0.5420\n",
            "Epoch 258/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9480 - acc: 0.5420\n",
            "Epoch 259/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9478 - acc: 0.5344\n",
            "Epoch 260/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9476 - acc: 0.5420\n",
            "Epoch 261/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9475 - acc: 0.5420\n",
            "Epoch 262/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9472 - acc: 0.5420\n",
            "Epoch 263/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9470 - acc: 0.5420\n",
            "Epoch 264/1000\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.9467 - acc: 0.5496\n",
            "Epoch 265/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9466 - acc: 0.5496\n",
            "Epoch 266/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9463 - acc: 0.5496\n",
            "Epoch 267/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9461 - acc: 0.5496\n",
            "Epoch 268/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9459 - acc: 0.5496\n",
            "Epoch 269/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9457 - acc: 0.5573\n",
            "Epoch 270/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9456 - acc: 0.5573\n",
            "Epoch 271/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9453 - acc: 0.5649\n",
            "Epoch 272/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9451 - acc: 0.5649\n",
            "Epoch 273/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9449 - acc: 0.5573\n",
            "Epoch 274/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9447 - acc: 0.5573\n",
            "Epoch 275/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9445 - acc: 0.5649\n",
            "Epoch 276/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9443 - acc: 0.5649\n",
            "Epoch 277/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9441 - acc: 0.5725\n",
            "Epoch 278/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9439 - acc: 0.5649\n",
            "Epoch 279/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9437 - acc: 0.5725\n",
            "Epoch 280/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9435 - acc: 0.5725\n",
            "Epoch 281/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9435 - acc: 0.5725\n",
            "Epoch 282/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9432 - acc: 0.5725\n",
            "Epoch 283/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9430 - acc: 0.5725\n",
            "Epoch 284/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9428 - acc: 0.5725\n",
            "Epoch 285/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9426 - acc: 0.5725\n",
            "Epoch 286/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9423 - acc: 0.5725\n",
            "Epoch 287/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9422 - acc: 0.5725\n",
            "Epoch 288/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9420 - acc: 0.5725\n",
            "Epoch 289/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9418 - acc: 0.5725\n",
            "Epoch 290/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9417 - acc: 0.5725\n",
            "Epoch 291/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9414 - acc: 0.5725\n",
            "Epoch 292/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9413 - acc: 0.5725\n",
            "Epoch 293/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9411 - acc: 0.5725\n",
            "Epoch 294/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9409 - acc: 0.5725\n",
            "Epoch 295/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9408 - acc: 0.5725\n",
            "Epoch 296/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9405 - acc: 0.5725\n",
            "Epoch 297/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9404 - acc: 0.5725\n",
            "Epoch 298/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9402 - acc: 0.5725\n",
            "Epoch 299/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9401 - acc: 0.5725\n",
            "Epoch 300/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9399 - acc: 0.5725\n",
            "Epoch 301/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.9398 - acc: 0.5725\n",
            "Epoch 302/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9397 - acc: 0.5725\n",
            "Epoch 303/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9395 - acc: 0.5725\n",
            "Epoch 304/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9392 - acc: 0.5725\n",
            "Epoch 305/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9391 - acc: 0.5725\n",
            "Epoch 306/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 0.9390 - acc: 0.5725\n",
            "Epoch 307/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9388 - acc: 0.5725\n",
            "Epoch 308/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9386 - acc: 0.5725\n",
            "Epoch 309/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9386 - acc: 0.5725\n",
            "Epoch 310/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9384 - acc: 0.5725\n",
            "Epoch 311/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9381 - acc: 0.5725\n",
            "Epoch 312/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9379 - acc: 0.5725\n",
            "Epoch 313/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9378 - acc: 0.5725\n",
            "Epoch 314/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9376 - acc: 0.5725\n",
            "Epoch 315/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9375 - acc: 0.5725\n",
            "Epoch 316/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9375 - acc: 0.5725\n",
            "Epoch 317/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9372 - acc: 0.5725\n",
            "Epoch 318/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9370 - acc: 0.5725\n",
            "Epoch 319/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9368 - acc: 0.5725\n",
            "Epoch 320/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9368 - acc: 0.5725\n",
            "Epoch 321/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9365 - acc: 0.5725\n",
            "Epoch 322/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9363 - acc: 0.5725\n",
            "Epoch 323/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9364 - acc: 0.5725\n",
            "Epoch 324/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9360 - acc: 0.5725\n",
            "Epoch 325/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9358 - acc: 0.5725\n",
            "Epoch 326/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9357 - acc: 0.5725\n",
            "Epoch 327/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9355 - acc: 0.5725\n",
            "Epoch 328/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9354 - acc: 0.5725\n",
            "Epoch 329/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9352 - acc: 0.5725\n",
            "Epoch 330/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9351 - acc: 0.5725\n",
            "Epoch 331/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9351 - acc: 0.5725\n",
            "Epoch 332/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9349 - acc: 0.5725\n",
            "Epoch 333/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.9346 - acc: 0.5802\n",
            "Epoch 334/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9344 - acc: 0.5802\n",
            "Epoch 335/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9343 - acc: 0.5802\n",
            "Epoch 336/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9342 - acc: 0.5802\n",
            "Epoch 337/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9341 - acc: 0.5802\n",
            "Epoch 338/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9339 - acc: 0.5802\n",
            "Epoch 339/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9337 - acc: 0.5802\n",
            "Epoch 340/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9335 - acc: 0.5802\n",
            "Epoch 341/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9334 - acc: 0.5802\n",
            "Epoch 342/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9333 - acc: 0.5802\n",
            "Epoch 343/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9331 - acc: 0.5802\n",
            "Epoch 344/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 345/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 346/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9326 - acc: 0.5802\n",
            "Epoch 347/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9324 - acc: 0.5802\n",
            "Epoch 348/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9324 - acc: 0.5802\n",
            "Epoch 349/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9322 - acc: 0.5878\n",
            "Epoch 350/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9320 - acc: 0.5878\n",
            "Epoch 351/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9319 - acc: 0.5878\n",
            "Epoch 352/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9318 - acc: 0.5878\n",
            "Epoch 353/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9316 - acc: 0.5954\n",
            "Epoch 354/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9315 - acc: 0.5878\n",
            "Epoch 355/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9313 - acc: 0.5954\n",
            "Epoch 356/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9312 - acc: 0.5954\n",
            "Epoch 357/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9310 - acc: 0.5954\n",
            "Epoch 358/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9309 - acc: 0.5954\n",
            "Epoch 359/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9307 - acc: 0.5954\n",
            "Epoch 360/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9306 - acc: 0.5954\n",
            "Epoch 361/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9304 - acc: 0.5954\n",
            "Epoch 362/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9303 - acc: 0.5954\n",
            "Epoch 363/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9301 - acc: 0.5954\n",
            "Epoch 364/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9299 - acc: 0.5954\n",
            "Epoch 365/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9299 - acc: 0.5954\n",
            "Epoch 366/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9298 - acc: 0.5954\n",
            "Epoch 367/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9296 - acc: 0.5954\n",
            "Epoch 368/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9294 - acc: 0.5954\n",
            "Epoch 369/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9293 - acc: 0.5954\n",
            "Epoch 370/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9293 - acc: 0.5954\n",
            "Epoch 371/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9292 - acc: 0.5954\n",
            "Epoch 372/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9290 - acc: 0.5878\n",
            "Epoch 373/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9289 - acc: 0.5878\n",
            "Epoch 374/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9286 - acc: 0.5878\n",
            "Epoch 375/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9286 - acc: 0.5878\n",
            "Epoch 376/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9284 - acc: 0.5878\n",
            "Epoch 377/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9283 - acc: 0.5878\n",
            "Epoch 378/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9282 - acc: 0.5878\n",
            "Epoch 379/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9280 - acc: 0.5878\n",
            "Epoch 380/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9279 - acc: 0.5878\n",
            "Epoch 381/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9278 - acc: 0.5878\n",
            "Epoch 382/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9277 - acc: 0.5878\n",
            "Epoch 383/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9275 - acc: 0.5878\n",
            "Epoch 384/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9275 - acc: 0.5878\n",
            "Epoch 385/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9273 - acc: 0.5878\n",
            "Epoch 386/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9273 - acc: 0.5878\n",
            "Epoch 387/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9272 - acc: 0.5878\n",
            "Epoch 388/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9270 - acc: 0.5878\n",
            "Epoch 389/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9269 - acc: 0.5878\n",
            "Epoch 390/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9267 - acc: 0.5878\n",
            "Epoch 391/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9267 - acc: 0.5878\n",
            "Epoch 392/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9266 - acc: 0.5878\n",
            "Epoch 393/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9263 - acc: 0.5878\n",
            "Epoch 394/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9263 - acc: 0.5954\n",
            "Epoch 395/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9261 - acc: 0.5878\n",
            "Epoch 396/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9259 - acc: 0.5878\n",
            "Epoch 397/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9258 - acc: 0.5954\n",
            "Epoch 398/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9257 - acc: 0.5878\n",
            "Epoch 399/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9257 - acc: 0.5954\n",
            "Epoch 400/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9256 - acc: 0.5954\n",
            "Epoch 401/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9253 - acc: 0.5954\n",
            "Epoch 402/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9252 - acc: 0.5954\n",
            "Epoch 403/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9251 - acc: 0.5954\n",
            "Epoch 404/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9250 - acc: 0.5954\n",
            "Epoch 405/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9249 - acc: 0.5954\n",
            "Epoch 406/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9247 - acc: 0.5954\n",
            "Epoch 407/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9249 - acc: 0.5954\n",
            "Epoch 408/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9245 - acc: 0.5954\n",
            "Epoch 409/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9245 - acc: 0.5954\n",
            "Epoch 410/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9243 - acc: 0.5954\n",
            "Epoch 411/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9241 - acc: 0.5954\n",
            "Epoch 412/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9239 - acc: 0.5954\n",
            "Epoch 413/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9239 - acc: 0.5954\n",
            "Epoch 414/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9237 - acc: 0.5954\n",
            "Epoch 415/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9235 - acc: 0.5954\n",
            "Epoch 416/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9235 - acc: 0.5954\n",
            "Epoch 417/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9234 - acc: 0.5954\n",
            "Epoch 418/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9231 - acc: 0.5954\n",
            "Epoch 419/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9230 - acc: 0.5954\n",
            "Epoch 420/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9229 - acc: 0.5954\n",
            "Epoch 421/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9229 - acc: 0.5954\n",
            "Epoch 422/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9227 - acc: 0.5954\n",
            "Epoch 423/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9226 - acc: 0.5954\n",
            "Epoch 424/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9224 - acc: 0.5954\n",
            "Epoch 425/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9224 - acc: 0.5954\n",
            "Epoch 426/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9222 - acc: 0.5954\n",
            "Epoch 427/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9220 - acc: 0.5954\n",
            "Epoch 428/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9219 - acc: 0.5954\n",
            "Epoch 429/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9219 - acc: 0.5954\n",
            "Epoch 430/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9216 - acc: 0.5954\n",
            "Epoch 431/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9215 - acc: 0.5954\n",
            "Epoch 432/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9215 - acc: 0.5954\n",
            "Epoch 433/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9214 - acc: 0.5954\n",
            "Epoch 434/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9213 - acc: 0.5954\n",
            "Epoch 435/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9210 - acc: 0.5954\n",
            "Epoch 436/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9210 - acc: 0.5954\n",
            "Epoch 437/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9209 - acc: 0.5954\n",
            "Epoch 438/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9207 - acc: 0.6031\n",
            "Epoch 439/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9206 - acc: 0.6031\n",
            "Epoch 440/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9204 - acc: 0.6031\n",
            "Epoch 441/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9204 - acc: 0.6031\n",
            "Epoch 442/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9203 - acc: 0.6031\n",
            "Epoch 443/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9201 - acc: 0.6107\n",
            "Epoch 444/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9200 - acc: 0.6107\n",
            "Epoch 445/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9199 - acc: 0.6107\n",
            "Epoch 446/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9198 - acc: 0.6031\n",
            "Epoch 447/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9198 - acc: 0.6107\n",
            "Epoch 448/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9196 - acc: 0.6031\n",
            "Epoch 449/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9194 - acc: 0.6107\n",
            "Epoch 450/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9193 - acc: 0.6031\n",
            "Epoch 451/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9191 - acc: 0.6031\n",
            "Epoch 452/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9191 - acc: 0.6031\n",
            "Epoch 453/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9188 - acc: 0.5954\n",
            "Epoch 454/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9187 - acc: 0.5954\n",
            "Epoch 455/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9186 - acc: 0.6031\n",
            "Epoch 456/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9185 - acc: 0.5954\n",
            "Epoch 457/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9184 - acc: 0.6031\n",
            "Epoch 458/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9183 - acc: 0.6031\n",
            "Epoch 459/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9181 - acc: 0.6031\n",
            "Epoch 460/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9182 - acc: 0.5954\n",
            "Epoch 461/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9180 - acc: 0.6031\n",
            "Epoch 462/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9179 - acc: 0.5954\n",
            "Epoch 463/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9178 - acc: 0.6031\n",
            "Epoch 464/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9176 - acc: 0.5954\n",
            "Epoch 465/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9175 - acc: 0.5954\n",
            "Epoch 466/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9174 - acc: 0.6031\n",
            "Epoch 467/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9173 - acc: 0.5954\n",
            "Epoch 468/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9172 - acc: 0.5954\n",
            "Epoch 469/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9171 - acc: 0.5954\n",
            "Epoch 470/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9169 - acc: 0.6031\n",
            "Epoch 471/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9168 - acc: 0.6031\n",
            "Epoch 472/1000\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.9167 - acc: 0.6031\n",
            "Epoch 473/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9165 - acc: 0.6031\n",
            "Epoch 474/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9164 - acc: 0.6031\n",
            "Epoch 475/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9164 - acc: 0.5954\n",
            "Epoch 476/1000\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.9164 - acc: 0.5954\n",
            "Epoch 477/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9161 - acc: 0.6031\n",
            "Epoch 478/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9159 - acc: 0.6031\n",
            "Epoch 479/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9159 - acc: 0.6031\n",
            "Epoch 480/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9157 - acc: 0.6031\n",
            "Epoch 481/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9157 - acc: 0.6031\n",
            "Epoch 482/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9155 - acc: 0.6031\n",
            "Epoch 483/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9154 - acc: 0.6031\n",
            "Epoch 484/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9154 - acc: 0.6031\n",
            "Epoch 485/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9151 - acc: 0.6031\n",
            "Epoch 486/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9151 - acc: 0.5954\n",
            "Epoch 487/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9149 - acc: 0.6031\n",
            "Epoch 488/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9148 - acc: 0.6031\n",
            "Epoch 489/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9146 - acc: 0.6031\n",
            "Epoch 490/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9145 - acc: 0.6031\n",
            "Epoch 491/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9144 - acc: 0.6031\n",
            "Epoch 492/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9142 - acc: 0.5954\n",
            "Epoch 493/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9141 - acc: 0.5954\n",
            "Epoch 494/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9140 - acc: 0.5954\n",
            "Epoch 495/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9141 - acc: 0.6031\n",
            "Epoch 496/1000\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.9138 - acc: 0.5954\n",
            "Epoch 497/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9137 - acc: 0.6031\n",
            "Epoch 498/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9135 - acc: 0.5954\n",
            "Epoch 499/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9135 - acc: 0.5954\n",
            "Epoch 500/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.9134 - acc: 0.5954\n",
            "Epoch 501/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9132 - acc: 0.5954\n",
            "Epoch 502/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9132 - acc: 0.5954\n",
            "Epoch 503/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9130 - acc: 0.6031\n",
            "Epoch 504/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9128 - acc: 0.6031\n",
            "Epoch 505/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9128 - acc: 0.6031\n",
            "Epoch 506/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9127 - acc: 0.6031\n",
            "Epoch 507/1000\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.9126 - acc: 0.6031\n",
            "Epoch 508/1000\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.9125 - acc: 0.6031\n",
            "Epoch 509/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9123 - acc: 0.6031\n",
            "Epoch 510/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9122 - acc: 0.6031\n",
            "Epoch 511/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9121 - acc: 0.6031\n",
            "Epoch 512/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9119 - acc: 0.6031\n",
            "Epoch 513/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9120 - acc: 0.6031\n",
            "Epoch 514/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9118 - acc: 0.5954\n",
            "Epoch 515/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9116 - acc: 0.5954\n",
            "Epoch 516/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9115 - acc: 0.5954\n",
            "Epoch 517/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9114 - acc: 0.5954\n",
            "Epoch 518/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9113 - acc: 0.5954\n",
            "Epoch 519/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9112 - acc: 0.5954\n",
            "Epoch 520/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9112 - acc: 0.5954\n",
            "Epoch 521/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9109 - acc: 0.6031\n",
            "Epoch 522/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9109 - acc: 0.5954\n",
            "Epoch 523/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9107 - acc: 0.6031\n",
            "Epoch 524/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9106 - acc: 0.6031\n",
            "Epoch 525/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9106 - acc: 0.5954\n",
            "Epoch 526/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9105 - acc: 0.5954\n",
            "Epoch 527/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9103 - acc: 0.5954\n",
            "Epoch 528/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9103 - acc: 0.5954\n",
            "Epoch 529/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9101 - acc: 0.5954\n",
            "Epoch 530/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9100 - acc: 0.5954\n",
            "Epoch 531/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9099 - acc: 0.6031\n",
            "Epoch 532/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9097 - acc: 0.5954\n",
            "Epoch 533/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9096 - acc: 0.6031\n",
            "Epoch 534/1000\n",
            "131/131 [==============================] - 0s 240us/step - loss: 0.9096 - acc: 0.6031\n",
            "Epoch 535/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9095 - acc: 0.6031\n",
            "Epoch 536/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9094 - acc: 0.6031\n",
            "Epoch 537/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9093 - acc: 0.5954\n",
            "Epoch 538/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9091 - acc: 0.6031\n",
            "Epoch 539/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9092 - acc: 0.6031\n",
            "Epoch 540/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9089 - acc: 0.6031\n",
            "Epoch 541/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9089 - acc: 0.5954\n",
            "Epoch 542/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9087 - acc: 0.5954\n",
            "Epoch 543/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9086 - acc: 0.6031\n",
            "Epoch 544/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9085 - acc: 0.5954\n",
            "Epoch 545/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9086 - acc: 0.6031\n",
            "Epoch 546/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9083 - acc: 0.6031\n",
            "Epoch 547/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9083 - acc: 0.5954\n",
            "Epoch 548/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9082 - acc: 0.6031\n",
            "Epoch 549/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9080 - acc: 0.5954\n",
            "Epoch 550/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9080 - acc: 0.6031\n",
            "Epoch 551/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9079 - acc: 0.5954\n",
            "Epoch 552/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9079 - acc: 0.5954\n",
            "Epoch 553/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9077 - acc: 0.5954\n",
            "Epoch 554/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9077 - acc: 0.6031\n",
            "Epoch 555/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9074 - acc: 0.6031\n",
            "Epoch 556/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9074 - acc: 0.5954\n",
            "Epoch 557/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9074 - acc: 0.5954\n",
            "Epoch 558/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9072 - acc: 0.5954\n",
            "Epoch 559/1000\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.9071 - acc: 0.6031\n",
            "Epoch 560/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9070 - acc: 0.6031\n",
            "Epoch 561/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9070 - acc: 0.5954\n",
            "Epoch 562/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9069 - acc: 0.5954\n",
            "Epoch 563/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9069 - acc: 0.5954\n",
            "Epoch 564/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9067 - acc: 0.6031\n",
            "Epoch 565/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9066 - acc: 0.5954\n",
            "Epoch 566/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9065 - acc: 0.5954\n",
            "Epoch 567/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9063 - acc: 0.5954\n",
            "Epoch 568/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9063 - acc: 0.5954\n",
            "Epoch 569/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9062 - acc: 0.5954\n",
            "Epoch 570/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9062 - acc: 0.5954\n",
            "Epoch 571/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9061 - acc: 0.5954\n",
            "Epoch 572/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9060 - acc: 0.6031\n",
            "Epoch 573/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9060 - acc: 0.6031\n",
            "Epoch 574/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9058 - acc: 0.6031\n",
            "Epoch 575/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9057 - acc: 0.6031\n",
            "Epoch 576/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9056 - acc: 0.6031\n",
            "Epoch 577/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9055 - acc: 0.6031\n",
            "Epoch 578/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9055 - acc: 0.6031\n",
            "Epoch 579/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9052 - acc: 0.6031\n",
            "Epoch 580/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9052 - acc: 0.6031\n",
            "Epoch 581/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9051 - acc: 0.6031\n",
            "Epoch 582/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9049 - acc: 0.6031\n",
            "Epoch 583/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9051 - acc: 0.5954\n",
            "Epoch 584/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9048 - acc: 0.6031\n",
            "Epoch 585/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9047 - acc: 0.6031\n",
            "Epoch 586/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9046 - acc: 0.6031\n",
            "Epoch 587/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9045 - acc: 0.6031\n",
            "Epoch 588/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9043 - acc: 0.6031\n",
            "Epoch 589/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9045 - acc: 0.6031\n",
            "Epoch 590/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9043 - acc: 0.6031\n",
            "Epoch 591/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9042 - acc: 0.6031\n",
            "Epoch 592/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9041 - acc: 0.6031\n",
            "Epoch 593/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9041 - acc: 0.6031\n",
            "Epoch 594/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9038 - acc: 0.5954\n",
            "Epoch 595/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9038 - acc: 0.5954\n",
            "Epoch 596/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9037 - acc: 0.5954\n",
            "Epoch 597/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 598/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 599/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 600/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 601/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 602/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 603/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9030 - acc: 0.5954\n",
            "Epoch 604/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9030 - acc: 0.5954\n",
            "Epoch 605/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9029 - acc: 0.5954\n",
            "Epoch 606/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9028 - acc: 0.5954\n",
            "Epoch 607/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9027 - acc: 0.5954\n",
            "Epoch 608/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9026 - acc: 0.5954\n",
            "Epoch 609/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9026 - acc: 0.6031\n",
            "Epoch 610/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9024 - acc: 0.5954\n",
            "Epoch 611/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9023 - acc: 0.5954\n",
            "Epoch 612/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9023 - acc: 0.5954\n",
            "Epoch 613/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9022 - acc: 0.5954\n",
            "Epoch 614/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9021 - acc: 0.5954\n",
            "Epoch 615/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9020 - acc: 0.5954\n",
            "Epoch 616/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9019 - acc: 0.5954\n",
            "Epoch 617/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9017 - acc: 0.5954\n",
            "Epoch 618/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9017 - acc: 0.5954\n",
            "Epoch 619/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9016 - acc: 0.5954\n",
            "Epoch 620/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9016 - acc: 0.6031\n",
            "Epoch 621/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9015 - acc: 0.5954\n",
            "Epoch 622/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9012 - acc: 0.6031\n",
            "Epoch 623/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9012 - acc: 0.5954\n",
            "Epoch 624/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9011 - acc: 0.6031\n",
            "Epoch 625/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9012 - acc: 0.6031\n",
            "Epoch 626/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9009 - acc: 0.6031\n",
            "Epoch 627/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9008 - acc: 0.6031\n",
            "Epoch 628/1000\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.9008 - acc: 0.6031\n",
            "Epoch 629/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9007 - acc: 0.6031\n",
            "Epoch 630/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9006 - acc: 0.6031\n",
            "Epoch 631/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9005 - acc: 0.6031\n",
            "Epoch 632/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9005 - acc: 0.6031\n",
            "Epoch 633/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9004 - acc: 0.5954\n",
            "Epoch 634/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9003 - acc: 0.6031\n",
            "Epoch 635/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9002 - acc: 0.6031\n",
            "Epoch 636/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9002 - acc: 0.6031\n",
            "Epoch 637/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9001 - acc: 0.6031\n",
            "Epoch 638/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9001 - acc: 0.6031\n",
            "Epoch 639/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8998 - acc: 0.6031\n",
            "Epoch 640/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8999 - acc: 0.6031\n",
            "Epoch 641/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8996 - acc: 0.6031\n",
            "Epoch 642/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8996 - acc: 0.5954\n",
            "Epoch 643/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8995 - acc: 0.6031\n",
            "Epoch 644/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8994 - acc: 0.6031\n",
            "Epoch 645/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8994 - acc: 0.6031\n",
            "Epoch 646/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8993 - acc: 0.5954\n",
            "Epoch 647/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8991 - acc: 0.6031\n",
            "Epoch 648/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8991 - acc: 0.6107\n",
            "Epoch 649/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8990 - acc: 0.6107\n",
            "Epoch 650/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8989 - acc: 0.6107\n",
            "Epoch 651/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8989 - acc: 0.5954\n",
            "Epoch 652/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8987 - acc: 0.6031\n",
            "Epoch 653/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8987 - acc: 0.6107\n",
            "Epoch 654/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8985 - acc: 0.6107\n",
            "Epoch 655/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8985 - acc: 0.6031\n",
            "Epoch 656/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8984 - acc: 0.6031\n",
            "Epoch 657/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 658/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 659/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 660/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8981 - acc: 0.6031\n",
            "Epoch 661/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 662/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8980 - acc: 0.6031\n",
            "Epoch 663/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 664/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 665/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8976 - acc: 0.6031\n",
            "Epoch 666/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8974 - acc: 0.6031\n",
            "Epoch 667/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8975 - acc: 0.6107\n",
            "Epoch 668/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.8973 - acc: 0.6031\n",
            "Epoch 669/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8973 - acc: 0.6031\n",
            "Epoch 670/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8972 - acc: 0.6031\n",
            "Epoch 671/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8971 - acc: 0.6031\n",
            "Epoch 672/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8970 - acc: 0.6031\n",
            "Epoch 673/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8969 - acc: 0.6031\n",
            "Epoch 674/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8968 - acc: 0.6031\n",
            "Epoch 675/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8967 - acc: 0.6031\n",
            "Epoch 676/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8966 - acc: 0.6031\n",
            "Epoch 677/1000\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.8966 - acc: 0.6031\n",
            "Epoch 678/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8965 - acc: 0.6107\n",
            "Epoch 679/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8964 - acc: 0.6107\n",
            "Epoch 680/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8963 - acc: 0.5954\n",
            "Epoch 681/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8962 - acc: 0.6031\n",
            "Epoch 682/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8962 - acc: 0.6031\n",
            "Epoch 683/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8960 - acc: 0.6031\n",
            "Epoch 684/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8960 - acc: 0.6107\n",
            "Epoch 685/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8958 - acc: 0.6031\n",
            "Epoch 686/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8958 - acc: 0.6031\n",
            "Epoch 687/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8959 - acc: 0.6107\n",
            "Epoch 688/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8956 - acc: 0.6031\n",
            "Epoch 689/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8957 - acc: 0.5954\n",
            "Epoch 690/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.8955 - acc: 0.6031\n",
            "Epoch 691/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8953 - acc: 0.5954\n",
            "Epoch 692/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8954 - acc: 0.5954\n",
            "Epoch 693/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8952 - acc: 0.6031\n",
            "Epoch 694/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8952 - acc: 0.5954\n",
            "Epoch 695/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8952 - acc: 0.6031\n",
            "Epoch 696/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8951 - acc: 0.6031\n",
            "Epoch 697/1000\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 698/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 699/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 700/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8947 - acc: 0.6031\n",
            "Epoch 701/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8945 - acc: 0.6031\n",
            "Epoch 702/1000\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.8946 - acc: 0.5954\n",
            "Epoch 703/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8944 - acc: 0.5954\n",
            "Epoch 704/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8944 - acc: 0.5954\n",
            "Epoch 705/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8946 - acc: 0.5954\n",
            "Epoch 706/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8942 - acc: 0.6031\n",
            "Epoch 707/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8941 - acc: 0.6031\n",
            "Epoch 708/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8941 - acc: 0.6031\n",
            "Epoch 709/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8940 - acc: 0.6031\n",
            "Epoch 710/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8939 - acc: 0.6031\n",
            "Epoch 711/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8939 - acc: 0.6031\n",
            "Epoch 712/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8938 - acc: 0.6031\n",
            "Epoch 713/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8938 - acc: 0.6031\n",
            "Epoch 714/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8937 - acc: 0.6031\n",
            "Epoch 715/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8936 - acc: 0.6031\n",
            "Epoch 716/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8935 - acc: 0.6031\n",
            "Epoch 717/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8933 - acc: 0.6031\n",
            "Epoch 718/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8934 - acc: 0.6031\n",
            "Epoch 719/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8933 - acc: 0.6031\n",
            "Epoch 720/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8931 - acc: 0.6031\n",
            "Epoch 721/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8931 - acc: 0.6031\n",
            "Epoch 722/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8930 - acc: 0.6031\n",
            "Epoch 723/1000\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.8929 - acc: 0.6031\n",
            "Epoch 724/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8929 - acc: 0.6031\n",
            "Epoch 725/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8928 - acc: 0.6107\n",
            "Epoch 726/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8927 - acc: 0.6107\n",
            "Epoch 727/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8927 - acc: 0.6031\n",
            "Epoch 728/1000\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.8926 - acc: 0.6031\n",
            "Epoch 729/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8924 - acc: 0.6031\n",
            "Epoch 730/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8924 - acc: 0.6031\n",
            "Epoch 731/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8923 - acc: 0.6031\n",
            "Epoch 732/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8922 - acc: 0.6031\n",
            "Epoch 733/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8923 - acc: 0.6031\n",
            "Epoch 734/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8921 - acc: 0.6031\n",
            "Epoch 735/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8920 - acc: 0.6031\n",
            "Epoch 736/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8919 - acc: 0.6031\n",
            "Epoch 737/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8918 - acc: 0.6031\n",
            "Epoch 738/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8917 - acc: 0.6031\n",
            "Epoch 739/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8917 - acc: 0.6107\n",
            "Epoch 740/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8916 - acc: 0.6031\n",
            "Epoch 741/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8915 - acc: 0.6031\n",
            "Epoch 742/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8915 - acc: 0.6031\n",
            "Epoch 743/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8913 - acc: 0.6107\n",
            "Epoch 744/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8912 - acc: 0.6031\n",
            "Epoch 745/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8911 - acc: 0.6031\n",
            "Epoch 746/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8911 - acc: 0.6107\n",
            "Epoch 747/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8910 - acc: 0.6031\n",
            "Epoch 748/1000\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.8910 - acc: 0.6107\n",
            "Epoch 749/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8909 - acc: 0.6107\n",
            "Epoch 750/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8908 - acc: 0.6107\n",
            "Epoch 751/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8906 - acc: 0.6107\n",
            "Epoch 752/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8905 - acc: 0.6031\n",
            "Epoch 753/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8905 - acc: 0.6031\n",
            "Epoch 754/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8904 - acc: 0.6031\n",
            "Epoch 755/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8902 - acc: 0.6031\n",
            "Epoch 756/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8903 - acc: 0.6031\n",
            "Epoch 757/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8902 - acc: 0.6031\n",
            "Epoch 758/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8903 - acc: 0.6031\n",
            "Epoch 759/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8900 - acc: 0.6031\n",
            "Epoch 760/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8900 - acc: 0.6031\n",
            "Epoch 761/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8899 - acc: 0.6031\n",
            "Epoch 762/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8898 - acc: 0.6031\n",
            "Epoch 763/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8897 - acc: 0.6031\n",
            "Epoch 764/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8897 - acc: 0.6031\n",
            "Epoch 765/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8896 - acc: 0.6031\n",
            "Epoch 766/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8895 - acc: 0.6031\n",
            "Epoch 767/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8893 - acc: 0.6031\n",
            "Epoch 768/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8894 - acc: 0.6031\n",
            "Epoch 769/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8892 - acc: 0.6031\n",
            "Epoch 770/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8891 - acc: 0.6031\n",
            "Epoch 771/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8892 - acc: 0.6031\n",
            "Epoch 772/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8889 - acc: 0.6031\n",
            "Epoch 773/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8889 - acc: 0.6031\n",
            "Epoch 774/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8888 - acc: 0.6031\n",
            "Epoch 775/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8886 - acc: 0.6031\n",
            "Epoch 776/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8887 - acc: 0.6031\n",
            "Epoch 777/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8885 - acc: 0.6031\n",
            "Epoch 778/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8886 - acc: 0.6031\n",
            "Epoch 779/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8885 - acc: 0.6031\n",
            "Epoch 780/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8884 - acc: 0.6031\n",
            "Epoch 781/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 782/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 783/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 784/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8881 - acc: 0.6031\n",
            "Epoch 785/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8880 - acc: 0.6031\n",
            "Epoch 786/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8879 - acc: 0.6031\n",
            "Epoch 787/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8877 - acc: 0.6031\n",
            "Epoch 788/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 789/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 790/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 791/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8874 - acc: 0.6031\n",
            "Epoch 792/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8873 - acc: 0.6031\n",
            "Epoch 793/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8872 - acc: 0.6031\n",
            "Epoch 794/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8873 - acc: 0.6031\n",
            "Epoch 795/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8870 - acc: 0.6031\n",
            "Epoch 796/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8870 - acc: 0.6031\n",
            "Epoch 797/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8869 - acc: 0.6031\n",
            "Epoch 798/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8868 - acc: 0.6031\n",
            "Epoch 799/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8867 - acc: 0.6031\n",
            "Epoch 800/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 801/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 802/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 803/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8865 - acc: 0.6031\n",
            "Epoch 804/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 805/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 806/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 807/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8862 - acc: 0.6031\n",
            "Epoch 808/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8861 - acc: 0.6031\n",
            "Epoch 809/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.8859 - acc: 0.5954\n",
            "Epoch 810/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8859 - acc: 0.6031\n",
            "Epoch 811/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8858 - acc: 0.6031\n",
            "Epoch 812/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8858 - acc: 0.5954\n",
            "Epoch 813/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.8856 - acc: 0.6031\n",
            "Epoch 814/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8855 - acc: 0.6031\n",
            "Epoch 815/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8854 - acc: 0.6031\n",
            "Epoch 816/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8855 - acc: 0.6031\n",
            "Epoch 817/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8853 - acc: 0.6031\n",
            "Epoch 818/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8852 - acc: 0.6031\n",
            "Epoch 819/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8851 - acc: 0.6031\n",
            "Epoch 820/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8851 - acc: 0.6031\n",
            "Epoch 821/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8850 - acc: 0.6031\n",
            "Epoch 822/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8849 - acc: 0.6031\n",
            "Epoch 823/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8847 - acc: 0.6031\n",
            "Epoch 824/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8848 - acc: 0.6031\n",
            "Epoch 825/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8847 - acc: 0.6031\n",
            "Epoch 826/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8845 - acc: 0.6031\n",
            "Epoch 827/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8845 - acc: 0.6031\n",
            "Epoch 828/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8844 - acc: 0.6031\n",
            "Epoch 829/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8843 - acc: 0.6031\n",
            "Epoch 830/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 831/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 832/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8840 - acc: 0.6031\n",
            "Epoch 833/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 834/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8839 - acc: 0.6031\n",
            "Epoch 835/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8840 - acc: 0.6031\n",
            "Epoch 836/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8838 - acc: 0.6031\n",
            "Epoch 837/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8837 - acc: 0.6031\n",
            "Epoch 838/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 839/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 840/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 841/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 842/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 843/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 844/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8832 - acc: 0.6031\n",
            "Epoch 845/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8832 - acc: 0.6031\n",
            "Epoch 846/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8831 - acc: 0.6031\n",
            "Epoch 847/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 848/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 849/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 850/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8827 - acc: 0.6031\n",
            "Epoch 851/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8828 - acc: 0.6031\n",
            "Epoch 852/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 853/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 854/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 855/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8824 - acc: 0.6031\n",
            "Epoch 856/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8823 - acc: 0.6031\n",
            "Epoch 857/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8823 - acc: 0.6031\n",
            "Epoch 858/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8822 - acc: 0.6031\n",
            "Epoch 859/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8822 - acc: 0.6031\n",
            "Epoch 860/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8820 - acc: 0.6031\n",
            "Epoch 861/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 862/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 863/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8818 - acc: 0.6031\n",
            "Epoch 864/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8818 - acc: 0.6031\n",
            "Epoch 865/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 866/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8817 - acc: 0.6031\n",
            "Epoch 867/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8816 - acc: 0.6031\n",
            "Epoch 868/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8816 - acc: 0.6031\n",
            "Epoch 869/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8814 - acc: 0.6031\n",
            "Epoch 870/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8814 - acc: 0.6031\n",
            "Epoch 871/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 872/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 873/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 874/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8811 - acc: 0.6031\n",
            "Epoch 875/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8810 - acc: 0.6031\n",
            "Epoch 876/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 877/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 878/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8808 - acc: 0.6031\n",
            "Epoch 879/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 880/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8805 - acc: 0.6031\n",
            "Epoch 881/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8806 - acc: 0.6031\n",
            "Epoch 882/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8806 - acc: 0.6031\n",
            "Epoch 883/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8804 - acc: 0.6031\n",
            "Epoch 884/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8803 - acc: 0.6031\n",
            "Epoch 885/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8803 - acc: 0.6031\n",
            "Epoch 886/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8802 - acc: 0.6031\n",
            "Epoch 887/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8802 - acc: 0.6031\n",
            "Epoch 888/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 889/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8801 - acc: 0.6031\n",
            "Epoch 890/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8799 - acc: 0.6031\n",
            "Epoch 891/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 892/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 893/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8797 - acc: 0.6031\n",
            "Epoch 894/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8796 - acc: 0.6031\n",
            "Epoch 895/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.8795 - acc: 0.6031\n",
            "Epoch 896/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8795 - acc: 0.6031\n",
            "Epoch 897/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8794 - acc: 0.6031\n",
            "Epoch 898/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8794 - acc: 0.6031\n",
            "Epoch 899/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8793 - acc: 0.6031\n",
            "Epoch 900/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 901/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8793 - acc: 0.6031\n",
            "Epoch 902/1000\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 903/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 904/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8789 - acc: 0.6031\n",
            "Epoch 905/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8789 - acc: 0.6031\n",
            "Epoch 906/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8790 - acc: 0.6031\n",
            "Epoch 907/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 908/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 909/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 910/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.8785 - acc: 0.6031\n",
            "Epoch 911/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8784 - acc: 0.6031\n",
            "Epoch 912/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8783 - acc: 0.6031\n",
            "Epoch 913/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8782 - acc: 0.6031\n",
            "Epoch 914/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8781 - acc: 0.6031\n",
            "Epoch 915/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8781 - acc: 0.6031\n",
            "Epoch 916/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8780 - acc: 0.6031\n",
            "Epoch 917/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8780 - acc: 0.6031\n",
            "Epoch 918/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8779 - acc: 0.6031\n",
            "Epoch 919/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 920/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 921/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 922/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8776 - acc: 0.6031\n",
            "Epoch 923/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8776 - acc: 0.6031\n",
            "Epoch 924/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8776 - acc: 0.5954\n",
            "Epoch 925/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8775 - acc: 0.5954\n",
            "Epoch 926/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8774 - acc: 0.6031\n",
            "Epoch 927/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8773 - acc: 0.6031\n",
            "Epoch 928/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8772 - acc: 0.6031\n",
            "Epoch 929/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8771 - acc: 0.5954\n",
            "Epoch 930/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8770 - acc: 0.6031\n",
            "Epoch 931/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8769 - acc: 0.6031\n",
            "Epoch 932/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8769 - acc: 0.5954\n",
            "Epoch 933/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 934/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 935/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 936/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8767 - acc: 0.5954\n",
            "Epoch 937/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 938/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 939/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 940/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8763 - acc: 0.6031\n",
            "Epoch 941/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8763 - acc: 0.6031\n",
            "Epoch 942/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8762 - acc: 0.6031\n",
            "Epoch 943/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 944/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8761 - acc: 0.6031\n",
            "Epoch 945/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 946/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 947/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 948/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 949/1000\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.8758 - acc: 0.6031\n",
            "Epoch 950/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8759 - acc: 0.6031\n",
            "Epoch 951/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8755 - acc: 0.6031\n",
            "Epoch 952/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8756 - acc: 0.5954\n",
            "Epoch 953/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8754 - acc: 0.6031\n",
            "Epoch 954/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8753 - acc: 0.6031\n",
            "Epoch 955/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8754 - acc: 0.5954\n",
            "Epoch 956/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8753 - acc: 0.5954\n",
            "Epoch 957/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8754 - acc: 0.5954\n",
            "Epoch 958/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8752 - acc: 0.5954\n",
            "Epoch 959/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8751 - acc: 0.5954\n",
            "Epoch 960/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8750 - acc: 0.5954\n",
            "Epoch 961/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8750 - acc: 0.5954\n",
            "Epoch 962/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8749 - acc: 0.5954\n",
            "Epoch 963/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8749 - acc: 0.5954\n",
            "Epoch 964/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8747 - acc: 0.5954\n",
            "Epoch 965/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 966/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 967/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 968/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8745 - acc: 0.5954\n",
            "Epoch 969/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8744 - acc: 0.5954\n",
            "Epoch 970/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8742 - acc: 0.5954\n",
            "Epoch 971/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8744 - acc: 0.5954\n",
            "Epoch 972/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8743 - acc: 0.5954\n",
            "Epoch 973/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8742 - acc: 0.5954\n",
            "Epoch 974/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8741 - acc: 0.5954\n",
            "Epoch 975/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8740 - acc: 0.5954\n",
            "Epoch 976/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8740 - acc: 0.5954\n",
            "Epoch 977/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8741 - acc: 0.5954\n",
            "Epoch 978/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8739 - acc: 0.5954\n",
            "Epoch 979/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8739 - acc: 0.5954\n",
            "Epoch 980/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8738 - acc: 0.5954\n",
            "Epoch 981/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8737 - acc: 0.5954\n",
            "Epoch 982/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8737 - acc: 0.5954\n",
            "Epoch 983/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8736 - acc: 0.5954\n",
            "Epoch 984/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8735 - acc: 0.5954\n",
            "Epoch 985/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8734 - acc: 0.5954\n",
            "Epoch 986/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8734 - acc: 0.5954\n",
            "Epoch 987/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 988/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 989/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8731 - acc: 0.5954\n",
            "Epoch 990/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8732 - acc: 0.5954\n",
            "Epoch 991/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 992/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 993/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 994/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 995/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8729 - acc: 0.5954\n",
            "Epoch 996/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8728 - acc: 0.5954\n",
            "Epoch 997/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8728 - acc: 0.5954\n",
            "Epoch 998/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8727 - acc: 0.5954\n",
            "Epoch 999/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8726 - acc: 0.5954\n",
            "Epoch 1000/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.8726 - acc: 0.5954\n",
            "34/34 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "bfcdc956-e7d5-4402-ae3d-eaa3c774f1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "7054a667-3ec5-44b5-a16b-f12c892bba8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20588235294117646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "f3afcdff-ef50-4e6e-96bb-32f99409007b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-91b9d254cb0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_stand_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_stand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_stand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data_stand' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "a8b2e761-e182-494c-a650-ce7dbcdf6e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JAOqZLBxkXFs"
      },
      "source": [
        "##Z score dei dati dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DisVOwPBkXF8",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6vzVp7KkXGU",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6fb8ef44-36c0-425a-8eb8-b90dbaea27b7",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1dac3719-8dd0-4eae-b561-b06fff110250",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a7fbc5c8-d088-4912-fa5b-858f4e75f059",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b7a797e7-f2a9-41df-85d0-a4cdc008c1f7",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 200\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 1.0956 - acc: 0.2069 - val_loss: 1.0482 - val_acc: 0.3182\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 1.0795 - acc: 0.3333 - val_loss: 1.0327 - val_acc: 0.3636\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 1.0640 - acc: 0.3793 - val_loss: 1.0181 - val_acc: 0.4091\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 1.0490 - acc: 0.4483 - val_loss: 1.0036 - val_acc: 0.4773\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 1.0348 - acc: 0.4828 - val_loss: 0.9903 - val_acc: 0.5455\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 203us/step - loss: 1.0214 - acc: 0.5172 - val_loss: 0.9776 - val_acc: 0.5455\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 201us/step - loss: 1.0085 - acc: 0.5287 - val_loss: 0.9656 - val_acc: 0.5682\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9964 - acc: 0.5747 - val_loss: 0.9538 - val_acc: 0.5682\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.9849 - acc: 0.5747 - val_loss: 0.9429 - val_acc: 0.5909\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.9740 - acc: 0.5977 - val_loss: 0.9324 - val_acc: 0.6136\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.9634 - acc: 0.6092 - val_loss: 0.9223 - val_acc: 0.6136\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9535 - acc: 0.6322 - val_loss: 0.9127 - val_acc: 0.6136\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.9437 - acc: 0.6437 - val_loss: 0.9032 - val_acc: 0.6818\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.9345 - acc: 0.6667 - val_loss: 0.8946 - val_acc: 0.7045\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.9255 - acc: 0.6667 - val_loss: 0.8859 - val_acc: 0.7045\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9168 - acc: 0.6667 - val_loss: 0.8775 - val_acc: 0.6818\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.9084 - acc: 0.6667 - val_loss: 0.8694 - val_acc: 0.7273\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9002 - acc: 0.6667 - val_loss: 0.8617 - val_acc: 0.7273\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8921 - acc: 0.6782 - val_loss: 0.8537 - val_acc: 0.7727\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8843 - acc: 0.6897 - val_loss: 0.8463 - val_acc: 0.7727\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8768 - acc: 0.7011 - val_loss: 0.8389 - val_acc: 0.7955\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8693 - acc: 0.7356 - val_loss: 0.8317 - val_acc: 0.7955\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8621 - acc: 0.7356 - val_loss: 0.8247 - val_acc: 0.7955\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8551 - acc: 0.7356 - val_loss: 0.8180 - val_acc: 0.7955\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8483 - acc: 0.7471 - val_loss: 0.8113 - val_acc: 0.7955\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8416 - acc: 0.7586 - val_loss: 0.8048 - val_acc: 0.8182\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 360us/step - loss: 0.8350 - acc: 0.7586 - val_loss: 0.7985 - val_acc: 0.8182\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8287 - acc: 0.7931 - val_loss: 0.7924 - val_acc: 0.8409\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8225 - acc: 0.8046 - val_loss: 0.7863 - val_acc: 0.8409\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8164 - acc: 0.8046 - val_loss: 0.7803 - val_acc: 0.8409\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8104 - acc: 0.8276 - val_loss: 0.7746 - val_acc: 0.8636\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8045 - acc: 0.8506 - val_loss: 0.7688 - val_acc: 0.8636\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7988 - acc: 0.8621 - val_loss: 0.7632 - val_acc: 0.8636\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 338us/step - loss: 0.7930 - acc: 0.8736 - val_loss: 0.7578 - val_acc: 0.8636\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7875 - acc: 0.8736 - val_loss: 0.7523 - val_acc: 0.8636\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7820 - acc: 0.8851 - val_loss: 0.7471 - val_acc: 0.8864\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.7767 - acc: 0.8851 - val_loss: 0.7419 - val_acc: 0.8864\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7715 - acc: 0.8851 - val_loss: 0.7368 - val_acc: 0.9091\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.7662 - acc: 0.8851 - val_loss: 0.7318 - val_acc: 0.9091\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.7611 - acc: 0.8966 - val_loss: 0.7269 - val_acc: 0.9318\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7561 - acc: 0.8966 - val_loss: 0.7220 - val_acc: 0.9318\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.7512 - acc: 0.9080 - val_loss: 0.7172 - val_acc: 0.9318\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7464 - acc: 0.9080 - val_loss: 0.7125 - val_acc: 0.9318\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.7417 - acc: 0.9080 - val_loss: 0.7079 - val_acc: 0.9318\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7370 - acc: 0.9080 - val_loss: 0.7033 - val_acc: 0.9318\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.7326 - acc: 0.9080 - val_loss: 0.6988 - val_acc: 0.9773\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7279 - acc: 0.9080 - val_loss: 0.6944 - val_acc: 0.9773\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7234 - acc: 0.9080 - val_loss: 0.6900 - val_acc: 0.9773\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7191 - acc: 0.9195 - val_loss: 0.6857 - val_acc: 0.9773\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.7147 - acc: 0.9195 - val_loss: 0.6815 - val_acc: 0.9773\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.7105 - acc: 0.9310 - val_loss: 0.6773 - val_acc: 0.9773\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7062 - acc: 0.9310 - val_loss: 0.6732 - val_acc: 0.9773\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7021 - acc: 0.9310 - val_loss: 0.6692 - val_acc: 0.9773\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.6980 - acc: 0.9425 - val_loss: 0.6651 - val_acc: 1.0000\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6939 - acc: 0.9425 - val_loss: 0.6611 - val_acc: 1.0000\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6899 - acc: 0.9425 - val_loss: 0.6572 - val_acc: 1.0000\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.6859 - acc: 0.9425 - val_loss: 0.6533 - val_acc: 1.0000\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6820 - acc: 0.9540 - val_loss: 0.6495 - val_acc: 1.0000\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.6780 - acc: 0.9540 - val_loss: 0.6457 - val_acc: 1.0000\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.6742 - acc: 0.9540 - val_loss: 0.6420 - val_acc: 1.0000\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6705 - acc: 0.9540 - val_loss: 0.6383 - val_acc: 1.0000\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.6667 - acc: 0.9540 - val_loss: 0.6346 - val_acc: 1.0000\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.6630 - acc: 0.9540 - val_loss: 0.6309 - val_acc: 1.0000\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.6594 - acc: 0.9540 - val_loss: 0.6274 - val_acc: 1.0000\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.6558 - acc: 0.9540 - val_loss: 0.6239 - val_acc: 1.0000\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.6523 - acc: 0.9540 - val_loss: 0.6204 - val_acc: 1.0000\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.6487 - acc: 0.9540 - val_loss: 0.6170 - val_acc: 1.0000\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6453 - acc: 0.9540 - val_loss: 0.6135 - val_acc: 1.0000\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.6418 - acc: 0.9540 - val_loss: 0.6101 - val_acc: 1.0000\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6384 - acc: 0.9540 - val_loss: 0.6068 - val_acc: 1.0000\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6351 - acc: 0.9540 - val_loss: 0.6035 - val_acc: 1.0000\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6317 - acc: 0.9540 - val_loss: 0.6002 - val_acc: 1.0000\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.6284 - acc: 0.9540 - val_loss: 0.5969 - val_acc: 1.0000\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.6251 - acc: 0.9540 - val_loss: 0.5937 - val_acc: 1.0000\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.6219 - acc: 0.9540 - val_loss: 0.5905 - val_acc: 1.0000\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6186 - acc: 0.9540 - val_loss: 0.5873 - val_acc: 1.0000\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6155 - acc: 0.9540 - val_loss: 0.5842 - val_acc: 1.0000\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6123 - acc: 0.9540 - val_loss: 0.5811 - val_acc: 1.0000\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6092 - acc: 0.9540 - val_loss: 0.5780 - val_acc: 1.0000\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 324us/step - loss: 0.6061 - acc: 0.9540 - val_loss: 0.5749 - val_acc: 1.0000\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.6030 - acc: 0.9540 - val_loss: 0.5719 - val_acc: 1.0000\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5999 - acc: 0.9540 - val_loss: 0.5689 - val_acc: 1.0000\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5969 - acc: 0.9540 - val_loss: 0.5659 - val_acc: 1.0000\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.5939 - acc: 0.9540 - val_loss: 0.5629 - val_acc: 1.0000\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5909 - acc: 0.9540 - val_loss: 0.5600 - val_acc: 1.0000\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.5880 - acc: 0.9540 - val_loss: 0.5571 - val_acc: 1.0000\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.5850 - acc: 0.9540 - val_loss: 0.5542 - val_acc: 1.0000\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5821 - acc: 0.9540 - val_loss: 0.5513 - val_acc: 1.0000\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5792 - acc: 0.9540 - val_loss: 0.5485 - val_acc: 1.0000\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5763 - acc: 0.9540 - val_loss: 0.5456 - val_acc: 1.0000\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.5735 - acc: 0.9540 - val_loss: 0.5428 - val_acc: 1.0000\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.5707 - acc: 0.9540 - val_loss: 0.5401 - val_acc: 1.0000\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5679 - acc: 0.9540 - val_loss: 0.5373 - val_acc: 1.0000\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5652 - acc: 0.9540 - val_loss: 0.5346 - val_acc: 1.0000\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 187us/step - loss: 0.5624 - acc: 0.9540 - val_loss: 0.5319 - val_acc: 1.0000\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5597 - acc: 0.9540 - val_loss: 0.5292 - val_acc: 1.0000\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.5569 - acc: 0.9655 - val_loss: 0.5266 - val_acc: 1.0000\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.5543 - acc: 0.9655 - val_loss: 0.5239 - val_acc: 1.0000\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5516 - acc: 0.9655 - val_loss: 0.5213 - val_acc: 1.0000\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5489 - acc: 0.9655 - val_loss: 0.5187 - val_acc: 1.0000\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5463 - acc: 0.9655 - val_loss: 0.5161 - val_acc: 1.0000\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5437 - acc: 0.9655 - val_loss: 0.5136 - val_acc: 1.0000\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5411 - acc: 0.9655 - val_loss: 0.5110 - val_acc: 1.0000\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5385 - acc: 0.9655 - val_loss: 0.5085 - val_acc: 1.0000\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.5359 - acc: 0.9655 - val_loss: 0.5060 - val_acc: 1.0000\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.5334 - acc: 0.9655 - val_loss: 0.5035 - val_acc: 1.0000\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5309 - acc: 0.9655 - val_loss: 0.5011 - val_acc: 1.0000\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5284 - acc: 0.9655 - val_loss: 0.4986 - val_acc: 1.0000\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.5259 - acc: 0.9655 - val_loss: 0.4962 - val_acc: 1.0000\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.5234 - acc: 0.9655 - val_loss: 0.4938 - val_acc: 1.0000\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5210 - acc: 0.9655 - val_loss: 0.4914 - val_acc: 1.0000\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5186 - acc: 0.9655 - val_loss: 0.4890 - val_acc: 1.0000\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5162 - acc: 0.9655 - val_loss: 0.4867 - val_acc: 1.0000\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.5138 - acc: 0.9655 - val_loss: 0.4844 - val_acc: 1.0000\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5114 - acc: 0.9655 - val_loss: 0.4821 - val_acc: 1.0000\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.5090 - acc: 0.9655 - val_loss: 0.4798 - val_acc: 1.0000\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.5067 - acc: 0.9770 - val_loss: 0.4775 - val_acc: 1.0000\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5044 - acc: 0.9770 - val_loss: 0.4752 - val_acc: 1.0000\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.5021 - acc: 0.9770 - val_loss: 0.4730 - val_acc: 1.0000\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.4998 - acc: 0.9770 - val_loss: 0.4707 - val_acc: 1.0000\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4974 - acc: 0.9770 - val_loss: 0.4685 - val_acc: 1.0000\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4952 - acc: 0.9770 - val_loss: 0.4663 - val_acc: 1.0000\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.4929 - acc: 0.9770 - val_loss: 0.4642 - val_acc: 1.0000\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.4907 - acc: 0.9770 - val_loss: 0.4620 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4885 - acc: 0.9770 - val_loss: 0.4598 - val_acc: 1.0000\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4863 - acc: 0.9770 - val_loss: 0.4577 - val_acc: 1.0000\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4840 - acc: 0.9770 - val_loss: 0.4556 - val_acc: 1.0000\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4819 - acc: 0.9770 - val_loss: 0.4535 - val_acc: 1.0000\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4797 - acc: 0.9770 - val_loss: 0.4514 - val_acc: 1.0000\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4775 - acc: 0.9770 - val_loss: 0.4493 - val_acc: 1.0000\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4754 - acc: 0.9770 - val_loss: 0.4473 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4733 - acc: 0.9770 - val_loss: 0.4452 - val_acc: 1.0000\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.4712 - acc: 0.9770 - val_loss: 0.4432 - val_acc: 1.0000\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.4691 - acc: 0.9770 - val_loss: 0.4412 - val_acc: 1.0000\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.4670 - acc: 0.9770 - val_loss: 0.4392 - val_acc: 1.0000\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4650 - acc: 0.9770 - val_loss: 0.4372 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4629 - acc: 0.9770 - val_loss: 0.4353 - val_acc: 1.0000\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4609 - acc: 0.9770 - val_loss: 0.4333 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4588 - acc: 0.9770 - val_loss: 0.4314 - val_acc: 1.0000\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.4567 - acc: 0.9770 - val_loss: 0.4295 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4548 - acc: 0.9770 - val_loss: 0.4276 - val_acc: 1.0000\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4528 - acc: 0.9770 - val_loss: 0.4257 - val_acc: 1.0000\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4508 - acc: 0.9770 - val_loss: 0.4238 - val_acc: 1.0000\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4488 - acc: 0.9770 - val_loss: 0.4219 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4469 - acc: 0.9770 - val_loss: 0.4201 - val_acc: 1.0000\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4449 - acc: 0.9770 - val_loss: 0.4182 - val_acc: 1.0000\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4430 - acc: 0.9770 - val_loss: 0.4164 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.4411 - acc: 0.9770 - val_loss: 0.4146 - val_acc: 1.0000\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4392 - acc: 0.9770 - val_loss: 0.4128 - val_acc: 1.0000\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4373 - acc: 0.9770 - val_loss: 0.4110 - val_acc: 1.0000\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4354 - acc: 0.9770 - val_loss: 0.4092 - val_acc: 1.0000\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4336 - acc: 0.9885 - val_loss: 0.4075 - val_acc: 1.0000\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4317 - acc: 0.9885 - val_loss: 0.4057 - val_acc: 1.0000\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4299 - acc: 0.9885 - val_loss: 0.4040 - val_acc: 1.0000\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4281 - acc: 0.9885 - val_loss: 0.4023 - val_acc: 1.0000\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4263 - acc: 0.9885 - val_loss: 0.4005 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4245 - acc: 0.9885 - val_loss: 0.3988 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4227 - acc: 0.9885 - val_loss: 0.3972 - val_acc: 1.0000\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4209 - acc: 0.9885 - val_loss: 0.3955 - val_acc: 1.0000\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4191 - acc: 0.9885 - val_loss: 0.3938 - val_acc: 1.0000\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4174 - acc: 0.9885 - val_loss: 0.3922 - val_acc: 1.0000\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4156 - acc: 0.9885 - val_loss: 0.3905 - val_acc: 1.0000\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4139 - acc: 0.9885 - val_loss: 0.3889 - val_acc: 1.0000\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4122 - acc: 0.9885 - val_loss: 0.3873 - val_acc: 1.0000\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4105 - acc: 0.9885 - val_loss: 0.3856 - val_acc: 1.0000\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4088 - acc: 0.9885 - val_loss: 0.3840 - val_acc: 1.0000\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4071 - acc: 0.9885 - val_loss: 0.3825 - val_acc: 1.0000\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4054 - acc: 0.9885 - val_loss: 0.3809 - val_acc: 1.0000\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4038 - acc: 0.9885 - val_loss: 0.3793 - val_acc: 1.0000\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.4021 - acc: 0.9885 - val_loss: 0.3778 - val_acc: 1.0000\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4005 - acc: 0.9885 - val_loss: 0.3762 - val_acc: 1.0000\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3988 - acc: 0.9885 - val_loss: 0.3747 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3972 - acc: 0.9885 - val_loss: 0.3731 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3956 - acc: 0.9885 - val_loss: 0.3716 - val_acc: 1.0000\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.3940 - acc: 0.9885 - val_loss: 0.3701 - val_acc: 1.0000\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.3924 - acc: 0.9885 - val_loss: 0.3686 - val_acc: 1.0000\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3908 - acc: 0.9885 - val_loss: 0.3671 - val_acc: 1.0000\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.3892 - acc: 0.9885 - val_loss: 0.3657 - val_acc: 1.0000\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3877 - acc: 0.9885 - val_loss: 0.3642 - val_acc: 1.0000\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3861 - acc: 0.9885 - val_loss: 0.3627 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.3846 - acc: 0.9885 - val_loss: 0.3613 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.3830 - acc: 0.9885 - val_loss: 0.3598 - val_acc: 1.0000\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.3815 - acc: 0.9885 - val_loss: 0.3584 - val_acc: 1.0000\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.3800 - acc: 0.9885 - val_loss: 0.3570 - val_acc: 1.0000\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3785 - acc: 0.9885 - val_loss: 0.3556 - val_acc: 1.0000\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3770 - acc: 0.9885 - val_loss: 0.3542 - val_acc: 1.0000\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 314us/step - loss: 0.3755 - acc: 0.9885 - val_loss: 0.3528 - val_acc: 1.0000\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3740 - acc: 0.9885 - val_loss: 0.3514 - val_acc: 1.0000\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3725 - acc: 0.9885 - val_loss: 0.3500 - val_acc: 1.0000\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3711 - acc: 0.9885 - val_loss: 0.3487 - val_acc: 1.0000\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3696 - acc: 0.9885 - val_loss: 0.3473 - val_acc: 1.0000\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3682 - acc: 0.9885 - val_loss: 0.3460 - val_acc: 1.0000\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.3667 - acc: 0.9885 - val_loss: 0.3446 - val_acc: 1.0000\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3653 - acc: 1.0000 - val_loss: 0.3433 - val_acc: 1.0000\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3638 - acc: 1.0000 - val_loss: 0.3420 - val_acc: 1.0000\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3624 - acc: 1.0000 - val_loss: 0.3407 - val_acc: 1.0000\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3611 - acc: 1.0000 - val_loss: 0.3394 - val_acc: 1.0000\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.3597 - acc: 1.0000 - val_loss: 0.3381 - val_acc: 1.0000\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.3583 - acc: 1.0000 - val_loss: 0.3368 - val_acc: 1.0000\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.3569 - acc: 1.0000 - val_loss: 0.3355 - val_acc: 1.0000\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 4ms/step - loss: 1.5660 - acc: 0.0000e+00 - val_loss: 1.4870 - val_acc: 0.0909\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.5505 - acc: 0.0000e+00 - val_loss: 1.4727 - val_acc: 0.0909\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.5344 - acc: 0.0115 - val_loss: 1.4585 - val_acc: 0.0909\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.5193 - acc: 0.0345 - val_loss: 1.4445 - val_acc: 0.0909\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.5045 - acc: 0.0460 - val_loss: 1.4311 - val_acc: 0.1136\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.4901 - acc: 0.0460 - val_loss: 1.4176 - val_acc: 0.1136\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 1.4755 - acc: 0.0690 - val_loss: 1.4047 - val_acc: 0.1364\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.4616 - acc: 0.0920 - val_loss: 1.3920 - val_acc: 0.1591\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 1.4479 - acc: 0.1149 - val_loss: 1.3796 - val_acc: 0.1591\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 1.4346 - acc: 0.1264 - val_loss: 1.3675 - val_acc: 0.2273\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 204us/step - loss: 1.4217 - acc: 0.1839 - val_loss: 1.3556 - val_acc: 0.2273\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 1.4094 - acc: 0.1954 - val_loss: 1.3442 - val_acc: 0.2955\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 1.3973 - acc: 0.2184 - val_loss: 1.3329 - val_acc: 0.3409\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.3856 - acc: 0.2529 - val_loss: 1.3223 - val_acc: 0.3636\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.3742 - acc: 0.2644 - val_loss: 1.3115 - val_acc: 0.3636\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 1.3631 - acc: 0.2759 - val_loss: 1.3010 - val_acc: 0.3636\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 275us/step - loss: 1.3520 - acc: 0.2989 - val_loss: 1.2910 - val_acc: 0.3636\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 1.3414 - acc: 0.3103 - val_loss: 1.2810 - val_acc: 0.3864\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 1.3310 - acc: 0.3448 - val_loss: 1.2710 - val_acc: 0.3864\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.3207 - acc: 0.3678 - val_loss: 1.2615 - val_acc: 0.3864\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.3107 - acc: 0.3908 - val_loss: 1.2519 - val_acc: 0.4318\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 1.3009 - acc: 0.3908 - val_loss: 1.2426 - val_acc: 0.4318\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 1.2913 - acc: 0.3908 - val_loss: 1.2338 - val_acc: 0.4318\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.2820 - acc: 0.3908 - val_loss: 1.2247 - val_acc: 0.4545\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.2724 - acc: 0.4138 - val_loss: 1.2158 - val_acc: 0.4773\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 1.2628 - acc: 0.4138 - val_loss: 1.2071 - val_acc: 0.5000\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 1.2537 - acc: 0.4253 - val_loss: 1.1982 - val_acc: 0.5000\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 267us/step - loss: 1.2443 - acc: 0.4598 - val_loss: 1.1896 - val_acc: 0.5000\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.2347 - acc: 0.4943 - val_loss: 1.1809 - val_acc: 0.5227\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.2252 - acc: 0.5172 - val_loss: 1.1721 - val_acc: 0.5455\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.2154 - acc: 0.5632 - val_loss: 1.1633 - val_acc: 0.5682\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 1.2056 - acc: 0.5747 - val_loss: 1.1547 - val_acc: 0.5682\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 1.1960 - acc: 0.6092 - val_loss: 1.1465 - val_acc: 0.5909\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.1867 - acc: 0.6322 - val_loss: 1.1385 - val_acc: 0.6136\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 1.1773 - acc: 0.6437 - val_loss: 1.1304 - val_acc: 0.6136\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.1681 - acc: 0.6667 - val_loss: 1.1225 - val_acc: 0.6136\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 1.1589 - acc: 0.6667 - val_loss: 1.1148 - val_acc: 0.6136\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 1.1497 - acc: 0.6782 - val_loss: 1.1070 - val_acc: 0.6364\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 1.1404 - acc: 0.6782 - val_loss: 1.0996 - val_acc: 0.6364\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 1.1315 - acc: 0.7011 - val_loss: 1.0922 - val_acc: 0.6364\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.1227 - acc: 0.7241 - val_loss: 1.0848 - val_acc: 0.6591\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 1.1142 - acc: 0.7356 - val_loss: 1.0771 - val_acc: 0.6818\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 1.1055 - acc: 0.7356 - val_loss: 1.0697 - val_acc: 0.6818\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.0973 - acc: 0.7471 - val_loss: 1.0622 - val_acc: 0.6818\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 1.0886 - acc: 0.7701 - val_loss: 1.0548 - val_acc: 0.6818\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 1.0802 - acc: 0.7701 - val_loss: 1.0473 - val_acc: 0.7045\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 1.0716 - acc: 0.7701 - val_loss: 1.0402 - val_acc: 0.7045\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 1.0632 - acc: 0.7816 - val_loss: 1.0330 - val_acc: 0.7500\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 283us/step - loss: 1.0548 - acc: 0.7816 - val_loss: 1.0258 - val_acc: 0.7727\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 1.0468 - acc: 0.7816 - val_loss: 1.0184 - val_acc: 0.7727\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 1.0384 - acc: 0.7931 - val_loss: 1.0113 - val_acc: 0.7727\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 1.0305 - acc: 0.8046 - val_loss: 1.0041 - val_acc: 0.7727\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.0226 - acc: 0.8276 - val_loss: 0.9971 - val_acc: 0.7727\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.0148 - acc: 0.8276 - val_loss: 0.9901 - val_acc: 0.7727\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.0070 - acc: 0.8276 - val_loss: 0.9835 - val_acc: 0.7727\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.9994 - acc: 0.8276 - val_loss: 0.9763 - val_acc: 0.7727\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.9921 - acc: 0.8276 - val_loss: 0.9693 - val_acc: 0.8182\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.9847 - acc: 0.8276 - val_loss: 0.9625 - val_acc: 0.8182\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.9775 - acc: 0.8276 - val_loss: 0.9557 - val_acc: 0.8182\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9703 - acc: 0.8276 - val_loss: 0.9492 - val_acc: 0.8182\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.9633 - acc: 0.8276 - val_loss: 0.9426 - val_acc: 0.8409\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.9563 - acc: 0.8276 - val_loss: 0.9362 - val_acc: 0.8409\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9495 - acc: 0.8276 - val_loss: 0.9296 - val_acc: 0.8636\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.9424 - acc: 0.8276 - val_loss: 0.9234 - val_acc: 0.8636\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9354 - acc: 0.8276 - val_loss: 0.9168 - val_acc: 0.8636\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.9282 - acc: 0.8276 - val_loss: 0.9104 - val_acc: 0.8636\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.9211 - acc: 0.8276 - val_loss: 0.9041 - val_acc: 0.8636\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9142 - acc: 0.8276 - val_loss: 0.8979 - val_acc: 0.8636\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9074 - acc: 0.8276 - val_loss: 0.8918 - val_acc: 0.8636\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9006 - acc: 0.8276 - val_loss: 0.8857 - val_acc: 0.8636\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8940 - acc: 0.8391 - val_loss: 0.8797 - val_acc: 0.8636\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.8873 - acc: 0.8506 - val_loss: 0.8738 - val_acc: 0.8636\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8806 - acc: 0.8506 - val_loss: 0.8679 - val_acc: 0.8636\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8740 - acc: 0.8621 - val_loss: 0.8621 - val_acc: 0.8636\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8675 - acc: 0.8621 - val_loss: 0.8563 - val_acc: 0.8636\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.8611 - acc: 0.8621 - val_loss: 0.8507 - val_acc: 0.8636\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8548 - acc: 0.8621 - val_loss: 0.8450 - val_acc: 0.8636\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8486 - acc: 0.8621 - val_loss: 0.8395 - val_acc: 0.8636\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 343us/step - loss: 0.8424 - acc: 0.8621 - val_loss: 0.8340 - val_acc: 0.8636\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8363 - acc: 0.8621 - val_loss: 0.8286 - val_acc: 0.8636\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8302 - acc: 0.8621 - val_loss: 0.8232 - val_acc: 0.8636\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8243 - acc: 0.8621 - val_loss: 0.8180 - val_acc: 0.8636\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8184 - acc: 0.8621 - val_loss: 0.8127 - val_acc: 0.8636\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8125 - acc: 0.8621 - val_loss: 0.8075 - val_acc: 0.8636\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8068 - acc: 0.8621 - val_loss: 0.8024 - val_acc: 0.8636\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8011 - acc: 0.8621 - val_loss: 0.7973 - val_acc: 0.8636\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.7955 - acc: 0.8621 - val_loss: 0.7923 - val_acc: 0.8636\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7898 - acc: 0.8621 - val_loss: 0.7874 - val_acc: 0.8636\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.7842 - acc: 0.8621 - val_loss: 0.7825 - val_acc: 0.8636\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7788 - acc: 0.8621 - val_loss: 0.7776 - val_acc: 0.8636\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.7734 - acc: 0.8621 - val_loss: 0.7728 - val_acc: 0.8636\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7679 - acc: 0.8621 - val_loss: 0.7681 - val_acc: 0.8636\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7627 - acc: 0.8621 - val_loss: 0.7635 - val_acc: 0.8636\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.7575 - acc: 0.8621 - val_loss: 0.7589 - val_acc: 0.8636\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.7523 - acc: 0.8621 - val_loss: 0.7544 - val_acc: 0.8636\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7471 - acc: 0.8621 - val_loss: 0.7500 - val_acc: 0.8636\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7421 - acc: 0.8621 - val_loss: 0.7457 - val_acc: 0.8636\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.7372 - acc: 0.8621 - val_loss: 0.7414 - val_acc: 0.8636\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7323 - acc: 0.8621 - val_loss: 0.7372 - val_acc: 0.8636\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.7274 - acc: 0.8621 - val_loss: 0.7331 - val_acc: 0.8636\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7228 - acc: 0.8621 - val_loss: 0.7290 - val_acc: 0.8636\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7180 - acc: 0.8621 - val_loss: 0.7250 - val_acc: 0.8636\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.7134 - acc: 0.8621 - val_loss: 0.7210 - val_acc: 0.8636\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7089 - acc: 0.8621 - val_loss: 0.7171 - val_acc: 0.8636\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.7044 - acc: 0.8621 - val_loss: 0.7133 - val_acc: 0.8636\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7000 - acc: 0.8621 - val_loss: 0.7096 - val_acc: 0.8636\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6956 - acc: 0.8621 - val_loss: 0.7062 - val_acc: 0.8636\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6913 - acc: 0.8736 - val_loss: 0.7029 - val_acc: 0.8636\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.6872 - acc: 0.8736 - val_loss: 0.6997 - val_acc: 0.8636\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6832 - acc: 0.8736 - val_loss: 0.6964 - val_acc: 0.8636\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6792 - acc: 0.8736 - val_loss: 0.6933 - val_acc: 0.8636\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6751 - acc: 0.8736 - val_loss: 0.6901 - val_acc: 0.8636\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6713 - acc: 0.8736 - val_loss: 0.6870 - val_acc: 0.8636\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6675 - acc: 0.8736 - val_loss: 0.6839 - val_acc: 0.8636\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6638 - acc: 0.8736 - val_loss: 0.6809 - val_acc: 0.8636\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6603 - acc: 0.8736 - val_loss: 0.6780 - val_acc: 0.8636\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.6567 - acc: 0.8736 - val_loss: 0.6751 - val_acc: 0.8636\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6533 - acc: 0.8736 - val_loss: 0.6722 - val_acc: 0.8636\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.6500 - acc: 0.8736 - val_loss: 0.6694 - val_acc: 0.8636\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6467 - acc: 0.8736 - val_loss: 0.6666 - val_acc: 0.8636\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6436 - acc: 0.8736 - val_loss: 0.6638 - val_acc: 0.8636\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6404 - acc: 0.8736 - val_loss: 0.6611 - val_acc: 0.8636\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6372 - acc: 0.8736 - val_loss: 0.6583 - val_acc: 0.8636\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6341 - acc: 0.8736 - val_loss: 0.6555 - val_acc: 0.8636\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6311 - acc: 0.8736 - val_loss: 0.6529 - val_acc: 0.8636\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.6279 - acc: 0.8736 - val_loss: 0.6502 - val_acc: 0.8636\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.6249 - acc: 0.8736 - val_loss: 0.6476 - val_acc: 0.8636\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6219 - acc: 0.8736 - val_loss: 0.6449 - val_acc: 0.8636\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.6190 - acc: 0.8736 - val_loss: 0.6423 - val_acc: 0.8636\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6161 - acc: 0.8736 - val_loss: 0.6397 - val_acc: 0.8636\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.6133 - acc: 0.8736 - val_loss: 0.6372 - val_acc: 0.8636\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.6105 - acc: 0.8736 - val_loss: 0.6346 - val_acc: 0.8636\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.6078 - acc: 0.8736 - val_loss: 0.6320 - val_acc: 0.8636\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.6050 - acc: 0.8736 - val_loss: 0.6295 - val_acc: 0.8636\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6023 - acc: 0.8736 - val_loss: 0.6270 - val_acc: 0.8636\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5995 - acc: 0.8736 - val_loss: 0.6245 - val_acc: 0.8636\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5969 - acc: 0.8736 - val_loss: 0.6221 - val_acc: 0.8636\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5941 - acc: 0.8736 - val_loss: 0.6195 - val_acc: 0.8636\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5915 - acc: 0.8736 - val_loss: 0.6171 - val_acc: 0.8636\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5888 - acc: 0.8736 - val_loss: 0.6147 - val_acc: 0.8636\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5862 - acc: 0.8736 - val_loss: 0.6122 - val_acc: 0.8636\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.5836 - acc: 0.8736 - val_loss: 0.6098 - val_acc: 0.8636\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.5809 - acc: 0.8736 - val_loss: 0.6074 - val_acc: 0.8636\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5784 - acc: 0.8736 - val_loss: 0.6050 - val_acc: 0.8636\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5757 - acc: 0.8736 - val_loss: 0.6026 - val_acc: 0.8636\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5733 - acc: 0.8736 - val_loss: 0.6003 - val_acc: 0.8636\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5707 - acc: 0.8736 - val_loss: 0.5980 - val_acc: 0.8636\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.5681 - acc: 0.8736 - val_loss: 0.5957 - val_acc: 0.8636\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5656 - acc: 0.8736 - val_loss: 0.5933 - val_acc: 0.8636\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5631 - acc: 0.8736 - val_loss: 0.5910 - val_acc: 0.8636\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.5607 - acc: 0.8736 - val_loss: 0.5888 - val_acc: 0.8636\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.5582 - acc: 0.8736 - val_loss: 0.5865 - val_acc: 0.8636\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5558 - acc: 0.8736 - val_loss: 0.5844 - val_acc: 0.8636\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.5534 - acc: 0.8736 - val_loss: 0.5822 - val_acc: 0.8636\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5510 - acc: 0.8736 - val_loss: 0.5799 - val_acc: 0.8636\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.5486 - acc: 0.8736 - val_loss: 0.5778 - val_acc: 0.8636\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5463 - acc: 0.8736 - val_loss: 0.5756 - val_acc: 0.8636\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5439 - acc: 0.8736 - val_loss: 0.5735 - val_acc: 0.8636\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.5416 - acc: 0.8736 - val_loss: 0.5714 - val_acc: 0.8636\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.5393 - acc: 0.8736 - val_loss: 0.5693 - val_acc: 0.8636\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.5370 - acc: 0.8736 - val_loss: 0.5672 - val_acc: 0.8636\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.5347 - acc: 0.8736 - val_loss: 0.5651 - val_acc: 0.8636\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5324 - acc: 0.8736 - val_loss: 0.5630 - val_acc: 0.8636\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.5301 - acc: 0.8736 - val_loss: 0.5609 - val_acc: 0.8636\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5279 - acc: 0.8736 - val_loss: 0.5589 - val_acc: 0.8636\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.5258 - acc: 0.8736 - val_loss: 0.5569 - val_acc: 0.8636\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.5236 - acc: 0.8736 - val_loss: 0.5548 - val_acc: 0.8636\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.5214 - acc: 0.8736 - val_loss: 0.5529 - val_acc: 0.8636\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.5193 - acc: 0.8736 - val_loss: 0.5509 - val_acc: 0.8636\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5172 - acc: 0.8736 - val_loss: 0.5489 - val_acc: 0.8636\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5151 - acc: 0.8736 - val_loss: 0.5470 - val_acc: 0.8636\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.5130 - acc: 0.8736 - val_loss: 0.5450 - val_acc: 0.8636\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.5109 - acc: 0.8736 - val_loss: 0.5431 - val_acc: 0.8636\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.5089 - acc: 0.8736 - val_loss: 0.5412 - val_acc: 0.8636\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.5069 - acc: 0.8736 - val_loss: 0.5392 - val_acc: 0.8636\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5048 - acc: 0.8736 - val_loss: 0.5373 - val_acc: 0.8636\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.5028 - acc: 0.8736 - val_loss: 0.5355 - val_acc: 0.8636\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5008 - acc: 0.8736 - val_loss: 0.5336 - val_acc: 0.8636\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4988 - acc: 0.8736 - val_loss: 0.5318 - val_acc: 0.8636\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4968 - acc: 0.8736 - val_loss: 0.5299 - val_acc: 0.8636\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4949 - acc: 0.8736 - val_loss: 0.5281 - val_acc: 0.8636\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.4929 - acc: 0.8736 - val_loss: 0.5262 - val_acc: 0.8636\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.4910 - acc: 0.8736 - val_loss: 0.5245 - val_acc: 0.8636\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4891 - acc: 0.8736 - val_loss: 0.5226 - val_acc: 0.8636\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4871 - acc: 0.8736 - val_loss: 0.5209 - val_acc: 0.8636\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4852 - acc: 0.8736 - val_loss: 0.5191 - val_acc: 0.8636\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.4834 - acc: 0.8736 - val_loss: 0.5173 - val_acc: 0.8636\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.4815 - acc: 0.8736 - val_loss: 0.5156 - val_acc: 0.8636\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.4797 - acc: 0.8736 - val_loss: 0.5138 - val_acc: 0.8636\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4778 - acc: 0.8736 - val_loss: 0.5121 - val_acc: 0.8636\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.4759 - acc: 0.8736 - val_loss: 0.5104 - val_acc: 0.8636\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.4741 - acc: 0.8736 - val_loss: 0.5087 - val_acc: 0.8636\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4723 - acc: 0.8736 - val_loss: 0.5070 - val_acc: 0.8636\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4705 - acc: 0.8736 - val_loss: 0.5053 - val_acc: 0.8636\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4688 - acc: 0.8736 - val_loss: 0.5036 - val_acc: 0.8636\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.4670 - acc: 0.8736 - val_loss: 0.5019 - val_acc: 0.8636\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.4652 - acc: 0.8736 - val_loss: 0.5002 - val_acc: 0.8636\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4635 - acc: 0.8736 - val_loss: 0.4986 - val_acc: 0.8636\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.4617 - acc: 0.8736 - val_loss: 0.4969 - val_acc: 0.8636\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4600 - acc: 0.8736 - val_loss: 0.4953 - val_acc: 0.8636\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/200\n",
            "88/88 [==============================] - 0s 4ms/step - loss: 1.4273 - acc: 0.3523 - val_loss: 1.5551 - val_acc: 0.3488\n",
            "Epoch 2/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.4094 - acc: 0.3523 - val_loss: 1.5319 - val_acc: 0.3488\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.3922 - acc: 0.3523 - val_loss: 1.5100 - val_acc: 0.3488\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.3759 - acc: 0.3523 - val_loss: 1.4889 - val_acc: 0.3488\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.3601 - acc: 0.3523 - val_loss: 1.4671 - val_acc: 0.3488\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.3446 - acc: 0.3523 - val_loss: 1.4479 - val_acc: 0.3488\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.3301 - acc: 0.3636 - val_loss: 1.4290 - val_acc: 0.3488\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.3160 - acc: 0.3636 - val_loss: 1.4101 - val_acc: 0.3488\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.3023 - acc: 0.3636 - val_loss: 1.3929 - val_acc: 0.3488\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.2893 - acc: 0.3636 - val_loss: 1.3758 - val_acc: 0.3488\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.2769 - acc: 0.3636 - val_loss: 1.3588 - val_acc: 0.3488\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 0s 264us/step - loss: 1.2646 - acc: 0.3636 - val_loss: 1.3434 - val_acc: 0.3488\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.2532 - acc: 0.3523 - val_loss: 1.3275 - val_acc: 0.3256\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 0s 259us/step - loss: 1.2419 - acc: 0.3523 - val_loss: 1.3127 - val_acc: 0.3256\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 0s 266us/step - loss: 1.2309 - acc: 0.3523 - val_loss: 1.2988 - val_acc: 0.3256\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.2202 - acc: 0.3523 - val_loss: 1.2846 - val_acc: 0.3256\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 0s 267us/step - loss: 1.2103 - acc: 0.3409 - val_loss: 1.2706 - val_acc: 0.3256\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.2000 - acc: 0.3523 - val_loss: 1.2579 - val_acc: 0.3256\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.1902 - acc: 0.3523 - val_loss: 1.2453 - val_acc: 0.3256\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.1810 - acc: 0.3523 - val_loss: 1.2328 - val_acc: 0.3256\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.1718 - acc: 0.3523 - val_loss: 1.2215 - val_acc: 0.3256\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.1630 - acc: 0.3523 - val_loss: 1.2097 - val_acc: 0.3256\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.1544 - acc: 0.3523 - val_loss: 1.1988 - val_acc: 0.3256\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.1460 - acc: 0.3523 - val_loss: 1.1876 - val_acc: 0.3256\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.1378 - acc: 0.3636 - val_loss: 1.1771 - val_acc: 0.3256\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.1299 - acc: 0.3636 - val_loss: 1.1668 - val_acc: 0.3256\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.1222 - acc: 0.3750 - val_loss: 1.1567 - val_acc: 0.3256\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1147 - acc: 0.3750 - val_loss: 1.1470 - val_acc: 0.3256\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.1073 - acc: 0.3750 - val_loss: 1.1371 - val_acc: 0.3256\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.1001 - acc: 0.3750 - val_loss: 1.1281 - val_acc: 0.3488\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0932 - acc: 0.3864 - val_loss: 1.1186 - val_acc: 0.3488\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0863 - acc: 0.3864 - val_loss: 1.1098 - val_acc: 0.3488\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0796 - acc: 0.3864 - val_loss: 1.1015 - val_acc: 0.3488\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0730 - acc: 0.4205 - val_loss: 1.0930 - val_acc: 0.3488\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.0667 - acc: 0.4205 - val_loss: 1.0845 - val_acc: 0.3488\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 1.0603 - acc: 0.4205 - val_loss: 1.0763 - val_acc: 0.3488\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0543 - acc: 0.4205 - val_loss: 1.0681 - val_acc: 0.3488\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 1.0481 - acc: 0.4318 - val_loss: 1.0606 - val_acc: 0.3488\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0421 - acc: 0.4432 - val_loss: 1.0529 - val_acc: 0.3488\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0363 - acc: 0.4432 - val_loss: 1.0454 - val_acc: 0.3488\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.0305 - acc: 0.4432 - val_loss: 1.0379 - val_acc: 0.3488\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 0s 350us/step - loss: 1.0249 - acc: 0.4545 - val_loss: 1.0306 - val_acc: 0.3721\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0193 - acc: 0.4545 - val_loss: 1.0232 - val_acc: 0.3721\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.0137 - acc: 0.4659 - val_loss: 1.0161 - val_acc: 0.3721\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.0084 - acc: 0.4659 - val_loss: 1.0095 - val_acc: 0.3953\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0027 - acc: 0.4886 - val_loss: 1.0025 - val_acc: 0.3953\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9975 - acc: 0.4886 - val_loss: 0.9955 - val_acc: 0.3953\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.9921 - acc: 0.4886 - val_loss: 0.9891 - val_acc: 0.3953\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 0s 321us/step - loss: 0.9869 - acc: 0.5000 - val_loss: 0.9825 - val_acc: 0.3953\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.9817 - acc: 0.5114 - val_loss: 0.9760 - val_acc: 0.3953\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 0s 320us/step - loss: 0.9766 - acc: 0.5227 - val_loss: 0.9694 - val_acc: 0.4419\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.9715 - acc: 0.5227 - val_loss: 0.9633 - val_acc: 0.4651\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.9666 - acc: 0.5455 - val_loss: 0.9569 - val_acc: 0.4884\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9618 - acc: 0.5682 - val_loss: 0.9507 - val_acc: 0.5116\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.9567 - acc: 0.5909 - val_loss: 0.9445 - val_acc: 0.5349\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.9517 - acc: 0.6136 - val_loss: 0.9384 - val_acc: 0.5349\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.9469 - acc: 0.6136 - val_loss: 0.9322 - val_acc: 0.5349\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.9420 - acc: 0.6136 - val_loss: 0.9262 - val_acc: 0.5581\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9373 - acc: 0.6250 - val_loss: 0.9201 - val_acc: 0.5814\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.9324 - acc: 0.6250 - val_loss: 0.9143 - val_acc: 0.6047\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.9276 - acc: 0.6364 - val_loss: 0.9087 - val_acc: 0.6279\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.9230 - acc: 0.6705 - val_loss: 0.9028 - val_acc: 0.6279\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9182 - acc: 0.6932 - val_loss: 0.8970 - val_acc: 0.6279\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.9136 - acc: 0.7045 - val_loss: 0.8912 - val_acc: 0.6977\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.9090 - acc: 0.7045 - val_loss: 0.8856 - val_acc: 0.7442\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9044 - acc: 0.7045 - val_loss: 0.8801 - val_acc: 0.7442\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8999 - acc: 0.7045 - val_loss: 0.8745 - val_acc: 0.7442\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8953 - acc: 0.7045 - val_loss: 0.8690 - val_acc: 0.7442\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 0s 297us/step - loss: 0.8909 - acc: 0.7159 - val_loss: 0.8634 - val_acc: 0.7674\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8863 - acc: 0.7273 - val_loss: 0.8580 - val_acc: 0.7674\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 0s 295us/step - loss: 0.8819 - acc: 0.7386 - val_loss: 0.8525 - val_acc: 0.8140\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8775 - acc: 0.7386 - val_loss: 0.8473 - val_acc: 0.8140\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8731 - acc: 0.7500 - val_loss: 0.8420 - val_acc: 0.8140\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8686 - acc: 0.7614 - val_loss: 0.8367 - val_acc: 0.8140\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8642 - acc: 0.7614 - val_loss: 0.8314 - val_acc: 0.8140\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8599 - acc: 0.7614 - val_loss: 0.8261 - val_acc: 0.8140\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8555 - acc: 0.7727 - val_loss: 0.8209 - val_acc: 0.8140\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8512 - acc: 0.7727 - val_loss: 0.8157 - val_acc: 0.8140\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8468 - acc: 0.7727 - val_loss: 0.8106 - val_acc: 0.8140\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8425 - acc: 0.7841 - val_loss: 0.8055 - val_acc: 0.8140\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8382 - acc: 0.7955 - val_loss: 0.8004 - val_acc: 0.8140\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8340 - acc: 0.7955 - val_loss: 0.7953 - val_acc: 0.8372\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.8298 - acc: 0.7955 - val_loss: 0.7901 - val_acc: 0.8372\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.8255 - acc: 0.7955 - val_loss: 0.7851 - val_acc: 0.8372\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8213 - acc: 0.8068 - val_loss: 0.7802 - val_acc: 0.8605\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8171 - acc: 0.8068 - val_loss: 0.7752 - val_acc: 0.8837\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8130 - acc: 0.8068 - val_loss: 0.7703 - val_acc: 0.8837\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.8087 - acc: 0.8182 - val_loss: 0.7653 - val_acc: 0.8837\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8046 - acc: 0.8182 - val_loss: 0.7605 - val_acc: 0.8837\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8005 - acc: 0.8182 - val_loss: 0.7556 - val_acc: 0.8837\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7964 - acc: 0.8182 - val_loss: 0.7508 - val_acc: 0.8837\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7924 - acc: 0.8182 - val_loss: 0.7459 - val_acc: 0.8837\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.7883 - acc: 0.8182 - val_loss: 0.7411 - val_acc: 0.8837\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7842 - acc: 0.8182 - val_loss: 0.7364 - val_acc: 0.8837\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7802 - acc: 0.8182 - val_loss: 0.7315 - val_acc: 0.8837\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 0s 359us/step - loss: 0.7762 - acc: 0.8182 - val_loss: 0.7267 - val_acc: 0.8837\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.7722 - acc: 0.8182 - val_loss: 0.7220 - val_acc: 0.8837\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7682 - acc: 0.8182 - val_loss: 0.7174 - val_acc: 0.8837\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7642 - acc: 0.8182 - val_loss: 0.7127 - val_acc: 0.8837\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7602 - acc: 0.8182 - val_loss: 0.7081 - val_acc: 0.8837\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.7562 - acc: 0.8182 - val_loss: 0.7034 - val_acc: 0.8837\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.7523 - acc: 0.8182 - val_loss: 0.6989 - val_acc: 0.8837\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.7484 - acc: 0.8295 - val_loss: 0.6943 - val_acc: 0.8837\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.7444 - acc: 0.8295 - val_loss: 0.6898 - val_acc: 0.8837\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.7405 - acc: 0.8295 - val_loss: 0.6852 - val_acc: 0.8837\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.7367 - acc: 0.8295 - val_loss: 0.6807 - val_acc: 0.8837\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7328 - acc: 0.8295 - val_loss: 0.6763 - val_acc: 0.8837\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7291 - acc: 0.8295 - val_loss: 0.6718 - val_acc: 0.8837\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7253 - acc: 0.8295 - val_loss: 0.6674 - val_acc: 0.8837\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7214 - acc: 0.8295 - val_loss: 0.6631 - val_acc: 0.8837\n",
            "Epoch 111/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7176 - acc: 0.8295 - val_loss: 0.6587 - val_acc: 0.8837\n",
            "Epoch 112/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7139 - acc: 0.8295 - val_loss: 0.6544 - val_acc: 0.8837\n",
            "Epoch 113/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.7101 - acc: 0.8295 - val_loss: 0.6500 - val_acc: 0.8837\n",
            "Epoch 114/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7064 - acc: 0.8409 - val_loss: 0.6458 - val_acc: 0.8837\n",
            "Epoch 115/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7028 - acc: 0.8409 - val_loss: 0.6415 - val_acc: 0.8837\n",
            "Epoch 116/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.6990 - acc: 0.8409 - val_loss: 0.6372 - val_acc: 0.8837\n",
            "Epoch 117/200\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.6953 - acc: 0.8523 - val_loss: 0.6330 - val_acc: 0.8837\n",
            "Epoch 118/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.6917 - acc: 0.8523 - val_loss: 0.6289 - val_acc: 0.8837\n",
            "Epoch 119/200\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.6881 - acc: 0.8523 - val_loss: 0.6248 - val_acc: 0.8837\n",
            "Epoch 120/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6845 - acc: 0.8523 - val_loss: 0.6206 - val_acc: 0.8837\n",
            "Epoch 121/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6808 - acc: 0.8523 - val_loss: 0.6166 - val_acc: 0.8837\n",
            "Epoch 122/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.6772 - acc: 0.8523 - val_loss: 0.6125 - val_acc: 0.8837\n",
            "Epoch 123/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.6737 - acc: 0.8523 - val_loss: 0.6085 - val_acc: 0.8837\n",
            "Epoch 124/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.6702 - acc: 0.8523 - val_loss: 0.6045 - val_acc: 0.8837\n",
            "Epoch 125/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6667 - acc: 0.8523 - val_loss: 0.6005 - val_acc: 0.8837\n",
            "Epoch 126/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.6632 - acc: 0.8523 - val_loss: 0.5965 - val_acc: 0.8837\n",
            "Epoch 127/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.6597 - acc: 0.8523 - val_loss: 0.5926 - val_acc: 0.8837\n",
            "Epoch 128/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.6563 - acc: 0.8523 - val_loss: 0.5887 - val_acc: 0.8837\n",
            "Epoch 129/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.6529 - acc: 0.8523 - val_loss: 0.5849 - val_acc: 0.8837\n",
            "Epoch 130/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6495 - acc: 0.8523 - val_loss: 0.5810 - val_acc: 0.8837\n",
            "Epoch 131/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6461 - acc: 0.8523 - val_loss: 0.5772 - val_acc: 0.8837\n",
            "Epoch 132/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.6427 - acc: 0.8523 - val_loss: 0.5735 - val_acc: 0.8837\n",
            "Epoch 133/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6394 - acc: 0.8523 - val_loss: 0.5697 - val_acc: 0.8837\n",
            "Epoch 134/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6360 - acc: 0.8523 - val_loss: 0.5660 - val_acc: 0.8837\n",
            "Epoch 135/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6327 - acc: 0.8636 - val_loss: 0.5623 - val_acc: 0.8837\n",
            "Epoch 136/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.6295 - acc: 0.8636 - val_loss: 0.5586 - val_acc: 0.8837\n",
            "Epoch 137/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.6262 - acc: 0.8636 - val_loss: 0.5550 - val_acc: 0.8837\n",
            "Epoch 138/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.6230 - acc: 0.8636 - val_loss: 0.5514 - val_acc: 0.8837\n",
            "Epoch 139/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.6198 - acc: 0.8636 - val_loss: 0.5479 - val_acc: 0.8837\n",
            "Epoch 140/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.6166 - acc: 0.8636 - val_loss: 0.5444 - val_acc: 0.8837\n",
            "Epoch 141/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.6134 - acc: 0.8636 - val_loss: 0.5408 - val_acc: 0.8837\n",
            "Epoch 142/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.6103 - acc: 0.8636 - val_loss: 0.5374 - val_acc: 0.8837\n",
            "Epoch 143/200\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.6072 - acc: 0.8636 - val_loss: 0.5340 - val_acc: 0.8837\n",
            "Epoch 144/200\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.6041 - acc: 0.8636 - val_loss: 0.5306 - val_acc: 0.8837\n",
            "Epoch 145/200\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.6010 - acc: 0.8636 - val_loss: 0.5272 - val_acc: 0.8837\n",
            "Epoch 146/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.5980 - acc: 0.8636 - val_loss: 0.5238 - val_acc: 0.8837\n",
            "Epoch 147/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.5950 - acc: 0.8636 - val_loss: 0.5205 - val_acc: 0.8837\n",
            "Epoch 148/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5920 - acc: 0.8636 - val_loss: 0.5172 - val_acc: 0.8837\n",
            "Epoch 149/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.5890 - acc: 0.8636 - val_loss: 0.5140 - val_acc: 0.8837\n",
            "Epoch 150/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5860 - acc: 0.8636 - val_loss: 0.5107 - val_acc: 0.8837\n",
            "Epoch 151/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.5830 - acc: 0.8636 - val_loss: 0.5075 - val_acc: 0.8837\n",
            "Epoch 152/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5802 - acc: 0.8636 - val_loss: 0.5043 - val_acc: 0.8837\n",
            "Epoch 153/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5772 - acc: 0.8636 - val_loss: 0.5012 - val_acc: 0.8837\n",
            "Epoch 154/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5744 - acc: 0.8636 - val_loss: 0.4981 - val_acc: 0.8837\n",
            "Epoch 155/200\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.5715 - acc: 0.8636 - val_loss: 0.4950 - val_acc: 0.8837\n",
            "Epoch 156/200\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.5687 - acc: 0.8636 - val_loss: 0.4920 - val_acc: 0.8837\n",
            "Epoch 157/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.5659 - acc: 0.8636 - val_loss: 0.4889 - val_acc: 0.8837\n",
            "Epoch 158/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.5631 - acc: 0.8636 - val_loss: 0.4859 - val_acc: 0.8837\n",
            "Epoch 159/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.5604 - acc: 0.8636 - val_loss: 0.4829 - val_acc: 0.8837\n",
            "Epoch 160/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.5576 - acc: 0.8636 - val_loss: 0.4800 - val_acc: 0.8837\n",
            "Epoch 161/200\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.5549 - acc: 0.8636 - val_loss: 0.4771 - val_acc: 0.8837\n",
            "Epoch 162/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5523 - acc: 0.8636 - val_loss: 0.4741 - val_acc: 0.8837\n",
            "Epoch 163/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5495 - acc: 0.8636 - val_loss: 0.4714 - val_acc: 0.8837\n",
            "Epoch 164/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5468 - acc: 0.8636 - val_loss: 0.4685 - val_acc: 0.8837\n",
            "Epoch 165/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5442 - acc: 0.8636 - val_loss: 0.4657 - val_acc: 0.8837\n",
            "Epoch 166/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5416 - acc: 0.8636 - val_loss: 0.4630 - val_acc: 0.8837\n",
            "Epoch 167/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5390 - acc: 0.8636 - val_loss: 0.4602 - val_acc: 0.8837\n",
            "Epoch 168/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.5364 - acc: 0.8636 - val_loss: 0.4575 - val_acc: 0.8837\n",
            "Epoch 169/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.5339 - acc: 0.8636 - val_loss: 0.4548 - val_acc: 0.8837\n",
            "Epoch 170/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5313 - acc: 0.8636 - val_loss: 0.4522 - val_acc: 0.8837\n",
            "Epoch 171/200\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.5289 - acc: 0.8636 - val_loss: 0.4495 - val_acc: 0.8837\n",
            "Epoch 172/200\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.5263 - acc: 0.8636 - val_loss: 0.4469 - val_acc: 0.8837\n",
            "Epoch 173/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5239 - acc: 0.8636 - val_loss: 0.4443 - val_acc: 0.8837\n",
            "Epoch 174/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.5215 - acc: 0.8750 - val_loss: 0.4418 - val_acc: 0.8837\n",
            "Epoch 175/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.5190 - acc: 0.8750 - val_loss: 0.4392 - val_acc: 0.8837\n",
            "Epoch 176/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5166 - acc: 0.8750 - val_loss: 0.4367 - val_acc: 0.8837\n",
            "Epoch 177/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.5143 - acc: 0.8750 - val_loss: 0.4342 - val_acc: 0.8837\n",
            "Epoch 178/200\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.5119 - acc: 0.8750 - val_loss: 0.4318 - val_acc: 0.8837\n",
            "Epoch 179/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.5096 - acc: 0.8750 - val_loss: 0.4294 - val_acc: 0.8837\n",
            "Epoch 180/200\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.5073 - acc: 0.8750 - val_loss: 0.4270 - val_acc: 0.8837\n",
            "Epoch 181/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5050 - acc: 0.8750 - val_loss: 0.4246 - val_acc: 0.8837\n",
            "Epoch 182/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.5028 - acc: 0.8750 - val_loss: 0.4222 - val_acc: 0.8837\n",
            "Epoch 183/200\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.5005 - acc: 0.8750 - val_loss: 0.4199 - val_acc: 0.8837\n",
            "Epoch 184/200\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.4983 - acc: 0.8750 - val_loss: 0.4176 - val_acc: 0.8837\n",
            "Epoch 185/200\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.4961 - acc: 0.8750 - val_loss: 0.4154 - val_acc: 0.8837\n",
            "Epoch 186/200\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.4939 - acc: 0.8750 - val_loss: 0.4131 - val_acc: 0.8837\n",
            "Epoch 187/200\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.4918 - acc: 0.8750 - val_loss: 0.4109 - val_acc: 0.8837\n",
            "Epoch 188/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.4896 - acc: 0.8750 - val_loss: 0.4086 - val_acc: 0.8837\n",
            "Epoch 189/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4875 - acc: 0.8750 - val_loss: 0.4065 - val_acc: 0.8837\n",
            "Epoch 190/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.4854 - acc: 0.8750 - val_loss: 0.4043 - val_acc: 0.8837\n",
            "Epoch 191/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.4832 - acc: 0.8750 - val_loss: 0.4022 - val_acc: 0.8837\n",
            "Epoch 192/200\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.4812 - acc: 0.8750 - val_loss: 0.4000 - val_acc: 0.8837\n",
            "Epoch 193/200\n",
            "88/88 [==============================] - 0s 284us/step - loss: 0.4792 - acc: 0.8750 - val_loss: 0.3980 - val_acc: 0.8837\n",
            "Epoch 194/200\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.4771 - acc: 0.8750 - val_loss: 0.3959 - val_acc: 0.8837\n",
            "Epoch 195/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.4751 - acc: 0.8750 - val_loss: 0.3938 - val_acc: 0.8837\n",
            "Epoch 196/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4731 - acc: 0.8750 - val_loss: 0.3918 - val_acc: 0.8837\n",
            "Epoch 197/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4711 - acc: 0.8750 - val_loss: 0.3898 - val_acc: 0.8837\n",
            "Epoch 198/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.4691 - acc: 0.8750 - val_loss: 0.3878 - val_acc: 0.8837\n",
            "Epoch 199/200\n",
            "88/88 [==============================] - 0s 312us/step - loss: 0.4672 - acc: 0.8750 - val_loss: 0.3858 - val_acc: 0.8837\n",
            "Epoch 200/200\n",
            "88/88 [==============================] - 0s 290us/step - loss: 0.4653 - acc: 0.8750 - val_loss: 0.3839 - val_acc: 0.8837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "ac0aad6e-4f0c-4911-d5aa-8d9a93d72ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.05997689,  1.78480965],\n",
              "       [-1.70377632,  2.62178086],\n",
              "       [-0.88024938,  1.10677205],\n",
              "       [-1.6221035 ,  2.53797672],\n",
              "       [-1.20438162,  1.42505216],\n",
              "       [-0.72831295, -0.54994201],\n",
              "       [-0.71544696, -1.05313817],\n",
              "       [-0.78016566, -0.87184816],\n",
              "       [-0.66616332, -1.04551976],\n",
              "       [-0.80814457, -1.1534213 ],\n",
              "       [-0.18117691, -0.5752502 ],\n",
              "       [-0.5797643 , -0.90332856],\n",
              "       [-0.75946851, -0.80650801],\n",
              "       [-0.47381325, -0.94095818],\n",
              "       [-0.51651934, -0.3343159 ],\n",
              "       [-0.9078223 , -0.51343018],\n",
              "       [-0.83040609, -0.52280685],\n",
              "       [-0.8364583 , -1.1044959 ],\n",
              "       [-1.06491561, -0.23177315],\n",
              "       [-1.09349754, -1.34108765],\n",
              "       [-1.44273181, -1.36811384],\n",
              "       [-0.39121396, -1.41060042],\n",
              "       [-0.96660223, -1.52141472],\n",
              "       [-0.29851281, -1.2365264 ],\n",
              "       [-0.66466281, -0.89831149],\n",
              "       [ 1.12079905,  0.10919806],\n",
              "       [ 0.41839628,  0.46087105],\n",
              "       [ 0.81585568,  0.3499919 ],\n",
              "       [ 0.6952246 , -0.22961609],\n",
              "       [ 0.78712685,  0.6056401 ],\n",
              "       [ 0.98738836,  0.40085179],\n",
              "       [ 1.18764219,  0.36694321],\n",
              "       [ 1.70546993,  0.15254344],\n",
              "       [ 1.84248269, -0.39151261],\n",
              "       [ 1.42391516,  0.0626175 ],\n",
              "       [ 1.3927998 ,  0.12226364],\n",
              "       [ 1.6269367 ,  0.7386622 ],\n",
              "       [ 1.87133538,  1.64617693],\n",
              "       [ 1.27102475, -0.06582761],\n",
              "       [ 0.9978794 ,  0.55048648],\n",
              "       [ 0.59853173,  0.27469505],\n",
              "       [ 1.40628344,  0.21564038],\n",
              "       [ 1.07460331,  0.42032253]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "73e12cde-c14b-450b-98eb-7b041eeeba54",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bb4c53c5-3df6-4198-fcd6-734693824ef7",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8af43eaa-9c12-4e93-fa53-098df0cbb569",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "83f56f43-f5d1-499a-8832-b8c6c6395adf",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a931790f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBM45F+U8ETjnXJT7f8xlZ/fFF3xDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c032b20b-9858-4289-c487-5a939cc9d178",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a930e8e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3gUVffA8e8hVIHQFaUIKkoPJSBK\nB1FEBBWlS1NRXsGO8iqWF8vPgqgo8opKUySgWFDAAlL1VQlIAEEEMUo39F4C5/fHncRNyCYbyGaT\n7Pk8T57szNyZPTvZ7Nm5d+69oqoYY4wJX/lCHYAxxpjQskRgjDFhzhKBMcaEOUsExhgT5iwRGGNM\nmLNEYIwxYc4SQRgQkQgROSgilbOybCiJyCUikuX3PovIVSIS77O8TkSaB1L2DJ7rHRF59Ez3D3ci\ncruILEhn+xIR6Zd9EeVe+UMdgDmdiBz0WTwHOAac9JbvVNUpmTmeqp4EimV12XCgqpdlxXFE5Hag\nt6q28jn27VlxbGPOliWCHEhVkz+IvW+ct6vqXH/lRSS/qiZmR2zGZMTej7mPVQ3lQiLyjIhME5Gp\nInIA6C0iV4jIDyKyV0S2ichoESnglc8vIioiVbzl973tc0TkgIj8T0SqZrast/1aEflNRPaJyOsi\n8p2/y/EAY7xTRDaIyB4RGe2zb4SIvCIiu0RkI9A+nfPzmIjEpFo3RkRGeY9vF5G13uv53fu27u9Y\nm0Wklff4HBF5z4vtF6BhqrLDRWSjd9xfRKSTt74O8AbQ3Kt22+lzbp/y2f8u77XvEpFPReT8QM5N\nZs5zUjwiMldEdovIdhF52Od5HvfOyX4RiRWRC9KqhvOtdvHO5yLveXYDw0WkmojM955jp3feSvjs\nf6H3GhO87a+JSGEv5ho+5c4XkcMiUsbf6/Up215cVd4+EXkNEJ9t6cYT9lTVfnLwDxAPXJVq3TPA\nceB6XDIvAjQCLsdd5V0E/AYM9srnBxSo4i2/D+wEooECwDTg/TMoey5wAOjsbXsAOAH08/NaAonx\nM6AEUAXYnfTagcHAL0BFoAywyL1903yei4CDQFGfY/8NRHvL13tlBGgDHAHqetuuAuJ9jrUZaOU9\nHgksAEoBFwJrUpXtCpzv/U16ejGc5227HViQKs73gae8x1d7MdYDCgNvAt8Gcm4yeZ5LADuAe4FC\nQCTQ2Nv2byAOqOa9hnpAaeCS1OcaWJL0d/ZeWyIwCIjAvR8vBdoCBb33yXfASJ/Xs9o7n0W98k29\nbeOAZ32e50HgEz+vM/mces9xELgR914c6sWUFKPfeOxHLRHk9B/8J4JvM9jvIeBD73FaH+7/9Snb\nCVh9BmUHAIt9tgmwDT+JIMAYm/hs/xh4yHu8CFdFlrStQ+oPp1TH/gHo6T2+FliXTtkvgLu9x+kl\ngr98/xbAv3zLpnHc1cB13uOMEsEk4DmfbZG4dqGKGZ2bTJ7nW4Glfsr9nhRvqvWBJIKNGcRwc9Lz\nAs2B7UBEGuWaAn8A4i2vAG7yc0zfRDAAWOKzLV9670XfeOxHrWooF9vkuyAi1UVklnepvx8YAZRN\nZ//tPo8Pk34Dsb+yF/jGoe4/bLO/gwQYY0DPBfyZTrwAHwA9vMc9veWkODqKyI9eNcFe3Lfx9M5V\nkvPTi0FE+olInFe9sReoHuBxwb2+5OOp6n5gD1DBp0xAf7MMznMl3Ad+WtLblpHU78fyIjJdRLZ4\nMUxMFUO8uhsTUlDV73Df5JuJSG2gMjArgOdP/V48hc97MYN4wp4lgtwr9a2Tb+G+gV6iqpHAE/jU\nkQbJNtw3VgBEREj5wZXa2cS4DfcBkiSj21unA1eJSAVc1dUHXoxFgI+A/8NV25QEvg4wju3+YhCR\ni4CxuOqRMt5xf/U5bka3um7FVTclHa84rgpqSwBxpZbeed4EXOxnP3/bDnkxneOzrnyqMqlf3wu4\nu93qeDH0SxXDhSIS4SeOyUBv3NXLdFU95qecrxTvDxHJh897M4N4wp4lgryjOLAPOOQ1tt2ZDc/5\nBdBARK4Xkfy4eudyQYpxOnCfiFTwGg4fSa+wqm7HVV9MxFULrfc2FcLVEycAJ0WkI67uONAYHhWR\nkuL6WQz22VYM92GYgMuJd+CuCJLsACr6NtqmMhW4TUTqikghXKJarKp+r7DSkd55nglUFpHBIlJI\nRCJFpLG37R3gGRG5WJx6IlIalwC3425KiBCRgfgkrXRiOATsE5FKuOqpJP8DdgHPiWuALyIiTX22\nv4eruumJSwqB+AKoJyKdvXN8Pynfi+nFE/YsEeQdDwJ9cY23b+EadYNKVXcA3YBRuH/si4Gfcd+8\nsjrGscA8YBWwFPetPiMf4Or8k6uFVHUv7kPiE1yD6824D5FAPIn75hkPzMHnQ0pVVwKvAz95ZS4D\nfvTZ9xtgPbBDRHyreJL2/xJXhfOJt39loFeAcaXm9zyr6j6gHdAFl5x+A1p6m18CPsWd5/24htvC\nXpXfHcCjuBsHLkn12tLyJNAYl5BmAjN8YkgEOgI1cFcHf+H+Dknb43F/52Oq+n0gL9jnvfiSF2Pl\nVDH6jcf80yBjzFnzLvW3Ajer6uJQx2NyLxGZjGuAfirUsYQD61BmzoqItMfdoXMEd/vhCdy3YmPO\niNfe0hmoE+pYwoVVDZmz1QzYiKsbvwa4McDGPWNOIyL/h+vL8Jyq/hXqeMKFVQ0ZY0yYsysCY4wJ\nc7mujaBs2bJapUqVUIdhjDG5yrJly3aqapq3d+e6RFClShViY2NDHYYxxuQqIuK3N75VDRljTJiz\nRGCMMWHOEoExxoQ5SwTGGBPmLBEYY0yYs0RgjDFhzhKBMcaEuVzXj8CYs7JvH3z8MZw4EepIjAFg\n/3747js4HsBb8qJbm1GnW80sj8ESgQkfp05Bly4wb16oIzEmWSRuUu1ALIocC7ktEXhDFL8GRADv\nqOrzqbZfCIzHzSS0G+h9hjMyGZOxF190SeCNN+DGG0MdjfHEx8PcuS5Ph8r338PX34TmuetFwSuv\nwGWXZVy2RYkSQYkhaInAm6RkDG42pM3AUhGZqaprfIqNBCar6iQRaYObnu/WYMVkwtgPP8Dw4dC1\nK/zrXyA2XW1GNm2COXOC+wG9fTu89BIcPhy85whEkSLwzMtw112QL5tbTgsVCv3bMZhXBI2BDaq6\nEUBEYnCTTfgmgprAA97j+bhp8ow5e4mJsGqV+xRLTIQePaBSJXjrrdD/152FBQvgjz+C/zw7dsBz\nz8GBA8F/rvbt3UVa2bLBfy5/Chd2H8jhKpiJoAJuPtIkm4HLU5WJA27CVR/dCBQXkTKquiuIcZm8\n7tQpuOEGmDXrn3UREbB4MZQsGbq4ztKMGXDzzRmXyypt2sDo0VCmTPCeIyLCJYBcnJvzhFA3Fj8E\nvCEi/YBFwBbgZOpCIjIQGAhQuXLl7IzPhMrOnWd+Z89777kk8NhjcLn33ePii6Fm1jeyBdO2ba5G\nC+DYMVdt0agRTJsW/OqLiAioUME+oMNFMBPBFqCSz3JFb10yVd2KuyJARIoBXVR1b+oDqeo4YBxA\ndHS0TamW102ZAr17n90xOneGp5/OsZ9kGzfCr7/6375hAzz+uLu1MEmpUhATA1WrBj8+E16CmQiW\nAtVEpCouAXQHevoWEJGywG5VPYWb+Hx8EOMxucW4cXDRRfDww2e2f6FCrg4lhEng6FFYuhROnnZ9\n6+4Z/89/Mr7gadnS1dMXLeqWK1WC0qWzPlZjgpYIVDVRRAYDX+FuHx2vqr+IyAggVlVnAq2A/xMR\nxVUN3R2seEwuER8PixbBM8/AnXeGOpo0HTjg2qH92bULhg6Fdev8l+nSBR580FXBpKVgQahbN/vv\nYDHhKahtBKo6G5idat0TPo8/Aj4KZgwml3n/ffe7V6+gP9WpU7BmjbupKFAbNsA997j6+/RUqgRT\np0L58qdvi4yE+vVzbK2VCUOhbiw2xpk+3X2N3rEDWrSAIMxLnZDg2qDBjTQxdCgsWZL549SqBWPG\n/FNlk1q+fK6NunjxM4/VmOxkicCE3rp10L8/XHKJu6l8wICzOtzOne6D3teHH8KTT8Lx4/+sK1EC\nXnsNMnMjWsGC0LZteN9zbvIeSwQme330ETz/PKjPzV+bN7uunbNnu3sWz9CxYzBiBLzwQtqNtF26\nwC23uMci0Lw5nH/+GT+dMXmGJQKTfVTd7TK7d0ODBv+sr1TJtZxmMgkcOODuzgFXdz9wIKxeDf36\nuc5QvipWhFatrF7emLRYIjDZJy7OfVK/+SYMGpSpXU+d+me4gxMn3EXFK6+kHAfnggtcP7IOHbIw\nZmPCgCUCk33eew8KFHADv2XCkSPQujX8+GPK9QMG/HNhUbCg6zpQqlQWxWpMGLFEYLLHyZPwwQdw\n3XWZHrzmgQdcEhg+/J8OVY0aQbNmQYjTmDBkicBkj+XL3ZjD3boFvMvRo+5On//+13UyfvrpIMZn\nTBizRGCyx8KF7nerVgEVj42Fvn1dh6877nAdjY0xwWEd2E32WLQILr007a62qXz0ETRp4voCzJnj\nhh4qUCAbYjQmTNkVgQm+U6fcXAABDKa/YYNrBG7UyCWBXDx9gDG5hl0RmOBbvRr27nVDR6RjzRo3\nenT+/G7MfUsCxmQPSwQm+JLaB/wkgpMn3bzy9eu7oYY+/DBzwz4YY86OVQ2Z4DpyBMaN42iFi+jQ\n/0J27z69yN698OefcNNNrq/Zeedlf5jGhDNLBCY4du+GlSth/HhYvZqbZA6/AdHRpxfNl8+ND9S1\nqw0BYUwoWCIwwdGtG8ydC8DzDCN/x/b88p4b8dMYk7NYIjBZ788/Ye5cjt02iC4z+7EushHL3ncT\nshhjch5LBCbrTZkCwFvFhzIroSo/fmFJwJiczO4aMllLFSZNIvHKFjw5sSqdO0PjxqEOyhiTnqAm\nAhFpLyLrRGSDiAxLY3tlEZkvIj+LyEoRsQGEc7vPP4fffmN64T7s3QtPPJHxLsaY0ApaIhCRCGAM\ncC1QE+ghIjVTFRsOTFfV+kB34M1gxWOC6+RJmPDsVvZ3vY3fikQx4NteKYaJNsbkXMG8ImgMbFDV\njap6HIgBOqcqo0BS7XEJYGsQ4zFBsn49tGp+ksrDbyX/8cPcf34Mb00szDvvhDoyY0wggtlYXAHY\n5LO8Gbg8VZmngK9FZAhQFLgqrQOJyEBgIEBl63KaI/z9N4wa5boLvP8+DNMXacu36NvvMuu26qEO\nzxiTCaFuLO4BTFTVikAH4D0ROS0mVR2nqtGqGl2uXLlsD9KklJjoOn+NHOlGCh0S/T8eP/E4dOuG\nDOgf6vCMMZkUzESwBajks1zRW+frNmA6gKr+DygMlA1iTCYLjBjhhg+aMAF2b9zLC3/1QCpVgrfe\nsq7BxuRCwUwES4FqIlJVRAriGoNnpirzF9AWQERq4BJBQhBjMmdp7lw3SUz//nDrrcB998HmzTB1\nqnUbNiaXCloiUNVEYDDwFbAWd3fQLyIyQkQ6ecUeBO4QkThgKtBPVTVYMZmzs2MH9O4NNWrA668D\nCQmu89g997iZZIwxuVJQexar6mxgdqp1T/g8XgM0DWYMJus89ZRrHJ43D4oWBd75wDUY3HZbqEMz\nxpyFUDcWm1xi0yZ49133mV+rlrdy4kQ3nGjyCmNMbmSJwATk+efd738PUzdmdIcOsGKFm2HeGJOr\nWSIwGdq7100r0KcPVF4wGYYNg40boXVr6NUr1OEZY86SjT5qMvThh3D0KNxz3e9w693QsqVrKIiI\nCHVoxpgsYFcEJkOTJkH16lBn3quucfj99y0JGJOHWCIw6dqwAb77Dvr3Oo7ETIUbboCKFUMdljEm\nC1kiMH7FxkKnTpA/PwwoPxt27XINBcaYPMUSgUnT9u2uKWD/fpg9G8rOmgTnnQdXXx3q0IwxWcwS\ngUnTyJGugfjbb6FdxLfw2WfQr5+7PDDG5Cn2X21O8/ffMHYs9OwJl5ZKgFa94bLL4PHHQx2aMSYI\n7IrApLB+vWsPPnIEHntU3ehyu3ZBTIw3roQxJq+xKwKTbN06aNQI8uVzd4hW/+Z1mDULRo+GqKhQ\nh2eMCRJLBAZwVwBdu0LBgu5uoSol9sAFj0DHjjB4cKjDM8YEkVUNGbZsgc6dYeVKmDwZqlThn+7E\nTz5pk80Yk8dZIghj998PBQpApUqu09i4cW4sOQDee89NPNCwYUhjNMYEn1UNhan334dXX4UuXdwo\n0rfeCpdcghtCYv16WLIEnnvOrgaMCQOWCMLQb7/BXXdB8+buZqDkrgG//w7NmrneZGAjixoTJiwR\nhJmjR12jcOHC8MEHPkng2DG34ehRN/nApZdC5cohjdUYkz2CmghEpD3wGhABvKOqz6fa/grQ2ls8\nBzhXVUsGM6Zw9+CDEBcHX3yRauy455+H5cvh009dy7ExJmwELRGISAQwBmgHbAaWishMb55iAFT1\nfp/yQ4D6wYrHwIwZ8Oab8NBDcN11PhtOnvynpdiSgDFhJ5h3DTUGNqjqRlU9DsQA6X3K9ACmBjGe\nsPX889C4sZtVsnFjePbZVAXmzYOtW10vYmNM2AlmIqgAbPJZ3uytO42IXAhUBb71s32giMSKSGxC\nQkKWB5qXxce7IYIOH3Z9w6ZPd53GUpg0CUqVguuvD0WIxpgQyymNxd2Bj1T1ZFobVXUcMA4gOjpa\nszOw3O7//s8NGfHll2nMJ/P66/Dii7BtG9x5JxQqFJIYjTGhFcwrgi1AJZ/lit66tHTHqoWy3B9/\nwIQJcNttfiYVe+cdd9vQ7bfD0KHZHp8xJmcIZiJYClQTkaoiUhD3YT8zdSERqQ6UAv4XxFjCzokT\n0Lu3u0303/9Oo8Du3bBqlcsS//2vN66EMSYcBS0RqGoiMBj4ClgLTFfVX0RkhIh08inaHYhRVavy\nyUIjRsD337ubgSpVSqPAokWgCq1aZXdoxpgcJqhtBKo6G5idat0TqZafCmYM4WjLFlf1f+ut0L27\nn0ILF7rLhUaNsjU2Y0zOY4PO5UEvvginTrmrAr8WLIArr7QGYmOMJYK85o8/XHXQrbemU+3/99+u\ne7FVCxljsESQp8TEQP36EBEBjz7qp5Aq3HGHu1uoS5dsjc8YkzPllH4E5iwtWeLuEmrc2E0uc8kl\nPhunT3eDC4G7W2jWLHjlFahZMySxGmNyFksEudz8+bB2res4VqWK6zgWGelT4Phx+Ne/3HhCpUq5\ndQMHwr33hiJcY0wOZIkgF/vrL7jmGtdnoHhxlxRSJAGA2bNh1y53FZA8/ZgxxvzDEkEu9sIL7ndc\nHFx8MRQtmkahyZPhvPPg6quzNTZjTO5hjcW51JYtboSI/v2hbl0/SWDXLtc20KuXzww0xhiTkiWC\nXGjxYjfNJMCwYX4Kqbp2gJMnbXhpY0y6LBHkMocPw7XXuhFFv/0Wqlb1U3D8eJgyBZ58EmrXztYY\njTG5iyWCXOaHH+DQIRg9Gpo29VNo9WoYMgTatIHHHsvW+IwxuY9VHOcyCxe6q4FmzdLYGBvrRhR9\n6SV3+9CUKa53mTHGpMMSQS6zYIHrPXzabaJHj0K7drB3r5uCbNYsKF8+FCEaY3IZqxrKRY4ehR9/\n9DNE0GefuSQwfTps3w5XXZXd4RljcqkME4GIDBGRUtkRjEnfjz/CsWPQsmUaGydOdBMPdOnyTw9i\nY4wJQCBXBOcBS0Vkuoi0FxEJdlAmbXPngkiq9oH4ePj0U/j6azfkaD67yDPGZE6GnxqqOhyoBrwL\n9APWi8hzInJxkGMzPlThgw/cjUDJX/hXr3YDx914o1vu2zdk8Rljcq+Avj5600hu934ScXMMfyQi\nLwYxNuPju+9g40bo0wfYutWt6NrVtRp/+y2sWQOXXhrqMI0xuVCGdw2JyL1AH2An8A4wVFVPiEg+\nYD3wcDr7tgdeAyKAd1T1+TTKdAWeAhSIU9WeZ/A68rxJk9wwEjdXi4OLm7iWYxH45hto3TrU4Rlj\ncrFAbh8tDdykqn/6rlTVUyLS0d9OIhIBjAHaAZtx7QwzVXWNT5lqwL+Bpqq6R0TOPZMXkdcdPOhu\nBurV6QDn9OsKpUu7acguughq1Ah1eMaYXC6QRDAH2J20ICKRQA1V/VFV16azX2Ngg6pu9PaLAToD\na3zK3AGMUdU9AKr6dybjDwtjx8L+/cqzuwfBhg1uvOkWLUIdljEmjwikjWAscNBn+aC3LiMVgE0+\ny5u9db4uBS4Vke9E5AevKuk0IjJQRGJFJDYhISGAp847Dh88xZTnN/F69Tcp+9UU+M9/LAkYY7JU\nIFcE4jUWA8lVQlnVIzk/7o6kVkBFYJGI1FHVvb6FVHUcMA4gOjpaUx8kL/v1+qGs2D3KXZO1bQv/\n/neoQzLG5DGBXBFsFJF7RKSA93MvsDGA/bYAlXyWK3rrfG0GZqrqCVX9A/gNlxgMwLFjXPLdRH46\np5WbYObjj23sIGNMlgskEdwFXIn7EN8MXA4MDGC/pUA1EakqIgWB7sDMVGU+xV0NICJlcVVFgSSZ\nsKBfzCLyxG6+a/qw6yx22gBDxhhz9jKs4vEacLtn9sCqmigig4GvcLePjlfVX0RkBBCrqjO9bVeL\nyBrgJO7W1F2Zfa686vBbk9lPeYre0C7UoRhj8rBA+hEUBm4DagGFk9ar6oCM9lXV2cDsVOue8Hms\nwAPej/GVkECRb2fxJvdx1RU2SKwxJngCqRp6DygPXAMsxNX1HwhmUAaYOpV8JxOJKdjXJhgzxgRV\nIIngElV9HDikqpOA63DtBCaYJk1iXdEGFGpYmwIFQh2MMSYvCyQRnPB+7xWR2kAJwHoAB9Pq1bB8\nOeOO96Vx41AHY4zJ6wKpfB7nzUcwHHfXTzHg8aBGFe7GjuVkvvxMPtGDydeEOhhjTF6XbiLwBpbb\n7w0BsQi4KFuiCmdz5sCbbzKOQXTsV45rrw11QMaYvC7dqiFVPUU6o4uaLLZlC/Tpw4Zz6jL6wpd5\n441QB2SMCQeBtBHMFZGHRKSSiJRO+gl6ZOEmMRF69uTU4SNcd3g6/QYVoWjRUAdljAkHgbQRdPN+\n3+2zTrFqoqyRkOBmFtuyBVau5JMb3mP9Z5fRq1eoAzPGhItAehZXzY5AwtaECa5doFUrTj37HA+9\n3Zu2baFixVAHZowJF4H0LO6T1npVnZz14YQZVZg4Ea68EubPZ3ksxD8GTz0V6sCMMeEkkKqhRj6P\nCwNtgeWAJYKzFRsLa9e62caABQvc6quvDl1IxpjwE0jV0BDfZREpCcQELaJw8u67ULiwm4QeWLQI\nqlWD888PcVzGmLASyF1DqR0CrN3gbC1YAG+/7RqKS5Tg5ElYvNgmHzPGZL9A2gg+x90lBC5x1ASm\nBzOoPO/vv6FnT/f1f+RIAFatgr17oWXLEMdmjAk7gbQRjPR5nAj8qaqbgxRP3nfqlJtkZs8e+PJL\nKFYMgIUL3WZLBMaY7BZIIvgL2KaqRwFEpIiIVFHV+KBGlpd8+CH88IN7/Oef8PXX8NZbULcu4PqS\nxcRA1apQuXII4zTGhKVAEsGHuKkqk5z01jVKu7hJ4cQJGDAAjh+HggXdun/9C+64I7nI00+7PDHZ\n7sMyxoRAIIkgv6oeT1pQ1ePeHMQmEMuXw8GDMH063HJLik0nTsBzz7lE0LevqzEyxpjsFshdQwki\n0ilpQUQ6AzsDObiItBeRdSKyQUSGpbG9n4gkiMgK7+f2wEPPJZI6B6RR+d+nj+s81rMnjBmTrVEZ\nY0yyQK4I7gKmiEjSWJibgTR7G/sSkQhgDNDO22epiMxU1TWpik5T1cGZiDl3WbgQatSAc1PO5bN9\nu2s6uO8+eOWVEMVmjDEE1qHsd6CJiBTzlg8GeOzGwAZV3QggIjFAZyB1Isi7EhNd54A06nw++ABO\nnoQ77wxBXMYY4yPDqiEReU5ESqrqQVU9KCKlROSZAI5dAdjks7zZW5daFxFZKSIfiUglPzEMFJFY\nEYlNSEgI4KlzgMWLXQPAwYNpVgtNmgSNG0P16iGIzRhjfATSRnCtqu5NWvBmK+uQRc//OVBFVesC\n3wCT0iqkquNUNVpVo8uVK5dFTx1Ee/ZAu3bw5JNQpAi0bk1sLNSq5fqQXXIJrFzp2giMMSbUAmkj\niBCRQqp6DFw/AqBQAPttAXy/4Vf01iVT1V0+i+8ALwZw3Jxv2jQ4dgy++cZ97Y+M5LFbXbtA+/au\nSNu2lgiMMTlDIIlgCjBPRCYAAvTDzzf3VJYC1USkKi4BdAd6+hYQkfNVdZu32AlYG2DcOdukSVC7\ntvu0F+GHH1wfshdfhKFDQx2cMcakFEhj8QsiEgdchRtz6CvgwgD2SxSRwV75CGC8qv4iIiOAWFWd\nCdzj3ZqaCOzGJZncbd061zts5EgQAVw/gTJlYNCgEMdmjDFpCOSKAGAHLgncAvwBzAhkJ1WdDcxO\nte4Jn8f/Bv4dYAw5nyo8+ijkz0/SXJOxsTB7Njz7bPKwQsYYk6P4TQQicinQw/vZCUwDRFVbZ1Ns\nuc+YMfDxx/DSS1C+PADPPAMlS8LgvNtTwhiTy6V319CvQBugo6o2U9XXceMMmbTs2eMaADp0gAce\nAGDFCvjsM7j/foiMDHF8xhjjR3qJ4CZgGzBfRN4Wkba4xmID7q6guDj45RdXJTRtGhw96hoE8rnT\n+swzLgHcc0+IYzXGmHT4TQSq+qmqdgeqA/OB+4BzRWSsiNisuvffD/XqubuD7r7bTUJfuzbUrw/A\n6tUwY4ZLAiVLhjZUY4xJTyB3DR0CPgA+EJFSuAbjR4CvgxxbznXkCEyZAtddBxdcAGPHuvU+dwol\nNQ7fd18I4zTGmABkas5iVd3j9fJtG6yAcoVPP4X9++HBB10D8ZVXprhTKGlAubvucreNGmNMThbo\n7aPG18SJcOGFbgyhfPlgzhyIj0++U2jqVDeg3IABIY3SGGMCkqkrAgPs3Alz57oRRb1GYSIjk6ed\nBNexuFEjN/q0McbkdJYIMmv5cjcBfdu0a8fi4txP377ZHJcxxpwhSwSZtWKF+x0VddqmU6fg4Yeh\ncGHo3j2b4zLGmDNkbQSZtWIFVK4MpUqdtumFF9zgcv/9rzUSG2NyD0sEmbVihes/4OPAAXjoIRg3\nDrp2hYEDQxSbMcacAasaynEOjCoAABqVSURBVIzDh93ooj6JYOFC10789tuuWmjSpOSuBMYYkytY\nIsiM1atdQ0C9ehw54joXt2oFERFuZsoXXnDtA8YYk5tYIsiMpIbievV49FF49VU3ukRcHDRtGtrQ\njDHmTFkbQWb8/DNERqIXVmHGDOjcGd54I9RBGWPM2bErgsxYvBiaNGH1L8KmTdCxY6gDMsaYs2eJ\nIFAJCW7I6VatmDXLrerQIbQhGWNMVghqIhCR9iKyTkQ2iMiwdMp1EREVkehgxnNWFi1yv1u25Isv\noEEDN/CoMcbkdkFLBCISAYwBrgVqAj1EpGYa5YoD9wI/BiuWLLFwIZxzDutLRPO//7kRqI0xJi8I\n5hVBY2CDqm5U1eNADNA5jXJPAy8AR4MYy9lbsIBTTa6ke5+ClCwJd94Z6oCMMSZrBDMRVAA2+Sxv\n9tYlE5EGQCVVnZXegURkoIjEikhsQkJC1keanvHjoVkzWLWKuYmtWL7crapQIeNdjTEmNwhZY7GI\n5ANGAQ9mVNabDCdaVaPLlSsX/OB8vfEGrF/Psas6cP9PPejZ0902aowxeUUwE8EWoJLPckVvXZLi\nQG1ggYjEA02AmTmqwfjECXenUL9+PNlwFmuPXcQTT4Q6KGOMyVrBTARLgWoiUlVECgLdgZlJG1V1\nn6qWVdUqqloF+AHopKqxQYwpc379FY4f51C1erzxBnTrBpddFuqgjDEmawUtEahqIjAY+ApYC0xX\n1V9EZISIdArW82Ypb0iJ7w9FcegQ/OtfIY7HGGOCIKhDTKjqbGB2qnVpVq6oaqtgxnJG4uKgcGFm\nb7iUIkXg8stDHZAxxmQ961mcnhUroE4d5i/OzxVXQMGCoQ7IGGOyniUCf1RhxQqOVY9i5Upo2TLU\nARljTHBYIvBnyxbYtYt1ReqhaonAGJN3WSJIiyoMGwb58vHV0ZYUKmTtA8aYvMsSQVrGj4cpUzjx\n2FOM+ro2rVvbzGPGmLzLEkFqq1fDkCHQti3jyj7K9u3wyCOhDsoYY4LHZijzdfgwdO0KkZEce/d9\n/q9pBM2bW/uAMSZvs0Tga9IkWLsWvvySR0eXZ8sWmDwZREIdmDHGBI9VDfmaOBHq1GHWiasZNcpN\nTN+mTaiDMsaY4LJEkGTtWvjpJ+jbl8efEGrUgJEjQx2UMcYEnyWC48dhxgwYMQIiIth7XS9WrHAD\nzNmdQsaYcGBtBNOmQZ8+7vGNN7J4fXnrQGaMCSuWCJYtgyJF4OefoUoVFg13YwpZBzJjTLiwRBAX\nB3XrJk80sHChSwJFioQ4LmOMySbh3UbgDSxHvXoAHDgAy5dDixYhjssYY7JReCeCv/6CvXuTE8H8\n+XDypLUPGGPCS3gngrg49zsqCoD334eyZS0RGGPCS3gnghUrXLfhOnXYswc++wx69rQJaIwx4SWo\niUBE2ovIOhHZICLD0th+l4isEpEVIrJERGoGM57TxMVBtWpQrBjTp7suBUl3khpjTLgIWiIQkQhg\nDHAtUBPokcYH/QeqWkdV6wEvAqOCFU+ali2DqChU4d13oVYtaNAgWyMwxpiQC+YVQWNgg6puVNXj\nQAzQ2beAqu73WSwKaBDjSemvv+DPP6FZM+bOhaVLYfBgG2DOGBN+gtmPoAKwyWd5M3BaNy0RuRt4\nACgIpDnEm4gMBAYCVK5cOWuiW7gQAG3ZihGDoWJF6N8/aw5tjDG5Scgbi1V1jKpeDDwCDPdTZpyq\nRqtqdLly5bLmiRcsgNKlWbK3NkuWuMlnChXKmkMbY0xuEsxEsAWo5LNc0VvnTwxwQxDjSWnBAmjR\ngnfG5yMyEgYMyLZnNsaYHCWYVUNLgWoiUhWXALoDPX0LiEg1VV3vLV4HrCc7bNoEGzdybOAQZjwN\nPXrAOedkyzMbk2VOnDjB5s2bOXr0aKhDMTlI4cKFqVixIgUKFAh4n6AlAlVNFJHBwFdABDBeVX8R\nkRFArKrOBAaLyFXACWAP0DdY8aTw7bcAfHO8JYcO2S2jJnfavHkzxYsXp0qVKojd5WAAVWXXrl1s\n3ryZqlWrBrxfUAedU9XZwOxU657weXxvMJ/frylToEoVXl8URdWq0KxZSKIw5qwcPXrUkoBJQUQo\nU6YMCQkJmdov5I3F2W7TJpg7l1O9+7BoST46d7ZbRk3uZUnApHYm74nwSwTvvw+qbGzWh6NHk8eb\nM8aYsBVeiUAVJk2C5s2J3XMxkDzenDEmk3bt2kW9evWoV68e5cuXp0KFCsnLx48fD+gY/fv3Z926\ndemWGTNmDFOmTMmKkI0f4TUxzY8/wrp1MHQocXFQoADUzN7RjYzJM8qUKcOKFSsAeOqppyhWrBgP\nPfRQijKqiqqSL1/a3zknTJiQ4fPcfffdZx9sNktMTCR//tzz8RpeVwSTJrmpx265hbg4qFHDRho1\necN990GrVln7c999ZxbLhg0bqFmzJr169aJWrVps27aNgQMHEh0dTa1atRgxYkRy2WbNmrFixQoS\nExMpWbIkw4YNIyoqiiuuuIK///4bgOHDh/Pqq68mlx82bBiNGzfmsssu4/vvvwfg0KFDdOnShZo1\na3LzzTcTHR2dnKR8PfnkkzRq1IjatWtz1113oepGtfntt99o06YNUVFRNGjQgPj4eACee+456tSp\nQ1RUFI899liKmAG2b9/OJZdcAsA777zDDTfcQOvWrbnmmmvYv38/bdq0oUGDBtStW5cvvvgiOY4J\nEyZQt25doqKi6N+/P/v27eOiiy4iMTERgD179qRYDrbwSQRHj0JMDNx0E0RGEhdn1ULGBMuvv/7K\n/fffz5o1a6hQoQLPP/88sbGxxMXF8c0337BmzZrT9tm3bx8tW7YkLi6OK664gvHjx6d5bFXlp59+\n4qWXXkpOKq+//jrly5dnzZo1PP744/z8889p7nvvvfeydOlSVq1axb59+/jyyy8B6NGjB/fffz9x\ncXF8//33nHvuuXz++efMmTOHn376ibi4OB588MEMX/fPP//Mxx9/zLx58yhSpAiffvopy5cvZ+7c\nudx///0AxMXF8cILL7BgwQLi4uJ4+eWXKVGiBE2bNk2OZ+rUqdxyyy3ZdlWRe65dztbMmW42sr59\nSUiArVstEZi8w/vCnGNcfPHFREdHJy9PnTqVd999l8TERLZu3cqaNWuomapetkiRIlx77bUANGzY\nkMWLF6d57Jtuuim5TNI39yVLlvDII48AEBUVRa1atdLcd968ebz00kscPXqUnTt30rBhQ5o0acLO\nnTu5/vrrAdchC2Du3LkMGDCAIt4E5qVLl87wdV999dWUKlUKcAlr2LBhLFmyhHz58rFp0yZ27tzJ\nt99+S7du3ZKPl/T79ttvZ/To0XTs2JEJEybw3nvvZfh8WSV8EoEItG4NbdoQN9+tskRgTHAULVo0\n+fH69et57bXX+OmnnyhZsiS9e/dOszd0QZ962oiICL/VIoW8QcHSK5OWw4cPM3jwYJYvX06FChUY\nPnz4GfXKzp8/P6dOnQI4bX/f1z158mT27dvH8uXLyZ8/PxUrVkz3+Vq2bMngwYOZP38+BQoUoHr1\n6pmO7UyFT9XQLbe4HsUREXz/vcsL9euHOihj8r79+/dTvHhxIiMj2bZtG1999VWWP0fTpk2ZPn06\nAKtWrUqz6unIkSPky5ePsmXLcuDAAWbMmAFAqVKlKFeuHJ9//jngPtwPHz5Mu3btGD9+PEeOHAFg\n9+7dAFSpUoVly5YB8NFHH/mNad++fZx77rnkz5+fb775hi1b3FBrbdq0Ydq0acnHS/oN0Lt3b3r1\n6kX/bB4KOXwSgY9Zs6BxYyhTJtSRGJP3NWjQgJo1a1K9enX69OlD06ZNs/w5hgwZwpYtW6hZsyb/\n+c9/qFmzJiVKlEhRpkyZMvTt25eaNWty7bXXcvnl/4yKP2XKFF5++WXq1q1Ls2bNSEhIoGPHjrRv\n357o6Gjq1avHK6+8AsDQoUN57bXXaNCgAXv27PEb06233sr3339PnTp1iImJoVq1aoCrunr44Ydp\n0aIF9erVY+jQocn79OrVi3379tGtW7esPD0ZkqRW89wiOjpaY2Njz3j/HTvg/PPhP/+Bxx/PwsCM\nyWZr166lRo0aoQ4jR0hMTCQxMZHChQuzfv16rr76atavX5+rbuEEiImJ4auvvgrottr0pPXeEJFl\nqhqdVvncdZaywJw5rl9Zx46hjsQYk1UOHjxI27ZtSUxMRFV56623cl0SGDRoEHPnzk2+cyg75a4z\nlQVmzYILLrChJYzJS0qWLJlcb59bjR07NmTPHVZtBKowbx5cc40NNGeMMUnCKhHs2eN+atcOdSTG\nGJNzhFUi+P139/vii0MbhzHG5CRhlQg2bnS/L7ootHEYY0xOEpaJIBMzuBlj/GjduvVpncNeffVV\nBg0alO5+xYoVA2Dr1q3cfPPNaZZp1aoVGd0m/uqrr3L48OHk5Q4dOrB3795AQjepBDURiEh7EVkn\nIhtEZFga2x8QkTUislJE5onIhcGM5/ff4dxzwXsfGmPOQo8ePYiJiUmxLiYmhh49egS0/wUXXJBu\nz9yMpE4Es2fPpmTJkmd8vOymqslDVYRa0BKBiEQAY4BrgZpADxFJPfr/z0C0qtYFPgJeDFY84K4I\nrH3A5EkhGIf65ptvZtasWcmT0MTHx7N161aaN2+efF9/gwYNqFOnDp999tlp+8fHx1Pbu3PjyJEj\ndO/enRo1anDjjTcmD+sA7v76pCGsn3zySQBGjx7N1q1bad26Na1btwbc0A87d+4EYNSoUdSuXZva\ntWsnD2EdHx9PjRo1uOOOO6hVqxZXX311iudJ8vnnn3P55ZdTv359rrrqKnbs2AG4vgr9+/enTp06\n1K1bN3mIii+//JIGDRoQFRVF27ZtATc/w8iRI5OPWbt2beLj44mPj+eyyy6jT58+1K5dm02bNqX5\n+gCWLl3KlVdeSVRUFI0bN+bAgQO0aNEixfDazZo1Iy4uLt2/UyCC2Y+gMbBBVTcCiEgM0BlIHgRE\nVef7lP8B6B3EeNi40SaqNyarlC5dmsaNGzNnzhw6d+5MTEwMXbt2RUQoXLgwn3zyCZGRkezcuZMm\nTZrQqVMnv/Ppjh07lnPOOYe1a9eycuVKGjRokLzt2WefpXTp0pw8eZK2bduycuVK7rnnHkaNGsX8\n+fMpW7ZsimMtW7aMCRMm8OOPP6KqXH755bRs2ZJSpUqxfv16pk6dyttvv03Xrl2ZMWMGvXun/Nhp\n1qwZP/zwAyLCO++8w4svvsjLL7/M008/TYkSJVi1ahXg5gxISEjgjjvuYNGiRVStWjXFuEH+rF+/\nnkmTJtGkSRO/r6969ep069aNadOm0ahRI/bv30+RIkW47bbbmDhxIq+++iq//fYbR48eJSoLRs8M\nZiKoAGzyWd4MXO6nLMBtwJy0NojIQGAgQOXKlc8omOPH3bz11lBs8qQQjUOdVD2UlAjeffddwFV7\nPProoyxatIh8+fKxZcsWduzYQfny5dM8zqJFi7jnnnsAqFu3LnXr1k3eNn36dMaNG0diYiLbtm1j\nzZo1KbantmTJEm688cbkkUBvuukmFi9eTKdOnahatSr1vN6kvsNY+9q8eTPdunVj27ZtHD9+nKpe\no+LcuXNTVIWVKlWKzz//nBYtWiSXCWSo6gsvvDA5Cfh7fSLC+eefT6NGjQCIjIwE4JZbbuHpp5/m\npZdeYvz48fTr1y/D5wtEjmgsFpHeQDTwUlrbVXWcqkaranS5cuXO6Dn+/BNOnbKqIWOyUufOnZk3\nbx7Lly/n8OHDNGzYEHCDuCUkJLBs2TJWrFjBeeedd0ZDPv/xxx+MHDmSefPmsXLlSq677rozOk6S\npCGswf8w1kOGDGHw4MGsWrWKt95666yHqoaUw1X7DlWd2dd3zjnn0K5dOz777DOmT59Or169Mh1b\nWoKZCLYAlXyWK3rrUhCRq4DHgE6qeixYwdito8ZkvWLFitG6dWsGDBiQopE4aQjmAgUKMH/+fP78\n8890j9OiRQs++OADAFavXs3KlSsBN4R10aJFKVGiBDt27GDOnH8qDYoXL86BAwdOO1bz5s359NNP\nOXz4MIcOHeKTTz6hefPmAb+mffv2UaFCBQAmTZqUvL5du3aMGTMmeXnPnj00adKERYsW8ccffwAp\nh6pevnw5AMuXL0/enpq/13fZZZexbds2li5dCsCBAweSk9btt9/OPffcQ6NGjZInwTlbwUwES4Fq\nIlJVRAoC3YGZvgVEpD7wFi4J/B3EWCwRGBMkPXr0IC4uLkUi6NWrF7GxsdSpU4fJkydnOMnKoEGD\nOHjwIDVq1OCJJ55IvrKIioqifv36VK9enZ49e6YYwnrgwIG0b98+ubE4SYMGDejXrx+NGzfm8ssv\n5/bbb6d+JiYfeeqpp7jlllto2LBhivaH4cOHs2fPHmrXrk1UVBTz58+nXLlyjBs3jptuuomoqKjk\n4aO7dOnC7t27qVWrFm+88QaXXnppms/l7/UVLFiQadOmMWTIEKKiomjXrl3ylULDhg2JjIzM0jkL\ngjoMtYh0AF4FIoDxqvqsiIwAYlV1pojMBeoA27xd/lLVTukd80yHof7sM5g4EWbMgHw5okLMmLNj\nw1CHp61bt9KqVSt+/fVX8vn5MMtRw1Cr6mxgdqp1T/g8viqYz++rc2f3Y4wxudXkyZN57LHHGDVq\nlN8kcCbCbhhqY4zJrfr06UOfPn2y/LhWSWJMLpbbZhg0wXcm7wlLBMbkUoULF2bXrl2WDEwyVWXX\nrl0ULlw4U/tZ1ZAxuVTFihXZvHkzCQkJoQ7F5CCFCxemYsWKmdrHEoExuVSBAgWSe7QaczasasgY\nY8KcJQJjjAlzlgiMMSbMBbVncTCISAKQ/sAlaSsL7MzicLKCxZU5OTUuyLmxWVyZk1PjgrOL7UJV\nTXPUzlyXCM6UiMT6614dShZX5uTUuCDnxmZxZU5OjQuCF5tVDRljTJizRGCMMWEunBLBuFAH4IfF\nlTk5NS7IubFZXJmTU+OCIMUWNm0Exhhj0hZOVwTGGGPSYInAGGPCXJ5PBCLSXkTWicgGERkWwjgq\nich8EVkjIr+IyL3e+qdEZIuIrPB+OoQovngRWeXFEOutKy0i34jIeu931kyQGnhMl/mclxUisl9E\n7gvFOROR8SLyt4is9lmX5vkRZ7T3nlspIg1CENtLIvKr9/yfiEhJb30VETnic+7+m81x+f3bici/\nvXO2TkSuyea4pvnEFC8iK7z12Xm+/H1GBP99pqp59gc3RebvwEVAQSAOqBmiWM4HGniPiwO/ATWB\np4CHcsC5igfKplr3IjDMezwMeCHEf8vtwIWhOGdAC6ABsDqj8wN0AOYAAjQBfgxBbFcD+b3HL/jE\nVsW3XAjiSvNv5/0vxAGFgKre/21EdsWVavvLwBMhOF/+PiOC/j7L61cEjYENqrpRVY8DMUBIJqxU\n1W2qutx7fABYC1QIRSyZ0BmY5D2eBNwQwljaAr+r6pn0Kj9rqroI2J1qtb/z0xmYrM4PQEkROT87\nY1PVr1U10Vv8AcjcuMRBiisdnYEYVT2mqn8AG3D/v9kal4gI0BWYGoznTk86nxFBf5/l9URQAdjk\ns7yZHPDhKyJVgPrAj96qwd6l3fjsrn7xocDXIrJMRAZ6685T1W3e4+3AeaEJDYDupPznzAnnzN/5\nyWnvuwG4b45JqorIzyKyUESahyCetP52OeWcNQd2qOp6n3XZfr5SfUYE/X2W1xNBjiMixYAZwH2q\nuh8YC1wM1AO24S5LQ6GZqjYArgXuFpEWvhvVXYuG5F5jESkIdAI+9FbllHOWLJTnJz0i8hiQCEzx\nVm0DKqtqfeAB4AMRiczGkHLc3y6VHqT8wpHt5yuNz4hkwXqf5fVEsAWo5LNc0VsXEiJSAPcHnqKq\nHwOo6g5VPamqp4C3CdLlcEZUdYv3+2/gEy+OHUmXmt7vv0MRGy45LVfVHV6MOeKc4f/85Ij3nYj0\nAzoCvbwPELyql13e42W4uvhLsyumdP52IT9nIpIfuAmYlrQuu89XWp8RZMP7LK8ngqVANRGp6n2r\n7A7MDEUgXt3ju8BaVR3ls963Tu9GYHXqfbMhtqIiUjzpMa6hcTXuXPX1ivUFPsvu2DwpvqXlhHPm\n8Xd+ZgJ9vLs6mgD7fC7ts4WItAceBjqp6mGf9eVEJMJ7fBFQDdiYjXH5+9vNBLqLSCERqerF9VN2\nxeW5CvhVVTcnrcjO8+XvM4LseJ9lR2t4KH9wLeu/4TL5YyGMoxnukm4lsML76QC8B6zy1s8Ezg9B\nbBfh7tiIA35JOk9AGWAesB6YC5QOQWxFgV1ACZ912X7OcIloG3ACVxd7m7/zg7uLY4z3nlsFRIcg\ntg24+uOk99p/vbJdvL/xCmA5cH02x+X3bwc85p2zdcC12RmXt34icFeqstl5vvx9RgT9fWZDTBhj\nTJjL61VDxhhjMmCJwBhjwpwlAmOMCXOWCIwxJsxZIjDGmDBnicAYj4iclJSjnWbZaLXeKJah6u9g\nTLryhzoAY3KQI6paL9RBGJPd7IrAmAx449O/KG6+hp9E5BJvfRUR+dYbQG2eiFT21p8nbg6AOO/n\nSu9QESLytjfW/NciUsQrf483Bv1KEYkJ0cs0YcwSgTH/KJKqaqibz7Z9qloHeAN41Vv3OjBJVevi\nBnUb7a0fDSxU1SjcuPe/eOurAWNUtRawF9drFdwY8/W949wVrBdnjD/Ws9gYj4gcVNViaayPB9qo\n6kZvULDtqlpGRHbihkg44a3fpqplRSQBqKiqx3yOUQX4RlWrecuPAAVU9RkR+RI4CHwKfKqqB4P8\nUo1Jwa4IjAmM+nmcGcd8Hp/knza663BjxjQAlnqjYBqTbSwRGBOYbj6//+c9/h43oi1AL2Cx93ge\nMAhARCJEpIS/g4pIPqCSqs4HHgFKAKddlRgTTPbNw5h/FBFv0nLPl6qadAtpKRFZiftW38NbNwSY\nICJDgQSgv7f+XmCciNyG++Y/CDfaZVoigPe9ZCHAaFXdm2WvyJgAWBuBMRnw2giiVXVnqGMxJhis\nasgYY8KcXREYY0yYsysCY4wJc5YIjDEmzFkiMMaYMGeJwBhjwpwlAmOMCXP/D8SlbP156CN6AAAA\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "52db6399-3cfc-4e22-9cce-c68582a9bc57",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.1954 - acc: 0.5420\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1574 - acc: 0.5725\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.1221 - acc: 0.5649\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0884 - acc: 0.5649\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.0563 - acc: 0.5649\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0275 - acc: 0.5649\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0002 - acc: 0.5649\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9746 - acc: 0.5573\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9505 - acc: 0.5573\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9285 - acc: 0.5649\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.9079 - acc: 0.5649\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8879 - acc: 0.5802\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8689 - acc: 0.5878\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8518 - acc: 0.6565\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8362 - acc: 0.7099\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8214 - acc: 0.7786\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8075 - acc: 0.8168\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7944 - acc: 0.8397\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.7817 - acc: 0.8779\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.7695 - acc: 0.9008\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.7577 - acc: 0.9160\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.7466 - acc: 0.9237\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.7363 - acc: 0.9237\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.7254 - acc: 0.9237\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 0s 240us/step - loss: 0.7154 - acc: 0.9237\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7059 - acc: 0.9237\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.6967 - acc: 0.9313\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.6875 - acc: 0.9313\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6786 - acc: 0.9313\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.6699 - acc: 0.9313\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.6617 - acc: 0.9313\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6535 - acc: 0.9313\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.6454 - acc: 0.9313\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.6376 - acc: 0.9313\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6300 - acc: 0.9313\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6227 - acc: 0.9313\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.6156 - acc: 0.9313\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.6086 - acc: 0.9313\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6018 - acc: 0.9313\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5950 - acc: 0.9313\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5884 - acc: 0.9313\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.5820 - acc: 0.9313\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5756 - acc: 0.9313\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.5692 - acc: 0.9313\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5631 - acc: 0.9313\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5570 - acc: 0.9313\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5510 - acc: 0.9237\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.5451 - acc: 0.9313\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.5394 - acc: 0.9313\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.5337 - acc: 0.9237\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.5282 - acc: 0.9237\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5227 - acc: 0.9237\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.5174 - acc: 0.9237\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.5121 - acc: 0.9313\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.5068 - acc: 0.9237\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5018 - acc: 0.9313\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.4966 - acc: 0.9313\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4917 - acc: 0.9313\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4866 - acc: 0.9313\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4818 - acc: 0.9313\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4768 - acc: 0.9389\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.4721 - acc: 0.9389\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.4674 - acc: 0.9389\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.4627 - acc: 0.9389\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.4581 - acc: 0.9389\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4536 - acc: 0.9389\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4491 - acc: 0.9389\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.4447 - acc: 0.9389\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.4403 - acc: 0.9389\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.4361 - acc: 0.9389\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4319 - acc: 0.9389\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4277 - acc: 0.9389\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4236 - acc: 0.9389\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4195 - acc: 0.9389\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.4154 - acc: 0.9389\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.4114 - acc: 0.9389\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4075 - acc: 0.9389\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4036 - acc: 0.9389\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3997 - acc: 0.9389\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.3959 - acc: 0.9389\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.3922 - acc: 0.9389\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3885 - acc: 0.9389\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.3849 - acc: 0.9389\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3812 - acc: 0.9389\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.3776 - acc: 0.9389\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.3742 - acc: 0.9389\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.3707 - acc: 0.9389\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.3673 - acc: 0.9389\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.3639 - acc: 0.9389\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.3606 - acc: 0.9389\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.3573 - acc: 0.9389\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.3540 - acc: 0.9389\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.3508 - acc: 0.9389\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3476 - acc: 0.9466\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.3445 - acc: 0.9466\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.3414 - acc: 0.9466\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3384 - acc: 0.9466\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.3353 - acc: 0.9466\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.3323 - acc: 0.9542\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3294 - acc: 0.9542\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.3264 - acc: 0.9542\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3235 - acc: 0.9542\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3206 - acc: 0.9542\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.3178 - acc: 0.9542\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3150 - acc: 0.9618\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3123 - acc: 0.9618\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3096 - acc: 0.9618\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.3069 - acc: 0.9618\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.3042 - acc: 0.9618\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3016 - acc: 0.9618\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.2989 - acc: 0.9618\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.2964 - acc: 0.9618\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.2937 - acc: 0.9618\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2912 - acc: 0.9618\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.2887 - acc: 0.9618\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2862 - acc: 0.9618\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.2838 - acc: 0.9618\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2814 - acc: 0.9618\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.2791 - acc: 0.9618\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2767 - acc: 0.9618\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.2744 - acc: 0.9618\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.2721 - acc: 0.9618\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2698 - acc: 0.9618\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2675 - acc: 0.9618\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2653 - acc: 0.9618\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.2630 - acc: 0.9618\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.2607 - acc: 0.9618\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.2586 - acc: 0.9618\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2565 - acc: 0.9618\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.2543 - acc: 0.9618\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.2522 - acc: 0.9618\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2502 - acc: 0.9618\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2481 - acc: 0.9618\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2461 - acc: 0.9618\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2441 - acc: 0.9618\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2421 - acc: 0.9618\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.2401 - acc: 0.9618\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2382 - acc: 0.9618\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.2364 - acc: 0.9618\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.2345 - acc: 0.9618\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2326 - acc: 0.9618\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2308 - acc: 0.9618\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2289 - acc: 0.9618\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2271 - acc: 0.9618\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.2253 - acc: 0.9618\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2236 - acc: 0.9618\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.2218 - acc: 0.9618\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.2201 - acc: 0.9618\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.2184 - acc: 0.9618\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2167 - acc: 0.9618\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2150 - acc: 0.9695\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2133 - acc: 0.9695\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2117 - acc: 0.9695\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.2100 - acc: 0.9695\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.2084 - acc: 0.9695\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.2069 - acc: 0.9695\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2053 - acc: 0.9695\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2038 - acc: 0.9771\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.2023 - acc: 0.9771\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2007 - acc: 0.9771\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.1992 - acc: 0.9771\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1978 - acc: 0.9771\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1964 - acc: 0.9771\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.1950 - acc: 0.9771\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.1936 - acc: 0.9771\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.1922 - acc: 0.9771\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1908 - acc: 0.9771\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.1895 - acc: 0.9771\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.1882 - acc: 0.9771\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1869 - acc: 0.9847\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.1855 - acc: 0.9847\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.1842 - acc: 0.9847\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.1829 - acc: 0.9847\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1817 - acc: 0.9847\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1804 - acc: 0.9847\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1791 - acc: 0.9847\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1779 - acc: 0.9847\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.1767 - acc: 0.9847\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1755 - acc: 0.9847\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.1743 - acc: 0.9847\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.1731 - acc: 0.9847\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1720 - acc: 0.9847\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.1708 - acc: 0.9847\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.1697 - acc: 0.9847\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.1686 - acc: 0.9847\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.1675 - acc: 0.9847\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.1665 - acc: 0.9847\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1654 - acc: 0.9847\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1643 - acc: 0.9847\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.1633 - acc: 0.9847\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.1623 - acc: 0.9847\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1612 - acc: 0.9847\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.1602 - acc: 0.9847\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1592 - acc: 0.9847\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1582 - acc: 0.9847\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.1572 - acc: 0.9847\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.1563 - acc: 0.9847\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1553 - acc: 0.9847\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.1543 - acc: 0.9847\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1534 - acc: 0.9847\n",
            "34/34 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35e1976b-f813-4182-8842-6b839d99bf30",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "67bd4a44-79b4-40ba-b65a-5b751506666b",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11764705882352941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZN9I7JNNUyS",
        "colab_type": "text"
      },
      "source": [
        "# Prova remove correlated features with treshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wACff-R4Ib",
        "colab_type": "text"
      },
      "source": [
        "https://campus.datacamp.com/courses/dimensionality-reduction-in-python/feature-selection-i-selecting-for-feature-information?ex=14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNufM8B3NYs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a positive correlation matrix\n",
        "corr_df = train_data_stand.corr().abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8ShRUvN63m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create and apply mask\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DgfSECpOwPu",
        "colab_type": "code",
        "outputId": "5d4050b2-1b27-4008-fda7-8db4bf4affda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [False,  True,  True, ...,  True,  True,  True],\n",
              "       [False, False,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [False, False, False, ...,  True,  True,  True],\n",
              "       [False, False, False, ..., False,  True,  True],\n",
              "       [False, False, False, ..., False, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xQLptVOw6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri_df = corr_df.mask(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDvePEPO8mn",
        "colab_type": "code",
        "outputId": "d101c796-70b3-4395-cf5d-b017c85ab044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tri_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>VoxelVolume</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <td>0.821982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MeshVolume</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <td>0.785606</td>\n",
              "      <td>0.964760</td>\n",
              "      <td>0.785457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sphericity</th>\n",
              "      <td>0.329751</td>\n",
              "      <td>0.678628</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.694906</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coarseness</th>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.586992</td>\n",
              "      <td>0.401370</td>\n",
              "      <td>0.546184</td>\n",
              "      <td>0.356708</td>\n",
              "      <td>0.569919</td>\n",
              "      <td>0.048602</td>\n",
              "      <td>0.813595</td>\n",
              "      <td>0.614989</td>\n",
              "      <td>0.094676</td>\n",
              "      <td>0.471734</td>\n",
              "      <td>0.598815</td>\n",
              "      <td>0.567029</td>\n",
              "      <td>0.554227</td>\n",
              "      <td>0.192052</td>\n",
              "      <td>0.605729</td>\n",
              "      <td>0.246122</td>\n",
              "      <td>0.406714</td>\n",
              "      <td>0.351606</td>\n",
              "      <td>0.669357</td>\n",
              "      <td>0.183179</td>\n",
              "      <td>0.731964</td>\n",
              "      <td>0.417524</td>\n",
              "      <td>0.077465</td>\n",
              "      <td>0.379163</td>\n",
              "      <td>0.437111</td>\n",
              "      <td>0.787425</td>\n",
              "      <td>0.182358</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>0.119708</td>\n",
              "      <td>0.324753</td>\n",
              "      <td>0.661086</td>\n",
              "      <td>0.326758</td>\n",
              "      <td>0.468498</td>\n",
              "      <td>0.481677</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.319663</td>\n",
              "      <td>0.667850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350082</td>\n",
              "      <td>0.292163</td>\n",
              "      <td>0.539925</td>\n",
              "      <td>0.345026</td>\n",
              "      <td>0.148147</td>\n",
              "      <td>0.251303</td>\n",
              "      <td>0.399211</td>\n",
              "      <td>0.336092</td>\n",
              "      <td>0.360524</td>\n",
              "      <td>0.381037</td>\n",
              "      <td>0.556410</td>\n",
              "      <td>0.412333</td>\n",
              "      <td>0.521415</td>\n",
              "      <td>0.480184</td>\n",
              "      <td>0.512788</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.194296</td>\n",
              "      <td>0.595126</td>\n",
              "      <td>0.552550</td>\n",
              "      <td>0.201612</td>\n",
              "      <td>0.272599</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.374723</td>\n",
              "      <td>0.426885</td>\n",
              "      <td>0.417651</td>\n",
              "      <td>0.272696</td>\n",
              "      <td>0.501061</td>\n",
              "      <td>0.667822</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.511486</td>\n",
              "      <td>0.366279</td>\n",
              "      <td>0.704958</td>\n",
              "      <td>0.523589</td>\n",
              "      <td>0.754391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Complexity</th>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.257297</td>\n",
              "      <td>0.277025</td>\n",
              "      <td>0.229141</td>\n",
              "      <td>0.182615</td>\n",
              "      <td>0.268450</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.214902</td>\n",
              "      <td>0.256226</td>\n",
              "      <td>0.019975</td>\n",
              "      <td>0.283738</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>0.441669</td>\n",
              "      <td>0.213055</td>\n",
              "      <td>0.423394</td>\n",
              "      <td>0.392267</td>\n",
              "      <td>0.103854</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.422629</td>\n",
              "      <td>0.058114</td>\n",
              "      <td>0.198407</td>\n",
              "      <td>0.214767</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.125140</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.275899</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.322120</td>\n",
              "      <td>0.410309</td>\n",
              "      <td>0.276311</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>0.275068</td>\n",
              "      <td>0.221594</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.125930</td>\n",
              "      <td>0.298911</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267658</td>\n",
              "      <td>0.198372</td>\n",
              "      <td>0.116352</td>\n",
              "      <td>0.255976</td>\n",
              "      <td>0.464523</td>\n",
              "      <td>0.267090</td>\n",
              "      <td>0.228132</td>\n",
              "      <td>0.195523</td>\n",
              "      <td>0.139835</td>\n",
              "      <td>0.198482</td>\n",
              "      <td>0.296969</td>\n",
              "      <td>0.359280</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>0.168824</td>\n",
              "      <td>0.228273</td>\n",
              "      <td>0.355694</td>\n",
              "      <td>0.220089</td>\n",
              "      <td>0.151630</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.011797</td>\n",
              "      <td>0.469854</td>\n",
              "      <td>0.036549</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.274863</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.351102</td>\n",
              "      <td>0.063147</td>\n",
              "      <td>0.152285</td>\n",
              "      <td>0.074162</td>\n",
              "      <td>0.313890</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.083326</td>\n",
              "      <td>0.166795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strength</th>\n",
              "      <td>0.510155</td>\n",
              "      <td>0.642559</td>\n",
              "      <td>0.509976</td>\n",
              "      <td>0.607597</td>\n",
              "      <td>0.339595</td>\n",
              "      <td>0.645544</td>\n",
              "      <td>0.021965</td>\n",
              "      <td>0.758610</td>\n",
              "      <td>0.670268</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.562243</td>\n",
              "      <td>0.659201</td>\n",
              "      <td>0.624228</td>\n",
              "      <td>0.620794</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.388883</td>\n",
              "      <td>0.341710</td>\n",
              "      <td>0.538344</td>\n",
              "      <td>0.425689</td>\n",
              "      <td>0.458791</td>\n",
              "      <td>0.226763</td>\n",
              "      <td>0.486770</td>\n",
              "      <td>0.290047</td>\n",
              "      <td>0.085559</td>\n",
              "      <td>0.272032</td>\n",
              "      <td>0.325476</td>\n",
              "      <td>0.564383</td>\n",
              "      <td>0.104481</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.099738</td>\n",
              "      <td>0.184149</td>\n",
              "      <td>0.141523</td>\n",
              "      <td>0.432332</td>\n",
              "      <td>0.139576</td>\n",
              "      <td>0.296993</td>\n",
              "      <td>0.271914</td>\n",
              "      <td>0.332061</td>\n",
              "      <td>0.195143</td>\n",
              "      <td>0.422246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126145</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.283099</td>\n",
              "      <td>0.224183</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>0.155122</td>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.240018</td>\n",
              "      <td>0.438144</td>\n",
              "      <td>0.264077</td>\n",
              "      <td>0.333647</td>\n",
              "      <td>0.530074</td>\n",
              "      <td>0.343866</td>\n",
              "      <td>0.337177</td>\n",
              "      <td>0.347487</td>\n",
              "      <td>0.027798</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.372899</td>\n",
              "      <td>0.365016</td>\n",
              "      <td>0.083608</td>\n",
              "      <td>0.329544</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.390861</td>\n",
              "      <td>0.567592</td>\n",
              "      <td>0.552634</td>\n",
              "      <td>0.329531</td>\n",
              "      <td>0.225342</td>\n",
              "      <td>0.451164</td>\n",
              "      <td>0.206294</td>\n",
              "      <td>0.221538</td>\n",
              "      <td>0.253885</td>\n",
              "      <td>0.383804</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.544355</td>\n",
              "      <td>0.611316</td>\n",
              "      <td>0.746738</td>\n",
              "      <td>0.074615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contrast.1</th>\n",
              "      <td>0.458069</td>\n",
              "      <td>0.592542</td>\n",
              "      <td>0.457835</td>\n",
              "      <td>0.555373</td>\n",
              "      <td>0.380133</td>\n",
              "      <td>0.588236</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>0.639843</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.065998</td>\n",
              "      <td>0.515229</td>\n",
              "      <td>0.623406</td>\n",
              "      <td>0.576338</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.699380</td>\n",
              "      <td>0.654218</td>\n",
              "      <td>0.165224</td>\n",
              "      <td>0.399133</td>\n",
              "      <td>0.462206</td>\n",
              "      <td>0.847990</td>\n",
              "      <td>0.404501</td>\n",
              "      <td>0.807251</td>\n",
              "      <td>0.503620</td>\n",
              "      <td>0.040094</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>0.507856</td>\n",
              "      <td>0.811825</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.674231</td>\n",
              "      <td>0.184830</td>\n",
              "      <td>0.427215</td>\n",
              "      <td>0.958897</td>\n",
              "      <td>0.507007</td>\n",
              "      <td>0.941239</td>\n",
              "      <td>0.822280</td>\n",
              "      <td>0.836388</td>\n",
              "      <td>0.858476</td>\n",
              "      <td>0.916872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777488</td>\n",
              "      <td>0.592929</td>\n",
              "      <td>0.882098</td>\n",
              "      <td>0.561351</td>\n",
              "      <td>0.652744</td>\n",
              "      <td>0.457706</td>\n",
              "      <td>0.693751</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.473433</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.601669</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.679507</td>\n",
              "      <td>0.566054</td>\n",
              "      <td>0.635554</td>\n",
              "      <td>0.154862</td>\n",
              "      <td>0.617903</td>\n",
              "      <td>0.665488</td>\n",
              "      <td>0.705727</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.390378</td>\n",
              "      <td>0.398000</td>\n",
              "      <td>0.502327</td>\n",
              "      <td>0.354174</td>\n",
              "      <td>0.437122</td>\n",
              "      <td>0.390572</td>\n",
              "      <td>0.610009</td>\n",
              "      <td>0.852276</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.242269</td>\n",
              "      <td>0.640887</td>\n",
              "      <td>0.494642</td>\n",
              "      <td>0.794082</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.759051</td>\n",
              "      <td>0.551508</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.374697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Busyness</th>\n",
              "      <td>0.937894</td>\n",
              "      <td>0.810919</td>\n",
              "      <td>0.937881</td>\n",
              "      <td>0.773599</td>\n",
              "      <td>0.339451</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.682675</td>\n",
              "      <td>0.843581</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>0.913619</td>\n",
              "      <td>0.867644</td>\n",
              "      <td>0.805965</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>0.176933</td>\n",
              "      <td>0.469093</td>\n",
              "      <td>0.023465</td>\n",
              "      <td>0.890974</td>\n",
              "      <td>0.884836</td>\n",
              "      <td>0.556691</td>\n",
              "      <td>0.356571</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.253053</td>\n",
              "      <td>0.425932</td>\n",
              "      <td>0.366938</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.364459</td>\n",
              "      <td>0.187027</td>\n",
              "      <td>0.262478</td>\n",
              "      <td>0.474823</td>\n",
              "      <td>0.313668</td>\n",
              "      <td>0.411803</td>\n",
              "      <td>0.469366</td>\n",
              "      <td>0.490211</td>\n",
              "      <td>0.354087</td>\n",
              "      <td>0.516006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294360</td>\n",
              "      <td>0.307158</td>\n",
              "      <td>0.382443</td>\n",
              "      <td>0.108961</td>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>0.428333</td>\n",
              "      <td>0.320272</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.365808</td>\n",
              "      <td>0.400702</td>\n",
              "      <td>0.915079</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.448260</td>\n",
              "      <td>0.462777</td>\n",
              "      <td>0.126498</td>\n",
              "      <td>0.276150</td>\n",
              "      <td>0.466663</td>\n",
              "      <td>0.497241</td>\n",
              "      <td>0.130804</td>\n",
              "      <td>0.770513</td>\n",
              "      <td>0.135920</td>\n",
              "      <td>0.372307</td>\n",
              "      <td>0.858147</td>\n",
              "      <td>0.912460</td>\n",
              "      <td>0.770458</td>\n",
              "      <td>0.381146</td>\n",
              "      <td>0.557089</td>\n",
              "      <td>0.516059</td>\n",
              "      <td>0.592938</td>\n",
              "      <td>0.403923</td>\n",
              "      <td>0.370737</td>\n",
              "      <td>0.363137</td>\n",
              "      <td>0.403091</td>\n",
              "      <td>0.382465</td>\n",
              "      <td>0.434561</td>\n",
              "      <td>0.093528</td>\n",
              "      <td>0.602417</td>\n",
              "      <td>0.418697</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "VoxelVolume                NaN                NaN  ...         NaN       NaN\n",
              "Maximum3DDiameter     0.821982                NaN  ...         NaN       NaN\n",
              "MeshVolume            0.999999           0.821800  ...         NaN       NaN\n",
              "MajorAxisLength       0.785606           0.964760  ...         NaN       NaN\n",
              "Sphericity            0.329751           0.678628  ...         NaN       NaN\n",
              "...                        ...                ...  ...         ...       ...\n",
              "Coarseness            0.401634           0.586992  ...         NaN       NaN\n",
              "Complexity            0.277302           0.257297  ...         NaN       NaN\n",
              "Strength              0.510155           0.642559  ...         NaN       NaN\n",
              "Contrast.1            0.458069           0.592542  ...         NaN       NaN\n",
              "Busyness              0.937894           0.810919  ...    0.418697       NaN\n",
              "\n",
              "[107 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1MkWP2O9hU",
        "colab_type": "code",
        "outputId": "72fe37f4-26ad-4a6d-9ca2-5732a582c13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Find columns that meet threshold\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.70)]\n",
        "to_drop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VoxelVolume',\n",
              " 'Maximum3DDiameter',\n",
              " 'MeshVolume',\n",
              " 'MajorAxisLength',\n",
              " 'LeastAxisLength',\n",
              " 'Elongation',\n",
              " 'SurfaceVolumeRatio',\n",
              " 'Maximum2DDiameterSlice',\n",
              " 'SurfaceArea',\n",
              " 'MinorAxisLength',\n",
              " 'Maximum2DDiameterColumn',\n",
              " 'Maximum2DDiameterRow',\n",
              " 'GrayLevelVariance',\n",
              " 'HighGrayLevelEmphasis',\n",
              " 'DependenceEntropy',\n",
              " 'DependenceNonUniformity',\n",
              " 'GrayLevelNonUniformity',\n",
              " 'SmallDependenceEmphasis',\n",
              " 'SmallDependenceHighGrayLevelEmphasis',\n",
              " 'DependenceNonUniformityNormalized',\n",
              " 'LargeDependenceEmphasis',\n",
              " 'LargeDependenceLowGrayLevelEmphasis',\n",
              " 'DependenceVariance',\n",
              " 'LargeDependenceHighGrayLevelEmphasis',\n",
              " 'SmallDependenceLowGrayLevelEmphasis',\n",
              " 'LowGrayLevelEmphasis',\n",
              " 'JointAverage',\n",
              " 'SumAverage',\n",
              " 'JointEntropy',\n",
              " 'ClusterShade',\n",
              " 'MaximumProbability',\n",
              " 'Idmn',\n",
              " 'JointEnergy',\n",
              " 'Contrast',\n",
              " 'DifferenceEntropy',\n",
              " 'InverseVariance',\n",
              " 'DifferenceVariance',\n",
              " 'Idn',\n",
              " 'Idm',\n",
              " 'Correlation',\n",
              " 'Autocorrelation',\n",
              " 'SumEntropy',\n",
              " 'SumSquares',\n",
              " 'ClusterProminence',\n",
              " 'Imc2',\n",
              " 'DifferenceAverage',\n",
              " 'Id',\n",
              " 'ClusterTendency',\n",
              " 'InterquartileRange',\n",
              " 'Skewness',\n",
              " 'Uniformity',\n",
              " 'Median',\n",
              " 'Energy',\n",
              " 'RobustMeanAbsoluteDeviation',\n",
              " 'MeanAbsoluteDeviation',\n",
              " 'Maximum',\n",
              " 'RootMeanSquared',\n",
              " 'Minimum',\n",
              " 'Entropy',\n",
              " 'Range',\n",
              " 'Variance',\n",
              " '10Percentile',\n",
              " 'Kurtosis',\n",
              " 'Mean',\n",
              " 'ShortRunLowGrayLevelEmphasis',\n",
              " 'GrayLevelVariance.1',\n",
              " 'LowGrayLevelRunEmphasis',\n",
              " 'GrayLevelNonUniformityNormalized',\n",
              " 'RunVariance',\n",
              " 'GrayLevelNonUniformity.1',\n",
              " 'LongRunEmphasis',\n",
              " 'ShortRunHighGrayLevelEmphasis',\n",
              " 'RunLengthNonUniformity',\n",
              " 'ShortRunEmphasis',\n",
              " 'LongRunHighGrayLevelEmphasis',\n",
              " 'RunPercentage',\n",
              " 'LongRunLowGrayLevelEmphasis',\n",
              " 'RunEntropy',\n",
              " 'HighGrayLevelRunEmphasis',\n",
              " 'RunLengthNonUniformityNormalized',\n",
              " 'ZoneVariance',\n",
              " 'SizeZoneNonUniformityNormalized',\n",
              " 'SizeZoneNonUniformity',\n",
              " 'GrayLevelNonUniformity.2',\n",
              " 'LargeAreaEmphasis',\n",
              " 'SmallAreaHighGrayLevelEmphasis',\n",
              " 'ZonePercentage',\n",
              " 'LowGrayLevelZoneEmphasis',\n",
              " 'SmallAreaLowGrayLevelEmphasis',\n",
              " 'Coarseness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKeereQoci",
        "colab_type": "text"
      },
      "source": [
        "The reason we used the mask to set half of the matrix to NA value is that we ewnt to avoid removing both features when thay have a strong correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R-tG-WPNXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "train_data_stand_reduced = train_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNUiUoZ0Vdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "test_data_stand_reduced = test_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-uuBifRO8G",
        "colab_type": "code",
        "outputId": "36f2591d-c481-48e2-e198-92c5a6a30732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wuGcLV0acQ",
        "colab_type": "code",
        "outputId": "7fdee798-f561-4033-8759-7d1a9a0271b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1qR3re7QwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_reduced = train_data_stand_reduced.to_numpy()\n",
        "test_data_stand_reduced = test_data_stand_reduced.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUG3a0o5W1O",
        "colab_type": "code",
        "outputId": "36e4e0cc-9e5a-4c06-ef84-3ed448f89222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8wkFES71GJ",
        "colab_type": "code",
        "outputId": "3adf95c8-cd56-46ee-a538-db6b90ac3f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljMaQeuSHe1",
        "colab_type": "text"
      },
      "source": [
        "funziona bene, però bisogna stare attenti a basarsi unicamente sul coefficiente di correlazione. Se y = x^2, x e y risulteranno scorrelate secondo il coeffiente di correlazione di Pearson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxuH4u741mLR",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(17,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmeGHBV1xs_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5hBUph6149E"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7NwNpEO149J"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MD1VB0S149O",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c2f96d60-6bca-4438-8038-8fccfc0543b4",
        "id": "VkpRnrey149Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_reduced, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eeb620be-fd15-4291-ffd7-bd7d831c79b9",
        "id": "F3zK4E6o149g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4b020b9-ff66-4fb1-dad2-a5dc8dea05bd",
        "id": "BDX_0MCd149o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9QlfChU149v",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvCKZfjj1490",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bjrjhWo1497"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Du7DFfm1498",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qsz_jnVasri",
        "colab_type": "code",
        "outputId": "494ce790-54ba-476c-f67d-5019cb3908b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "55ded20c-5846-48ad-8583-1d1aa85ab9b7",
        "id": "kHNpJxBL14-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 200\n",
        "all_acc_histories_reduced = []\n",
        "all_loss_histories_reduced = []\n",
        "all_val_acc_histories_reduced = []\n",
        "all_val_loss_histories_reduced = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_reduced[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_reduced[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history_reduced = history.history['acc']\n",
        "  all_acc_histories_reduced.append(acc_history)\n",
        "\n",
        "  loss_history_reduced = history.history['loss']\n",
        "  all_loss_histories_reduced.append(loss_history)\n",
        "\n",
        "  acc_val_history_reduced = history.history['val_acc']\n",
        "  all_val_acc_histories_reduced.append(acc_val_history)\n",
        "\n",
        "  loss_val_history_reduced = history.history['val_loss']\n",
        "  all_val_loss_histories_reduced.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 5ms/step - loss: 1.2852 - acc: 0.3678 - val_loss: 1.2351 - val_acc: 0.4545\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.1662 - acc: 0.4023 - val_loss: 1.1834 - val_acc: 0.5227\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.0859 - acc: 0.4368 - val_loss: 1.1589 - val_acc: 0.5227\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 1.0316 - acc: 0.4483 - val_loss: 1.1529 - val_acc: 0.5455\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.9955 - acc: 0.4828 - val_loss: 1.1452 - val_acc: 0.5455\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9708 - acc: 0.5287 - val_loss: 1.1419 - val_acc: 0.5682\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.9524 - acc: 0.5517 - val_loss: 1.1426 - val_acc: 0.5682\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.9334 - acc: 0.5747 - val_loss: 1.1414 - val_acc: 0.5682\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.9187 - acc: 0.5862 - val_loss: 1.1395 - val_acc: 0.5909\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.9059 - acc: 0.5862 - val_loss: 1.1375 - val_acc: 0.5909\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 319us/step - loss: 0.8948 - acc: 0.6092 - val_loss: 1.1398 - val_acc: 0.5909\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8847 - acc: 0.5862 - val_loss: 1.1395 - val_acc: 0.5909\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 350us/step - loss: 0.8806 - acc: 0.6092 - val_loss: 1.1422 - val_acc: 0.5909\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.8729 - acc: 0.5977 - val_loss: 1.1426 - val_acc: 0.5682\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8609 - acc: 0.6207 - val_loss: 1.1404 - val_acc: 0.5682\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.8524 - acc: 0.6092 - val_loss: 1.1423 - val_acc: 0.5455\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8468 - acc: 0.6322 - val_loss: 1.1468 - val_acc: 0.5682\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8386 - acc: 0.6667 - val_loss: 1.1507 - val_acc: 0.5455\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8320 - acc: 0.6552 - val_loss: 1.1508 - val_acc: 0.5227\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8293 - acc: 0.6667 - val_loss: 1.1537 - val_acc: 0.5227\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8191 - acc: 0.6782 - val_loss: 1.1541 - val_acc: 0.5227\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8154 - acc: 0.6782 - val_loss: 1.1575 - val_acc: 0.5227\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8105 - acc: 0.6782 - val_loss: 1.1618 - val_acc: 0.5227\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8055 - acc: 0.6897 - val_loss: 1.1643 - val_acc: 0.5455\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8026 - acc: 0.6782 - val_loss: 1.1670 - val_acc: 0.5227\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7971 - acc: 0.6897 - val_loss: 1.1696 - val_acc: 0.5000\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.7941 - acc: 0.6897 - val_loss: 1.1748 - val_acc: 0.5227\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.7906 - acc: 0.6782 - val_loss: 1.1758 - val_acc: 0.5000\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.7821 - acc: 0.6782 - val_loss: 1.1796 - val_acc: 0.5000\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7801 - acc: 0.6782 - val_loss: 1.1828 - val_acc: 0.5227\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7759 - acc: 0.6782 - val_loss: 1.1871 - val_acc: 0.5455\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7705 - acc: 0.6897 - val_loss: 1.1921 - val_acc: 0.5455\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.7658 - acc: 0.6897 - val_loss: 1.1936 - val_acc: 0.5455\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.7591 - acc: 0.6897 - val_loss: 1.1986 - val_acc: 0.5455\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7556 - acc: 0.6897 - val_loss: 1.2031 - val_acc: 0.5455\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.7535 - acc: 0.6897 - val_loss: 1.2042 - val_acc: 0.5227\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.7472 - acc: 0.6897 - val_loss: 1.2088 - val_acc: 0.5227\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7437 - acc: 0.6897 - val_loss: 1.2130 - val_acc: 0.5455\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7393 - acc: 0.6897 - val_loss: 1.2175 - val_acc: 0.5227\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.7356 - acc: 0.6897 - val_loss: 1.2215 - val_acc: 0.5455\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.7326 - acc: 0.6897 - val_loss: 1.2247 - val_acc: 0.5455\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.7283 - acc: 0.6897 - val_loss: 1.2298 - val_acc: 0.5455\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7254 - acc: 0.6897 - val_loss: 1.2320 - val_acc: 0.5455\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7204 - acc: 0.6897 - val_loss: 1.2381 - val_acc: 0.5455\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7167 - acc: 0.6897 - val_loss: 1.2447 - val_acc: 0.5227\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.7167 - acc: 0.6782 - val_loss: 1.2478 - val_acc: 0.5682\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 354us/step - loss: 0.7118 - acc: 0.6897 - val_loss: 1.2515 - val_acc: 0.5682\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.7088 - acc: 0.6782 - val_loss: 1.2548 - val_acc: 0.5000\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.7032 - acc: 0.7011 - val_loss: 1.2600 - val_acc: 0.5227\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7013 - acc: 0.6897 - val_loss: 1.2643 - val_acc: 0.5227\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6984 - acc: 0.7011 - val_loss: 1.2695 - val_acc: 0.5227\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6943 - acc: 0.6897 - val_loss: 1.2741 - val_acc: 0.5000\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.6898 - acc: 0.7011 - val_loss: 1.2781 - val_acc: 0.5227\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6870 - acc: 0.6782 - val_loss: 1.2838 - val_acc: 0.5227\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6836 - acc: 0.7011 - val_loss: 1.2850 - val_acc: 0.5000\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6793 - acc: 0.7011 - val_loss: 1.2907 - val_acc: 0.5227\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6762 - acc: 0.7011 - val_loss: 1.2959 - val_acc: 0.5000\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 286us/step - loss: 0.6736 - acc: 0.7011 - val_loss: 1.2994 - val_acc: 0.5227\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.6734 - acc: 0.7126 - val_loss: 1.3060 - val_acc: 0.5000\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6695 - acc: 0.7011 - val_loss: 1.3105 - val_acc: 0.5000\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.6640 - acc: 0.7011 - val_loss: 1.3141 - val_acc: 0.5000\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6627 - acc: 0.7011 - val_loss: 1.3171 - val_acc: 0.5000\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6602 - acc: 0.7126 - val_loss: 1.3223 - val_acc: 0.5000\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6566 - acc: 0.6897 - val_loss: 1.3270 - val_acc: 0.5000\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.6556 - acc: 0.7011 - val_loss: 1.3292 - val_acc: 0.5227\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 435us/step - loss: 0.6525 - acc: 0.7126 - val_loss: 1.3346 - val_acc: 0.5227\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 300us/step - loss: 0.6485 - acc: 0.6897 - val_loss: 1.3414 - val_acc: 0.5000\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.6471 - acc: 0.6897 - val_loss: 1.3460 - val_acc: 0.5000\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6427 - acc: 0.7011 - val_loss: 1.3519 - val_acc: 0.5000\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6426 - acc: 0.7011 - val_loss: 1.3567 - val_acc: 0.5000\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6366 - acc: 0.7011 - val_loss: 1.3579 - val_acc: 0.5227\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6389 - acc: 0.6897 - val_loss: 1.3624 - val_acc: 0.5227\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6354 - acc: 0.7126 - val_loss: 1.3673 - val_acc: 0.5227\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6312 - acc: 0.6897 - val_loss: 1.3718 - val_acc: 0.5227\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6298 - acc: 0.7126 - val_loss: 1.3782 - val_acc: 0.5227\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6267 - acc: 0.7126 - val_loss: 1.3805 - val_acc: 0.5227\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6236 - acc: 0.7126 - val_loss: 1.3856 - val_acc: 0.5227\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6202 - acc: 0.7126 - val_loss: 1.3911 - val_acc: 0.5000\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6168 - acc: 0.7011 - val_loss: 1.3968 - val_acc: 0.5000\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6140 - acc: 0.7241 - val_loss: 1.4029 - val_acc: 0.5000\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.6146 - acc: 0.7126 - val_loss: 1.4089 - val_acc: 0.5227\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6136 - acc: 0.7126 - val_loss: 1.4136 - val_acc: 0.5000\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6127 - acc: 0.7126 - val_loss: 1.4191 - val_acc: 0.5000\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.6043 - acc: 0.7356 - val_loss: 1.4240 - val_acc: 0.5000\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.6043 - acc: 0.7241 - val_loss: 1.4288 - val_acc: 0.5227\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.6032 - acc: 0.7011 - val_loss: 1.4382 - val_acc: 0.5000\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.6019 - acc: 0.7011 - val_loss: 1.4391 - val_acc: 0.5000\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5974 - acc: 0.6897 - val_loss: 1.4462 - val_acc: 0.5000\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5922 - acc: 0.7241 - val_loss: 1.4491 - val_acc: 0.5227\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.5959 - acc: 0.7241 - val_loss: 1.4546 - val_acc: 0.5227\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5877 - acc: 0.7356 - val_loss: 1.4607 - val_acc: 0.5000\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5878 - acc: 0.7241 - val_loss: 1.4668 - val_acc: 0.5000\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5827 - acc: 0.7126 - val_loss: 1.4735 - val_acc: 0.5000\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.5806 - acc: 0.7471 - val_loss: 1.4760 - val_acc: 0.5227\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.5797 - acc: 0.7241 - val_loss: 1.4811 - val_acc: 0.5227\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5776 - acc: 0.7241 - val_loss: 1.4890 - val_acc: 0.4773\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5775 - acc: 0.7471 - val_loss: 1.4900 - val_acc: 0.5227\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5720 - acc: 0.7701 - val_loss: 1.4963 - val_acc: 0.5000\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5727 - acc: 0.7356 - val_loss: 1.5022 - val_acc: 0.5000\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5669 - acc: 0.7471 - val_loss: 1.5046 - val_acc: 0.5227\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5673 - acc: 0.7701 - val_loss: 1.5094 - val_acc: 0.5000\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5640 - acc: 0.7586 - val_loss: 1.5146 - val_acc: 0.5227\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5606 - acc: 0.7586 - val_loss: 1.5225 - val_acc: 0.5227\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5609 - acc: 0.7471 - val_loss: 1.5253 - val_acc: 0.5227\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5609 - acc: 0.7471 - val_loss: 1.5361 - val_acc: 0.5000\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5549 - acc: 0.7586 - val_loss: 1.5429 - val_acc: 0.5227\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.5532 - acc: 0.7586 - val_loss: 1.5498 - val_acc: 0.5000\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5529 - acc: 0.7471 - val_loss: 1.5495 - val_acc: 0.5000\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5498 - acc: 0.7586 - val_loss: 1.5544 - val_acc: 0.5000\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.5450 - acc: 0.7586 - val_loss: 1.5627 - val_acc: 0.5000\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5421 - acc: 0.7701 - val_loss: 1.5691 - val_acc: 0.5227\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5394 - acc: 0.7586 - val_loss: 1.5742 - val_acc: 0.5000\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5366 - acc: 0.7701 - val_loss: 1.5791 - val_acc: 0.5000\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5365 - acc: 0.7701 - val_loss: 1.5822 - val_acc: 0.5000\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5346 - acc: 0.7586 - val_loss: 1.5890 - val_acc: 0.5000\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.5314 - acc: 0.7471 - val_loss: 1.5960 - val_acc: 0.5000\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5281 - acc: 0.7471 - val_loss: 1.5998 - val_acc: 0.5000\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5250 - acc: 0.7816 - val_loss: 1.6097 - val_acc: 0.5000\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5238 - acc: 0.7586 - val_loss: 1.6090 - val_acc: 0.5000\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5231 - acc: 0.7471 - val_loss: 1.6153 - val_acc: 0.5000\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5183 - acc: 0.7816 - val_loss: 1.6247 - val_acc: 0.5000\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5171 - acc: 0.7816 - val_loss: 1.6328 - val_acc: 0.5000\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5175 - acc: 0.7816 - val_loss: 1.6342 - val_acc: 0.5000\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5111 - acc: 0.7816 - val_loss: 1.6380 - val_acc: 0.5000\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5093 - acc: 0.7701 - val_loss: 1.6438 - val_acc: 0.5000\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5069 - acc: 0.7816 - val_loss: 1.6508 - val_acc: 0.5000\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.5040 - acc: 0.7701 - val_loss: 1.6628 - val_acc: 0.5000\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5046 - acc: 0.7586 - val_loss: 1.6669 - val_acc: 0.5000\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4967 - acc: 0.7701 - val_loss: 1.6673 - val_acc: 0.5000\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4973 - acc: 0.7586 - val_loss: 1.6751 - val_acc: 0.5000\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.4957 - acc: 0.7931 - val_loss: 1.6898 - val_acc: 0.5000\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.4927 - acc: 0.7816 - val_loss: 1.6929 - val_acc: 0.5000\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.4911 - acc: 0.7816 - val_loss: 1.6955 - val_acc: 0.5000\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4912 - acc: 0.8161 - val_loss: 1.7018 - val_acc: 0.5000\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.4885 - acc: 0.7931 - val_loss: 1.7053 - val_acc: 0.4773\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.4852 - acc: 0.7816 - val_loss: 1.7091 - val_acc: 0.4773\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4836 - acc: 0.7931 - val_loss: 1.7150 - val_acc: 0.4773\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4789 - acc: 0.8046 - val_loss: 1.7188 - val_acc: 0.4773\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.4783 - acc: 0.8161 - val_loss: 1.7220 - val_acc: 0.4773\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4761 - acc: 0.8046 - val_loss: 1.7274 - val_acc: 0.4773\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4751 - acc: 0.8161 - val_loss: 1.7338 - val_acc: 0.4773\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4727 - acc: 0.8046 - val_loss: 1.7341 - val_acc: 0.4773\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4692 - acc: 0.8046 - val_loss: 1.7400 - val_acc: 0.4773\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4657 - acc: 0.8161 - val_loss: 1.7462 - val_acc: 0.4773\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4661 - acc: 0.7931 - val_loss: 1.7467 - val_acc: 0.4773\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.4639 - acc: 0.8161 - val_loss: 1.7533 - val_acc: 0.4773\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4599 - acc: 0.8276 - val_loss: 1.7564 - val_acc: 0.4773\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 317us/step - loss: 0.4599 - acc: 0.8161 - val_loss: 1.7620 - val_acc: 0.4773\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.4554 - acc: 0.8161 - val_loss: 1.7669 - val_acc: 0.4773\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.4555 - acc: 0.8276 - val_loss: 1.7693 - val_acc: 0.4773\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.4529 - acc: 0.8161 - val_loss: 1.7745 - val_acc: 0.4773\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.4509 - acc: 0.8161 - val_loss: 1.7808 - val_acc: 0.4545\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 325us/step - loss: 0.4502 - acc: 0.8276 - val_loss: 1.7847 - val_acc: 0.4545\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.4471 - acc: 0.8276 - val_loss: 1.7890 - val_acc: 0.4545\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4417 - acc: 0.8391 - val_loss: 1.7954 - val_acc: 0.4545\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4421 - acc: 0.8391 - val_loss: 1.8015 - val_acc: 0.4545\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.4391 - acc: 0.8276 - val_loss: 1.8032 - val_acc: 0.4545\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.4406 - acc: 0.8276 - val_loss: 1.8101 - val_acc: 0.4545\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4346 - acc: 0.8391 - val_loss: 1.8134 - val_acc: 0.4545\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4349 - acc: 0.8276 - val_loss: 1.8193 - val_acc: 0.4318\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4301 - acc: 0.8276 - val_loss: 1.8265 - val_acc: 0.4318\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4314 - acc: 0.8276 - val_loss: 1.8272 - val_acc: 0.4318\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4269 - acc: 0.8276 - val_loss: 1.8318 - val_acc: 0.4318\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.4280 - acc: 0.8276 - val_loss: 1.8345 - val_acc: 0.4318\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4251 - acc: 0.8391 - val_loss: 1.8417 - val_acc: 0.4318\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.4219 - acc: 0.8276 - val_loss: 1.8468 - val_acc: 0.4091\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4187 - acc: 0.8506 - val_loss: 1.8524 - val_acc: 0.4091\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4203 - acc: 0.8276 - val_loss: 1.8572 - val_acc: 0.4091\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4169 - acc: 0.8276 - val_loss: 1.8606 - val_acc: 0.4091\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4152 - acc: 0.8276 - val_loss: 1.8673 - val_acc: 0.4091\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.4128 - acc: 0.8391 - val_loss: 1.8713 - val_acc: 0.4091\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4112 - acc: 0.8506 - val_loss: 1.8731 - val_acc: 0.4091\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4092 - acc: 0.8506 - val_loss: 1.8799 - val_acc: 0.4091\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.4050 - acc: 0.8506 - val_loss: 1.8861 - val_acc: 0.4318\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.4070 - acc: 0.8506 - val_loss: 1.8917 - val_acc: 0.4091\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4018 - acc: 0.8621 - val_loss: 1.8976 - val_acc: 0.4091\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.4006 - acc: 0.8621 - val_loss: 1.9015 - val_acc: 0.4091\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4007 - acc: 0.8391 - val_loss: 1.9086 - val_acc: 0.4091\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.3977 - acc: 0.8621 - val_loss: 1.9139 - val_acc: 0.4091\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3929 - acc: 0.8736 - val_loss: 1.9200 - val_acc: 0.4091\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.3915 - acc: 0.8736 - val_loss: 1.9242 - val_acc: 0.4091\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3933 - acc: 0.8851 - val_loss: 1.9273 - val_acc: 0.4091\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3918 - acc: 0.8621 - val_loss: 1.9345 - val_acc: 0.4091\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3875 - acc: 0.8736 - val_loss: 1.9433 - val_acc: 0.4091\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.3843 - acc: 0.8851 - val_loss: 1.9450 - val_acc: 0.4091\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.3844 - acc: 0.8736 - val_loss: 1.9488 - val_acc: 0.4091\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3817 - acc: 0.8851 - val_loss: 1.9566 - val_acc: 0.4091\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3796 - acc: 0.8851 - val_loss: 1.9627 - val_acc: 0.4091\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3783 - acc: 0.8736 - val_loss: 1.9663 - val_acc: 0.4091\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.3756 - acc: 0.8966 - val_loss: 1.9742 - val_acc: 0.4091\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.3739 - acc: 0.8851 - val_loss: 1.9784 - val_acc: 0.4091\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.3703 - acc: 0.8851 - val_loss: 1.9831 - val_acc: 0.4091\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 351us/step - loss: 0.3719 - acc: 0.9080 - val_loss: 1.9862 - val_acc: 0.4091\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 363us/step - loss: 0.3687 - acc: 0.8966 - val_loss: 1.9909 - val_acc: 0.4318\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.3652 - acc: 0.8851 - val_loss: 1.9966 - val_acc: 0.4091\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3663 - acc: 0.8851 - val_loss: 2.0003 - val_acc: 0.4091\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3628 - acc: 0.9080 - val_loss: 2.0061 - val_acc: 0.4091\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.3617 - acc: 0.8966 - val_loss: 2.0133 - val_acc: 0.4091\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3604 - acc: 0.9080 - val_loss: 2.0188 - val_acc: 0.4091\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.3604 - acc: 0.9195 - val_loss: 2.0222 - val_acc: 0.4091\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 6ms/step - loss: 1.5228 - acc: 0.4138 - val_loss: 1.1639 - val_acc: 0.4318\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.3265 - acc: 0.4138 - val_loss: 1.0803 - val_acc: 0.4318\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.2118 - acc: 0.3563 - val_loss: 1.0293 - val_acc: 0.4318\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.1403 - acc: 0.3908 - val_loss: 1.0045 - val_acc: 0.4545\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.0966 - acc: 0.4023 - val_loss: 0.9879 - val_acc: 0.5000\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 1.0656 - acc: 0.4483 - val_loss: 0.9780 - val_acc: 0.5227\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.0503 - acc: 0.4368 - val_loss: 0.9718 - val_acc: 0.5455\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.0302 - acc: 0.4598 - val_loss: 0.9677 - val_acc: 0.5000\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 1.0172 - acc: 0.4598 - val_loss: 0.9663 - val_acc: 0.5000\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.0018 - acc: 0.4828 - val_loss: 0.9659 - val_acc: 0.5227\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 339us/step - loss: 0.9942 - acc: 0.4828 - val_loss: 0.9648 - val_acc: 0.4773\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9835 - acc: 0.5172 - val_loss: 0.9652 - val_acc: 0.4773\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.9783 - acc: 0.5287 - val_loss: 0.9652 - val_acc: 0.4545\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.9707 - acc: 0.5172 - val_loss: 0.9664 - val_acc: 0.4773\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.9632 - acc: 0.5172 - val_loss: 0.9674 - val_acc: 0.4318\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9595 - acc: 0.5402 - val_loss: 0.9687 - val_acc: 0.4318\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9510 - acc: 0.5287 - val_loss: 0.9701 - val_acc: 0.4091\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9482 - acc: 0.5402 - val_loss: 0.9710 - val_acc: 0.4091\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9432 - acc: 0.5172 - val_loss: 0.9737 - val_acc: 0.4091\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.9368 - acc: 0.5172 - val_loss: 0.9763 - val_acc: 0.4091\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9331 - acc: 0.5287 - val_loss: 0.9768 - val_acc: 0.4091\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9279 - acc: 0.5402 - val_loss: 0.9799 - val_acc: 0.4091\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.9257 - acc: 0.5287 - val_loss: 0.9808 - val_acc: 0.4318\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.9221 - acc: 0.5402 - val_loss: 0.9831 - val_acc: 0.4545\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.9189 - acc: 0.5402 - val_loss: 0.9841 - val_acc: 0.4545\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.9152 - acc: 0.5517 - val_loss: 0.9875 - val_acc: 0.4545\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9109 - acc: 0.5517 - val_loss: 0.9892 - val_acc: 0.4545\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9081 - acc: 0.5402 - val_loss: 0.9913 - val_acc: 0.4318\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.9053 - acc: 0.5517 - val_loss: 0.9939 - val_acc: 0.4318\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9040 - acc: 0.5517 - val_loss: 0.9959 - val_acc: 0.4318\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.8995 - acc: 0.5402 - val_loss: 0.9986 - val_acc: 0.4318\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8971 - acc: 0.5517 - val_loss: 1.0012 - val_acc: 0.4318\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8937 - acc: 0.5517 - val_loss: 1.0018 - val_acc: 0.4318\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8915 - acc: 0.5517 - val_loss: 1.0038 - val_acc: 0.4318\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8887 - acc: 0.5747 - val_loss: 1.0073 - val_acc: 0.4318\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8868 - acc: 0.5517 - val_loss: 1.0078 - val_acc: 0.4318\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8844 - acc: 0.5747 - val_loss: 1.0125 - val_acc: 0.4318\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8835 - acc: 0.5632 - val_loss: 1.0129 - val_acc: 0.4318\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8793 - acc: 0.5747 - val_loss: 1.0156 - val_acc: 0.4318\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8783 - acc: 0.5632 - val_loss: 1.0170 - val_acc: 0.4318\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8748 - acc: 0.5862 - val_loss: 1.0192 - val_acc: 0.4318\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8719 - acc: 0.5862 - val_loss: 1.0212 - val_acc: 0.4545\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8685 - acc: 0.5747 - val_loss: 1.0240 - val_acc: 0.4545\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.8671 - acc: 0.5747 - val_loss: 1.0254 - val_acc: 0.4545\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.8646 - acc: 0.5862 - val_loss: 1.0279 - val_acc: 0.4545\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8627 - acc: 0.5977 - val_loss: 1.0289 - val_acc: 0.4545\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8600 - acc: 0.5747 - val_loss: 1.0311 - val_acc: 0.4545\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.8577 - acc: 0.5747 - val_loss: 1.0326 - val_acc: 0.4545\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8560 - acc: 0.5862 - val_loss: 1.0351 - val_acc: 0.4545\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8546 - acc: 0.5862 - val_loss: 1.0376 - val_acc: 0.4545\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.8538 - acc: 0.5862 - val_loss: 1.0382 - val_acc: 0.4545\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8498 - acc: 0.5862 - val_loss: 1.0431 - val_acc: 0.4545\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8469 - acc: 0.5977 - val_loss: 1.0439 - val_acc: 0.4545\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8442 - acc: 0.6092 - val_loss: 1.0455 - val_acc: 0.4545\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.8423 - acc: 0.6092 - val_loss: 1.0492 - val_acc: 0.4545\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8413 - acc: 0.5977 - val_loss: 1.0495 - val_acc: 0.4545\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8380 - acc: 0.6092 - val_loss: 1.0526 - val_acc: 0.4545\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8359 - acc: 0.5977 - val_loss: 1.0534 - val_acc: 0.4545\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.8342 - acc: 0.6092 - val_loss: 1.0571 - val_acc: 0.4545\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8305 - acc: 0.6092 - val_loss: 1.0587 - val_acc: 0.4545\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8295 - acc: 0.6207 - val_loss: 1.0609 - val_acc: 0.4545\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8261 - acc: 0.6207 - val_loss: 1.0636 - val_acc: 0.4545\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8259 - acc: 0.6207 - val_loss: 1.0655 - val_acc: 0.4545\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.8221 - acc: 0.6207 - val_loss: 1.0692 - val_acc: 0.4545\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.8214 - acc: 0.6207 - val_loss: 1.0714 - val_acc: 0.4545\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8205 - acc: 0.6207 - val_loss: 1.0731 - val_acc: 0.4545\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.8180 - acc: 0.6207 - val_loss: 1.0753 - val_acc: 0.4773\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 335us/step - loss: 0.8152 - acc: 0.5977 - val_loss: 1.0788 - val_acc: 0.4773\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.8140 - acc: 0.6322 - val_loss: 1.0789 - val_acc: 0.4773\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.8113 - acc: 0.6322 - val_loss: 1.0810 - val_acc: 0.4773\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8096 - acc: 0.6322 - val_loss: 1.0848 - val_acc: 0.4773\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.8067 - acc: 0.6322 - val_loss: 1.0867 - val_acc: 0.4773\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.8044 - acc: 0.6322 - val_loss: 1.0892 - val_acc: 0.4773\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8042 - acc: 0.6322 - val_loss: 1.0914 - val_acc: 0.4318\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.8034 - acc: 0.6437 - val_loss: 1.0935 - val_acc: 0.4318\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8005 - acc: 0.6322 - val_loss: 1.0972 - val_acc: 0.4318\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.7980 - acc: 0.6437 - val_loss: 1.0988 - val_acc: 0.4318\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7950 - acc: 0.6437 - val_loss: 1.1013 - val_acc: 0.4318\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7941 - acc: 0.6322 - val_loss: 1.1053 - val_acc: 0.4318\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7910 - acc: 0.6437 - val_loss: 1.1059 - val_acc: 0.4091\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7907 - acc: 0.6437 - val_loss: 1.1090 - val_acc: 0.4091\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7879 - acc: 0.6437 - val_loss: 1.1118 - val_acc: 0.4091\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.7871 - acc: 0.6437 - val_loss: 1.1151 - val_acc: 0.4091\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7830 - acc: 0.6322 - val_loss: 1.1180 - val_acc: 0.4091\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.7826 - acc: 0.6437 - val_loss: 1.1217 - val_acc: 0.3864\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7793 - acc: 0.6437 - val_loss: 1.1208 - val_acc: 0.3864\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7784 - acc: 0.6437 - val_loss: 1.1254 - val_acc: 0.3864\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7755 - acc: 0.6437 - val_loss: 1.1266 - val_acc: 0.3864\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7742 - acc: 0.6552 - val_loss: 1.1282 - val_acc: 0.3864\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7720 - acc: 0.6552 - val_loss: 1.1304 - val_acc: 0.3864\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.7705 - acc: 0.6552 - val_loss: 1.1344 - val_acc: 0.3864\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7692 - acc: 0.6667 - val_loss: 1.1378 - val_acc: 0.3864\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7657 - acc: 0.6667 - val_loss: 1.1392 - val_acc: 0.3864\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7632 - acc: 0.6667 - val_loss: 1.1410 - val_acc: 0.3864\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7632 - acc: 0.6667 - val_loss: 1.1428 - val_acc: 0.3864\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7595 - acc: 0.6667 - val_loss: 1.1481 - val_acc: 0.3864\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7568 - acc: 0.6667 - val_loss: 1.1492 - val_acc: 0.3864\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7548 - acc: 0.6667 - val_loss: 1.1533 - val_acc: 0.3636\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.7544 - acc: 0.6667 - val_loss: 1.1543 - val_acc: 0.3636\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7504 - acc: 0.6782 - val_loss: 1.1578 - val_acc: 0.3636\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 331us/step - loss: 0.7498 - acc: 0.6782 - val_loss: 1.1636 - val_acc: 0.3864\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.7475 - acc: 0.6782 - val_loss: 1.1638 - val_acc: 0.3864\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.7464 - acc: 0.6782 - val_loss: 1.1685 - val_acc: 0.3864\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 314us/step - loss: 0.7436 - acc: 0.6782 - val_loss: 1.1706 - val_acc: 0.3864\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.7422 - acc: 0.6667 - val_loss: 1.1704 - val_acc: 0.3636\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.7372 - acc: 0.6782 - val_loss: 1.1747 - val_acc: 0.3864\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.7360 - acc: 0.6782 - val_loss: 1.1771 - val_acc: 0.3864\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.7337 - acc: 0.6782 - val_loss: 1.1839 - val_acc: 0.3864\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7333 - acc: 0.6782 - val_loss: 1.1839 - val_acc: 0.3864\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 310us/step - loss: 0.7324 - acc: 0.6667 - val_loss: 1.1867 - val_acc: 0.3864\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.7285 - acc: 0.6667 - val_loss: 1.1934 - val_acc: 0.3864\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7256 - acc: 0.6667 - val_loss: 1.1982 - val_acc: 0.4091\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 307us/step - loss: 0.7247 - acc: 0.6667 - val_loss: 1.2019 - val_acc: 0.3864\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.7216 - acc: 0.6552 - val_loss: 1.2065 - val_acc: 0.3864\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.7190 - acc: 0.6667 - val_loss: 1.2103 - val_acc: 0.3864\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.7198 - acc: 0.6552 - val_loss: 1.2129 - val_acc: 0.3864\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7149 - acc: 0.6552 - val_loss: 1.2186 - val_acc: 0.3864\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.7154 - acc: 0.6552 - val_loss: 1.2237 - val_acc: 0.3864\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7126 - acc: 0.6552 - val_loss: 1.2254 - val_acc: 0.3864\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7107 - acc: 0.6552 - val_loss: 1.2297 - val_acc: 0.4091\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7089 - acc: 0.6667 - val_loss: 1.2317 - val_acc: 0.3636\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7061 - acc: 0.6667 - val_loss: 1.2347 - val_acc: 0.3864\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7040 - acc: 0.6667 - val_loss: 1.2362 - val_acc: 0.3636\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.7022 - acc: 0.6667 - val_loss: 1.2395 - val_acc: 0.3636\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.7014 - acc: 0.6667 - val_loss: 1.2418 - val_acc: 0.3636\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6984 - acc: 0.6782 - val_loss: 1.2445 - val_acc: 0.3864\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.6950 - acc: 0.6782 - val_loss: 1.2462 - val_acc: 0.4091\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6929 - acc: 0.6782 - val_loss: 1.2520 - val_acc: 0.3864\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6934 - acc: 0.6782 - val_loss: 1.2545 - val_acc: 0.3864\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.6903 - acc: 0.6782 - val_loss: 1.2615 - val_acc: 0.4091\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6880 - acc: 0.6782 - val_loss: 1.2642 - val_acc: 0.3864\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6861 - acc: 0.6782 - val_loss: 1.2633 - val_acc: 0.4091\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6837 - acc: 0.6782 - val_loss: 1.2664 - val_acc: 0.4091\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.6830 - acc: 0.6782 - val_loss: 1.2725 - val_acc: 0.4318\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6804 - acc: 0.6782 - val_loss: 1.2729 - val_acc: 0.4318\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.6784 - acc: 0.6782 - val_loss: 1.2769 - val_acc: 0.4318\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 335us/step - loss: 0.6760 - acc: 0.6782 - val_loss: 1.2804 - val_acc: 0.3864\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.6745 - acc: 0.6782 - val_loss: 1.2844 - val_acc: 0.4318\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6728 - acc: 0.6782 - val_loss: 1.2886 - val_acc: 0.4318\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.6726 - acc: 0.6897 - val_loss: 1.2881 - val_acc: 0.4318\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.6690 - acc: 0.6897 - val_loss: 1.2912 - val_acc: 0.4318\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6670 - acc: 0.6897 - val_loss: 1.2942 - val_acc: 0.4318\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6644 - acc: 0.6897 - val_loss: 1.2955 - val_acc: 0.4318\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6623 - acc: 0.6897 - val_loss: 1.2957 - val_acc: 0.4318\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6609 - acc: 0.6897 - val_loss: 1.3028 - val_acc: 0.4318\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.6599 - acc: 0.6897 - val_loss: 1.3034 - val_acc: 0.4318\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6574 - acc: 0.6897 - val_loss: 1.3068 - val_acc: 0.4318\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.6562 - acc: 0.6897 - val_loss: 1.3105 - val_acc: 0.4318\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.6536 - acc: 0.6897 - val_loss: 1.3136 - val_acc: 0.4318\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6521 - acc: 0.6897 - val_loss: 1.3147 - val_acc: 0.4318\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6496 - acc: 0.6897 - val_loss: 1.3175 - val_acc: 0.4318\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6463 - acc: 0.6897 - val_loss: 1.3160 - val_acc: 0.4545\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6460 - acc: 0.6897 - val_loss: 1.3205 - val_acc: 0.4545\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6434 - acc: 0.6897 - val_loss: 1.3220 - val_acc: 0.4545\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6414 - acc: 0.7011 - val_loss: 1.3222 - val_acc: 0.4545\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.6392 - acc: 0.6897 - val_loss: 1.3225 - val_acc: 0.4545\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6384 - acc: 0.6897 - val_loss: 1.3308 - val_acc: 0.4545\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6345 - acc: 0.6897 - val_loss: 1.3329 - val_acc: 0.4545\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6328 - acc: 0.6897 - val_loss: 1.3320 - val_acc: 0.4545\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.6317 - acc: 0.7011 - val_loss: 1.3398 - val_acc: 0.4773\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6288 - acc: 0.7011 - val_loss: 1.3392 - val_acc: 0.4545\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.6273 - acc: 0.7011 - val_loss: 1.3436 - val_acc: 0.4773\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6263 - acc: 0.7011 - val_loss: 1.3463 - val_acc: 0.4773\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6238 - acc: 0.7126 - val_loss: 1.3512 - val_acc: 0.4773\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.6231 - acc: 0.7126 - val_loss: 1.3534 - val_acc: 0.4773\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6196 - acc: 0.7126 - val_loss: 1.3576 - val_acc: 0.4773\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.6192 - acc: 0.7011 - val_loss: 1.3601 - val_acc: 0.4773\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6172 - acc: 0.7011 - val_loss: 1.3637 - val_acc: 0.4773\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.6134 - acc: 0.7126 - val_loss: 1.3615 - val_acc: 0.4773\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6112 - acc: 0.7126 - val_loss: 1.3613 - val_acc: 0.4773\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.6090 - acc: 0.7126 - val_loss: 1.3633 - val_acc: 0.4773\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6080 - acc: 0.7126 - val_loss: 1.3635 - val_acc: 0.4773\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.6057 - acc: 0.7356 - val_loss: 1.3683 - val_acc: 0.4773\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6046 - acc: 0.7241 - val_loss: 1.3769 - val_acc: 0.4773\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.6008 - acc: 0.7241 - val_loss: 1.3813 - val_acc: 0.4773\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.6005 - acc: 0.7126 - val_loss: 1.3809 - val_acc: 0.4773\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5998 - acc: 0.7241 - val_loss: 1.3883 - val_acc: 0.4773\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.5957 - acc: 0.7356 - val_loss: 1.3887 - val_acc: 0.4773\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.5963 - acc: 0.7356 - val_loss: 1.3994 - val_acc: 0.4773\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5911 - acc: 0.7471 - val_loss: 1.3952 - val_acc: 0.4773\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5911 - acc: 0.7356 - val_loss: 1.3997 - val_acc: 0.4773\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5902 - acc: 0.7356 - val_loss: 1.4039 - val_acc: 0.4773\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5866 - acc: 0.7356 - val_loss: 1.4070 - val_acc: 0.5000\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5872 - acc: 0.7471 - val_loss: 1.4027 - val_acc: 0.5000\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.5830 - acc: 0.7471 - val_loss: 1.4096 - val_acc: 0.5000\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5819 - acc: 0.7471 - val_loss: 1.4158 - val_acc: 0.5000\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5801 - acc: 0.7356 - val_loss: 1.4200 - val_acc: 0.5000\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5779 - acc: 0.7471 - val_loss: 1.4229 - val_acc: 0.5000\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5768 - acc: 0.7471 - val_loss: 1.4266 - val_acc: 0.5000\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.5742 - acc: 0.7471 - val_loss: 1.4330 - val_acc: 0.5000\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5722 - acc: 0.7471 - val_loss: 1.4312 - val_acc: 0.5000\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5701 - acc: 0.7471 - val_loss: 1.4382 - val_acc: 0.5000\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5689 - acc: 0.7471 - val_loss: 1.4430 - val_acc: 0.5000\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5677 - acc: 0.7471 - val_loss: 1.4486 - val_acc: 0.5000\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5677 - acc: 0.7471 - val_loss: 1.4504 - val_acc: 0.5000\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.5631 - acc: 0.7471 - val_loss: 1.4505 - val_acc: 0.5000\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5632 - acc: 0.7471 - val_loss: 1.4563 - val_acc: 0.5000\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.5598 - acc: 0.7586 - val_loss: 1.4544 - val_acc: 0.5000\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.5589 - acc: 0.7586 - val_loss: 1.4595 - val_acc: 0.5000\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5601 - acc: 0.7471 - val_loss: 1.4646 - val_acc: 0.5000\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/200\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 1.2554 - acc: 0.3636 - val_loss: 1.0964 - val_acc: 0.4186\n",
            "Epoch 2/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.1828 - acc: 0.3409 - val_loss: 1.0426 - val_acc: 0.4186\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.1204 - acc: 0.3750 - val_loss: 1.0148 - val_acc: 0.4419\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.0827 - acc: 0.3750 - val_loss: 0.9974 - val_acc: 0.4651\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0487 - acc: 0.4091 - val_loss: 0.9894 - val_acc: 0.4651\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0203 - acc: 0.4432 - val_loss: 0.9847 - val_acc: 0.4651\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.9983 - acc: 0.4545 - val_loss: 0.9827 - val_acc: 0.4651\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9792 - acc: 0.4659 - val_loss: 0.9812 - val_acc: 0.4419\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9631 - acc: 0.5114 - val_loss: 0.9834 - val_acc: 0.4419\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9486 - acc: 0.5455 - val_loss: 0.9835 - val_acc: 0.4651\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9330 - acc: 0.5341 - val_loss: 0.9880 - val_acc: 0.4651\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.9218 - acc: 0.5568 - val_loss: 0.9895 - val_acc: 0.4884\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.9112 - acc: 0.5568 - val_loss: 0.9933 - val_acc: 0.4884\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8981 - acc: 0.5568 - val_loss: 0.9987 - val_acc: 0.5116\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8886 - acc: 0.5455 - val_loss: 1.0026 - val_acc: 0.5349\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8819 - acc: 0.5568 - val_loss: 1.0068 - val_acc: 0.5349\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 0s 295us/step - loss: 0.8696 - acc: 0.5795 - val_loss: 1.0105 - val_acc: 0.5814\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8633 - acc: 0.6023 - val_loss: 1.0133 - val_acc: 0.5814\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.8554 - acc: 0.6023 - val_loss: 1.0156 - val_acc: 0.5814\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8473 - acc: 0.6136 - val_loss: 1.0206 - val_acc: 0.5814\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8416 - acc: 0.6136 - val_loss: 1.0255 - val_acc: 0.5814\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8347 - acc: 0.6136 - val_loss: 1.0288 - val_acc: 0.5814\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8272 - acc: 0.6250 - val_loss: 1.0323 - val_acc: 0.6047\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8211 - acc: 0.6250 - val_loss: 1.0375 - val_acc: 0.5814\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8150 - acc: 0.6250 - val_loss: 1.0389 - val_acc: 0.5814\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8100 - acc: 0.6364 - val_loss: 1.0423 - val_acc: 0.5814\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8028 - acc: 0.6364 - val_loss: 1.0475 - val_acc: 0.6047\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.7993 - acc: 0.6364 - val_loss: 1.0514 - val_acc: 0.6047\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7937 - acc: 0.6591 - val_loss: 1.0547 - val_acc: 0.5814\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7878 - acc: 0.6591 - val_loss: 1.0584 - val_acc: 0.5814\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7810 - acc: 0.6818 - val_loss: 1.0617 - val_acc: 0.5814\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7780 - acc: 0.6705 - val_loss: 1.0657 - val_acc: 0.6047\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.7718 - acc: 0.6818 - val_loss: 1.0687 - val_acc: 0.5814\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7658 - acc: 0.6818 - val_loss: 1.0733 - val_acc: 0.5814\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7614 - acc: 0.6818 - val_loss: 1.0769 - val_acc: 0.5814\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.7560 - acc: 0.6932 - val_loss: 1.0792 - val_acc: 0.5581\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7519 - acc: 0.6932 - val_loss: 1.0835 - val_acc: 0.5814\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7472 - acc: 0.6932 - val_loss: 1.0876 - val_acc: 0.5814\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7428 - acc: 0.7045 - val_loss: 1.0885 - val_acc: 0.5814\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.7412 - acc: 0.7045 - val_loss: 1.0943 - val_acc: 0.5814\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7329 - acc: 0.7273 - val_loss: 1.0952 - val_acc: 0.5814\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7297 - acc: 0.7159 - val_loss: 1.0960 - val_acc: 0.5814\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7244 - acc: 0.7159 - val_loss: 1.1015 - val_acc: 0.5814\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7195 - acc: 0.7159 - val_loss: 1.1031 - val_acc: 0.5814\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7164 - acc: 0.7159 - val_loss: 1.1066 - val_acc: 0.5814\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7131 - acc: 0.7159 - val_loss: 1.1079 - val_acc: 0.5814\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7075 - acc: 0.7159 - val_loss: 1.1142 - val_acc: 0.5581\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7029 - acc: 0.7273 - val_loss: 1.1164 - val_acc: 0.5814\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.6982 - acc: 0.7159 - val_loss: 1.1179 - val_acc: 0.5581\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6963 - acc: 0.7159 - val_loss: 1.1236 - val_acc: 0.5581\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6913 - acc: 0.7159 - val_loss: 1.1239 - val_acc: 0.5814\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.6857 - acc: 0.7159 - val_loss: 1.1278 - val_acc: 0.5581\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.6818 - acc: 0.7273 - val_loss: 1.1345 - val_acc: 0.5581\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6790 - acc: 0.7159 - val_loss: 1.1380 - val_acc: 0.5814\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6747 - acc: 0.7159 - val_loss: 1.1403 - val_acc: 0.5814\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.6712 - acc: 0.7159 - val_loss: 1.1425 - val_acc: 0.5581\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.6674 - acc: 0.7159 - val_loss: 1.1478 - val_acc: 0.5581\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6643 - acc: 0.7159 - val_loss: 1.1497 - val_acc: 0.5581\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.6600 - acc: 0.7045 - val_loss: 1.1528 - val_acc: 0.5581\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.6568 - acc: 0.7159 - val_loss: 1.1553 - val_acc: 0.5581\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6547 - acc: 0.7386 - val_loss: 1.1633 - val_acc: 0.5581\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6494 - acc: 0.7273 - val_loss: 1.1626 - val_acc: 0.5581\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.6447 - acc: 0.7386 - val_loss: 1.1693 - val_acc: 0.5814\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.6403 - acc: 0.7273 - val_loss: 1.1703 - val_acc: 0.5581\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.6358 - acc: 0.7614 - val_loss: 1.1748 - val_acc: 0.5581\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.6330 - acc: 0.7500 - val_loss: 1.1765 - val_acc: 0.5581\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 0s 300us/step - loss: 0.6285 - acc: 0.7273 - val_loss: 1.1795 - val_acc: 0.5581\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.6242 - acc: 0.7500 - val_loss: 1.1858 - val_acc: 0.5581\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6206 - acc: 0.7386 - val_loss: 1.1888 - val_acc: 0.5581\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6166 - acc: 0.7614 - val_loss: 1.1918 - val_acc: 0.5581\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6132 - acc: 0.7273 - val_loss: 1.1965 - val_acc: 0.5581\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.6079 - acc: 0.7614 - val_loss: 1.1978 - val_acc: 0.5581\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.6063 - acc: 0.7614 - val_loss: 1.2027 - val_acc: 0.5581\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.6003 - acc: 0.7727 - val_loss: 1.2055 - val_acc: 0.5814\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.5966 - acc: 0.7614 - val_loss: 1.2108 - val_acc: 0.5814\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.5930 - acc: 0.7727 - val_loss: 1.2138 - val_acc: 0.5814\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5903 - acc: 0.7614 - val_loss: 1.2170 - val_acc: 0.5581\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.5843 - acc: 0.7727 - val_loss: 1.2186 - val_acc: 0.5814\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5819 - acc: 0.7727 - val_loss: 1.2236 - val_acc: 0.5814\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5788 - acc: 0.7614 - val_loss: 1.2246 - val_acc: 0.5814\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5735 - acc: 0.7841 - val_loss: 1.2281 - val_acc: 0.6047\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.5708 - acc: 0.7727 - val_loss: 1.2309 - val_acc: 0.5814\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.5670 - acc: 0.7841 - val_loss: 1.2340 - val_acc: 0.5814\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.5639 - acc: 0.7727 - val_loss: 1.2352 - val_acc: 0.5814\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.5598 - acc: 0.7955 - val_loss: 1.2420 - val_acc: 0.5814\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 0s 340us/step - loss: 0.5550 - acc: 0.7841 - val_loss: 1.2422 - val_acc: 0.5814\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.5521 - acc: 0.7727 - val_loss: 1.2446 - val_acc: 0.5814\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.5502 - acc: 0.7955 - val_loss: 1.2549 - val_acc: 0.5814\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.5458 - acc: 0.7841 - val_loss: 1.2595 - val_acc: 0.5814\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5441 - acc: 0.7614 - val_loss: 1.2596 - val_acc: 0.5814\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.5408 - acc: 0.7955 - val_loss: 1.2713 - val_acc: 0.5814\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5367 - acc: 0.7955 - val_loss: 1.2708 - val_acc: 0.5814\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.5327 - acc: 0.7955 - val_loss: 1.2738 - val_acc: 0.5814\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5292 - acc: 0.7955 - val_loss: 1.2764 - val_acc: 0.5814\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.5271 - acc: 0.7955 - val_loss: 1.2811 - val_acc: 0.5814\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5225 - acc: 0.7955 - val_loss: 1.2854 - val_acc: 0.5814\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.5207 - acc: 0.7955 - val_loss: 1.2899 - val_acc: 0.5814\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.5159 - acc: 0.8068 - val_loss: 1.2966 - val_acc: 0.5814\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5138 - acc: 0.8068 - val_loss: 1.3000 - val_acc: 0.5814\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.5106 - acc: 0.8068 - val_loss: 1.3027 - val_acc: 0.5814\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.5080 - acc: 0.8068 - val_loss: 1.3082 - val_acc: 0.5814\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.5059 - acc: 0.8068 - val_loss: 1.3114 - val_acc: 0.5814\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 0s 353us/step - loss: 0.5016 - acc: 0.7955 - val_loss: 1.3149 - val_acc: 0.5814\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4979 - acc: 0.8068 - val_loss: 1.3179 - val_acc: 0.5814\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.4952 - acc: 0.8295 - val_loss: 1.3214 - val_acc: 0.5814\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.4925 - acc: 0.8068 - val_loss: 1.3293 - val_acc: 0.5581\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.4915 - acc: 0.8182 - val_loss: 1.3360 - val_acc: 0.5581\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.4880 - acc: 0.8182 - val_loss: 1.3347 - val_acc: 0.5581\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.4830 - acc: 0.8409 - val_loss: 1.3461 - val_acc: 0.5581\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4807 - acc: 0.8409 - val_loss: 1.3501 - val_acc: 0.5581\n",
            "Epoch 111/200\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.4789 - acc: 0.8409 - val_loss: 1.3541 - val_acc: 0.5581\n",
            "Epoch 112/200\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.4750 - acc: 0.8523 - val_loss: 1.3603 - val_acc: 0.5581\n",
            "Epoch 113/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.4728 - acc: 0.8295 - val_loss: 1.3649 - val_acc: 0.5581\n",
            "Epoch 114/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.4706 - acc: 0.8409 - val_loss: 1.3666 - val_acc: 0.5581\n",
            "Epoch 115/200\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.4691 - acc: 0.8409 - val_loss: 1.3753 - val_acc: 0.5581\n",
            "Epoch 116/200\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.4658 - acc: 0.8409 - val_loss: 1.3822 - val_acc: 0.5581\n",
            "Epoch 117/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.4626 - acc: 0.8409 - val_loss: 1.3855 - val_acc: 0.5349\n",
            "Epoch 118/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4599 - acc: 0.8409 - val_loss: 1.3899 - val_acc: 0.5349\n",
            "Epoch 119/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.4579 - acc: 0.8295 - val_loss: 1.3917 - val_acc: 0.5349\n",
            "Epoch 120/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.4556 - acc: 0.8295 - val_loss: 1.4005 - val_acc: 0.5349\n",
            "Epoch 121/200\n",
            "88/88 [==============================] - 0s 293us/step - loss: 0.4526 - acc: 0.8409 - val_loss: 1.4059 - val_acc: 0.5349\n",
            "Epoch 122/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4512 - acc: 0.8409 - val_loss: 1.4089 - val_acc: 0.5349\n",
            "Epoch 123/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.4483 - acc: 0.8409 - val_loss: 1.4118 - val_acc: 0.5349\n",
            "Epoch 124/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4457 - acc: 0.8409 - val_loss: 1.4137 - val_acc: 0.5349\n",
            "Epoch 125/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.4443 - acc: 0.8295 - val_loss: 1.4233 - val_acc: 0.5349\n",
            "Epoch 126/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.4418 - acc: 0.8523 - val_loss: 1.4302 - val_acc: 0.5349\n",
            "Epoch 127/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.4387 - acc: 0.8636 - val_loss: 1.4301 - val_acc: 0.5349\n",
            "Epoch 128/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.4368 - acc: 0.8636 - val_loss: 1.4416 - val_acc: 0.5349\n",
            "Epoch 129/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.4348 - acc: 0.8409 - val_loss: 1.4421 - val_acc: 0.5349\n",
            "Epoch 130/200\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.4326 - acc: 0.8409 - val_loss: 1.4483 - val_acc: 0.5349\n",
            "Epoch 131/200\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.4302 - acc: 0.8636 - val_loss: 1.4528 - val_acc: 0.5349\n",
            "Epoch 132/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.4289 - acc: 0.8523 - val_loss: 1.4549 - val_acc: 0.5349\n",
            "Epoch 133/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4263 - acc: 0.8636 - val_loss: 1.4625 - val_acc: 0.5349\n",
            "Epoch 134/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4246 - acc: 0.8636 - val_loss: 1.4655 - val_acc: 0.5581\n",
            "Epoch 135/200\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.4211 - acc: 0.8636 - val_loss: 1.4713 - val_acc: 0.5581\n",
            "Epoch 136/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.4197 - acc: 0.8636 - val_loss: 1.4784 - val_acc: 0.5581\n",
            "Epoch 137/200\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.4193 - acc: 0.8523 - val_loss: 1.4762 - val_acc: 0.5581\n",
            "Epoch 138/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4159 - acc: 0.8523 - val_loss: 1.4922 - val_acc: 0.5581\n",
            "Epoch 139/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4151 - acc: 0.8523 - val_loss: 1.4898 - val_acc: 0.5581\n",
            "Epoch 140/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.4134 - acc: 0.8523 - val_loss: 1.4909 - val_acc: 0.5581\n",
            "Epoch 141/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.4103 - acc: 0.8636 - val_loss: 1.4964 - val_acc: 0.5581\n",
            "Epoch 142/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.4085 - acc: 0.8636 - val_loss: 1.5042 - val_acc: 0.5581\n",
            "Epoch 143/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4063 - acc: 0.8636 - val_loss: 1.5099 - val_acc: 0.5581\n",
            "Epoch 144/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.4047 - acc: 0.8636 - val_loss: 1.5119 - val_acc: 0.5581\n",
            "Epoch 145/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.4033 - acc: 0.8636 - val_loss: 1.5161 - val_acc: 0.5581\n",
            "Epoch 146/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.4008 - acc: 0.8636 - val_loss: 1.5174 - val_acc: 0.5581\n",
            "Epoch 147/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3985 - acc: 0.8636 - val_loss: 1.5249 - val_acc: 0.5581\n",
            "Epoch 148/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.3979 - acc: 0.8636 - val_loss: 1.5271 - val_acc: 0.5581\n",
            "Epoch 149/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.3959 - acc: 0.8636 - val_loss: 1.5270 - val_acc: 0.5581\n",
            "Epoch 150/200\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.3929 - acc: 0.8636 - val_loss: 1.5375 - val_acc: 0.5581\n",
            "Epoch 151/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.3921 - acc: 0.8636 - val_loss: 1.5352 - val_acc: 0.5814\n",
            "Epoch 152/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.3890 - acc: 0.8636 - val_loss: 1.5429 - val_acc: 0.5581\n",
            "Epoch 153/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.3877 - acc: 0.8636 - val_loss: 1.5459 - val_acc: 0.5814\n",
            "Epoch 154/200\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.3853 - acc: 0.8636 - val_loss: 1.5501 - val_acc: 0.5581\n",
            "Epoch 155/200\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.3833 - acc: 0.8636 - val_loss: 1.5559 - val_acc: 0.5581\n",
            "Epoch 156/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3812 - acc: 0.8636 - val_loss: 1.5591 - val_acc: 0.5581\n",
            "Epoch 157/200\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.3803 - acc: 0.8636 - val_loss: 1.5628 - val_acc: 0.5814\n",
            "Epoch 158/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3780 - acc: 0.8636 - val_loss: 1.5708 - val_acc: 0.5814\n",
            "Epoch 159/200\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.3746 - acc: 0.8750 - val_loss: 1.5757 - val_acc: 0.5814\n",
            "Epoch 160/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3728 - acc: 0.8864 - val_loss: 1.5785 - val_acc: 0.5581\n",
            "Epoch 161/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.3709 - acc: 0.8864 - val_loss: 1.5751 - val_acc: 0.5814\n",
            "Epoch 162/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3701 - acc: 0.8864 - val_loss: 1.5879 - val_acc: 0.5581\n",
            "Epoch 163/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.3670 - acc: 0.8864 - val_loss: 1.5868 - val_acc: 0.5581\n",
            "Epoch 164/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.3647 - acc: 0.8864 - val_loss: 1.5882 - val_acc: 0.5581\n",
            "Epoch 165/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.3640 - acc: 0.8864 - val_loss: 1.5947 - val_acc: 0.5581\n",
            "Epoch 166/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3611 - acc: 0.8864 - val_loss: 1.5963 - val_acc: 0.5581\n",
            "Epoch 167/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.3603 - acc: 0.8864 - val_loss: 1.6023 - val_acc: 0.5581\n",
            "Epoch 168/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.3585 - acc: 0.8977 - val_loss: 1.6044 - val_acc: 0.5581\n",
            "Epoch 169/200\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.3564 - acc: 0.8977 - val_loss: 1.6145 - val_acc: 0.5581\n",
            "Epoch 170/200\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.3543 - acc: 0.8977 - val_loss: 1.6102 - val_acc: 0.5349\n",
            "Epoch 171/200\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.3538 - acc: 0.8977 - val_loss: 1.6127 - val_acc: 0.5349\n",
            "Epoch 172/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.3513 - acc: 0.8864 - val_loss: 1.6177 - val_acc: 0.5349\n",
            "Epoch 173/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.3489 - acc: 0.8977 - val_loss: 1.6243 - val_acc: 0.5349\n",
            "Epoch 174/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.3480 - acc: 0.8977 - val_loss: 1.6256 - val_acc: 0.5349\n",
            "Epoch 175/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.3462 - acc: 0.8977 - val_loss: 1.6315 - val_acc: 0.5349\n",
            "Epoch 176/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3444 - acc: 0.8977 - val_loss: 1.6377 - val_acc: 0.5349\n",
            "Epoch 177/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.3424 - acc: 0.8977 - val_loss: 1.6422 - val_acc: 0.5349\n",
            "Epoch 178/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.3409 - acc: 0.8977 - val_loss: 1.6400 - val_acc: 0.5349\n",
            "Epoch 179/200\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.3389 - acc: 0.8977 - val_loss: 1.6467 - val_acc: 0.5349\n",
            "Epoch 180/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3380 - acc: 0.8977 - val_loss: 1.6522 - val_acc: 0.5349\n",
            "Epoch 181/200\n",
            "88/88 [==============================] - 0s 334us/step - loss: 0.3354 - acc: 0.8977 - val_loss: 1.6585 - val_acc: 0.5349\n",
            "Epoch 182/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.3347 - acc: 0.8977 - val_loss: 1.6556 - val_acc: 0.5349\n",
            "Epoch 183/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.3330 - acc: 0.8977 - val_loss: 1.6601 - val_acc: 0.5349\n",
            "Epoch 184/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3312 - acc: 0.8977 - val_loss: 1.6634 - val_acc: 0.5349\n",
            "Epoch 185/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.3301 - acc: 0.8977 - val_loss: 1.6698 - val_acc: 0.5349\n",
            "Epoch 186/200\n",
            "88/88 [==============================] - 0s 335us/step - loss: 0.3285 - acc: 0.8977 - val_loss: 1.6673 - val_acc: 0.5349\n",
            "Epoch 187/200\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.3266 - acc: 0.8977 - val_loss: 1.6702 - val_acc: 0.5349\n",
            "Epoch 188/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3252 - acc: 0.8977 - val_loss: 1.6749 - val_acc: 0.5349\n",
            "Epoch 189/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.3244 - acc: 0.8977 - val_loss: 1.6734 - val_acc: 0.5349\n",
            "Epoch 190/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.3229 - acc: 0.8977 - val_loss: 1.6745 - val_acc: 0.5349\n",
            "Epoch 191/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.3205 - acc: 0.8977 - val_loss: 1.6794 - val_acc: 0.5349\n",
            "Epoch 192/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.3187 - acc: 0.8977 - val_loss: 1.6824 - val_acc: 0.5349\n",
            "Epoch 193/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.3182 - acc: 0.8977 - val_loss: 1.6854 - val_acc: 0.5349\n",
            "Epoch 194/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3162 - acc: 0.8977 - val_loss: 1.6904 - val_acc: 0.5349\n",
            "Epoch 195/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3155 - acc: 0.8977 - val_loss: 1.6929 - val_acc: 0.5349\n",
            "Epoch 196/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.3145 - acc: 0.8977 - val_loss: 1.6933 - val_acc: 0.5349\n",
            "Epoch 197/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.3119 - acc: 0.8977 - val_loss: 1.6972 - val_acc: 0.5349\n",
            "Epoch 198/200\n",
            "88/88 [==============================] - 0s 284us/step - loss: 0.3113 - acc: 0.8977 - val_loss: 1.6979 - val_acc: 0.5349\n",
            "Epoch 199/200\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.3089 - acc: 0.8977 - val_loss: 1.7020 - val_acc: 0.5349\n",
            "Epoch 200/200\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.3083 - acc: 0.8977 - val_loss: 1.7028 - val_acc: 0.5349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2d1f19ec-b4e7-436e-ea6d-b332a351d7a9",
        "id": "6OH7qa6a14-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.09052215e-01, -1.30570140e+00, -1.50987053e+00,\n",
              "         4.78413407e-01, -5.48688420e-01, -2.19260239e-01,\n",
              "        -3.87514118e-01, -2.35668681e-01, -3.27761894e-01,\n",
              "        -3.30961271e-01, -8.29963414e-01,  1.09070485e+00,\n",
              "        -4.68298254e-01, -2.54403631e-01,  2.05362316e-01,\n",
              "        -1.58492380e-01, -8.76044573e-01],\n",
              "       [ 1.93706034e+00,  2.47792697e-01, -6.14324644e-01,\n",
              "        -1.43358516e+00,  1.74280631e-01, -1.01385115e+00,\n",
              "        -5.98180597e-01,  7.79397065e-02, -3.34625115e-01,\n",
              "        -3.30973016e-01, -1.31203202e+00,  4.46338623e-02,\n",
              "        -1.30913346e+00, -5.65265481e-01,  2.19195593e+00,\n",
              "         9.70100653e-01, -1.05149009e+00],\n",
              "       [ 7.34257050e-01,  9.68988268e-02,  3.85769839e-01,\n",
              "         1.61200050e-01, -5.40961150e-01, -3.83658358e-01,\n",
              "        -1.13750969e-01, -2.28591476e-01, -1.15055286e-01,\n",
              "        -3.30299955e-01,  1.35982249e-01,  3.71533408e-01,\n",
              "        -1.21254356e-01, -4.84378805e-01, -5.30458542e-01,\n",
              "        -4.87994766e-01,  1.88481002e-01],\n",
              "       [ 7.68510240e-01, -1.82042583e-01, -3.33326380e-01,\n",
              "         6.50849331e-02, -5.40195473e-01, -8.49453029e-01,\n",
              "         1.35406834e-01, -8.66383484e-01, -3.31690446e-01,\n",
              "        -3.30963175e-01, -7.37179436e-01,  4.04397633e-01,\n",
              "         3.35949635e-01,  3.71018266e-01, -4.24856494e-01,\n",
              "         1.10650982e+00, -5.76281334e-01],\n",
              "       [-2.67444175e-01,  2.78483283e-01,  5.18248153e-01,\n",
              "         1.05778670e+00, -5.44927981e-01, -2.74059612e-01,\n",
              "        -3.14211181e-01,  1.39098834e+00,  3.52354987e-01,\n",
              "         1.74307426e+00,  1.16373157e+00, -1.00481406e+00,\n",
              "        -2.31777499e-01, -7.50286880e-01, -4.86354308e-01,\n",
              "        -7.66223485e-01,  4.69101139e-01],\n",
              "       [ 1.54957687e+00,  1.90539890e+00, -2.22052320e-01,\n",
              "         3.87307146e-01, -5.45340485e-01, -1.01385115e+00,\n",
              "        -2.73626628e-01, -6.67618363e-01, -3.33713506e-01,\n",
              "        -3.30970545e-01, -1.05552128e+00, -1.16554466e+00,\n",
              "         7.92773556e-01, -6.04756035e-03, -1.36103435e-01,\n",
              "         2.84618093e-01, -7.68000271e-01],\n",
              "       [ 1.87054098e+00,  1.34674226e+00, -1.02135859e+00,\n",
              "        -6.88962901e-01,  1.41898628e+00, -7.67253969e-01,\n",
              "        -2.64847015e-01, -5.26713015e-01, -3.29415595e-01,\n",
              "        -3.30965430e-01, -1.76967570e+00,  1.59330175e+00,\n",
              "        -1.24929440e+00, -4.09476984e-01,  2.95159541e-01,\n",
              "         2.24992687e+00, -7.63153541e-01],\n",
              "       [ 1.40230428e+00,  4.89008981e-01, -1.59088213e+00,\n",
              "        -7.77936324e-01,  6.37226628e-01, -8.76852715e-01,\n",
              "        -2.27818116e-01, -3.15371130e-01, -3.33772401e-01,\n",
              "        -3.30969539e-01, -9.21826930e-01,  2.84393304e+00,\n",
              "        -1.70848419e+00, -1.46089724e-01,  1.24885248e+00,\n",
              "         1.45114145e+00, -9.68926187e-01],\n",
              "       [ 1.49637582e+00,  1.54770319e+00, -6.67501968e-01,\n",
              "        -1.28154597e+00, -5.45678923e-01, -1.06865052e+00,\n",
              "         5.59983970e-01, -8.66709024e-01, -3.31610893e-01,\n",
              "        -3.30972296e-01, -1.64687168e+00, -5.07742521e-01,\n",
              "         1.05796673e-01,  6.81620219e-01,  1.91473680e-01,\n",
              "         5.11821617e+00, -6.81890737e-01],\n",
              "       [ 1.00150735e+00, -8.58201787e-01,  3.11646964e-01,\n",
              "        -1.43442573e+00, -5.49929348e-01, -9.04252402e-01,\n",
              "        -5.28211743e-01, -1.20002808e-01, -3.35769175e-01,\n",
              "        -3.30973760e-01, -4.42456637e-01,  2.35041057e+00,\n",
              "        -1.82929991e+00,  2.68994980e-01,  2.63286133e+00,\n",
              "         1.29973450e+00, -1.07303619e+00],\n",
              "       [-6.27982153e-01,  1.82083041e+00,  6.89179021e-01,\n",
              "         1.26719711e+00, -5.43758936e-01, -1.64460866e-01,\n",
              "        -6.68394416e-02,  9.78698647e-01,  6.80756634e-01,\n",
              "         2.73073776e+00,  1.92168550e+00, -1.03524748e+00,\n",
              "         5.49325368e-01,  4.74349821e-01, -4.82848243e-01,\n",
              "        -8.08082795e-01,  9.01514579e-01],\n",
              "       [ 5.22267532e-01, -1.06875098e-01, -6.47157410e-01,\n",
              "        -2.81002626e-02, -5.47216346e-01,  5.47366256e-02,\n",
              "        -9.22810177e-01,  2.97018333e-01, -1.85535435e-01,\n",
              "        -3.30751288e-01, -1.01051841e+00,  5.33572761e-01,\n",
              "        -8.41591880e-01, -1.03863818e+00, -5.91370564e-01,\n",
              "        -4.78529164e-01,  7.65523623e-02],\n",
              "       [ 2.31051081e-01, -3.20246036e-01,  3.33606515e-01,\n",
              "        -1.92448965e+00, -5.47077623e-01, -1.39470679e+00,\n",
              "         1.71658188e-01, -8.49813810e-01, -3.35016786e-01,\n",
              "        -3.30973339e-01, -1.13057908e+00, -3.96647241e-02,\n",
              "        -7.76402925e-02, -1.46431122e-01,  1.57365298e+00,\n",
              "         2.06583169e+00, -9.92270817e-01],\n",
              "       [-6.44453936e-01, -8.78723589e-03, -1.04418142e-01,\n",
              "        -5.22952367e-01,  9.09313099e-01,  1.69871781e+00,\n",
              "        -1.21474566e+00,  1.75610763e+00, -9.44599440e-02,\n",
              "        -3.30471019e-01,  8.05783791e-01, -1.11896137e+00,\n",
              "        -5.71419421e-01, -1.13904175e+00,  2.36554102e-01,\n",
              "        -7.98738149e-01, -5.48945706e-01],\n",
              "       [-3.48340628e-01, -2.04547197e+00, -1.03467134e+00,\n",
              "        -1.55390505e-02, -5.46627799e-01, -1.15084958e+00,\n",
              "        -6.69297508e-02, -7.37879216e-01, -3.34679763e-01,\n",
              "        -3.30971699e-01, -1.03184069e+00,  1.50423112e+00,\n",
              "        -6.09364543e-01,  1.28156402e-01,  5.77786575e-02,\n",
              "         1.87632640e+00, -7.99203206e-01],\n",
              "       [ 1.31428729e+00,  8.33731470e-01, -2.06133494e+00,\n",
              "        -4.94935111e-01, -1.48787803e-01, -4.93257104e-01,\n",
              "        -1.11052146e+00,  2.96631799e-01, -3.34734762e-01,\n",
              "        -3.30973644e-01, -1.88126819e+00,  8.14095169e-01,\n",
              "        -1.86868564e+00, -9.97315097e-01,  1.51500876e+00,\n",
              "         6.33359739e-01, -1.04880671e+00],\n",
              "       [-7.78919476e-02, -8.97044120e-03,  6.51523027e-01,\n",
              "         7.49297997e-01, -5.27596185e-01, -5.20656791e-01,\n",
              "        -4.28916598e-02,  4.08624768e-01,  1.37001203e+00,\n",
              "         4.65521584e+00,  7.28554740e-01, -6.10275581e-01,\n",
              "         4.10570346e-01,  3.16088294e-01, -7.78599488e-01,\n",
              "        -7.78277706e-01,  2.46846217e+00],\n",
              "       [-1.07862168e+00, -9.07261519e-01, -1.76684217e+00,\n",
              "         4.54004496e-01, -5.45792945e-01, -8.22053342e-01,\n",
              "        -1.15501373e-01, -6.66255332e-01, -3.34572715e-01,\n",
              "        -3.30971879e-01, -1.27247672e+00,  1.63479878e+00,\n",
              "        -7.17735476e-01,  3.80034857e-01, -3.52185277e-01,\n",
              "         2.09857952e+00, -5.85517130e-01],\n",
              "       [-4.10778722e-01, -1.47731624e+00,  2.03066111e-02,\n",
              "         5.14359698e-01, -5.42388460e-01,  9.04126907e-01,\n",
              "         1.61247582e-01, -6.87811940e-01, -3.23281141e-01,\n",
              "        -3.30938728e-01, -8.53130201e-03, -2.23899349e-02,\n",
              "         4.65265450e-01, -1.25718098e-01, -5.02362406e-01,\n",
              "         3.93224819e-01, -3.54460457e-01],\n",
              "       [-2.67691522e+00, -7.83949702e-01, -4.35930764e-01,\n",
              "         7.82858235e-01, -5.43538425e-01, -3.56258672e-01,\n",
              "        -2.29288143e-01, -4.04234755e-01, -3.23171479e-01,\n",
              "        -3.30952033e-01, -6.84554256e-01,  1.72157557e-02,\n",
              "         1.25295199e-01, -5.44093976e-01, -6.52698092e-01,\n",
              "         2.90087082e-01, -1.25754879e-01],\n",
              "       [ 8.66821394e-01, -3.81661127e-01, -9.31529767e-01,\n",
              "        -1.08735653e-01,  2.68419262e+00, -8.22053342e-01,\n",
              "        -1.16075046e+00,  4.28848179e-01, -2.98252121e-01,\n",
              "        -3.30895891e-01, -5.72195095e-01,  2.93311022e-01,\n",
              "        -9.78331074e-01, -7.76096481e-01, -3.14273460e-01,\n",
              "        -3.22049443e-01, -5.37955363e-01],\n",
              "       [ 7.23052291e-02,  1.04327058e+00, -6.85581854e-01,\n",
              "         3.47382948e-01,  2.82555274e+00, -8.22053342e-01,\n",
              "        -8.49521291e-01, -1.35127259e-01, -3.32460424e-01,\n",
              "        -3.30967487e-01, -9.50662606e-01, -1.34840990e-01,\n",
              "        -4.28277955e-01, -5.22481418e-01, -6.28998767e-02,\n",
              "         3.20908629e-01, -7.80744615e-01],\n",
              "       [ 1.20793562e+00, -1.03223723e+00, -1.62396721e+00,\n",
              "         3.11100130e-01,  1.49808593e+00, -9.86451461e-01,\n",
              "        -4.48263664e-01, -2.91367343e-01, -3.31854994e-01,\n",
              "        -3.30968638e-01, -1.24663188e+00,  2.52799032e-01,\n",
              "        -6.54119703e-01, -4.47802506e-01,  1.69383467e-01,\n",
              "         1.04489784e+00, -8.04946099e-01],\n",
              "       [-6.87842207e-01,  1.72331016e+00,  9.61258968e-01,\n",
              "         5.87565237e-01, -5.46222606e-01, -1.64460866e-01,\n",
              "         7.23101620e-01,  7.19138408e-01, -1.35740067e-01,\n",
              "        -3.30621865e-01,  9.72960680e-01,  5.02376472e-01,\n",
              "        -3.50762360e-01, -6.61316119e-01,  2.15088506e-01,\n",
              "        -5.02291020e-01, -5.16875211e-01],\n",
              "       [-7.06431308e-01, -9.89748699e-03,  1.32801751e+00,\n",
              "        -1.37618450e+00, -5.39625628e-01,  6.60326170e+00,\n",
              "         3.83648904e+00, -1.69002919e+00, -2.97148140e-01,\n",
              "        -3.30858604e-01,  2.98396859e+00, -8.38653288e-01,\n",
              "         2.67323855e+00,  1.97331940e+00,  4.59022522e-01,\n",
              "        -5.16599725e-01, -7.19356677e-01],\n",
              "       [ 2.06934501e+00,  1.21207305e+00, -8.02529761e-01,\n",
              "        -3.23665850e+00,  3.52435672e-01, -1.01385115e+00,\n",
              "         5.82901521e-02, -7.18648977e-01, -3.35432838e-01,\n",
              "        -3.30974284e-01, -1.66414662e+00,  2.66798240e+00,\n",
              "        -1.79401751e+00,  3.67356718e-01,  2.40487403e+00,\n",
              "         4.02913212e+00, -1.04661333e+00],\n",
              "       [-4.81158272e-02, -2.32919315e-01,  1.22501708e+00,\n",
              "        -8.12476213e-01, -4.97813037e-01,  1.01372565e+00,\n",
              "         1.33700890e+00, -1.28646438e+00, -2.95623368e-01,\n",
              "        -3.30844390e-01, -3.13005653e-01, -3.33040465e-01,\n",
              "         1.52391557e+00,  1.11048723e+00, -7.64915655e-01,\n",
              "         4.28233562e-01,  4.28892001e-01],\n",
              "       [-9.82392012e-01, -1.50981574e-01,  4.83117627e-01,\n",
              "         2.22916324e-01, -5.37588700e-01, -5.48621204e-02,\n",
              "        -3.23635973e-01,  2.80315082e-01,  1.13406449e-01,\n",
              "        -3.29875677e-01,  8.18070701e-01, -1.43941597e+00,\n",
              "         1.04414111e+00,  1.63780813e-01, -7.28423285e-01,\n",
              "        -7.65482987e-01,  8.05402450e-01],\n",
              "       [ 6.97724794e-01,  1.63023754e-01, -1.75187371e+00,\n",
              "         1.14426911e+00, -5.48805576e-01,  8.21363121e-02,\n",
              "        -1.28708397e+00,  8.40567159e-01, -2.89064255e-01,\n",
              "        -3.30913130e-01, -1.78303534e-01, -1.94079236e+00,\n",
              "         1.47162088e-01, -7.67067466e-01, -4.77411270e-01,\n",
              "        -6.59369742e-01, -4.29973239e-01],\n",
              "       [ 1.47188959e+00,  5.62595026e-01, -1.49271155e+00,\n",
              "         1.02886770e+00,  6.05581723e-01, -3.56258672e-01,\n",
              "        -9.01440816e-01,  4.88482086e-01, -3.20638346e-01,\n",
              "        -3.30944186e-01, -3.05015407e-01,  4.24598068e-01,\n",
              "        -1.03575902e+00, -6.13459514e-01,  7.86519903e-01,\n",
              "        -3.65165869e-01, -8.72051850e-01],\n",
              "       [ 1.78305420e+00,  1.37711411e+00,  1.49916132e-01,\n",
              "        -5.15432158e-01, -5.46664013e-01, -1.17824927e+00,\n",
              "        -7.14148531e-01, -3.26597763e-01, -3.31802117e-01,\n",
              "        -3.30964935e-01, -1.10153207e+00, -3.25076660e-01,\n",
              "        -2.76899548e-01, -4.81278689e-01,  1.63951096e-01,\n",
              "         7.64810335e-01, -8.13461099e-01],\n",
              "       [ 1.33916856e+00,  1.74729366e+00,  9.32576683e-01,\n",
              "        -3.92248540e-01, -5.42436939e-01, -5.48056477e-01,\n",
              "         5.42853861e-01, -3.37319011e-01,  1.32217300e-01,\n",
              "        -3.30650739e-01,  9.93278185e-02,  2.18568209e+00,\n",
              "        -7.52734919e-01, -5.58159325e-01, -2.94368551e-01,\n",
              "         3.00978986e-04, -1.51329794e-01],\n",
              "       [-4.34428134e-01,  3.02004017e-01,  1.01503799e-01,\n",
              "        -7.25101197e-01, -5.28519067e-01,  8.21927847e-01,\n",
              "         1.45567081e+00, -7.68295455e-01,  4.14797781e-01,\n",
              "         2.25528874e+00,  5.75813151e-01,  6.77986198e-01,\n",
              "         7.49259558e-01, -2.17678153e-01, -7.81576726e-01,\n",
              "        -6.63814520e-01,  1.22155484e+00],\n",
              "       [-1.31924424e+00, -2.13873130e+00, -1.64219371e+00,\n",
              "         5.92161706e-01,  3.05030254e-01, -1.09661493e-01,\n",
              "        -8.13304283e-01,  4.59418668e-01, -2.47396709e-01,\n",
              "        -3.30904876e-01, -6.48364725e-01,  2.66347433e-01,\n",
              "        -5.20524304e-01, -8.21052616e-01,  4.23155858e-01,\n",
              "        -7.44445162e-01, -7.34242330e-01],\n",
              "       [ 4.98694935e-01,  7.35612793e-01,  6.35586964e-01,\n",
              "         6.54250049e-01,  3.16485533e-02, -1.64460866e-01,\n",
              "        -3.77661612e-01,  2.09671116e+00, -2.80318175e-01,\n",
              "        -3.30830305e-01,  6.43998628e-01,  5.19144703e-01,\n",
              "        -1.77373780e+00, -1.05512656e+00,  3.58224772e+00,\n",
              "        -7.01535374e-01, -9.40195936e-01],\n",
              "       [-5.53790101e-01,  1.03819108e+00,  1.37825865e+00,\n",
              "        -5.27827103e-01, -4.81328114e-01,  9.58926280e-01,\n",
              "         1.12437799e+00, -1.09948055e+00, -1.42972054e-01,\n",
              "        -3.30380716e-01,  1.64834270e-01, -1.08663847e+00,\n",
              "         1.87716536e+00,  3.95541488e+00, -6.54081800e-01,\n",
              "        -5.41593407e-01,  9.48322491e-01],\n",
              "       [-1.26926479e+00, -6.36680117e-01,  1.01970124e+00,\n",
              "        -5.33978697e-01, -5.33738171e-01,  9.86325966e-01,\n",
              "         1.25522324e+00, -6.93947595e-01, -1.94759811e-01,\n",
              "        -3.30667183e-01,  1.30284625e+00, -2.03162186e+00,\n",
              "         2.27840513e+00,  4.24096956e+00,  6.55272926e-01,\n",
              "        -7.58507969e-01, -2.33948455e-01],\n",
              "       [-3.20926151e-02, -6.03988430e-01, -7.78588951e-01,\n",
              "         6.46497882e-01,  1.92052260e+00,  5.75330669e-01,\n",
              "         1.94163568e-01,  1.55176070e-01, -2.84214659e-01,\n",
              "        -3.30810446e-01,  1.22361023e+00, -4.23498024e-02,\n",
              "         1.00554220e-01, -3.74807906e-01,  4.69542622e-01,\n",
              "        -6.39069582e-01, -7.50674863e-01],\n",
              "       [-1.04351660e+00, -1.81410010e+00,  3.47721555e-01,\n",
              "        -1.29945878e-02, -5.39025604e-01, -4.11058045e-01,\n",
              "        -3.10854978e-01, -4.68380791e-01, -2.99799844e-01,\n",
              "        -3.30884312e-01, -6.04430429e-01,  3.95954001e-01,\n",
              "        -2.75961036e-02, -3.55011848e-01, -6.40261061e-01,\n",
              "         1.25434692e-01,  1.69221243e-03],\n",
              "       [-1.70779933e+00, -1.82177506e+00,  6.30653686e-01,\n",
              "         7.72815870e-02, -5.41667522e-01, -7.94653656e-01,\n",
              "        -2.36091860e-01, -5.49058455e-01, -3.32324336e-01,\n",
              "        -3.30964704e-01, -2.78998649e-01,  7.31967045e-01,\n",
              "        -5.20673201e-02,  1.47627164e+00, -2.60634812e-01,\n",
              "         1.91408748e-01, -6.63483974e-01],\n",
              "       [-2.61293931e-01, -8.16561598e-01,  1.04338706e+00,\n",
              "        -1.15650705e+00, -5.41614180e-01,  2.60290747e+00,\n",
              "         1.30285401e+00, -7.54923071e-01, -2.82469107e-01,\n",
              "        -3.30793928e-01,  5.28076978e-01, -1.69820568e-01,\n",
              "         8.39421745e-01, -4.61928261e-01, -2.21762703e-01,\n",
              "        -2.37855717e-01, -5.14552169e-01],\n",
              "       [ 8.62558431e-01,  1.62239571e+00,  3.56990345e-01,\n",
              "        -2.80327055e-01, -5.41526545e-01,  9.04126907e-01,\n",
              "         7.81323904e-01, -7.68937274e-01, -1.92343026e-01,\n",
              "        -3.30479367e-01,  3.40655060e-01,  9.32793053e-01,\n",
              "         3.24131351e-01, -4.79406591e-01, -4.16664194e-01,\n",
              "        -3.06035404e-01, -1.41367216e-01],\n",
              "       [ 1.09128720e-01, -1.44962712e+00, -1.13746835e+00,\n",
              "        -1.59265282e-02,  2.15276655e+00, -2.19260239e-01,\n",
              "        -2.57819423e-01, -6.92733551e-01, -3.35814248e-01,\n",
              "        -3.30974439e-01, -1.20398783e+00,  9.16355465e-01,\n",
              "        -4.47333051e-01,  1.32357085e-01,  2.86341634e-01,\n",
              "         1.47032779e+00, -8.80778234e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WoqA4rT14-Q",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b913cd5-2fc3-430d-d0b0-21b1599efc13",
        "id": "YEyjhsg414-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2109e582-22ac-4bd3-b3d7-de653f9cb0d9",
        "id": "9WylvzXG14-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw9GTwCC14-f",
        "colab": {}
      },
      "source": [
        "average_acc_history_reduced = [np.mean([x[i] for x in all_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_reduced = [np.mean([x[i] for x in all_loss_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_acc_history_reduced = [np.mean([x[i] for x in all_val_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_loss_history_reduced = [np.mean([x[i] for x in all_val_loss_histories_reduced]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9dfc4d2d-9870-48a6-d0f6-7e595ef1771b",
        "id": "MdMC3doS14-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "outputId": "6e488e75-1309-4117-8240-c3c32d0def6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 10)                180       \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrcg4Bx625fC"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9_gKwYk25fK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCcaAgTi25fZ",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15e601a1-0eec-4799-a15c-2978e611e486",
        "id": "6H8nOl_X25fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_reduced, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_reduced, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a92d7db70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMh2BG152967"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "27561d40-f05a-4582-cfac-1ae7f2ec35f3",
        "id": "WNEq_n08297J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_reduced, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_reduced, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a92d0b0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5BQnjMi93Dgn"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c98b7599-8470-4e7f-8c8b-0ed5ad365c31",
        "id": "yANP0XPF3Dg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_reduced, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_reduced, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 1.2255 - acc: 0.3130\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.1124 - acc: 0.4122\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0401 - acc: 0.4885\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9896 - acc: 0.5496\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9585 - acc: 0.5802\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9361 - acc: 0.6031\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9200 - acc: 0.6260\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8996 - acc: 0.6183\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8865 - acc: 0.6260\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8760 - acc: 0.6260\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8648 - acc: 0.6336\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8563 - acc: 0.6565\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8488 - acc: 0.6565\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.8386 - acc: 0.6565\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8345 - acc: 0.6565\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8299 - acc: 0.6794\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8245 - acc: 0.6794\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8177 - acc: 0.6794\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8139 - acc: 0.6870\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8103 - acc: 0.6718\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 0s 264us/step - loss: 0.8041 - acc: 0.6718\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.7974 - acc: 0.6794\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.7946 - acc: 0.6718\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.7916 - acc: 0.6794\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.7870 - acc: 0.6641\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.7822 - acc: 0.6947\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.7811 - acc: 0.6641\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.7750 - acc: 0.7023\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.7717 - acc: 0.6870\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.7671 - acc: 0.6870\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.7666 - acc: 0.6947\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7610 - acc: 0.6870\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7597 - acc: 0.7099\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7538 - acc: 0.7023\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.7499 - acc: 0.7099\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7472 - acc: 0.6947\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7445 - acc: 0.7023\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.7427 - acc: 0.6947\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7378 - acc: 0.7023\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.7361 - acc: 0.7023\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.7320 - acc: 0.6794\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.7321 - acc: 0.7099\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.7274 - acc: 0.7176\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.7249 - acc: 0.6870\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7202 - acc: 0.7099\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.7165 - acc: 0.6947\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.7168 - acc: 0.7176\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7140 - acc: 0.7099\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7093 - acc: 0.7099\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7063 - acc: 0.7099\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7037 - acc: 0.7099\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.7022 - acc: 0.7099\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6985 - acc: 0.7099\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.6939 - acc: 0.7328\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6912 - acc: 0.7252\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6871 - acc: 0.7252\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6858 - acc: 0.6947\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.6828 - acc: 0.7328\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 0s 224us/step - loss: 0.6771 - acc: 0.7176\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.6745 - acc: 0.7252\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6709 - acc: 0.7328\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.6680 - acc: 0.7099\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6643 - acc: 0.7099\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6617 - acc: 0.7252\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.6582 - acc: 0.7252\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.6545 - acc: 0.7252\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.6523 - acc: 0.7252\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6498 - acc: 0.7481\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.6457 - acc: 0.7405\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6455 - acc: 0.7557\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.6399 - acc: 0.7481\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.6355 - acc: 0.7405\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6332 - acc: 0.7405\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.6328 - acc: 0.7557\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6268 - acc: 0.7634\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6275 - acc: 0.7481\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6213 - acc: 0.7557\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6197 - acc: 0.7481\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.6150 - acc: 0.7481\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.6146 - acc: 0.7328\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.6099 - acc: 0.7405\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6074 - acc: 0.7557\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.6054 - acc: 0.7634\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6030 - acc: 0.7481\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.6015 - acc: 0.7634\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5978 - acc: 0.7634\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5959 - acc: 0.7710\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5908 - acc: 0.7557\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5888 - acc: 0.7786\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5887 - acc: 0.7710\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.5848 - acc: 0.7634\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.5814 - acc: 0.7710\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.5792 - acc: 0.7863\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5781 - acc: 0.7710\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5750 - acc: 0.7939\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5727 - acc: 0.7939\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5713 - acc: 0.7786\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.5707 - acc: 0.7939\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5663 - acc: 0.7786\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5657 - acc: 0.7710\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.5618 - acc: 0.8015\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.5589 - acc: 0.7939\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.5560 - acc: 0.7939\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5554 - acc: 0.7939\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.5516 - acc: 0.8015\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5502 - acc: 0.7939\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5473 - acc: 0.7863\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.5450 - acc: 0.7939\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.5426 - acc: 0.8015\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.5417 - acc: 0.8015\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.5370 - acc: 0.8015\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.5391 - acc: 0.8092\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5320 - acc: 0.8092\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5328 - acc: 0.8015\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.5290 - acc: 0.7939\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5270 - acc: 0.8015\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5240 - acc: 0.8015\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5227 - acc: 0.8168\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.5196 - acc: 0.8092\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5182 - acc: 0.7939\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5140 - acc: 0.8015\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5143 - acc: 0.8092\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5101 - acc: 0.8092\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.5094 - acc: 0.7939\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.5061 - acc: 0.8092\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5035 - acc: 0.8244\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4995 - acc: 0.8244\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5017 - acc: 0.8244\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4974 - acc: 0.8168\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4959 - acc: 0.8244\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.4941 - acc: 0.8168\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.4916 - acc: 0.8092\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4889 - acc: 0.8244\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4881 - acc: 0.8092\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4848 - acc: 0.8015\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.4847 - acc: 0.8168\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4811 - acc: 0.8168\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.4782 - acc: 0.8092\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4781 - acc: 0.8244\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.4728 - acc: 0.8092\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.4748 - acc: 0.8092\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4724 - acc: 0.8092\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4681 - acc: 0.8168\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4672 - acc: 0.7939\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.4646 - acc: 0.8168\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4643 - acc: 0.8168\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4629 - acc: 0.8168\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.4610 - acc: 0.8244\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4577 - acc: 0.8244\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.4558 - acc: 0.8092\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 0s 277us/step - loss: 0.4552 - acc: 0.8244\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4539 - acc: 0.8244\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4503 - acc: 0.8168\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.4489 - acc: 0.8168\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4451 - acc: 0.8321\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4495 - acc: 0.8397\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4421 - acc: 0.8321\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4428 - acc: 0.8168\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4394 - acc: 0.8321\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4371 - acc: 0.8321\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4357 - acc: 0.8168\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4338 - acc: 0.8244\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4320 - acc: 0.8321\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.4322 - acc: 0.8244\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4297 - acc: 0.8321\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.4268 - acc: 0.8321\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4260 - acc: 0.8397\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4249 - acc: 0.8321\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4233 - acc: 0.8244\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4229 - acc: 0.8321\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4202 - acc: 0.8321\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.4180 - acc: 0.8168\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4152 - acc: 0.8244\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4151 - acc: 0.8321\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4135 - acc: 0.8321\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4113 - acc: 0.8244\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4107 - acc: 0.8321\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4084 - acc: 0.8473\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4061 - acc: 0.8397\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.4055 - acc: 0.8321\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.4020 - acc: 0.8244\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.4014 - acc: 0.8397\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3996 - acc: 0.8321\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3966 - acc: 0.8626\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.3967 - acc: 0.8550\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.3949 - acc: 0.8550\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3905 - acc: 0.8702\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3906 - acc: 0.8550\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.3912 - acc: 0.8702\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3880 - acc: 0.8550\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 0s 257us/step - loss: 0.3862 - acc: 0.8779\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 0s 236us/step - loss: 0.3857 - acc: 0.8473\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3827 - acc: 0.8702\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3875 - acc: 0.8626\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.3826 - acc: 0.8550\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3808 - acc: 0.8779\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.3798 - acc: 0.8702\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.3782 - acc: 0.8626\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3762 - acc: 0.8702\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3753 - acc: 0.8702\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5e05c29-c9fa-4dac-fd13-ba29c13c7d1b",
        "id": "kQLgWQtS3DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "30cfd131-b234-466d-b914-b989b79198c6",
        "id": "_c5TGT4H3DhR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2647058823529412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMbKL5Yw3DhZ"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viwAabDN6yW9",
        "colab_type": "text"
      },
      "source": [
        "#Using pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nicqs7CG65iH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f0f18ce-0b5f-4588-9153-c9069ec7f14a"
      },
      "source": [
        "!pip install tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 56.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Collecting scipy==1.4.1; python_version >= \"3\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/29/162476fd44203116e7980cfbd9352eef9db37c49445d1fec35509022f6aa/scipy-1.4.1-cp36-cp36m-manylinux1_x86_64.whl (26.1MB)\n",
            "\u001b[K     |████████████████████████████████| 26.1MB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (45.1.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: -ensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: -ensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: wrangle 0.6.7 has requirement scipy==1.2, but you'll have scipy 1.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, scipy, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: scipy 1.2.0\n",
            "    Uninstalling scipy-1.2.0:\n",
            "      Successfully uninstalled scipy-1.2.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "Successfully installed scipy-1.4.1 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scipy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tPhecjM6xuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.wrappers.scikit_learn import KerasRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gihe2no0B6po",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4s2z0QPE8e5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhmQysgZ7R5J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu'))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.005, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aevQ3mxf63mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# wrap the model using the function you created\n",
        "clf = KerasRegressor(build_fn=build_model, epochs=1000, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKhggrQs7lx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "steps = [('scaler', StandardScaler()), ('red_dim', PCA()), ('clf', clf)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwxH7hpP73EG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline(steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fhCwXF0-juS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df3f4b6f-4e62-4599-bcf8-824dd25bf3a6"
      },
      "source": [
        "scores = cross_val_score(pipeline, train_data, one_hot_train_labels, cv=3, scoring='accuracy')"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "87/87 [==============================] - 0s 532us/step - loss: 1.6561 - accuracy: 0.3333\n",
            "Epoch 2/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 1.4802 - accuracy: 0.3678\n",
            "Epoch 3/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.3504 - accuracy: 0.3908\n",
            "Epoch 4/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 1.2682 - accuracy: 0.4023\n",
            "Epoch 5/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 1.2160 - accuracy: 0.4253\n",
            "Epoch 6/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 1.1765 - accuracy: 0.4943\n",
            "Epoch 7/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.1436 - accuracy: 0.4943\n",
            "Epoch 8/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 1.1167 - accuracy: 0.5057\n",
            "Epoch 9/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 1.0902 - accuracy: 0.5172\n",
            "Epoch 10/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 1.0699 - accuracy: 0.5402\n",
            "Epoch 11/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 1.0477 - accuracy: 0.5402\n",
            "Epoch 12/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.0322 - accuracy: 0.5517\n",
            "Epoch 13/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 1.0157 - accuracy: 0.5402\n",
            "Epoch 14/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.9988 - accuracy: 0.5517\n",
            "Epoch 15/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.9861 - accuracy: 0.5862\n",
            "Epoch 16/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9729 - accuracy: 0.6092\n",
            "Epoch 17/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.9612 - accuracy: 0.6667\n",
            "Epoch 18/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9522 - accuracy: 0.6552\n",
            "Epoch 19/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.9407 - accuracy: 0.6552\n",
            "Epoch 20/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9316 - accuracy: 0.6552\n",
            "Epoch 21/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.9227 - accuracy: 0.6552\n",
            "Epoch 22/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.9148 - accuracy: 0.6552\n",
            "Epoch 23/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.9066 - accuracy: 0.6552\n",
            "Epoch 24/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.9011 - accuracy: 0.6667\n",
            "Epoch 25/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.8911 - accuracy: 0.6782\n",
            "Epoch 26/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.8861 - accuracy: 0.6897\n",
            "Epoch 27/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.8789 - accuracy: 0.6782\n",
            "Epoch 28/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.8714 - accuracy: 0.6667\n",
            "Epoch 29/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.8661 - accuracy: 0.6782\n",
            "Epoch 30/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.8593 - accuracy: 0.6897\n",
            "Epoch 31/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.8531 - accuracy: 0.6897\n",
            "Epoch 32/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.8477 - accuracy: 0.6897\n",
            "Epoch 33/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.8425 - accuracy: 0.6782\n",
            "Epoch 34/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.8376 - accuracy: 0.6897\n",
            "Epoch 35/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.8312 - accuracy: 0.6782\n",
            "Epoch 36/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.8270 - accuracy: 0.6897\n",
            "Epoch 37/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.8214 - accuracy: 0.6897\n",
            "Epoch 38/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.8159 - accuracy: 0.6897\n",
            "Epoch 39/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.8111 - accuracy: 0.7126\n",
            "Epoch 40/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.8064 - accuracy: 0.7126\n",
            "Epoch 41/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.8022 - accuracy: 0.7126\n",
            "Epoch 42/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.7976 - accuracy: 0.7011\n",
            "Epoch 43/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.7952 - accuracy: 0.7241\n",
            "Epoch 44/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.7892 - accuracy: 0.7241\n",
            "Epoch 45/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.7854 - accuracy: 0.7241\n",
            "Epoch 46/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.7814 - accuracy: 0.7241\n",
            "Epoch 47/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.7781 - accuracy: 0.7241\n",
            "Epoch 48/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.7744 - accuracy: 0.7241\n",
            "Epoch 49/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.7713 - accuracy: 0.7241\n",
            "Epoch 50/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.7696 - accuracy: 0.7126\n",
            "Epoch 51/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7646 - accuracy: 0.7241\n",
            "Epoch 52/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7620 - accuracy: 0.7241\n",
            "Epoch 53/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.7577 - accuracy: 0.7241\n",
            "Epoch 54/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.7547 - accuracy: 0.7241\n",
            "Epoch 55/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7513 - accuracy: 0.7241\n",
            "Epoch 56/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.7492 - accuracy: 0.7241\n",
            "Epoch 57/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.7459 - accuracy: 0.7241\n",
            "Epoch 58/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.7438 - accuracy: 0.7241\n",
            "Epoch 59/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.7406 - accuracy: 0.7241\n",
            "Epoch 60/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.7374 - accuracy: 0.7241\n",
            "Epoch 61/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.7357 - accuracy: 0.7241\n",
            "Epoch 62/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7340 - accuracy: 0.7241\n",
            "Epoch 63/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.7305 - accuracy: 0.7241\n",
            "Epoch 64/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.7282 - accuracy: 0.7241\n",
            "Epoch 65/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.7266 - accuracy: 0.7241\n",
            "Epoch 66/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.7233 - accuracy: 0.7241\n",
            "Epoch 67/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7208 - accuracy: 0.7241\n",
            "Epoch 68/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.7186 - accuracy: 0.7241\n",
            "Epoch 69/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7159 - accuracy: 0.7241\n",
            "Epoch 70/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.7137 - accuracy: 0.7241\n",
            "Epoch 71/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.7120 - accuracy: 0.7241\n",
            "Epoch 72/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.7101 - accuracy: 0.7241\n",
            "Epoch 73/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.7085 - accuracy: 0.7241\n",
            "Epoch 74/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.7060 - accuracy: 0.7241\n",
            "Epoch 75/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.7039 - accuracy: 0.7241\n",
            "Epoch 76/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.7028 - accuracy: 0.7241\n",
            "Epoch 77/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.7007 - accuracy: 0.7126\n",
            "Epoch 78/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.6977 - accuracy: 0.7126\n",
            "Epoch 79/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6961 - accuracy: 0.7126\n",
            "Epoch 80/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6948 - accuracy: 0.7126\n",
            "Epoch 81/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6933 - accuracy: 0.7126\n",
            "Epoch 82/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6920 - accuracy: 0.7126\n",
            "Epoch 83/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6911 - accuracy: 0.7126\n",
            "Epoch 84/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6883 - accuracy: 0.7126\n",
            "Epoch 85/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.6875 - accuracy: 0.7126\n",
            "Epoch 86/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6852 - accuracy: 0.7126\n",
            "Epoch 87/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.6841 - accuracy: 0.7126\n",
            "Epoch 88/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.6828 - accuracy: 0.7126\n",
            "Epoch 89/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.6795 - accuracy: 0.7126\n",
            "Epoch 90/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.6791 - accuracy: 0.7126\n",
            "Epoch 91/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.6767 - accuracy: 0.7126\n",
            "Epoch 92/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6753 - accuracy: 0.7126\n",
            "Epoch 93/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6743 - accuracy: 0.7126\n",
            "Epoch 94/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6726 - accuracy: 0.7126\n",
            "Epoch 95/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6710 - accuracy: 0.7126\n",
            "Epoch 96/1000\n",
            "87/87 [==============================] - 0s 109us/step - loss: 0.6699 - accuracy: 0.7241\n",
            "Epoch 97/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6684 - accuracy: 0.7241\n",
            "Epoch 98/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6668 - accuracy: 0.7241\n",
            "Epoch 99/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6661 - accuracy: 0.7241\n",
            "Epoch 100/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6647 - accuracy: 0.7241\n",
            "Epoch 101/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6628 - accuracy: 0.7241\n",
            "Epoch 102/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6620 - accuracy: 0.7241\n",
            "Epoch 103/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6609 - accuracy: 0.7241\n",
            "Epoch 104/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.6592 - accuracy: 0.7241\n",
            "Epoch 105/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6590 - accuracy: 0.7241\n",
            "Epoch 106/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.6564 - accuracy: 0.7241\n",
            "Epoch 107/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.6547 - accuracy: 0.7356\n",
            "Epoch 108/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.6531 - accuracy: 0.7356\n",
            "Epoch 109/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6521 - accuracy: 0.7356\n",
            "Epoch 110/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6507 - accuracy: 0.7356\n",
            "Epoch 111/1000\n",
            "87/87 [==============================] - 0s 62us/step - loss: 0.6505 - accuracy: 0.7356\n",
            "Epoch 112/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.6481 - accuracy: 0.7356\n",
            "Epoch 113/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6479 - accuracy: 0.7241\n",
            "Epoch 114/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.6461 - accuracy: 0.7356\n",
            "Epoch 115/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6444 - accuracy: 0.7356\n",
            "Epoch 116/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.6444 - accuracy: 0.7241\n",
            "Epoch 117/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.6432 - accuracy: 0.7241\n",
            "Epoch 118/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6409 - accuracy: 0.7356\n",
            "Epoch 119/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.6400 - accuracy: 0.7356\n",
            "Epoch 120/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.6402 - accuracy: 0.7356\n",
            "Epoch 121/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.6375 - accuracy: 0.7356\n",
            "Epoch 122/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6361 - accuracy: 0.7356\n",
            "Epoch 123/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.6359 - accuracy: 0.7471\n",
            "Epoch 124/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.6343 - accuracy: 0.7471\n",
            "Epoch 125/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6342 - accuracy: 0.7356\n",
            "Epoch 126/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6318 - accuracy: 0.7356\n",
            "Epoch 127/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6318 - accuracy: 0.7356\n",
            "Epoch 128/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.6298 - accuracy: 0.7356\n",
            "Epoch 129/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6288 - accuracy: 0.7356\n",
            "Epoch 130/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6282 - accuracy: 0.7356\n",
            "Epoch 131/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6269 - accuracy: 0.7241\n",
            "Epoch 132/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.6256 - accuracy: 0.7356\n",
            "Epoch 133/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6254 - accuracy: 0.7356\n",
            "Epoch 134/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.6232 - accuracy: 0.7356\n",
            "Epoch 135/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.6241 - accuracy: 0.7471\n",
            "Epoch 136/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.6217 - accuracy: 0.7356\n",
            "Epoch 137/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.6214 - accuracy: 0.7241\n",
            "Epoch 138/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.6205 - accuracy: 0.7241\n",
            "Epoch 139/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6189 - accuracy: 0.7241\n",
            "Epoch 140/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.6180 - accuracy: 0.7356\n",
            "Epoch 141/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.6171 - accuracy: 0.7356\n",
            "Epoch 142/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.6159 - accuracy: 0.7356\n",
            "Epoch 143/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.6160 - accuracy: 0.7241\n",
            "Epoch 144/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6143 - accuracy: 0.7241\n",
            "Epoch 145/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.6142 - accuracy: 0.7241\n",
            "Epoch 146/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.6122 - accuracy: 0.7241\n",
            "Epoch 147/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.6117 - accuracy: 0.7241\n",
            "Epoch 148/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.6107 - accuracy: 0.7241\n",
            "Epoch 149/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.6096 - accuracy: 0.7241\n",
            "Epoch 150/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.6094 - accuracy: 0.7356\n",
            "Epoch 151/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.6087 - accuracy: 0.7356\n",
            "Epoch 152/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.6073 - accuracy: 0.7241\n",
            "Epoch 153/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6062 - accuracy: 0.7241\n",
            "Epoch 154/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6056 - accuracy: 0.7356\n",
            "Epoch 155/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.6045 - accuracy: 0.7356\n",
            "Epoch 156/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.6036 - accuracy: 0.7471\n",
            "Epoch 157/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.6048 - accuracy: 0.7241\n",
            "Epoch 158/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6018 - accuracy: 0.7241\n",
            "Epoch 159/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.6011 - accuracy: 0.7356\n",
            "Epoch 160/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5999 - accuracy: 0.7241\n",
            "Epoch 161/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.6003 - accuracy: 0.7241\n",
            "Epoch 162/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.5987 - accuracy: 0.7241\n",
            "Epoch 163/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5985 - accuracy: 0.7241\n",
            "Epoch 164/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5975 - accuracy: 0.7356\n",
            "Epoch 165/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.5972 - accuracy: 0.7356\n",
            "Epoch 166/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5954 - accuracy: 0.7241\n",
            "Epoch 167/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5947 - accuracy: 0.7356\n",
            "Epoch 168/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5943 - accuracy: 0.7356\n",
            "Epoch 169/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5931 - accuracy: 0.7356\n",
            "Epoch 170/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5922 - accuracy: 0.7356\n",
            "Epoch 171/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5909 - accuracy: 0.7356\n",
            "Epoch 172/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5906 - accuracy: 0.7241\n",
            "Epoch 173/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5896 - accuracy: 0.7241\n",
            "Epoch 174/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5886 - accuracy: 0.7356\n",
            "Epoch 175/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5884 - accuracy: 0.7241\n",
            "Epoch 176/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5865 - accuracy: 0.7471\n",
            "Epoch 177/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5874 - accuracy: 0.7471\n",
            "Epoch 178/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5847 - accuracy: 0.7356\n",
            "Epoch 179/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.5846 - accuracy: 0.7241\n",
            "Epoch 180/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5854 - accuracy: 0.7356\n",
            "Epoch 181/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5829 - accuracy: 0.7241\n",
            "Epoch 182/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5838 - accuracy: 0.7241\n",
            "Epoch 183/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.5810 - accuracy: 0.7241\n",
            "Epoch 184/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.5810 - accuracy: 0.7241\n",
            "Epoch 185/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5791 - accuracy: 0.7241\n",
            "Epoch 186/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5784 - accuracy: 0.7356\n",
            "Epoch 187/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5782 - accuracy: 0.7356\n",
            "Epoch 188/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.5770 - accuracy: 0.7356\n",
            "Epoch 189/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5767 - accuracy: 0.7241\n",
            "Epoch 190/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5772 - accuracy: 0.7241\n",
            "Epoch 191/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5750 - accuracy: 0.7471\n",
            "Epoch 192/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5748 - accuracy: 0.7586\n",
            "Epoch 193/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5752 - accuracy: 0.7241\n",
            "Epoch 194/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5723 - accuracy: 0.7356\n",
            "Epoch 195/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5724 - accuracy: 0.7586\n",
            "Epoch 196/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5707 - accuracy: 0.7586\n",
            "Epoch 197/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5706 - accuracy: 0.7471\n",
            "Epoch 198/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5697 - accuracy: 0.7586\n",
            "Epoch 199/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5691 - accuracy: 0.7471\n",
            "Epoch 200/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.5701 - accuracy: 0.7356\n",
            "Epoch 201/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5686 - accuracy: 0.7356\n",
            "Epoch 202/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.5662 - accuracy: 0.7701\n",
            "Epoch 203/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5670 - accuracy: 0.7471\n",
            "Epoch 204/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.5665 - accuracy: 0.7701\n",
            "Epoch 205/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.5647 - accuracy: 0.7701\n",
            "Epoch 206/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5636 - accuracy: 0.7701\n",
            "Epoch 207/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5633 - accuracy: 0.7701\n",
            "Epoch 208/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5623 - accuracy: 0.7701\n",
            "Epoch 209/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5617 - accuracy: 0.7701\n",
            "Epoch 210/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5611 - accuracy: 0.7701\n",
            "Epoch 211/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5613 - accuracy: 0.7701\n",
            "Epoch 212/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5603 - accuracy: 0.7701\n",
            "Epoch 213/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.5594 - accuracy: 0.7471\n",
            "Epoch 214/1000\n",
            "87/87 [==============================] - 0s 61us/step - loss: 0.5583 - accuracy: 0.7586\n",
            "Epoch 215/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5583 - accuracy: 0.7586\n",
            "Epoch 216/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5578 - accuracy: 0.7701\n",
            "Epoch 217/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.5590 - accuracy: 0.7471\n",
            "Epoch 218/1000\n",
            "87/87 [==============================] - 0s 104us/step - loss: 0.5554 - accuracy: 0.7701\n",
            "Epoch 219/1000\n",
            "87/87 [==============================] - 0s 129us/step - loss: 0.5546 - accuracy: 0.7701\n",
            "Epoch 220/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.5547 - accuracy: 0.7586\n",
            "Epoch 221/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5531 - accuracy: 0.7586\n",
            "Epoch 222/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5524 - accuracy: 0.7701\n",
            "Epoch 223/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5522 - accuracy: 0.7701\n",
            "Epoch 224/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5512 - accuracy: 0.7701\n",
            "Epoch 225/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5508 - accuracy: 0.7701\n",
            "Epoch 226/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5491 - accuracy: 0.7701\n",
            "Epoch 227/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5491 - accuracy: 0.7701\n",
            "Epoch 228/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5482 - accuracy: 0.7701\n",
            "Epoch 229/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5467 - accuracy: 0.7701\n",
            "Epoch 230/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.5466 - accuracy: 0.7701\n",
            "Epoch 231/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5458 - accuracy: 0.7701\n",
            "Epoch 232/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5469 - accuracy: 0.7586\n",
            "Epoch 233/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5437 - accuracy: 0.7701\n",
            "Epoch 234/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5442 - accuracy: 0.7701\n",
            "Epoch 235/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.5421 - accuracy: 0.7701\n",
            "Epoch 236/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5417 - accuracy: 0.7701\n",
            "Epoch 237/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5411 - accuracy: 0.7701\n",
            "Epoch 238/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.5402 - accuracy: 0.7701\n",
            "Epoch 239/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5396 - accuracy: 0.7701\n",
            "Epoch 240/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5393 - accuracy: 0.7701\n",
            "Epoch 241/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5378 - accuracy: 0.7701\n",
            "Epoch 242/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5375 - accuracy: 0.7586\n",
            "Epoch 243/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5368 - accuracy: 0.7701\n",
            "Epoch 244/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5382 - accuracy: 0.7586\n",
            "Epoch 245/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5366 - accuracy: 0.7701\n",
            "Epoch 246/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5350 - accuracy: 0.7701\n",
            "Epoch 247/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5335 - accuracy: 0.7701\n",
            "Epoch 248/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5327 - accuracy: 0.7701\n",
            "Epoch 249/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5329 - accuracy: 0.7586\n",
            "Epoch 250/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5330 - accuracy: 0.7701\n",
            "Epoch 251/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5318 - accuracy: 0.7586\n",
            "Epoch 252/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5303 - accuracy: 0.7701\n",
            "Epoch 253/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5293 - accuracy: 0.7701\n",
            "Epoch 254/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5288 - accuracy: 0.7701\n",
            "Epoch 255/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5279 - accuracy: 0.7701\n",
            "Epoch 256/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5285 - accuracy: 0.7701\n",
            "Epoch 257/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5283 - accuracy: 0.7701\n",
            "Epoch 258/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.5272 - accuracy: 0.7701\n",
            "Epoch 259/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5257 - accuracy: 0.7701\n",
            "Epoch 260/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5251 - accuracy: 0.7701\n",
            "Epoch 261/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.5263 - accuracy: 0.7701\n",
            "Epoch 262/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.5232 - accuracy: 0.7701\n",
            "Epoch 263/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5236 - accuracy: 0.7701\n",
            "Epoch 264/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5226 - accuracy: 0.7701\n",
            "Epoch 265/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5217 - accuracy: 0.7586\n",
            "Epoch 266/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5218 - accuracy: 0.7701\n",
            "Epoch 267/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.5197 - accuracy: 0.7586\n",
            "Epoch 268/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5199 - accuracy: 0.7586\n",
            "Epoch 269/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5190 - accuracy: 0.7701\n",
            "Epoch 270/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5188 - accuracy: 0.7701\n",
            "Epoch 271/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.5173 - accuracy: 0.7701\n",
            "Epoch 272/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5176 - accuracy: 0.7586\n",
            "Epoch 273/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.5159 - accuracy: 0.7586\n",
            "Epoch 274/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.5160 - accuracy: 0.7586\n",
            "Epoch 275/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.5152 - accuracy: 0.7586\n",
            "Epoch 276/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.5167 - accuracy: 0.7701\n",
            "Epoch 277/1000\n",
            "87/87 [==============================] - 0s 187us/step - loss: 0.5138 - accuracy: 0.7701\n",
            "Epoch 278/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5126 - accuracy: 0.7701\n",
            "Epoch 279/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5125 - accuracy: 0.7701\n",
            "Epoch 280/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.5121 - accuracy: 0.7701\n",
            "Epoch 281/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.5110 - accuracy: 0.7701\n",
            "Epoch 282/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.5115 - accuracy: 0.7701\n",
            "Epoch 283/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5104 - accuracy: 0.7701\n",
            "Epoch 284/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5091 - accuracy: 0.7701\n",
            "Epoch 285/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.5092 - accuracy: 0.7701\n",
            "Epoch 286/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5086 - accuracy: 0.7701\n",
            "Epoch 287/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.5087 - accuracy: 0.7701\n",
            "Epoch 288/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5079 - accuracy: 0.7701\n",
            "Epoch 289/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5056 - accuracy: 0.7701\n",
            "Epoch 290/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5080 - accuracy: 0.7701\n",
            "Epoch 291/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.5042 - accuracy: 0.7701\n",
            "Epoch 292/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.5039 - accuracy: 0.7701\n",
            "Epoch 293/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.5045 - accuracy: 0.7701\n",
            "Epoch 294/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5036 - accuracy: 0.7701\n",
            "Epoch 295/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.5024 - accuracy: 0.7701\n",
            "Epoch 296/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.5013 - accuracy: 0.7701\n",
            "Epoch 297/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.5012 - accuracy: 0.7701\n",
            "Epoch 298/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.5003 - accuracy: 0.7701\n",
            "Epoch 299/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4994 - accuracy: 0.7701\n",
            "Epoch 300/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4992 - accuracy: 0.7701\n",
            "Epoch 301/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4986 - accuracy: 0.7701\n",
            "Epoch 302/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4992 - accuracy: 0.7701\n",
            "Epoch 303/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4976 - accuracy: 0.7701\n",
            "Epoch 304/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4971 - accuracy: 0.7816\n",
            "Epoch 305/1000\n",
            "87/87 [==============================] - 0s 143us/step - loss: 0.4961 - accuracy: 0.7816\n",
            "Epoch 306/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4965 - accuracy: 0.7816\n",
            "Epoch 307/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4950 - accuracy: 0.7816\n",
            "Epoch 308/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4942 - accuracy: 0.7816\n",
            "Epoch 309/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4939 - accuracy: 0.7816\n",
            "Epoch 310/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4941 - accuracy: 0.7816\n",
            "Epoch 311/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4932 - accuracy: 0.7816\n",
            "Epoch 312/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4916 - accuracy: 0.7816\n",
            "Epoch 313/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4912 - accuracy: 0.7816\n",
            "Epoch 314/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4922 - accuracy: 0.7816\n",
            "Epoch 315/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4907 - accuracy: 0.7931\n",
            "Epoch 316/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4896 - accuracy: 0.7816\n",
            "Epoch 317/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4889 - accuracy: 0.7816\n",
            "Epoch 318/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4888 - accuracy: 0.7816\n",
            "Epoch 319/1000\n",
            "87/87 [==============================] - 0s 65us/step - loss: 0.4878 - accuracy: 0.7931\n",
            "Epoch 320/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.4886 - accuracy: 0.7931\n",
            "Epoch 321/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.4865 - accuracy: 0.7816\n",
            "Epoch 322/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.4863 - accuracy: 0.7816\n",
            "Epoch 323/1000\n",
            "87/87 [==============================] - 0s 177us/step - loss: 0.4861 - accuracy: 0.7816\n",
            "Epoch 324/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4850 - accuracy: 0.7816\n",
            "Epoch 325/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4840 - accuracy: 0.7931\n",
            "Epoch 326/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4838 - accuracy: 0.7816\n",
            "Epoch 327/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4834 - accuracy: 0.7931\n",
            "Epoch 328/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.4828 - accuracy: 0.7931\n",
            "Epoch 329/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4826 - accuracy: 0.7931\n",
            "Epoch 330/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4832 - accuracy: 0.7931\n",
            "Epoch 331/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4811 - accuracy: 0.7931\n",
            "Epoch 332/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4804 - accuracy: 0.7931\n",
            "Epoch 333/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4796 - accuracy: 0.7931\n",
            "Epoch 334/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4788 - accuracy: 0.7931\n",
            "Epoch 335/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4787 - accuracy: 0.7931\n",
            "Epoch 336/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4792 - accuracy: 0.7931\n",
            "Epoch 337/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4770 - accuracy: 0.7931\n",
            "Epoch 338/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4770 - accuracy: 0.7931\n",
            "Epoch 339/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4767 - accuracy: 0.7931\n",
            "Epoch 340/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4755 - accuracy: 0.7931\n",
            "Epoch 341/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4754 - accuracy: 0.7931\n",
            "Epoch 342/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4747 - accuracy: 0.7931\n",
            "Epoch 343/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4770 - accuracy: 0.7931\n",
            "Epoch 344/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4731 - accuracy: 0.7931\n",
            "Epoch 345/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4733 - accuracy: 0.7931\n",
            "Epoch 346/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.4730 - accuracy: 0.7931\n",
            "Epoch 347/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4719 - accuracy: 0.7931\n",
            "Epoch 348/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4717 - accuracy: 0.7931\n",
            "Epoch 349/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.4705 - accuracy: 0.7931\n",
            "Epoch 350/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4704 - accuracy: 0.7931\n",
            "Epoch 351/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4697 - accuracy: 0.7931\n",
            "Epoch 352/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4694 - accuracy: 0.7931\n",
            "Epoch 353/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4693 - accuracy: 0.7931\n",
            "Epoch 354/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4696 - accuracy: 0.8046\n",
            "Epoch 355/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4679 - accuracy: 0.8046\n",
            "Epoch 356/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4670 - accuracy: 0.8046\n",
            "Epoch 357/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.4669 - accuracy: 0.8046\n",
            "Epoch 358/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4666 - accuracy: 0.8046\n",
            "Epoch 359/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4650 - accuracy: 0.8046\n",
            "Epoch 360/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4660 - accuracy: 0.8046\n",
            "Epoch 361/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4657 - accuracy: 0.8046\n",
            "Epoch 362/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4661 - accuracy: 0.8046\n",
            "Epoch 363/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4634 - accuracy: 0.8046\n",
            "Epoch 364/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4629 - accuracy: 0.8046\n",
            "Epoch 365/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4626 - accuracy: 0.8046\n",
            "Epoch 366/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4630 - accuracy: 0.8046\n",
            "Epoch 367/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4619 - accuracy: 0.8046\n",
            "Epoch 368/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4609 - accuracy: 0.8046\n",
            "Epoch 369/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4603 - accuracy: 0.8046\n",
            "Epoch 370/1000\n",
            "87/87 [==============================] - 0s 60us/step - loss: 0.4596 - accuracy: 0.8046\n",
            "Epoch 371/1000\n",
            "87/87 [==============================] - 0s 43us/step - loss: 0.4592 - accuracy: 0.8046\n",
            "Epoch 372/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.4583 - accuracy: 0.8046\n",
            "Epoch 373/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.4593 - accuracy: 0.8046\n",
            "Epoch 374/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4578 - accuracy: 0.8046\n",
            "Epoch 375/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4576 - accuracy: 0.8046\n",
            "Epoch 376/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4567 - accuracy: 0.8046\n",
            "Epoch 377/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4581 - accuracy: 0.8046\n",
            "Epoch 378/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.4579 - accuracy: 0.8046\n",
            "Epoch 379/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4547 - accuracy: 0.8046\n",
            "Epoch 380/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4555 - accuracy: 0.8046\n",
            "Epoch 381/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4553 - accuracy: 0.8046\n",
            "Epoch 382/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4542 - accuracy: 0.8046\n",
            "Epoch 383/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4540 - accuracy: 0.8046\n",
            "Epoch 384/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4523 - accuracy: 0.8046\n",
            "Epoch 385/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4518 - accuracy: 0.8046\n",
            "Epoch 386/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4518 - accuracy: 0.8046\n",
            "Epoch 387/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4510 - accuracy: 0.8046\n",
            "Epoch 388/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4502 - accuracy: 0.8046\n",
            "Epoch 389/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.4498 - accuracy: 0.8046\n",
            "Epoch 390/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.4495 - accuracy: 0.8046\n",
            "Epoch 391/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.4487 - accuracy: 0.8046\n",
            "Epoch 392/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4511 - accuracy: 0.8046\n",
            "Epoch 393/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4497 - accuracy: 0.8046\n",
            "Epoch 394/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4484 - accuracy: 0.8046\n",
            "Epoch 395/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.4472 - accuracy: 0.8046\n",
            "Epoch 396/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4462 - accuracy: 0.8046\n",
            "Epoch 397/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4466 - accuracy: 0.8046\n",
            "Epoch 398/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4454 - accuracy: 0.8046\n",
            "Epoch 399/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4445 - accuracy: 0.8046\n",
            "Epoch 400/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4463 - accuracy: 0.8046\n",
            "Epoch 401/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4445 - accuracy: 0.8046\n",
            "Epoch 402/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4450 - accuracy: 0.8046\n",
            "Epoch 403/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4427 - accuracy: 0.8046\n",
            "Epoch 404/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4421 - accuracy: 0.8046\n",
            "Epoch 405/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4417 - accuracy: 0.8046\n",
            "Epoch 406/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4407 - accuracy: 0.8046\n",
            "Epoch 407/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4406 - accuracy: 0.8046\n",
            "Epoch 408/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4403 - accuracy: 0.8046\n",
            "Epoch 409/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4395 - accuracy: 0.8046\n",
            "Epoch 410/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.4408 - accuracy: 0.7931\n",
            "Epoch 411/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4386 - accuracy: 0.7931\n",
            "Epoch 412/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4383 - accuracy: 0.7931\n",
            "Epoch 413/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4388 - accuracy: 0.8046\n",
            "Epoch 414/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4380 - accuracy: 0.7931\n",
            "Epoch 415/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4365 - accuracy: 0.7931\n",
            "Epoch 416/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4365 - accuracy: 0.7931\n",
            "Epoch 417/1000\n",
            "87/87 [==============================] - 0s 111us/step - loss: 0.4371 - accuracy: 0.7931\n",
            "Epoch 418/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.4354 - accuracy: 0.7931\n",
            "Epoch 419/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4354 - accuracy: 0.7931\n",
            "Epoch 420/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4354 - accuracy: 0.7931\n",
            "Epoch 421/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4338 - accuracy: 0.7931\n",
            "Epoch 422/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4357 - accuracy: 0.7931\n",
            "Epoch 423/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.4328 - accuracy: 0.7931\n",
            "Epoch 424/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4320 - accuracy: 0.7931\n",
            "Epoch 425/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4320 - accuracy: 0.7931\n",
            "Epoch 426/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4314 - accuracy: 0.7931\n",
            "Epoch 427/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.4310 - accuracy: 0.7931\n",
            "Epoch 428/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4310 - accuracy: 0.7931\n",
            "Epoch 429/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.4315 - accuracy: 0.7931\n",
            "Epoch 430/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4298 - accuracy: 0.7931\n",
            "Epoch 431/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4293 - accuracy: 0.7931\n",
            "Epoch 432/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4286 - accuracy: 0.7931\n",
            "Epoch 433/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4280 - accuracy: 0.7931\n",
            "Epoch 434/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.4274 - accuracy: 0.7931\n",
            "Epoch 435/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4302 - accuracy: 0.7931\n",
            "Epoch 436/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.4261 - accuracy: 0.7931\n",
            "Epoch 437/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4260 - accuracy: 0.7931\n",
            "Epoch 438/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4270 - accuracy: 0.7931\n",
            "Epoch 439/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4259 - accuracy: 0.7931\n",
            "Epoch 440/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.4250 - accuracy: 0.7931\n",
            "Epoch 441/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.4251 - accuracy: 0.7931\n",
            "Epoch 442/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4257 - accuracy: 0.7931\n",
            "Epoch 443/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.4233 - accuracy: 0.7931\n",
            "Epoch 444/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.4240 - accuracy: 0.7931\n",
            "Epoch 445/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4226 - accuracy: 0.7931\n",
            "Epoch 446/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4228 - accuracy: 0.7931\n",
            "Epoch 447/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4221 - accuracy: 0.7931\n",
            "Epoch 448/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4206 - accuracy: 0.7931\n",
            "Epoch 449/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4221 - accuracy: 0.7931\n",
            "Epoch 450/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4197 - accuracy: 0.7931\n",
            "Epoch 451/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.4214 - accuracy: 0.7931\n",
            "Epoch 452/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4191 - accuracy: 0.7931\n",
            "Epoch 453/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.4186 - accuracy: 0.7931\n",
            "Epoch 454/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4176 - accuracy: 0.7931\n",
            "Epoch 455/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4190 - accuracy: 0.8046\n",
            "Epoch 456/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4177 - accuracy: 0.7931\n",
            "Epoch 457/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4175 - accuracy: 0.7931\n",
            "Epoch 458/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4160 - accuracy: 0.7931\n",
            "Epoch 459/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.4167 - accuracy: 0.7931\n",
            "Epoch 460/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4150 - accuracy: 0.7931\n",
            "Epoch 461/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4147 - accuracy: 0.7931\n",
            "Epoch 462/1000\n",
            "87/87 [==============================] - 0s 188us/step - loss: 0.4139 - accuracy: 0.7931\n",
            "Epoch 463/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.4182 - accuracy: 0.7931\n",
            "Epoch 464/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4130 - accuracy: 0.7931\n",
            "Epoch 465/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4131 - accuracy: 0.8046\n",
            "Epoch 466/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.4126 - accuracy: 0.8046\n",
            "Epoch 467/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.4127 - accuracy: 0.8046\n",
            "Epoch 468/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4108 - accuracy: 0.8046\n",
            "Epoch 469/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.4123 - accuracy: 0.7931\n",
            "Epoch 470/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.4094 - accuracy: 0.7931\n",
            "Epoch 471/1000\n",
            "87/87 [==============================] - 0s 115us/step - loss: 0.4100 - accuracy: 0.8046\n",
            "Epoch 472/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4107 - accuracy: 0.8046\n",
            "Epoch 473/1000\n",
            "87/87 [==============================] - 0s 105us/step - loss: 0.4089 - accuracy: 0.7931\n",
            "Epoch 474/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.4125 - accuracy: 0.7931\n",
            "Epoch 475/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4075 - accuracy: 0.8046\n",
            "Epoch 476/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.4069 - accuracy: 0.8046\n",
            "Epoch 477/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4071 - accuracy: 0.8046\n",
            "Epoch 478/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.4064 - accuracy: 0.8046\n",
            "Epoch 479/1000\n",
            "87/87 [==============================] - 0s 105us/step - loss: 0.4068 - accuracy: 0.8046\n",
            "Epoch 480/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.4053 - accuracy: 0.8046\n",
            "Epoch 481/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.4047 - accuracy: 0.8046\n",
            "Epoch 482/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.4057 - accuracy: 0.8046\n",
            "Epoch 483/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.4040 - accuracy: 0.8046\n",
            "Epoch 484/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.4038 - accuracy: 0.8046\n",
            "Epoch 485/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.4043 - accuracy: 0.8046\n",
            "Epoch 486/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4028 - accuracy: 0.8161\n",
            "Epoch 487/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.4019 - accuracy: 0.8161\n",
            "Epoch 488/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.4013 - accuracy: 0.8046\n",
            "Epoch 489/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.4018 - accuracy: 0.8046\n",
            "Epoch 490/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.4010 - accuracy: 0.8046\n",
            "Epoch 491/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3999 - accuracy: 0.8046\n",
            "Epoch 492/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.4001 - accuracy: 0.8046\n",
            "Epoch 493/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.3990 - accuracy: 0.8046\n",
            "Epoch 494/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.3987 - accuracy: 0.8046\n",
            "Epoch 495/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.3982 - accuracy: 0.8046\n",
            "Epoch 496/1000\n",
            "87/87 [==============================] - 0s 63us/step - loss: 0.3983 - accuracy: 0.8046\n",
            "Epoch 497/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3983 - accuracy: 0.8046\n",
            "Epoch 498/1000\n",
            "87/87 [==============================] - 0s 116us/step - loss: 0.3972 - accuracy: 0.8161\n",
            "Epoch 499/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3966 - accuracy: 0.8046\n",
            "Epoch 500/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3964 - accuracy: 0.8046\n",
            "Epoch 501/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3959 - accuracy: 0.8046\n",
            "Epoch 502/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.3953 - accuracy: 0.8046\n",
            "Epoch 503/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3975 - accuracy: 0.8046\n",
            "Epoch 504/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.3988 - accuracy: 0.8046\n",
            "Epoch 505/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3936 - accuracy: 0.8046\n",
            "Epoch 506/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.3944 - accuracy: 0.8046\n",
            "Epoch 507/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.3929 - accuracy: 0.8276\n",
            "Epoch 508/1000\n",
            "87/87 [==============================] - 0s 98us/step - loss: 0.3940 - accuracy: 0.8276\n",
            "Epoch 509/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3921 - accuracy: 0.8391\n",
            "Epoch 510/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3917 - accuracy: 0.8046\n",
            "Epoch 511/1000\n",
            "87/87 [==============================] - 0s 104us/step - loss: 0.3917 - accuracy: 0.8161\n",
            "Epoch 512/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3908 - accuracy: 0.8046\n",
            "Epoch 513/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3900 - accuracy: 0.8046\n",
            "Epoch 514/1000\n",
            "87/87 [==============================] - 0s 134us/step - loss: 0.3898 - accuracy: 0.8161\n",
            "Epoch 515/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3899 - accuracy: 0.8046\n",
            "Epoch 516/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3884 - accuracy: 0.8046\n",
            "Epoch 517/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3883 - accuracy: 0.8161\n",
            "Epoch 518/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3888 - accuracy: 0.8161\n",
            "Epoch 519/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3880 - accuracy: 0.8161\n",
            "Epoch 520/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3871 - accuracy: 0.8161\n",
            "Epoch 521/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3887 - accuracy: 0.8161\n",
            "Epoch 522/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3882 - accuracy: 0.8276\n",
            "Epoch 523/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.3876 - accuracy: 0.8276\n",
            "Epoch 524/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3851 - accuracy: 0.8391\n",
            "Epoch 525/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3866 - accuracy: 0.8391\n",
            "Epoch 526/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3844 - accuracy: 0.8276\n",
            "Epoch 527/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3848 - accuracy: 0.8161\n",
            "Epoch 528/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3844 - accuracy: 0.8276\n",
            "Epoch 529/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3835 - accuracy: 0.8276\n",
            "Epoch 530/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3836 - accuracy: 0.8276\n",
            "Epoch 531/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3830 - accuracy: 0.8276\n",
            "Epoch 532/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3818 - accuracy: 0.8276\n",
            "Epoch 533/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3825 - accuracy: 0.8161\n",
            "Epoch 534/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.3818 - accuracy: 0.8391\n",
            "Epoch 535/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3814 - accuracy: 0.8276\n",
            "Epoch 536/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3817 - accuracy: 0.8391\n",
            "Epoch 537/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3812 - accuracy: 0.8276\n",
            "Epoch 538/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3824 - accuracy: 0.8276\n",
            "Epoch 539/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3792 - accuracy: 0.8276\n",
            "Epoch 540/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3790 - accuracy: 0.8276\n",
            "Epoch 541/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3787 - accuracy: 0.8391\n",
            "Epoch 542/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3799 - accuracy: 0.8276\n",
            "Epoch 543/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3772 - accuracy: 0.8276\n",
            "Epoch 544/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3782 - accuracy: 0.8276\n",
            "Epoch 545/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3768 - accuracy: 0.8391\n",
            "Epoch 546/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3764 - accuracy: 0.8391\n",
            "Epoch 547/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3774 - accuracy: 0.8391\n",
            "Epoch 548/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3761 - accuracy: 0.8506\n",
            "Epoch 549/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3766 - accuracy: 0.8506\n",
            "Epoch 550/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3746 - accuracy: 0.8391\n",
            "Epoch 551/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.3765 - accuracy: 0.8391\n",
            "Epoch 552/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3736 - accuracy: 0.8391\n",
            "Epoch 553/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3735 - accuracy: 0.8506\n",
            "Epoch 554/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3750 - accuracy: 0.8391\n",
            "Epoch 555/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3722 - accuracy: 0.8506\n",
            "Epoch 556/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3731 - accuracy: 0.8506\n",
            "Epoch 557/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3722 - accuracy: 0.8506\n",
            "Epoch 558/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3727 - accuracy: 0.8506\n",
            "Epoch 559/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3716 - accuracy: 0.8506\n",
            "Epoch 560/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3705 - accuracy: 0.8506\n",
            "Epoch 561/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3700 - accuracy: 0.8506\n",
            "Epoch 562/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3701 - accuracy: 0.8506\n",
            "Epoch 563/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3689 - accuracy: 0.8506\n",
            "Epoch 564/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3699 - accuracy: 0.8506\n",
            "Epoch 565/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3685 - accuracy: 0.8506\n",
            "Epoch 566/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3682 - accuracy: 0.8506\n",
            "Epoch 567/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3686 - accuracy: 0.8506\n",
            "Epoch 568/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3696 - accuracy: 0.8391\n",
            "Epoch 569/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3676 - accuracy: 0.8506\n",
            "Epoch 570/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3666 - accuracy: 0.8506\n",
            "Epoch 571/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3666 - accuracy: 0.8621\n",
            "Epoch 572/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3653 - accuracy: 0.8621\n",
            "Epoch 573/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3659 - accuracy: 0.8621\n",
            "Epoch 574/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3645 - accuracy: 0.8506\n",
            "Epoch 575/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3644 - accuracy: 0.8506\n",
            "Epoch 576/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3647 - accuracy: 0.8506\n",
            "Epoch 577/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3644 - accuracy: 0.8621\n",
            "Epoch 578/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3644 - accuracy: 0.8621\n",
            "Epoch 579/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3632 - accuracy: 0.8506\n",
            "Epoch 580/1000\n",
            "87/87 [==============================] - 0s 58us/step - loss: 0.3625 - accuracy: 0.8506\n",
            "Epoch 581/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3646 - accuracy: 0.8276\n",
            "Epoch 582/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3619 - accuracy: 0.8506\n",
            "Epoch 583/1000\n",
            "87/87 [==============================] - 0s 99us/step - loss: 0.3612 - accuracy: 0.8506\n",
            "Epoch 584/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3613 - accuracy: 0.8506\n",
            "Epoch 585/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3601 - accuracy: 0.8506\n",
            "Epoch 586/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3599 - accuracy: 0.8506\n",
            "Epoch 587/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3596 - accuracy: 0.8506\n",
            "Epoch 588/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3608 - accuracy: 0.8621\n",
            "Epoch 589/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3589 - accuracy: 0.8621\n",
            "Epoch 590/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3596 - accuracy: 0.8506\n",
            "Epoch 591/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3583 - accuracy: 0.8506\n",
            "Epoch 592/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3574 - accuracy: 0.8621\n",
            "Epoch 593/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3582 - accuracy: 0.8621\n",
            "Epoch 594/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3568 - accuracy: 0.8621\n",
            "Epoch 595/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.3576 - accuracy: 0.8621\n",
            "Epoch 596/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3584 - accuracy: 0.8506\n",
            "Epoch 597/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3552 - accuracy: 0.8506\n",
            "Epoch 598/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3564 - accuracy: 0.8621\n",
            "Epoch 599/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3566 - accuracy: 0.8621\n",
            "Epoch 600/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3543 - accuracy: 0.8621\n",
            "Epoch 601/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3552 - accuracy: 0.8506\n",
            "Epoch 602/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3542 - accuracy: 0.8736\n",
            "Epoch 603/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3539 - accuracy: 0.8736\n",
            "Epoch 604/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3527 - accuracy: 0.8736\n",
            "Epoch 605/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3542 - accuracy: 0.8736\n",
            "Epoch 606/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3527 - accuracy: 0.8621\n",
            "Epoch 607/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3530 - accuracy: 0.8736\n",
            "Epoch 608/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3516 - accuracy: 0.8506\n",
            "Epoch 609/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3513 - accuracy: 0.8621\n",
            "Epoch 610/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3521 - accuracy: 0.8621\n",
            "Epoch 611/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3505 - accuracy: 0.8736\n",
            "Epoch 612/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3501 - accuracy: 0.8736\n",
            "Epoch 613/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3509 - accuracy: 0.8736\n",
            "Epoch 614/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3500 - accuracy: 0.8736\n",
            "Epoch 615/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3490 - accuracy: 0.8736\n",
            "Epoch 616/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3492 - accuracy: 0.8736\n",
            "Epoch 617/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3504 - accuracy: 0.8736\n",
            "Epoch 618/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3480 - accuracy: 0.8736\n",
            "Epoch 619/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3476 - accuracy: 0.8736\n",
            "Epoch 620/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.3476 - accuracy: 0.8736\n",
            "Epoch 621/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3468 - accuracy: 0.8736\n",
            "Epoch 622/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3473 - accuracy: 0.8736\n",
            "Epoch 623/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3465 - accuracy: 0.8736\n",
            "Epoch 624/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3458 - accuracy: 0.8736\n",
            "Epoch 625/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3452 - accuracy: 0.8621\n",
            "Epoch 626/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3451 - accuracy: 0.8736\n",
            "Epoch 627/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3460 - accuracy: 0.8736\n",
            "Epoch 628/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3458 - accuracy: 0.8736\n",
            "Epoch 629/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3442 - accuracy: 0.8736\n",
            "Epoch 630/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3453 - accuracy: 0.8621\n",
            "Epoch 631/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3431 - accuracy: 0.8621\n",
            "Epoch 632/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3431 - accuracy: 0.8621\n",
            "Epoch 633/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3420 - accuracy: 0.8621\n",
            "Epoch 634/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3444 - accuracy: 0.8621\n",
            "Epoch 635/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3428 - accuracy: 0.8621\n",
            "Epoch 636/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3427 - accuracy: 0.8621\n",
            "Epoch 637/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3407 - accuracy: 0.8736\n",
            "Epoch 638/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3410 - accuracy: 0.8736\n",
            "Epoch 639/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3401 - accuracy: 0.8736\n",
            "Epoch 640/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3396 - accuracy: 0.8736\n",
            "Epoch 641/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3396 - accuracy: 0.8736\n",
            "Epoch 642/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3385 - accuracy: 0.8736\n",
            "Epoch 643/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3384 - accuracy: 0.8621\n",
            "Epoch 644/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3391 - accuracy: 0.8736\n",
            "Epoch 645/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3386 - accuracy: 0.8621\n",
            "Epoch 646/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3371 - accuracy: 0.8621\n",
            "Epoch 647/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3366 - accuracy: 0.8736\n",
            "Epoch 648/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.3370 - accuracy: 0.8736\n",
            "Epoch 649/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3371 - accuracy: 0.8736\n",
            "Epoch 650/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3358 - accuracy: 0.8736\n",
            "Epoch 651/1000\n",
            "87/87 [==============================] - 0s 60us/step - loss: 0.3356 - accuracy: 0.8621\n",
            "Epoch 652/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3356 - accuracy: 0.8736\n",
            "Epoch 653/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3359 - accuracy: 0.8736\n",
            "Epoch 654/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3355 - accuracy: 0.8736\n",
            "Epoch 655/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3345 - accuracy: 0.8736\n",
            "Epoch 656/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3338 - accuracy: 0.8736\n",
            "Epoch 657/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3336 - accuracy: 0.8621\n",
            "Epoch 658/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3341 - accuracy: 0.8736\n",
            "Epoch 659/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3324 - accuracy: 0.8736\n",
            "Epoch 660/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3324 - accuracy: 0.8736\n",
            "Epoch 661/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3341 - accuracy: 0.8736\n",
            "Epoch 662/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3315 - accuracy: 0.8736\n",
            "Epoch 663/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3347 - accuracy: 0.8736\n",
            "Epoch 664/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3305 - accuracy: 0.8736\n",
            "Epoch 665/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.3308 - accuracy: 0.8736\n",
            "Epoch 666/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3305 - accuracy: 0.8736\n",
            "Epoch 667/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3308 - accuracy: 0.8621\n",
            "Epoch 668/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.3288 - accuracy: 0.8736\n",
            "Epoch 669/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3289 - accuracy: 0.8736\n",
            "Epoch 670/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.3302 - accuracy: 0.8736\n",
            "Epoch 671/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3276 - accuracy: 0.8851\n",
            "Epoch 672/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.3298 - accuracy: 0.8851\n",
            "Epoch 673/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3288 - accuracy: 0.8851\n",
            "Epoch 674/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.3286 - accuracy: 0.8851\n",
            "Epoch 675/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3267 - accuracy: 0.8736\n",
            "Epoch 676/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3275 - accuracy: 0.8736\n",
            "Epoch 677/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3276 - accuracy: 0.8851\n",
            "Epoch 678/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3261 - accuracy: 0.8736\n",
            "Epoch 679/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3282 - accuracy: 0.8621\n",
            "Epoch 680/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.3248 - accuracy: 0.8621\n",
            "Epoch 681/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3272 - accuracy: 0.8851\n",
            "Epoch 682/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3240 - accuracy: 0.8851\n",
            "Epoch 683/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.3246 - accuracy: 0.8851\n",
            "Epoch 684/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3236 - accuracy: 0.8851\n",
            "Epoch 685/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3234 - accuracy: 0.8851\n",
            "Epoch 686/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3233 - accuracy: 0.8851\n",
            "Epoch 687/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3223 - accuracy: 0.8851\n",
            "Epoch 688/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.3244 - accuracy: 0.8736\n",
            "Epoch 689/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.3220 - accuracy: 0.8851\n",
            "Epoch 690/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3224 - accuracy: 0.8851\n",
            "Epoch 691/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3230 - accuracy: 0.8851\n",
            "Epoch 692/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3206 - accuracy: 0.8851\n",
            "Epoch 693/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.3209 - accuracy: 0.8851\n",
            "Epoch 694/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3202 - accuracy: 0.8851\n",
            "Epoch 695/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3201 - accuracy: 0.8736\n",
            "Epoch 696/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3207 - accuracy: 0.8736\n",
            "Epoch 697/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.3210 - accuracy: 0.8851\n",
            "Epoch 698/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.3195 - accuracy: 0.8851\n",
            "Epoch 699/1000\n",
            "87/87 [==============================] - 0s 168us/step - loss: 0.3183 - accuracy: 0.8851\n",
            "Epoch 700/1000\n",
            "87/87 [==============================] - 0s 59us/step - loss: 0.3184 - accuracy: 0.8851\n",
            "Epoch 701/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.3181 - accuracy: 0.8851\n",
            "Epoch 702/1000\n",
            "87/87 [==============================] - 0s 130us/step - loss: 0.3179 - accuracy: 0.8851\n",
            "Epoch 703/1000\n",
            "87/87 [==============================] - 0s 185us/step - loss: 0.3184 - accuracy: 0.8851\n",
            "Epoch 704/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.3181 - accuracy: 0.8851\n",
            "Epoch 705/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.3174 - accuracy: 0.8851\n",
            "Epoch 706/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3177 - accuracy: 0.8851\n",
            "Epoch 707/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3161 - accuracy: 0.8851\n",
            "Epoch 708/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3179 - accuracy: 0.8851\n",
            "Epoch 709/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3163 - accuracy: 0.8851\n",
            "Epoch 710/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3146 - accuracy: 0.8851\n",
            "Epoch 711/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3179 - accuracy: 0.8851\n",
            "Epoch 712/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3163 - accuracy: 0.8851\n",
            "Epoch 713/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3145 - accuracy: 0.8851\n",
            "Epoch 714/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3136 - accuracy: 0.8851\n",
            "Epoch 715/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.3137 - accuracy: 0.8851\n",
            "Epoch 716/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3130 - accuracy: 0.8851\n",
            "Epoch 717/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3131 - accuracy: 0.8851\n",
            "Epoch 718/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3152 - accuracy: 0.8851\n",
            "Epoch 719/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3118 - accuracy: 0.8851\n",
            "Epoch 720/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3117 - accuracy: 0.8851\n",
            "Epoch 721/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.3109 - accuracy: 0.8851\n",
            "Epoch 722/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.3111 - accuracy: 0.8851\n",
            "Epoch 723/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3105 - accuracy: 0.8851\n",
            "Epoch 724/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3103 - accuracy: 0.8851\n",
            "Epoch 725/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.3099 - accuracy: 0.8851\n",
            "Epoch 726/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3097 - accuracy: 0.8851\n",
            "Epoch 727/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.3118 - accuracy: 0.8851\n",
            "Epoch 728/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.3088 - accuracy: 0.8851\n",
            "Epoch 729/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3083 - accuracy: 0.8851\n",
            "Epoch 730/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3084 - accuracy: 0.8851\n",
            "Epoch 731/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3092 - accuracy: 0.8851\n",
            "Epoch 732/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.3076 - accuracy: 0.8851\n",
            "Epoch 733/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.3072 - accuracy: 0.8851\n",
            "Epoch 734/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.3077 - accuracy: 0.8851\n",
            "Epoch 735/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3062 - accuracy: 0.8966\n",
            "Epoch 736/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.3065 - accuracy: 0.8851\n",
            "Epoch 737/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3066 - accuracy: 0.8851\n",
            "Epoch 738/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3062 - accuracy: 0.8966\n",
            "Epoch 739/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.3053 - accuracy: 0.8966\n",
            "Epoch 740/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.3046 - accuracy: 0.8966\n",
            "Epoch 741/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.3061 - accuracy: 0.8966\n",
            "Epoch 742/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.3039 - accuracy: 0.8966\n",
            "Epoch 743/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.3040 - accuracy: 0.8851\n",
            "Epoch 744/1000\n",
            "87/87 [==============================] - 0s 127us/step - loss: 0.3036 - accuracy: 0.8966\n",
            "Epoch 745/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.3028 - accuracy: 0.8966\n",
            "Epoch 746/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.3029 - accuracy: 0.8966\n",
            "Epoch 747/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.3018 - accuracy: 0.8966\n",
            "Epoch 748/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.3032 - accuracy: 0.8966\n",
            "Epoch 749/1000\n",
            "87/87 [==============================] - 0s 68us/step - loss: 0.3014 - accuracy: 0.8966\n",
            "Epoch 750/1000\n",
            "87/87 [==============================] - 0s 68us/step - loss: 0.3014 - accuracy: 0.8966\n",
            "Epoch 751/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.3008 - accuracy: 0.8966\n",
            "Epoch 752/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.3007 - accuracy: 0.8966\n",
            "Epoch 753/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.3001 - accuracy: 0.8966\n",
            "Epoch 754/1000\n",
            "87/87 [==============================] - 0s 132us/step - loss: 0.2999 - accuracy: 0.8966\n",
            "Epoch 755/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2994 - accuracy: 0.8966\n",
            "Epoch 756/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2998 - accuracy: 0.8966\n",
            "Epoch 757/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2992 - accuracy: 0.8966\n",
            "Epoch 758/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2997 - accuracy: 0.8966\n",
            "Epoch 759/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2984 - accuracy: 0.8966\n",
            "Epoch 760/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2982 - accuracy: 0.8966\n",
            "Epoch 761/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2979 - accuracy: 0.8966\n",
            "Epoch 762/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2969 - accuracy: 0.8966\n",
            "Epoch 763/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2971 - accuracy: 0.8966\n",
            "Epoch 764/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2962 - accuracy: 0.8966\n",
            "Epoch 765/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2966 - accuracy: 0.8966\n",
            "Epoch 766/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2955 - accuracy: 0.8966\n",
            "Epoch 767/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2952 - accuracy: 0.8966\n",
            "Epoch 768/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2993 - accuracy: 0.8966\n",
            "Epoch 769/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2971 - accuracy: 0.8966\n",
            "Epoch 770/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2959 - accuracy: 0.8966\n",
            "Epoch 771/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2938 - accuracy: 0.8966\n",
            "Epoch 772/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2944 - accuracy: 0.8966\n",
            "Epoch 773/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2949 - accuracy: 0.8966\n",
            "Epoch 774/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2936 - accuracy: 0.8966\n",
            "Epoch 775/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2942 - accuracy: 0.8966\n",
            "Epoch 776/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2921 - accuracy: 0.8966\n",
            "Epoch 777/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2926 - accuracy: 0.8966\n",
            "Epoch 778/1000\n",
            "87/87 [==============================] - 0s 107us/step - loss: 0.2920 - accuracy: 0.8966\n",
            "Epoch 779/1000\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.2915 - accuracy: 0.8966\n",
            "Epoch 780/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2907 - accuracy: 0.8966\n",
            "Epoch 781/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2915 - accuracy: 0.8966\n",
            "Epoch 782/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2902 - accuracy: 0.8966\n",
            "Epoch 783/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2912 - accuracy: 0.8966\n",
            "Epoch 784/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2900 - accuracy: 0.8966\n",
            "Epoch 785/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2902 - accuracy: 0.8966\n",
            "Epoch 786/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2902 - accuracy: 0.8966\n",
            "Epoch 787/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2894 - accuracy: 0.8966\n",
            "Epoch 788/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2882 - accuracy: 0.8966\n",
            "Epoch 789/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2893 - accuracy: 0.8966\n",
            "Epoch 790/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2882 - accuracy: 0.8966\n",
            "Epoch 791/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2884 - accuracy: 0.8966\n",
            "Epoch 792/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2888 - accuracy: 0.8966\n",
            "Epoch 793/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2880 - accuracy: 0.8966\n",
            "Epoch 794/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2883 - accuracy: 0.8966\n",
            "Epoch 795/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2883 - accuracy: 0.9080\n",
            "Epoch 796/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2859 - accuracy: 0.8966\n",
            "Epoch 797/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2869 - accuracy: 0.8966\n",
            "Epoch 798/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2849 - accuracy: 0.8966\n",
            "Epoch 799/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2866 - accuracy: 0.8966\n",
            "Epoch 800/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2841 - accuracy: 0.8966\n",
            "Epoch 801/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.2856 - accuracy: 0.8966\n",
            "Epoch 802/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2851 - accuracy: 0.9080\n",
            "Epoch 803/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.2830 - accuracy: 0.9080\n",
            "Epoch 804/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2823 - accuracy: 0.9195\n",
            "Epoch 805/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2824 - accuracy: 0.9195\n",
            "Epoch 806/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2811 - accuracy: 0.9080\n",
            "Epoch 807/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2821 - accuracy: 0.9080\n",
            "Epoch 808/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2813 - accuracy: 0.9080\n",
            "Epoch 809/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2826 - accuracy: 0.9080\n",
            "Epoch 810/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2804 - accuracy: 0.9080\n",
            "Epoch 811/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.2817 - accuracy: 0.9080\n",
            "Epoch 812/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2799 - accuracy: 0.9080\n",
            "Epoch 813/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2803 - accuracy: 0.9195\n",
            "Epoch 814/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2789 - accuracy: 0.9195\n",
            "Epoch 815/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2796 - accuracy: 0.9195\n",
            "Epoch 816/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2793 - accuracy: 0.9195\n",
            "Epoch 817/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2779 - accuracy: 0.9080\n",
            "Epoch 818/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2783 - accuracy: 0.9080\n",
            "Epoch 819/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2770 - accuracy: 0.9195\n",
            "Epoch 820/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2768 - accuracy: 0.9195\n",
            "Epoch 821/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2773 - accuracy: 0.9195\n",
            "Epoch 822/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2761 - accuracy: 0.9195\n",
            "Epoch 823/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2776 - accuracy: 0.9080\n",
            "Epoch 824/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2747 - accuracy: 0.9080\n",
            "Epoch 825/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2763 - accuracy: 0.9080\n",
            "Epoch 826/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2754 - accuracy: 0.9080\n",
            "Epoch 827/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2748 - accuracy: 0.9080\n",
            "Epoch 828/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2740 - accuracy: 0.9195\n",
            "Epoch 829/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2734 - accuracy: 0.9195\n",
            "Epoch 830/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2736 - accuracy: 0.9310\n",
            "Epoch 831/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2725 - accuracy: 0.9195\n",
            "Epoch 832/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2751 - accuracy: 0.9195\n",
            "Epoch 833/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2716 - accuracy: 0.9310\n",
            "Epoch 834/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2723 - accuracy: 0.9195\n",
            "Epoch 835/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2715 - accuracy: 0.9195\n",
            "Epoch 836/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2721 - accuracy: 0.9195\n",
            "Epoch 837/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2740 - accuracy: 0.9195\n",
            "Epoch 838/1000\n",
            "87/87 [==============================] - 0s 125us/step - loss: 0.2706 - accuracy: 0.9080\n",
            "Epoch 839/1000\n",
            "87/87 [==============================] - 0s 64us/step - loss: 0.2712 - accuracy: 0.9195\n",
            "Epoch 840/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2712 - accuracy: 0.9195\n",
            "Epoch 841/1000\n",
            "87/87 [==============================] - 0s 70us/step - loss: 0.2695 - accuracy: 0.9195\n",
            "Epoch 842/1000\n",
            "87/87 [==============================] - 0s 178us/step - loss: 0.2695 - accuracy: 0.9195\n",
            "Epoch 843/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2697 - accuracy: 0.9195\n",
            "Epoch 844/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.2683 - accuracy: 0.9195\n",
            "Epoch 845/1000\n",
            "87/87 [==============================] - 0s 114us/step - loss: 0.2677 - accuracy: 0.9195\n",
            "Epoch 846/1000\n",
            "87/87 [==============================] - 0s 124us/step - loss: 0.2680 - accuracy: 0.9195\n",
            "Epoch 847/1000\n",
            "87/87 [==============================] - 0s 131us/step - loss: 0.2677 - accuracy: 0.9195\n",
            "Epoch 848/1000\n",
            "87/87 [==============================] - 0s 132us/step - loss: 0.2673 - accuracy: 0.9195\n",
            "Epoch 849/1000\n",
            "87/87 [==============================] - 0s 133us/step - loss: 0.2694 - accuracy: 0.9080\n",
            "Epoch 850/1000\n",
            "87/87 [==============================] - 0s 129us/step - loss: 0.2671 - accuracy: 0.9195\n",
            "Epoch 851/1000\n",
            "87/87 [==============================] - 0s 142us/step - loss: 0.2662 - accuracy: 0.9195\n",
            "Epoch 852/1000\n",
            "87/87 [==============================] - 0s 102us/step - loss: 0.2680 - accuracy: 0.9310\n",
            "Epoch 853/1000\n",
            "87/87 [==============================] - 0s 117us/step - loss: 0.2666 - accuracy: 0.9310\n",
            "Epoch 854/1000\n",
            "87/87 [==============================] - 0s 127us/step - loss: 0.2653 - accuracy: 0.9195\n",
            "Epoch 855/1000\n",
            "87/87 [==============================] - 0s 122us/step - loss: 0.2647 - accuracy: 0.9195\n",
            "Epoch 856/1000\n",
            "87/87 [==============================] - 0s 113us/step - loss: 0.2665 - accuracy: 0.9310\n",
            "Epoch 857/1000\n",
            "87/87 [==============================] - 0s 109us/step - loss: 0.2634 - accuracy: 0.9195\n",
            "Epoch 858/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.2642 - accuracy: 0.9195\n",
            "Epoch 859/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2645 - accuracy: 0.9310\n",
            "Epoch 860/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.2629 - accuracy: 0.9195\n",
            "Epoch 861/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2651 - accuracy: 0.9195\n",
            "Epoch 862/1000\n",
            "87/87 [==============================] - 0s 114us/step - loss: 0.2626 - accuracy: 0.9310\n",
            "Epoch 863/1000\n",
            "87/87 [==============================] - 0s 103us/step - loss: 0.2628 - accuracy: 0.9310\n",
            "Epoch 864/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.2632 - accuracy: 0.9310\n",
            "Epoch 865/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2630 - accuracy: 0.9195\n",
            "Epoch 866/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2651 - accuracy: 0.9310\n",
            "Epoch 867/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2618 - accuracy: 0.9310\n",
            "Epoch 868/1000\n",
            "87/87 [==============================] - 0s 109us/step - loss: 0.2621 - accuracy: 0.9195\n",
            "Epoch 869/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2603 - accuracy: 0.9310\n",
            "Epoch 870/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2605 - accuracy: 0.9310\n",
            "Epoch 871/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2603 - accuracy: 0.9310\n",
            "Epoch 872/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2591 - accuracy: 0.9310\n",
            "Epoch 873/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2597 - accuracy: 0.9310\n",
            "Epoch 874/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2615 - accuracy: 0.9195\n",
            "Epoch 875/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2608 - accuracy: 0.9310\n",
            "Epoch 876/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2579 - accuracy: 0.9195\n",
            "Epoch 877/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2576 - accuracy: 0.9310\n",
            "Epoch 878/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2579 - accuracy: 0.9310\n",
            "Epoch 879/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.2574 - accuracy: 0.9195\n",
            "Epoch 880/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2593 - accuracy: 0.9310\n",
            "Epoch 881/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2563 - accuracy: 0.9195\n",
            "Epoch 882/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2566 - accuracy: 0.9195\n",
            "Epoch 883/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2562 - accuracy: 0.9195\n",
            "Epoch 884/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2558 - accuracy: 0.9310\n",
            "Epoch 885/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2553 - accuracy: 0.9310\n",
            "Epoch 886/1000\n",
            "87/87 [==============================] - 0s 107us/step - loss: 0.2559 - accuracy: 0.9310\n",
            "Epoch 887/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2546 - accuracy: 0.9310\n",
            "Epoch 888/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2540 - accuracy: 0.9310\n",
            "Epoch 889/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2535 - accuracy: 0.9310\n",
            "Epoch 890/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2542 - accuracy: 0.9195\n",
            "Epoch 891/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2532 - accuracy: 0.9195\n",
            "Epoch 892/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2528 - accuracy: 0.9310\n",
            "Epoch 893/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2539 - accuracy: 0.9310\n",
            "Epoch 894/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2533 - accuracy: 0.9310\n",
            "Epoch 895/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2541 - accuracy: 0.9310\n",
            "Epoch 896/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2518 - accuracy: 0.9310\n",
            "Epoch 897/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2519 - accuracy: 0.9310\n",
            "Epoch 898/1000\n",
            "87/87 [==============================] - 0s 101us/step - loss: 0.2525 - accuracy: 0.9425\n",
            "Epoch 899/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2505 - accuracy: 0.9425\n",
            "Epoch 900/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2519 - accuracy: 0.9310\n",
            "Epoch 901/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2500 - accuracy: 0.9310\n",
            "Epoch 902/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2504 - accuracy: 0.9425\n",
            "Epoch 903/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2507 - accuracy: 0.9425\n",
            "Epoch 904/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2508 - accuracy: 0.9310\n",
            "Epoch 905/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2515 - accuracy: 0.9310\n",
            "Epoch 906/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2490 - accuracy: 0.9540\n",
            "Epoch 907/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2495 - accuracy: 0.9540\n",
            "Epoch 908/1000\n",
            "87/87 [==============================] - 0s 94us/step - loss: 0.2487 - accuracy: 0.9310\n",
            "Epoch 909/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2487 - accuracy: 0.9310\n",
            "Epoch 910/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2486 - accuracy: 0.9425\n",
            "Epoch 911/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2470 - accuracy: 0.9425\n",
            "Epoch 912/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2483 - accuracy: 0.9425\n",
            "Epoch 913/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2471 - accuracy: 0.9425\n",
            "Epoch 914/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2474 - accuracy: 0.9425\n",
            "Epoch 915/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2475 - accuracy: 0.9195\n",
            "Epoch 916/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2471 - accuracy: 0.9195\n",
            "Epoch 917/1000\n",
            "87/87 [==============================] - 0s 93us/step - loss: 0.2466 - accuracy: 0.9425\n",
            "Epoch 918/1000\n",
            "87/87 [==============================] - 0s 90us/step - loss: 0.2450 - accuracy: 0.9425\n",
            "Epoch 919/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2473 - accuracy: 0.9425\n",
            "Epoch 920/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2461 - accuracy: 0.9195\n",
            "Epoch 921/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2442 - accuracy: 0.9310\n",
            "Epoch 922/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2453 - accuracy: 0.9425\n",
            "Epoch 923/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2438 - accuracy: 0.9425\n",
            "Epoch 924/1000\n",
            "87/87 [==============================] - 0s 67us/step - loss: 0.2450 - accuracy: 0.9425\n",
            "Epoch 925/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2439 - accuracy: 0.9310\n",
            "Epoch 926/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2440 - accuracy: 0.9425\n",
            "Epoch 927/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2427 - accuracy: 0.9425\n",
            "Epoch 928/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2431 - accuracy: 0.9425\n",
            "Epoch 929/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2436 - accuracy: 0.9425\n",
            "Epoch 930/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2423 - accuracy: 0.9540\n",
            "Epoch 931/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2430 - accuracy: 0.9310\n",
            "Epoch 932/1000\n",
            "87/87 [==============================] - 0s 66us/step - loss: 0.2416 - accuracy: 0.9425\n",
            "Epoch 933/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2437 - accuracy: 0.9425\n",
            "Epoch 934/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2407 - accuracy: 0.9425\n",
            "Epoch 935/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2418 - accuracy: 0.9425\n",
            "Epoch 936/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2423 - accuracy: 0.9425\n",
            "Epoch 937/1000\n",
            "87/87 [==============================] - 0s 100us/step - loss: 0.2420 - accuracy: 0.9425\n",
            "Epoch 938/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2398 - accuracy: 0.9425\n",
            "Epoch 939/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2404 - accuracy: 0.9425\n",
            "Epoch 940/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2397 - accuracy: 0.9425\n",
            "Epoch 941/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2385 - accuracy: 0.9425\n",
            "Epoch 942/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2389 - accuracy: 0.9425\n",
            "Epoch 943/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2384 - accuracy: 0.9425\n",
            "Epoch 944/1000\n",
            "87/87 [==============================] - 0s 57us/step - loss: 0.2382 - accuracy: 0.9425\n",
            "Epoch 945/1000\n",
            "87/87 [==============================] - 0s 73us/step - loss: 0.2385 - accuracy: 0.9425\n",
            "Epoch 946/1000\n",
            "87/87 [==============================] - 0s 95us/step - loss: 0.2369 - accuracy: 0.9425\n",
            "Epoch 947/1000\n",
            "87/87 [==============================] - 0s 89us/step - loss: 0.2364 - accuracy: 0.9425\n",
            "Epoch 948/1000\n",
            "87/87 [==============================] - 0s 92us/step - loss: 0.2369 - accuracy: 0.9425\n",
            "Epoch 949/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 950/1000\n",
            "87/87 [==============================] - 0s 96us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 951/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2359 - accuracy: 0.9425\n",
            "Epoch 952/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2360 - accuracy: 0.9425\n",
            "Epoch 953/1000\n",
            "87/87 [==============================] - 0s 91us/step - loss: 0.2369 - accuracy: 0.9540\n",
            "Epoch 954/1000\n",
            "87/87 [==============================] - 0s 86us/step - loss: 0.2360 - accuracy: 0.9425\n",
            "Epoch 955/1000\n",
            "87/87 [==============================] - 0s 69us/step - loss: 0.2354 - accuracy: 0.9425\n",
            "Epoch 956/1000\n",
            "87/87 [==============================] - 0s 68us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 957/1000\n",
            "87/87 [==============================] - 0s 67us/step - loss: 0.2365 - accuracy: 0.9425\n",
            "Epoch 958/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2337 - accuracy: 0.9425\n",
            "Epoch 959/1000\n",
            "87/87 [==============================] - 0s 97us/step - loss: 0.2335 - accuracy: 0.9425\n",
            "Epoch 960/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2341 - accuracy: 0.9425\n",
            "Epoch 961/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2330 - accuracy: 0.9425\n",
            "Epoch 962/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2339 - accuracy: 0.9425\n",
            "Epoch 963/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2326 - accuracy: 0.9425\n",
            "Epoch 964/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2335 - accuracy: 0.9425\n",
            "Epoch 965/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2323 - accuracy: 0.9425\n",
            "Epoch 966/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2325 - accuracy: 0.9425\n",
            "Epoch 967/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2312 - accuracy: 0.9540\n",
            "Epoch 968/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2313 - accuracy: 0.9540\n",
            "Epoch 969/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2306 - accuracy: 0.9540\n",
            "Epoch 970/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2308 - accuracy: 0.9425\n",
            "Epoch 971/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2299 - accuracy: 0.9425\n",
            "Epoch 972/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2313 - accuracy: 0.9425\n",
            "Epoch 973/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2297 - accuracy: 0.9425\n",
            "Epoch 974/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2307 - accuracy: 0.9425\n",
            "Epoch 975/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2292 - accuracy: 0.9425\n",
            "Epoch 976/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2292 - accuracy: 0.9425\n",
            "Epoch 977/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2287 - accuracy: 0.9425\n",
            "Epoch 978/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2287 - accuracy: 0.9425\n",
            "Epoch 979/1000\n",
            "87/87 [==============================] - 0s 85us/step - loss: 0.2283 - accuracy: 0.9425\n",
            "Epoch 980/1000\n",
            "87/87 [==============================] - 0s 74us/step - loss: 0.2277 - accuracy: 0.9425\n",
            "Epoch 981/1000\n",
            "87/87 [==============================] - 0s 81us/step - loss: 0.2298 - accuracy: 0.9425\n",
            "Epoch 982/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2282 - accuracy: 0.9425\n",
            "Epoch 983/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2311 - accuracy: 0.9425\n",
            "Epoch 984/1000\n",
            "87/87 [==============================] - 0s 87us/step - loss: 0.2265 - accuracy: 0.9425\n",
            "Epoch 985/1000\n",
            "87/87 [==============================] - 0s 88us/step - loss: 0.2263 - accuracy: 0.9425\n",
            "Epoch 986/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2265 - accuracy: 0.9425\n",
            "Epoch 987/1000\n",
            "87/87 [==============================] - 0s 78us/step - loss: 0.2255 - accuracy: 0.9425\n",
            "Epoch 988/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2253 - accuracy: 0.9425\n",
            "Epoch 989/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2262 - accuracy: 0.9425\n",
            "Epoch 990/1000\n",
            "87/87 [==============================] - 0s 82us/step - loss: 0.2267 - accuracy: 0.9425\n",
            "Epoch 991/1000\n",
            "87/87 [==============================] - 0s 77us/step - loss: 0.2254 - accuracy: 0.9540\n",
            "Epoch 992/1000\n",
            "87/87 [==============================] - 0s 76us/step - loss: 0.2260 - accuracy: 0.9425\n",
            "Epoch 993/1000\n",
            "87/87 [==============================] - 0s 83us/step - loss: 0.2254 - accuracy: 0.9425\n",
            "Epoch 994/1000\n",
            "87/87 [==============================] - 0s 84us/step - loss: 0.2258 - accuracy: 0.9425\n",
            "Epoch 995/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2242 - accuracy: 0.9425\n",
            "Epoch 996/1000\n",
            "87/87 [==============================] - 0s 72us/step - loss: 0.2239 - accuracy: 0.9425\n",
            "Epoch 997/1000\n",
            "87/87 [==============================] - 0s 71us/step - loss: 0.2241 - accuracy: 0.9425\n",
            "Epoch 998/1000\n",
            "87/87 [==============================] - 0s 80us/step - loss: 0.2232 - accuracy: 0.9425\n",
            "Epoch 999/1000\n",
            "87/87 [==============================] - 0s 79us/step - loss: 0.2238 - accuracy: 0.9425\n",
            "Epoch 1000/1000\n",
            "87/87 [==============================] - 0s 75us/step - loss: 0.2225 - accuracy: 0.9540\n",
            "44/44 [==============================] - 0s 606us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-5e636cf8b2b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                                 error_score=error_score)\n\u001b[0m\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m             error_score=error_score)\n\u001b[0;32m--> 236\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 score = scorer._score(cached_call, estimator,\n\u001b[0;32m---> 87\u001b[0;31m                                       *args, **kwargs)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[0;32m--> 212\u001b[0;31m                                                  **self._kwargs)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 90\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multilabel-indicator and continuous-multioutput targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK7gz80tB10x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
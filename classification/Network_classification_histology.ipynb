{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Network_classification_histology.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonardoub/SCRIPT_PALERMO/blob/master/classification/Network_classification_histology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck9uZtF_gzU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln0sTf8q1IrI",
        "colab_type": "text"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyyNl4gxhEwD",
        "colab_type": "code",
        "outputId": "ba123e7a-2bbd-4c99-a119-3223fcaabb37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "#load data from Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "#%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCkUXesZhMzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset_path = '/gdrive/My Drive/AIM_PA/database_training2.csv'\n",
        "test_dataset_path = '/gdrive/My Drive/AIM_PA/database_nostro_without_nan.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TczPxOpEhTXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train = pd.read_csv(train_dataset_path)\n",
        "df_test = pd.read_csv(test_dataset_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll-87QSVhqhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulSbeCedhuxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test.rename(columns={'Survival.time (months)':'Surv_time_months'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbcwLGg3iNSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_train.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)\n",
        "df_test.rename(columns={'Overall.Stage':'Overall_Stage'}, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKKv4iKghWWn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = df_train.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQdR4izXiT0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = df_test.drop(['Histology', 'Surv_time_months', 'OS', 'deadstatus.event','Overall_Stage'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu46pqnPhnCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels = df_train.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS5wIylYmsQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels = df_test.Histology"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPx7PMDnXM3",
        "colab_type": "text"
      },
      "source": [
        "##Z score dei dati"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4Qji2EnVV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data_stand = train_data - mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOVOoNOvm0Yx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand = test_data - mean\n",
        "test_data_stand /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00VohsAyokpq",
        "colab_type": "text"
      },
      "source": [
        "##Vettorizzare i label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RvS_9ISpxRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_index={'adenocarcinoma':0, 'large cell':1, 'squamous cell carcinoma':2}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiPW9U0XrWY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_labels_dec = [word_index[label] for label in train_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4SBiKFQsKFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_labels_dec = [word_index[label] for label in test_labels]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IMbTYR7okJq",
        "colab_type": "code",
        "outputId": "7f3f0544-8479-4e39-e8cb-670a21c42d7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "from keras.utils.np_utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Frv4FDNn6Qu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_train_labels = to_categorical(train_labels_dec)\n",
        "one_hot_test_labels = to_categorical(test_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn0tkOGc3LKN",
        "colab_type": "text"
      },
      "source": [
        "#PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS76u6iu3Seg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCjC4zqJ3bui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=0.85, svd_solver='full')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLUCf9qX4p_e",
        "colab_type": "code",
        "outputId": "476b74fb-6581-458c-abcc-57ddc28f2780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "pca.fit(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PCA(copy=True, iterated_power='auto', n_components=0.85, random_state=None,\n",
              "    svd_solver='full', tol=0.0, whiten=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfyaKgNZ44o3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_pca = pca.transform(train_data_stand)\n",
        "test_data_stand_pca = pca.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz9C4nl05b_g",
        "colab_type": "code",
        "outputId": "e22c954d-44cc-4f10-cb49-2ef3c380d3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_pca.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nRJJ4WxMgyIt"
      },
      "source": [
        "##Z score dei dati dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNbcqlgbgyI5",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_pca.mean(axis=0)\n",
        "std = train_data_stand_pca.std(axis=0)\n",
        "train_data_stand_pca = train_data_stand_pca - mean\n",
        "train_data_stand_pca /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZA9GJO6gyJO",
        "colab": {}
      },
      "source": [
        "test_data_stand_pca = test_data_stand_pca - mean\n",
        "test_data_stand_pca /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wSKvSu4s5ip",
        "colab_type": "text"
      },
      "source": [
        "#Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJTbHiq0D-4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShwM6YMqsxxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAzbu7P1VylY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyqbUCK5wOVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OAEgN31tHVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(7,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.005, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIxDu50pBeiz",
        "colab_type": "text"
      },
      "source": [
        "#Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyLcvedUBpxA",
        "colab_type": "text"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY1apcZ19gFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaBDM-PtBx5V",
        "colab_type": "code",
        "outputId": "03854704-132e-4291-89e4-d8b83361797a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_pca, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me-XQzPyD1gi",
        "colab_type": "code",
        "outputId": "1e930d57-e6d3-43a4-f40a-fc4b497e8db5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgdGK-8FK-U_",
        "colab_type": "code",
        "outputId": "79d6d566-29e7-4b76-9ef4-a6fbd33de0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBJg0XD4Shhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5Sq8r9GEPx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X5B3lasRcsR",
        "colab_type": "text"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Sl23XX-uUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K8a1I3yU9FS",
        "colab_type": "code",
        "outputId": "359c6c99-8ca4-40ae-b8c1-f57c016a68b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 300\n",
        "all_acc_histories = []\n",
        "all_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_val_loss_histories = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/300\n",
            "87/87 [==============================] - 1s 7ms/step - loss: 1.1639 - acc: 0.5632 - val_loss: 1.3481 - val_acc: 0.4091\n",
            "Epoch 2/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 1.1417 - acc: 0.5632 - val_loss: 1.3274 - val_acc: 0.4091\n",
            "Epoch 3/300\n",
            "87/87 [==============================] - 0s 332us/step - loss: 1.1208 - acc: 0.5747 - val_loss: 1.3075 - val_acc: 0.3864\n",
            "Epoch 4/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 1.1019 - acc: 0.5747 - val_loss: 1.2902 - val_acc: 0.4318\n",
            "Epoch 5/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 1.0871 - acc: 0.5632 - val_loss: 1.2741 - val_acc: 0.4773\n",
            "Epoch 6/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.0705 - acc: 0.5517 - val_loss: 1.2613 - val_acc: 0.4545\n",
            "Epoch 7/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 1.0570 - acc: 0.5402 - val_loss: 1.2495 - val_acc: 0.4318\n",
            "Epoch 8/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 1.0452 - acc: 0.5517 - val_loss: 1.2373 - val_acc: 0.4318\n",
            "Epoch 9/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.0337 - acc: 0.5517 - val_loss: 1.2278 - val_acc: 0.4318\n",
            "Epoch 10/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 1.0229 - acc: 0.5517 - val_loss: 1.2184 - val_acc: 0.4318\n",
            "Epoch 11/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 1.0136 - acc: 0.5632 - val_loss: 1.2101 - val_acc: 0.4318\n",
            "Epoch 12/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 1.0049 - acc: 0.5632 - val_loss: 1.2026 - val_acc: 0.4545\n",
            "Epoch 13/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.9974 - acc: 0.5632 - val_loss: 1.1968 - val_acc: 0.4545\n",
            "Epoch 14/300\n",
            "87/87 [==============================] - 0s 304us/step - loss: 0.9895 - acc: 0.5862 - val_loss: 1.1899 - val_acc: 0.4545\n",
            "Epoch 15/300\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.9831 - acc: 0.5862 - val_loss: 1.1833 - val_acc: 0.4545\n",
            "Epoch 16/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.9774 - acc: 0.5862 - val_loss: 1.1795 - val_acc: 0.4773\n",
            "Epoch 17/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.9707 - acc: 0.5862 - val_loss: 1.1741 - val_acc: 0.4773\n",
            "Epoch 18/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.9652 - acc: 0.5862 - val_loss: 1.1693 - val_acc: 0.4773\n",
            "Epoch 19/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.9607 - acc: 0.5862 - val_loss: 1.1644 - val_acc: 0.4773\n",
            "Epoch 20/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.9573 - acc: 0.5747 - val_loss: 1.1603 - val_acc: 0.4773\n",
            "Epoch 21/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.9518 - acc: 0.5747 - val_loss: 1.1566 - val_acc: 0.4773\n",
            "Epoch 22/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.9481 - acc: 0.5747 - val_loss: 1.1533 - val_acc: 0.4773\n",
            "Epoch 23/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.9442 - acc: 0.5862 - val_loss: 1.1509 - val_acc: 0.4773\n",
            "Epoch 24/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.9409 - acc: 0.5747 - val_loss: 1.1484 - val_acc: 0.4773\n",
            "Epoch 25/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.9375 - acc: 0.5747 - val_loss: 1.1450 - val_acc: 0.4773\n",
            "Epoch 26/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.9340 - acc: 0.5747 - val_loss: 1.1420 - val_acc: 0.4773\n",
            "Epoch 27/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.9321 - acc: 0.5747 - val_loss: 1.1398 - val_acc: 0.4773\n",
            "Epoch 28/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.9287 - acc: 0.5747 - val_loss: 1.1376 - val_acc: 0.4773\n",
            "Epoch 29/300\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.9269 - acc: 0.5747 - val_loss: 1.1358 - val_acc: 0.4773\n",
            "Epoch 30/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.9239 - acc: 0.5747 - val_loss: 1.1340 - val_acc: 0.4773\n",
            "Epoch 31/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.9218 - acc: 0.5747 - val_loss: 1.1321 - val_acc: 0.4773\n",
            "Epoch 32/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.9192 - acc: 0.5747 - val_loss: 1.1303 - val_acc: 0.5000\n",
            "Epoch 33/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.9178 - acc: 0.5747 - val_loss: 1.1283 - val_acc: 0.4773\n",
            "Epoch 34/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.9156 - acc: 0.5747 - val_loss: 1.1263 - val_acc: 0.4773\n",
            "Epoch 35/300\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.9137 - acc: 0.5747 - val_loss: 1.1237 - val_acc: 0.4773\n",
            "Epoch 36/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.9120 - acc: 0.5862 - val_loss: 1.1229 - val_acc: 0.4773\n",
            "Epoch 37/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.9108 - acc: 0.5747 - val_loss: 1.1198 - val_acc: 0.4773\n",
            "Epoch 38/300\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.9090 - acc: 0.5747 - val_loss: 1.1182 - val_acc: 0.4773\n",
            "Epoch 39/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.9078 - acc: 0.5747 - val_loss: 1.1170 - val_acc: 0.4773\n",
            "Epoch 40/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.9068 - acc: 0.5747 - val_loss: 1.1151 - val_acc: 0.4773\n",
            "Epoch 41/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.9052 - acc: 0.5862 - val_loss: 1.1137 - val_acc: 0.4773\n",
            "Epoch 42/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.9038 - acc: 0.5747 - val_loss: 1.1122 - val_acc: 0.4773\n",
            "Epoch 43/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.9026 - acc: 0.5862 - val_loss: 1.1108 - val_acc: 0.4773\n",
            "Epoch 44/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9012 - acc: 0.5977 - val_loss: 1.1097 - val_acc: 0.4773\n",
            "Epoch 45/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.9010 - acc: 0.5862 - val_loss: 1.1090 - val_acc: 0.4773\n",
            "Epoch 46/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.8998 - acc: 0.5862 - val_loss: 1.1077 - val_acc: 0.4773\n",
            "Epoch 47/300\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.8990 - acc: 0.5977 - val_loss: 1.1073 - val_acc: 0.4773\n",
            "Epoch 48/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8977 - acc: 0.5977 - val_loss: 1.1054 - val_acc: 0.4773\n",
            "Epoch 49/300\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.8973 - acc: 0.5977 - val_loss: 1.1056 - val_acc: 0.4773\n",
            "Epoch 50/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8971 - acc: 0.5862 - val_loss: 1.1046 - val_acc: 0.4773\n",
            "Epoch 51/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8961 - acc: 0.5977 - val_loss: 1.1045 - val_acc: 0.4773\n",
            "Epoch 52/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8955 - acc: 0.5977 - val_loss: 1.1031 - val_acc: 0.4773\n",
            "Epoch 53/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8950 - acc: 0.5862 - val_loss: 1.1034 - val_acc: 0.4773\n",
            "Epoch 54/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8940 - acc: 0.5977 - val_loss: 1.1020 - val_acc: 0.4773\n",
            "Epoch 55/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8934 - acc: 0.5977 - val_loss: 1.1014 - val_acc: 0.4773\n",
            "Epoch 56/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8932 - acc: 0.5977 - val_loss: 1.1006 - val_acc: 0.4773\n",
            "Epoch 57/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8923 - acc: 0.5977 - val_loss: 1.1003 - val_acc: 0.4773\n",
            "Epoch 58/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8919 - acc: 0.5862 - val_loss: 1.1003 - val_acc: 0.4773\n",
            "Epoch 59/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8912 - acc: 0.5977 - val_loss: 1.0994 - val_acc: 0.4773\n",
            "Epoch 60/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8910 - acc: 0.5977 - val_loss: 1.0989 - val_acc: 0.4773\n",
            "Epoch 61/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8908 - acc: 0.5977 - val_loss: 1.0985 - val_acc: 0.4773\n",
            "Epoch 62/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8908 - acc: 0.5977 - val_loss: 1.0975 - val_acc: 0.4773\n",
            "Epoch 63/300\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8899 - acc: 0.5977 - val_loss: 1.0973 - val_acc: 0.4773\n",
            "Epoch 64/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8898 - acc: 0.5977 - val_loss: 1.0968 - val_acc: 0.4773\n",
            "Epoch 65/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8889 - acc: 0.5977 - val_loss: 1.0967 - val_acc: 0.4773\n",
            "Epoch 66/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8884 - acc: 0.5977 - val_loss: 1.0968 - val_acc: 0.4773\n",
            "Epoch 67/300\n",
            "87/87 [==============================] - 0s 307us/step - loss: 0.8887 - acc: 0.5977 - val_loss: 1.0964 - val_acc: 0.4773\n",
            "Epoch 68/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8879 - acc: 0.5977 - val_loss: 1.0958 - val_acc: 0.4773\n",
            "Epoch 69/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8875 - acc: 0.5977 - val_loss: 1.0951 - val_acc: 0.4773\n",
            "Epoch 70/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8874 - acc: 0.5862 - val_loss: 1.0947 - val_acc: 0.4773\n",
            "Epoch 71/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8868 - acc: 0.5862 - val_loss: 1.0945 - val_acc: 0.4773\n",
            "Epoch 72/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.8866 - acc: 0.5862 - val_loss: 1.0944 - val_acc: 0.4773\n",
            "Epoch 73/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8861 - acc: 0.5977 - val_loss: 1.0945 - val_acc: 0.4545\n",
            "Epoch 74/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8862 - acc: 0.5977 - val_loss: 1.0936 - val_acc: 0.4773\n",
            "Epoch 75/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8860 - acc: 0.5862 - val_loss: 1.0935 - val_acc: 0.4545\n",
            "Epoch 76/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8854 - acc: 0.6092 - val_loss: 1.0936 - val_acc: 0.4545\n",
            "Epoch 77/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8848 - acc: 0.5977 - val_loss: 1.0933 - val_acc: 0.4545\n",
            "Epoch 78/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.8848 - acc: 0.5977 - val_loss: 1.0938 - val_acc: 0.4545\n",
            "Epoch 79/300\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.8845 - acc: 0.6092 - val_loss: 1.0929 - val_acc: 0.4545\n",
            "Epoch 80/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8839 - acc: 0.6092 - val_loss: 1.0928 - val_acc: 0.4545\n",
            "Epoch 81/300\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8843 - acc: 0.5977 - val_loss: 1.0927 - val_acc: 0.4545\n",
            "Epoch 82/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8834 - acc: 0.5977 - val_loss: 1.0924 - val_acc: 0.4545\n",
            "Epoch 83/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8831 - acc: 0.5977 - val_loss: 1.0920 - val_acc: 0.4545\n",
            "Epoch 84/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8832 - acc: 0.5977 - val_loss: 1.0912 - val_acc: 0.4545\n",
            "Epoch 85/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.8832 - acc: 0.5977 - val_loss: 1.0910 - val_acc: 0.4545\n",
            "Epoch 86/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8829 - acc: 0.5977 - val_loss: 1.0905 - val_acc: 0.4545\n",
            "Epoch 87/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8826 - acc: 0.5977 - val_loss: 1.0913 - val_acc: 0.4545\n",
            "Epoch 88/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8821 - acc: 0.5977 - val_loss: 1.0903 - val_acc: 0.4545\n",
            "Epoch 89/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8820 - acc: 0.6092 - val_loss: 1.0899 - val_acc: 0.4545\n",
            "Epoch 90/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8816 - acc: 0.6207 - val_loss: 1.0899 - val_acc: 0.4545\n",
            "Epoch 91/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8817 - acc: 0.6092 - val_loss: 1.0894 - val_acc: 0.4545\n",
            "Epoch 92/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8820 - acc: 0.5977 - val_loss: 1.0892 - val_acc: 0.4545\n",
            "Epoch 93/300\n",
            "87/87 [==============================] - 0s 198us/step - loss: 0.8812 - acc: 0.6092 - val_loss: 1.0889 - val_acc: 0.4545\n",
            "Epoch 94/300\n",
            "87/87 [==============================] - 0s 316us/step - loss: 0.8811 - acc: 0.6092 - val_loss: 1.0890 - val_acc: 0.4545\n",
            "Epoch 95/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8808 - acc: 0.6092 - val_loss: 1.0888 - val_acc: 0.4545\n",
            "Epoch 96/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8808 - acc: 0.6092 - val_loss: 1.0886 - val_acc: 0.4545\n",
            "Epoch 97/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8802 - acc: 0.6092 - val_loss: 1.0886 - val_acc: 0.4545\n",
            "Epoch 98/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8803 - acc: 0.6092 - val_loss: 1.0884 - val_acc: 0.4545\n",
            "Epoch 99/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8800 - acc: 0.6092 - val_loss: 1.0885 - val_acc: 0.4545\n",
            "Epoch 100/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.8798 - acc: 0.6092 - val_loss: 1.0887 - val_acc: 0.4545\n",
            "Epoch 101/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.8804 - acc: 0.6092 - val_loss: 1.0886 - val_acc: 0.4545\n",
            "Epoch 102/300\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.8792 - acc: 0.6092 - val_loss: 1.0879 - val_acc: 0.4545\n",
            "Epoch 103/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.8793 - acc: 0.6092 - val_loss: 1.0875 - val_acc: 0.4545\n",
            "Epoch 104/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8791 - acc: 0.6092 - val_loss: 1.0872 - val_acc: 0.4545\n",
            "Epoch 105/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.8788 - acc: 0.6092 - val_loss: 1.0876 - val_acc: 0.4545\n",
            "Epoch 106/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8788 - acc: 0.6092 - val_loss: 1.0870 - val_acc: 0.4545\n",
            "Epoch 107/300\n",
            "87/87 [==============================] - 0s 312us/step - loss: 0.8785 - acc: 0.6092 - val_loss: 1.0872 - val_acc: 0.4545\n",
            "Epoch 108/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8784 - acc: 0.6092 - val_loss: 1.0868 - val_acc: 0.4545\n",
            "Epoch 109/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8781 - acc: 0.6092 - val_loss: 1.0865 - val_acc: 0.4545\n",
            "Epoch 110/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8778 - acc: 0.6092 - val_loss: 1.0864 - val_acc: 0.4545\n",
            "Epoch 111/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8781 - acc: 0.6092 - val_loss: 1.0873 - val_acc: 0.4545\n",
            "Epoch 112/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8781 - acc: 0.6092 - val_loss: 1.0870 - val_acc: 0.4545\n",
            "Epoch 113/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8783 - acc: 0.6092 - val_loss: 1.0859 - val_acc: 0.4545\n",
            "Epoch 114/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8774 - acc: 0.6092 - val_loss: 1.0855 - val_acc: 0.4545\n",
            "Epoch 115/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8769 - acc: 0.6092 - val_loss: 1.0857 - val_acc: 0.4545\n",
            "Epoch 116/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8773 - acc: 0.6092 - val_loss: 1.0858 - val_acc: 0.4545\n",
            "Epoch 117/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8772 - acc: 0.6092 - val_loss: 1.0858 - val_acc: 0.4545\n",
            "Epoch 118/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.8766 - acc: 0.6092 - val_loss: 1.0862 - val_acc: 0.4545\n",
            "Epoch 119/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8765 - acc: 0.6092 - val_loss: 1.0856 - val_acc: 0.4545\n",
            "Epoch 120/300\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.8761 - acc: 0.6092 - val_loss: 1.0861 - val_acc: 0.4545\n",
            "Epoch 121/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.8757 - acc: 0.6092 - val_loss: 1.0858 - val_acc: 0.4545\n",
            "Epoch 122/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8759 - acc: 0.6092 - val_loss: 1.0862 - val_acc: 0.4545\n",
            "Epoch 123/300\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.8755 - acc: 0.6092 - val_loss: 1.0853 - val_acc: 0.4545\n",
            "Epoch 124/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.8754 - acc: 0.6092 - val_loss: 1.0852 - val_acc: 0.4545\n",
            "Epoch 125/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.8756 - acc: 0.5977 - val_loss: 1.0858 - val_acc: 0.4545\n",
            "Epoch 126/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.8753 - acc: 0.6092 - val_loss: 1.0860 - val_acc: 0.4545\n",
            "Epoch 127/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8746 - acc: 0.6092 - val_loss: 1.0841 - val_acc: 0.4545\n",
            "Epoch 128/300\n",
            "87/87 [==============================] - 0s 318us/step - loss: 0.8747 - acc: 0.6092 - val_loss: 1.0847 - val_acc: 0.4545\n",
            "Epoch 129/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.8747 - acc: 0.6092 - val_loss: 1.0849 - val_acc: 0.4545\n",
            "Epoch 130/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8746 - acc: 0.6092 - val_loss: 1.0839 - val_acc: 0.4545\n",
            "Epoch 131/300\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.8742 - acc: 0.6092 - val_loss: 1.0848 - val_acc: 0.4545\n",
            "Epoch 132/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.8736 - acc: 0.6092 - val_loss: 1.0847 - val_acc: 0.4545\n",
            "Epoch 133/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8738 - acc: 0.6092 - val_loss: 1.0841 - val_acc: 0.4545\n",
            "Epoch 134/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.8737 - acc: 0.6092 - val_loss: 1.0843 - val_acc: 0.4545\n",
            "Epoch 135/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.8739 - acc: 0.6092 - val_loss: 1.0846 - val_acc: 0.4545\n",
            "Epoch 136/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.8734 - acc: 0.6207 - val_loss: 1.0847 - val_acc: 0.4545\n",
            "Epoch 137/300\n",
            "87/87 [==============================] - 0s 308us/step - loss: 0.8730 - acc: 0.6092 - val_loss: 1.0846 - val_acc: 0.4545\n",
            "Epoch 138/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8730 - acc: 0.6092 - val_loss: 1.0843 - val_acc: 0.4545\n",
            "Epoch 139/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8729 - acc: 0.6092 - val_loss: 1.0840 - val_acc: 0.4545\n",
            "Epoch 140/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8723 - acc: 0.6207 - val_loss: 1.0840 - val_acc: 0.4545\n",
            "Epoch 141/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8723 - acc: 0.6207 - val_loss: 1.0845 - val_acc: 0.4545\n",
            "Epoch 142/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.8723 - acc: 0.6207 - val_loss: 1.0838 - val_acc: 0.4545\n",
            "Epoch 143/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8720 - acc: 0.6207 - val_loss: 1.0843 - val_acc: 0.4545\n",
            "Epoch 144/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.8719 - acc: 0.6092 - val_loss: 1.0844 - val_acc: 0.4545\n",
            "Epoch 145/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8718 - acc: 0.6092 - val_loss: 1.0843 - val_acc: 0.4545\n",
            "Epoch 146/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8716 - acc: 0.6207 - val_loss: 1.0839 - val_acc: 0.4545\n",
            "Epoch 147/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8717 - acc: 0.6207 - val_loss: 1.0847 - val_acc: 0.4545\n",
            "Epoch 148/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8717 - acc: 0.6207 - val_loss: 1.0838 - val_acc: 0.4545\n",
            "Epoch 149/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8713 - acc: 0.6207 - val_loss: 1.0838 - val_acc: 0.4545\n",
            "Epoch 150/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8713 - acc: 0.6207 - val_loss: 1.0839 - val_acc: 0.4545\n",
            "Epoch 151/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.8711 - acc: 0.6207 - val_loss: 1.0841 - val_acc: 0.4545\n",
            "Epoch 152/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8709 - acc: 0.6207 - val_loss: 1.0838 - val_acc: 0.4545\n",
            "Epoch 153/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8711 - acc: 0.6207 - val_loss: 1.0840 - val_acc: 0.4545\n",
            "Epoch 154/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8713 - acc: 0.6207 - val_loss: 1.0840 - val_acc: 0.4545\n",
            "Epoch 155/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8706 - acc: 0.6207 - val_loss: 1.0831 - val_acc: 0.4545\n",
            "Epoch 156/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8705 - acc: 0.6207 - val_loss: 1.0836 - val_acc: 0.4545\n",
            "Epoch 157/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8703 - acc: 0.6207 - val_loss: 1.0835 - val_acc: 0.4545\n",
            "Epoch 158/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8699 - acc: 0.6322 - val_loss: 1.0837 - val_acc: 0.4545\n",
            "Epoch 159/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8699 - acc: 0.6322 - val_loss: 1.0841 - val_acc: 0.4545\n",
            "Epoch 160/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8697 - acc: 0.6207 - val_loss: 1.0843 - val_acc: 0.4545\n",
            "Epoch 161/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.8705 - acc: 0.6322 - val_loss: 1.0839 - val_acc: 0.4545\n",
            "Epoch 162/300\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8690 - acc: 0.6552 - val_loss: 1.0843 - val_acc: 0.4773\n",
            "Epoch 163/300\n",
            "87/87 [==============================] - 0s 304us/step - loss: 0.8694 - acc: 0.6207 - val_loss: 1.0850 - val_acc: 0.4773\n",
            "Epoch 164/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.8691 - acc: 0.6322 - val_loss: 1.0852 - val_acc: 0.4773\n",
            "Epoch 165/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.8688 - acc: 0.6437 - val_loss: 1.0849 - val_acc: 0.4773\n",
            "Epoch 166/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8687 - acc: 0.6437 - val_loss: 1.0847 - val_acc: 0.4773\n",
            "Epoch 167/300\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.8686 - acc: 0.6437 - val_loss: 1.0850 - val_acc: 0.4773\n",
            "Epoch 168/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8687 - acc: 0.6437 - val_loss: 1.0851 - val_acc: 0.4773\n",
            "Epoch 169/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.8682 - acc: 0.6552 - val_loss: 1.0855 - val_acc: 0.4773\n",
            "Epoch 170/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8680 - acc: 0.6437 - val_loss: 1.0858 - val_acc: 0.4773\n",
            "Epoch 171/300\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.8681 - acc: 0.6552 - val_loss: 1.0853 - val_acc: 0.4773\n",
            "Epoch 172/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8678 - acc: 0.6322 - val_loss: 1.0858 - val_acc: 0.4773\n",
            "Epoch 173/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8676 - acc: 0.6552 - val_loss: 1.0862 - val_acc: 0.4773\n",
            "Epoch 174/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.8674 - acc: 0.6552 - val_loss: 1.0860 - val_acc: 0.4773\n",
            "Epoch 175/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8673 - acc: 0.6437 - val_loss: 1.0860 - val_acc: 0.4773\n",
            "Epoch 176/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8673 - acc: 0.6437 - val_loss: 1.0860 - val_acc: 0.4773\n",
            "Epoch 177/300\n",
            "87/87 [==============================] - 0s 319us/step - loss: 0.8668 - acc: 0.6552 - val_loss: 1.0866 - val_acc: 0.4773\n",
            "Epoch 178/300\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.8667 - acc: 0.6322 - val_loss: 1.0868 - val_acc: 0.4773\n",
            "Epoch 179/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.8671 - acc: 0.6322 - val_loss: 1.0864 - val_acc: 0.4773\n",
            "Epoch 180/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.8668 - acc: 0.6552 - val_loss: 1.0867 - val_acc: 0.4773\n",
            "Epoch 181/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8667 - acc: 0.6552 - val_loss: 1.0870 - val_acc: 0.4773\n",
            "Epoch 182/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8661 - acc: 0.6437 - val_loss: 1.0875 - val_acc: 0.4773\n",
            "Epoch 183/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.8660 - acc: 0.6437 - val_loss: 1.0872 - val_acc: 0.4773\n",
            "Epoch 184/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.8662 - acc: 0.6437 - val_loss: 1.0871 - val_acc: 0.4773\n",
            "Epoch 185/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8660 - acc: 0.6437 - val_loss: 1.0871 - val_acc: 0.4773\n",
            "Epoch 186/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.8658 - acc: 0.6322 - val_loss: 1.0878 - val_acc: 0.4773\n",
            "Epoch 187/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8656 - acc: 0.6437 - val_loss: 1.0880 - val_acc: 0.4773\n",
            "Epoch 188/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8659 - acc: 0.6437 - val_loss: 1.0876 - val_acc: 0.4773\n",
            "Epoch 189/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8655 - acc: 0.6437 - val_loss: 1.0879 - val_acc: 0.4773\n",
            "Epoch 190/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8657 - acc: 0.6322 - val_loss: 1.0884 - val_acc: 0.4773\n",
            "Epoch 191/300\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8651 - acc: 0.6437 - val_loss: 1.0880 - val_acc: 0.4773\n",
            "Epoch 192/300\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8655 - acc: 0.6322 - val_loss: 1.0884 - val_acc: 0.4773\n",
            "Epoch 193/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8647 - acc: 0.6437 - val_loss: 1.0881 - val_acc: 0.4773\n",
            "Epoch 194/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8646 - acc: 0.6437 - val_loss: 1.0887 - val_acc: 0.4773\n",
            "Epoch 195/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8641 - acc: 0.6437 - val_loss: 1.0889 - val_acc: 0.4773\n",
            "Epoch 196/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8642 - acc: 0.6322 - val_loss: 1.0886 - val_acc: 0.4773\n",
            "Epoch 197/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.8641 - acc: 0.6322 - val_loss: 1.0891 - val_acc: 0.4773\n",
            "Epoch 198/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8643 - acc: 0.6437 - val_loss: 1.0888 - val_acc: 0.4773\n",
            "Epoch 199/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8643 - acc: 0.6322 - val_loss: 1.0890 - val_acc: 0.4773\n",
            "Epoch 200/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8635 - acc: 0.6437 - val_loss: 1.0898 - val_acc: 0.4773\n",
            "Epoch 201/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8631 - acc: 0.6437 - val_loss: 1.0894 - val_acc: 0.4773\n",
            "Epoch 202/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8635 - acc: 0.6322 - val_loss: 1.0891 - val_acc: 0.4773\n",
            "Epoch 203/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8633 - acc: 0.6207 - val_loss: 1.0900 - val_acc: 0.4773\n",
            "Epoch 204/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8628 - acc: 0.6437 - val_loss: 1.0903 - val_acc: 0.4773\n",
            "Epoch 205/300\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8631 - acc: 0.6322 - val_loss: 1.0904 - val_acc: 0.4773\n",
            "Epoch 206/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8626 - acc: 0.6322 - val_loss: 1.0909 - val_acc: 0.4773\n",
            "Epoch 207/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8621 - acc: 0.6322 - val_loss: 1.0910 - val_acc: 0.4773\n",
            "Epoch 208/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8621 - acc: 0.6322 - val_loss: 1.0916 - val_acc: 0.4773\n",
            "Epoch 209/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.8618 - acc: 0.6322 - val_loss: 1.0914 - val_acc: 0.4773\n",
            "Epoch 210/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.8625 - acc: 0.6322 - val_loss: 1.0912 - val_acc: 0.4545\n",
            "Epoch 211/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8615 - acc: 0.6322 - val_loss: 1.0928 - val_acc: 0.4545\n",
            "Epoch 212/300\n",
            "87/87 [==============================] - 0s 295us/step - loss: 0.8615 - acc: 0.6437 - val_loss: 1.0922 - val_acc: 0.4545\n",
            "Epoch 213/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.8615 - acc: 0.6437 - val_loss: 1.0919 - val_acc: 0.4545\n",
            "Epoch 214/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.8607 - acc: 0.6322 - val_loss: 1.0926 - val_acc: 0.4545\n",
            "Epoch 215/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.8605 - acc: 0.6322 - val_loss: 1.0930 - val_acc: 0.4545\n",
            "Epoch 216/300\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.8600 - acc: 0.6322 - val_loss: 1.0929 - val_acc: 0.4545\n",
            "Epoch 217/300\n",
            "87/87 [==============================] - 0s 300us/step - loss: 0.8606 - acc: 0.6322 - val_loss: 1.0929 - val_acc: 0.4545\n",
            "Epoch 218/300\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.8608 - acc: 0.6322 - val_loss: 1.0941 - val_acc: 0.4545\n",
            "Epoch 219/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8598 - acc: 0.6322 - val_loss: 1.0938 - val_acc: 0.4545\n",
            "Epoch 220/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.8603 - acc: 0.6322 - val_loss: 1.0941 - val_acc: 0.4545\n",
            "Epoch 221/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8591 - acc: 0.6322 - val_loss: 1.0944 - val_acc: 0.4545\n",
            "Epoch 222/300\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.8593 - acc: 0.6437 - val_loss: 1.0955 - val_acc: 0.4545\n",
            "Epoch 223/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8596 - acc: 0.6322 - val_loss: 1.0951 - val_acc: 0.4318\n",
            "Epoch 224/300\n",
            "87/87 [==============================] - 0s 311us/step - loss: 0.8589 - acc: 0.6322 - val_loss: 1.0962 - val_acc: 0.4545\n",
            "Epoch 225/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8583 - acc: 0.6437 - val_loss: 1.0966 - val_acc: 0.4318\n",
            "Epoch 226/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8584 - acc: 0.6322 - val_loss: 1.0968 - val_acc: 0.4318\n",
            "Epoch 227/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8578 - acc: 0.6322 - val_loss: 1.0970 - val_acc: 0.4318\n",
            "Epoch 228/300\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8576 - acc: 0.6437 - val_loss: 1.0973 - val_acc: 0.4318\n",
            "Epoch 229/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8576 - acc: 0.6322 - val_loss: 1.0979 - val_acc: 0.4318\n",
            "Epoch 230/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8576 - acc: 0.6437 - val_loss: 1.0979 - val_acc: 0.4318\n",
            "Epoch 231/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8573 - acc: 0.6437 - val_loss: 1.0982 - val_acc: 0.4318\n",
            "Epoch 232/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.8568 - acc: 0.6437 - val_loss: 1.0984 - val_acc: 0.4318\n",
            "Epoch 233/300\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8568 - acc: 0.6437 - val_loss: 1.0986 - val_acc: 0.4318\n",
            "Epoch 234/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8569 - acc: 0.6437 - val_loss: 1.0988 - val_acc: 0.4318\n",
            "Epoch 235/300\n",
            "87/87 [==============================] - 0s 318us/step - loss: 0.8563 - acc: 0.6552 - val_loss: 1.0993 - val_acc: 0.4318\n",
            "Epoch 236/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.8562 - acc: 0.6437 - val_loss: 1.0990 - val_acc: 0.4318\n",
            "Epoch 237/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.8560 - acc: 0.6437 - val_loss: 1.0996 - val_acc: 0.4318\n",
            "Epoch 238/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8557 - acc: 0.6437 - val_loss: 1.0998 - val_acc: 0.4318\n",
            "Epoch 239/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8561 - acc: 0.6437 - val_loss: 1.1005 - val_acc: 0.4318\n",
            "Epoch 240/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8556 - acc: 0.6667 - val_loss: 1.1009 - val_acc: 0.4318\n",
            "Epoch 241/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8553 - acc: 0.6437 - val_loss: 1.1011 - val_acc: 0.4318\n",
            "Epoch 242/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8557 - acc: 0.6667 - val_loss: 1.1013 - val_acc: 0.4318\n",
            "Epoch 243/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8553 - acc: 0.6552 - val_loss: 1.1019 - val_acc: 0.4318\n",
            "Epoch 244/300\n",
            "87/87 [==============================] - 0s 316us/step - loss: 0.8547 - acc: 0.6552 - val_loss: 1.1017 - val_acc: 0.4545\n",
            "Epoch 245/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.8545 - acc: 0.6667 - val_loss: 1.1018 - val_acc: 0.4545\n",
            "Epoch 246/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8543 - acc: 0.6667 - val_loss: 1.1023 - val_acc: 0.4545\n",
            "Epoch 247/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8541 - acc: 0.6667 - val_loss: 1.1024 - val_acc: 0.4545\n",
            "Epoch 248/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8540 - acc: 0.6667 - val_loss: 1.1025 - val_acc: 0.4545\n",
            "Epoch 249/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8537 - acc: 0.6667 - val_loss: 1.1026 - val_acc: 0.4545\n",
            "Epoch 250/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8535 - acc: 0.6667 - val_loss: 1.1030 - val_acc: 0.4545\n",
            "Epoch 251/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8533 - acc: 0.6552 - val_loss: 1.1035 - val_acc: 0.4545\n",
            "Epoch 252/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8534 - acc: 0.6667 - val_loss: 1.1034 - val_acc: 0.4545\n",
            "Epoch 253/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8529 - acc: 0.6667 - val_loss: 1.1034 - val_acc: 0.4545\n",
            "Epoch 254/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8524 - acc: 0.6552 - val_loss: 1.1036 - val_acc: 0.4773\n",
            "Epoch 255/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8521 - acc: 0.6667 - val_loss: 1.1036 - val_acc: 0.4545\n",
            "Epoch 256/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.8523 - acc: 0.6667 - val_loss: 1.1035 - val_acc: 0.4773\n",
            "Epoch 257/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8527 - acc: 0.6552 - val_loss: 1.1040 - val_acc: 0.4773\n",
            "Epoch 258/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8515 - acc: 0.6667 - val_loss: 1.1039 - val_acc: 0.4773\n",
            "Epoch 259/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8512 - acc: 0.6667 - val_loss: 1.1037 - val_acc: 0.4773\n",
            "Epoch 260/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8514 - acc: 0.6667 - val_loss: 1.1041 - val_acc: 0.4545\n",
            "Epoch 261/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8514 - acc: 0.6667 - val_loss: 1.1039 - val_acc: 0.4773\n",
            "Epoch 262/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8505 - acc: 0.6667 - val_loss: 1.1040 - val_acc: 0.4773\n",
            "Epoch 263/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8504 - acc: 0.6667 - val_loss: 1.1052 - val_acc: 0.4773\n",
            "Epoch 264/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.8514 - acc: 0.6667 - val_loss: 1.1046 - val_acc: 0.4773\n",
            "Epoch 265/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8501 - acc: 0.6667 - val_loss: 1.1051 - val_acc: 0.4773\n",
            "Epoch 266/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.8499 - acc: 0.6667 - val_loss: 1.1051 - val_acc: 0.4773\n",
            "Epoch 267/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8494 - acc: 0.6667 - val_loss: 1.1055 - val_acc: 0.4773\n",
            "Epoch 268/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8495 - acc: 0.6552 - val_loss: 1.1058 - val_acc: 0.4773\n",
            "Epoch 269/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8497 - acc: 0.6667 - val_loss: 1.1051 - val_acc: 0.4773\n",
            "Epoch 270/300\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.8487 - acc: 0.6667 - val_loss: 1.1060 - val_acc: 0.4773\n",
            "Epoch 271/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.8491 - acc: 0.6667 - val_loss: 1.1060 - val_acc: 0.4773\n",
            "Epoch 272/300\n",
            "87/87 [==============================] - 0s 298us/step - loss: 0.8486 - acc: 0.6667 - val_loss: 1.1056 - val_acc: 0.4773\n",
            "Epoch 273/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8487 - acc: 0.6667 - val_loss: 1.1059 - val_acc: 0.4773\n",
            "Epoch 274/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8481 - acc: 0.6667 - val_loss: 1.1067 - val_acc: 0.4773\n",
            "Epoch 275/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8480 - acc: 0.6667 - val_loss: 1.1070 - val_acc: 0.4773\n",
            "Epoch 276/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8478 - acc: 0.6667 - val_loss: 1.1067 - val_acc: 0.4773\n",
            "Epoch 277/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.8471 - acc: 0.6667 - val_loss: 1.1067 - val_acc: 0.4773\n",
            "Epoch 278/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.8471 - acc: 0.6667 - val_loss: 1.1070 - val_acc: 0.4773\n",
            "Epoch 279/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8472 - acc: 0.6667 - val_loss: 1.1077 - val_acc: 0.4773\n",
            "Epoch 280/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8468 - acc: 0.6667 - val_loss: 1.1074 - val_acc: 0.4773\n",
            "Epoch 281/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8465 - acc: 0.6667 - val_loss: 1.1079 - val_acc: 0.4773\n",
            "Epoch 282/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8465 - acc: 0.6667 - val_loss: 1.1080 - val_acc: 0.4773\n",
            "Epoch 283/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8461 - acc: 0.6667 - val_loss: 1.1093 - val_acc: 0.4773\n",
            "Epoch 284/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8460 - acc: 0.6552 - val_loss: 1.1089 - val_acc: 0.4773\n",
            "Epoch 285/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8460 - acc: 0.6667 - val_loss: 1.1091 - val_acc: 0.4773\n",
            "Epoch 286/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8459 - acc: 0.6667 - val_loss: 1.1091 - val_acc: 0.4773\n",
            "Epoch 287/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.8462 - acc: 0.6667 - val_loss: 1.1097 - val_acc: 0.4773\n",
            "Epoch 288/300\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.8451 - acc: 0.6667 - val_loss: 1.1096 - val_acc: 0.4773\n",
            "Epoch 289/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8445 - acc: 0.6667 - val_loss: 1.1095 - val_acc: 0.4773\n",
            "Epoch 290/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8447 - acc: 0.6667 - val_loss: 1.1099 - val_acc: 0.4773\n",
            "Epoch 291/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8446 - acc: 0.6667 - val_loss: 1.1103 - val_acc: 0.4773\n",
            "Epoch 292/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8441 - acc: 0.6667 - val_loss: 1.1105 - val_acc: 0.4773\n",
            "Epoch 293/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8439 - acc: 0.6552 - val_loss: 1.1103 - val_acc: 0.4773\n",
            "Epoch 294/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8436 - acc: 0.6667 - val_loss: 1.1104 - val_acc: 0.4773\n",
            "Epoch 295/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8441 - acc: 0.6667 - val_loss: 1.1110 - val_acc: 0.4773\n",
            "Epoch 296/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8432 - acc: 0.6667 - val_loss: 1.1108 - val_acc: 0.4773\n",
            "Epoch 297/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8436 - acc: 0.6667 - val_loss: 1.1111 - val_acc: 0.4773\n",
            "Epoch 298/300\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.8433 - acc: 0.6667 - val_loss: 1.1119 - val_acc: 0.4773\n",
            "Epoch 299/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8430 - acc: 0.6552 - val_loss: 1.1114 - val_acc: 0.4773\n",
            "Epoch 300/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8426 - acc: 0.6667 - val_loss: 1.1120 - val_acc: 0.4773\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/300\n",
            "87/87 [==============================] - 1s 7ms/step - loss: 1.3376 - acc: 0.4368 - val_loss: 1.0736 - val_acc: 0.4318\n",
            "Epoch 2/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 1.3044 - acc: 0.4368 - val_loss: 1.0500 - val_acc: 0.4318\n",
            "Epoch 3/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.2765 - acc: 0.4598 - val_loss: 1.0314 - val_acc: 0.4318\n",
            "Epoch 4/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 1.2493 - acc: 0.4483 - val_loss: 1.0136 - val_acc: 0.4545\n",
            "Epoch 5/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 1.2244 - acc: 0.4483 - val_loss: 0.9995 - val_acc: 0.4545\n",
            "Epoch 6/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 1.2041 - acc: 0.4598 - val_loss: 0.9857 - val_acc: 0.5000\n",
            "Epoch 7/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 1.1857 - acc: 0.4598 - val_loss: 0.9725 - val_acc: 0.5227\n",
            "Epoch 8/300\n",
            "87/87 [==============================] - 0s 204us/step - loss: 1.1688 - acc: 0.4713 - val_loss: 0.9644 - val_acc: 0.5227\n",
            "Epoch 9/300\n",
            "87/87 [==============================] - 0s 194us/step - loss: 1.1536 - acc: 0.4713 - val_loss: 0.9538 - val_acc: 0.5455\n",
            "Epoch 10/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.1409 - acc: 0.4713 - val_loss: 0.9468 - val_acc: 0.5455\n",
            "Epoch 11/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 1.1284 - acc: 0.4713 - val_loss: 0.9388 - val_acc: 0.5227\n",
            "Epoch 12/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 1.1171 - acc: 0.4828 - val_loss: 0.9314 - val_acc: 0.5227\n",
            "Epoch 13/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.1077 - acc: 0.5057 - val_loss: 0.9232 - val_acc: 0.5682\n",
            "Epoch 14/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.0962 - acc: 0.5057 - val_loss: 0.9190 - val_acc: 0.5682\n",
            "Epoch 15/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 1.0884 - acc: 0.5057 - val_loss: 0.9134 - val_acc: 0.5682\n",
            "Epoch 16/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 1.0787 - acc: 0.5172 - val_loss: 0.9083 - val_acc: 0.5909\n",
            "Epoch 17/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 1.0703 - acc: 0.5172 - val_loss: 0.9037 - val_acc: 0.5682\n",
            "Epoch 18/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 1.0621 - acc: 0.5517 - val_loss: 0.8998 - val_acc: 0.5682\n",
            "Epoch 19/300\n",
            "87/87 [==============================] - 0s 286us/step - loss: 1.0547 - acc: 0.5517 - val_loss: 0.8966 - val_acc: 0.5682\n",
            "Epoch 20/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 1.0483 - acc: 0.5402 - val_loss: 0.8932 - val_acc: 0.5682\n",
            "Epoch 21/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 1.0416 - acc: 0.5402 - val_loss: 0.8907 - val_acc: 0.5682\n",
            "Epoch 22/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 1.0359 - acc: 0.5402 - val_loss: 0.8888 - val_acc: 0.5682\n",
            "Epoch 23/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 1.0298 - acc: 0.5517 - val_loss: 0.8871 - val_acc: 0.5682\n",
            "Epoch 24/300\n",
            "87/87 [==============================] - 0s 225us/step - loss: 1.0257 - acc: 0.5402 - val_loss: 0.8860 - val_acc: 0.5909\n",
            "Epoch 25/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 1.0211 - acc: 0.5517 - val_loss: 0.8857 - val_acc: 0.5909\n",
            "Epoch 26/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 1.0183 - acc: 0.5402 - val_loss: 0.8847 - val_acc: 0.6136\n",
            "Epoch 27/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.0144 - acc: 0.5402 - val_loss: 0.8837 - val_acc: 0.6136\n",
            "Epoch 28/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 1.0105 - acc: 0.5517 - val_loss: 0.8826 - val_acc: 0.6136\n",
            "Epoch 29/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 1.0073 - acc: 0.5517 - val_loss: 0.8817 - val_acc: 0.6136\n",
            "Epoch 30/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 1.0032 - acc: 0.5517 - val_loss: 0.8815 - val_acc: 0.6136\n",
            "Epoch 31/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.9999 - acc: 0.5402 - val_loss: 0.8807 - val_acc: 0.6136\n",
            "Epoch 32/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9969 - acc: 0.5402 - val_loss: 0.8805 - val_acc: 0.6136\n",
            "Epoch 33/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.9941 - acc: 0.5402 - val_loss: 0.8799 - val_acc: 0.6136\n",
            "Epoch 34/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.9922 - acc: 0.5402 - val_loss: 0.8800 - val_acc: 0.6136\n",
            "Epoch 35/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.9887 - acc: 0.5402 - val_loss: 0.8792 - val_acc: 0.6136\n",
            "Epoch 36/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9873 - acc: 0.5402 - val_loss: 0.8786 - val_acc: 0.6136\n",
            "Epoch 37/300\n",
            "87/87 [==============================] - 0s 304us/step - loss: 0.9840 - acc: 0.5517 - val_loss: 0.8795 - val_acc: 0.5909\n",
            "Epoch 38/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.9811 - acc: 0.5402 - val_loss: 0.8790 - val_acc: 0.6136\n",
            "Epoch 39/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.9791 - acc: 0.5632 - val_loss: 0.8790 - val_acc: 0.5909\n",
            "Epoch 40/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.9769 - acc: 0.5747 - val_loss: 0.8785 - val_acc: 0.6136\n",
            "Epoch 41/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.9748 - acc: 0.5632 - val_loss: 0.8784 - val_acc: 0.6364\n",
            "Epoch 42/300\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.9728 - acc: 0.5747 - val_loss: 0.8781 - val_acc: 0.6591\n",
            "Epoch 43/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.9711 - acc: 0.5632 - val_loss: 0.8784 - val_acc: 0.6591\n",
            "Epoch 44/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.9687 - acc: 0.5747 - val_loss: 0.8789 - val_acc: 0.6591\n",
            "Epoch 45/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9662 - acc: 0.5747 - val_loss: 0.8784 - val_acc: 0.6591\n",
            "Epoch 46/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.9649 - acc: 0.5747 - val_loss: 0.8792 - val_acc: 0.6591\n",
            "Epoch 47/300\n",
            "87/87 [==============================] - 0s 195us/step - loss: 0.9635 - acc: 0.5632 - val_loss: 0.8787 - val_acc: 0.6591\n",
            "Epoch 48/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.9614 - acc: 0.5747 - val_loss: 0.8786 - val_acc: 0.6591\n",
            "Epoch 49/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.9597 - acc: 0.5862 - val_loss: 0.8792 - val_acc: 0.6591\n",
            "Epoch 50/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9575 - acc: 0.5862 - val_loss: 0.8790 - val_acc: 0.6591\n",
            "Epoch 51/300\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.9555 - acc: 0.5862 - val_loss: 0.8797 - val_acc: 0.6591\n",
            "Epoch 52/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9538 - acc: 0.5862 - val_loss: 0.8799 - val_acc: 0.6591\n",
            "Epoch 53/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.9526 - acc: 0.5862 - val_loss: 0.8803 - val_acc: 0.6591\n",
            "Epoch 54/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9503 - acc: 0.5862 - val_loss: 0.8805 - val_acc: 0.6591\n",
            "Epoch 55/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.9484 - acc: 0.5862 - val_loss: 0.8810 - val_acc: 0.6591\n",
            "Epoch 56/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9474 - acc: 0.5862 - val_loss: 0.8815 - val_acc: 0.6591\n",
            "Epoch 57/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.9452 - acc: 0.5862 - val_loss: 0.8815 - val_acc: 0.6591\n",
            "Epoch 58/300\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.9440 - acc: 0.5862 - val_loss: 0.8818 - val_acc: 0.6591\n",
            "Epoch 59/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.9428 - acc: 0.5862 - val_loss: 0.8825 - val_acc: 0.6591\n",
            "Epoch 60/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9408 - acc: 0.5862 - val_loss: 0.8832 - val_acc: 0.6591\n",
            "Epoch 61/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.9389 - acc: 0.5862 - val_loss: 0.8831 - val_acc: 0.6591\n",
            "Epoch 62/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.9380 - acc: 0.5862 - val_loss: 0.8834 - val_acc: 0.6591\n",
            "Epoch 63/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.9355 - acc: 0.5862 - val_loss: 0.8840 - val_acc: 0.6818\n",
            "Epoch 64/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.9348 - acc: 0.5977 - val_loss: 0.8841 - val_acc: 0.6818\n",
            "Epoch 65/300\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.9338 - acc: 0.5977 - val_loss: 0.8843 - val_acc: 0.6818\n",
            "Epoch 66/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.9326 - acc: 0.5977 - val_loss: 0.8848 - val_acc: 0.6818\n",
            "Epoch 67/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.9313 - acc: 0.5977 - val_loss: 0.8847 - val_acc: 0.6818\n",
            "Epoch 68/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.9311 - acc: 0.5977 - val_loss: 0.8843 - val_acc: 0.6818\n",
            "Epoch 69/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9295 - acc: 0.5977 - val_loss: 0.8853 - val_acc: 0.6818\n",
            "Epoch 70/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.9293 - acc: 0.5862 - val_loss: 0.8851 - val_acc: 0.6818\n",
            "Epoch 71/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.9293 - acc: 0.5977 - val_loss: 0.8851 - val_acc: 0.6818\n",
            "Epoch 72/300\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.9269 - acc: 0.5977 - val_loss: 0.8859 - val_acc: 0.6818\n",
            "Epoch 73/300\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.9262 - acc: 0.5977 - val_loss: 0.8855 - val_acc: 0.6818\n",
            "Epoch 74/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.9256 - acc: 0.5977 - val_loss: 0.8854 - val_acc: 0.6818\n",
            "Epoch 75/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.9246 - acc: 0.5977 - val_loss: 0.8865 - val_acc: 0.6818\n",
            "Epoch 76/300\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.9241 - acc: 0.5977 - val_loss: 0.8870 - val_acc: 0.6818\n",
            "Epoch 77/300\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.9231 - acc: 0.5977 - val_loss: 0.8868 - val_acc: 0.6818\n",
            "Epoch 78/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9223 - acc: 0.5977 - val_loss: 0.8871 - val_acc: 0.6818\n",
            "Epoch 79/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9213 - acc: 0.5977 - val_loss: 0.8875 - val_acc: 0.7045\n",
            "Epoch 80/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9207 - acc: 0.5977 - val_loss: 0.8875 - val_acc: 0.7045\n",
            "Epoch 81/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.9197 - acc: 0.5977 - val_loss: 0.8886 - val_acc: 0.7045\n",
            "Epoch 82/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9185 - acc: 0.5977 - val_loss: 0.8882 - val_acc: 0.7045\n",
            "Epoch 83/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.9181 - acc: 0.5977 - val_loss: 0.8883 - val_acc: 0.7045\n",
            "Epoch 84/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9173 - acc: 0.5977 - val_loss: 0.8889 - val_acc: 0.7045\n",
            "Epoch 85/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9165 - acc: 0.5977 - val_loss: 0.8888 - val_acc: 0.7045\n",
            "Epoch 86/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.9157 - acc: 0.5977 - val_loss: 0.8893 - val_acc: 0.7045\n",
            "Epoch 87/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.9154 - acc: 0.5977 - val_loss: 0.8895 - val_acc: 0.7045\n",
            "Epoch 88/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.9149 - acc: 0.6092 - val_loss: 0.8894 - val_acc: 0.7045\n",
            "Epoch 89/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.9142 - acc: 0.5862 - val_loss: 0.8903 - val_acc: 0.7045\n",
            "Epoch 90/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.9134 - acc: 0.5977 - val_loss: 0.8907 - val_acc: 0.7045\n",
            "Epoch 91/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.9123 - acc: 0.6092 - val_loss: 0.8908 - val_acc: 0.7045\n",
            "Epoch 92/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9121 - acc: 0.5862 - val_loss: 0.8919 - val_acc: 0.7045\n",
            "Epoch 93/300\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.9112 - acc: 0.5977 - val_loss: 0.8923 - val_acc: 0.7045\n",
            "Epoch 94/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.9103 - acc: 0.6092 - val_loss: 0.8925 - val_acc: 0.7045\n",
            "Epoch 95/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.9100 - acc: 0.5977 - val_loss: 0.8929 - val_acc: 0.7045\n",
            "Epoch 96/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.9097 - acc: 0.5977 - val_loss: 0.8932 - val_acc: 0.7045\n",
            "Epoch 97/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.9088 - acc: 0.5977 - val_loss: 0.8932 - val_acc: 0.7045\n",
            "Epoch 98/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9083 - acc: 0.5977 - val_loss: 0.8937 - val_acc: 0.7045\n",
            "Epoch 99/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.9075 - acc: 0.5977 - val_loss: 0.8941 - val_acc: 0.7045\n",
            "Epoch 100/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.9068 - acc: 0.5977 - val_loss: 0.8946 - val_acc: 0.7045\n",
            "Epoch 101/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.9068 - acc: 0.6092 - val_loss: 0.8947 - val_acc: 0.7045\n",
            "Epoch 102/300\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.9062 - acc: 0.5977 - val_loss: 0.8952 - val_acc: 0.7045\n",
            "Epoch 103/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9052 - acc: 0.5977 - val_loss: 0.8959 - val_acc: 0.7045\n",
            "Epoch 104/300\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.9048 - acc: 0.5977 - val_loss: 0.8958 - val_acc: 0.7045\n",
            "Epoch 105/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.9036 - acc: 0.5977 - val_loss: 0.8967 - val_acc: 0.7045\n",
            "Epoch 106/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.9034 - acc: 0.5977 - val_loss: 0.8965 - val_acc: 0.7045\n",
            "Epoch 107/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.9027 - acc: 0.5977 - val_loss: 0.8972 - val_acc: 0.7045\n",
            "Epoch 108/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.9024 - acc: 0.5977 - val_loss: 0.8977 - val_acc: 0.7045\n",
            "Epoch 109/300\n",
            "87/87 [==============================] - 0s 293us/step - loss: 0.9015 - acc: 0.5977 - val_loss: 0.8978 - val_acc: 0.7045\n",
            "Epoch 110/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.9011 - acc: 0.5977 - val_loss: 0.8980 - val_acc: 0.7045\n",
            "Epoch 111/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.9005 - acc: 0.5977 - val_loss: 0.8985 - val_acc: 0.7045\n",
            "Epoch 112/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9001 - acc: 0.5977 - val_loss: 0.8992 - val_acc: 0.7045\n",
            "Epoch 113/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.8995 - acc: 0.5977 - val_loss: 0.8996 - val_acc: 0.7045\n",
            "Epoch 114/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8990 - acc: 0.5977 - val_loss: 0.8995 - val_acc: 0.7045\n",
            "Epoch 115/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8985 - acc: 0.5977 - val_loss: 0.9001 - val_acc: 0.7045\n",
            "Epoch 116/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.8976 - acc: 0.5977 - val_loss: 0.9004 - val_acc: 0.7045\n",
            "Epoch 117/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8970 - acc: 0.5977 - val_loss: 0.9010 - val_acc: 0.7045\n",
            "Epoch 118/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8965 - acc: 0.5977 - val_loss: 0.9012 - val_acc: 0.6818\n",
            "Epoch 119/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8965 - acc: 0.5977 - val_loss: 0.9017 - val_acc: 0.7045\n",
            "Epoch 120/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8952 - acc: 0.5977 - val_loss: 0.9024 - val_acc: 0.6818\n",
            "Epoch 121/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8953 - acc: 0.5977 - val_loss: 0.9028 - val_acc: 0.6818\n",
            "Epoch 122/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.8947 - acc: 0.5977 - val_loss: 0.9032 - val_acc: 0.6818\n",
            "Epoch 123/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8946 - acc: 0.5977 - val_loss: 0.9035 - val_acc: 0.6818\n",
            "Epoch 124/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8932 - acc: 0.5977 - val_loss: 0.9040 - val_acc: 0.6818\n",
            "Epoch 125/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8934 - acc: 0.5977 - val_loss: 0.9045 - val_acc: 0.6818\n",
            "Epoch 126/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.8928 - acc: 0.5977 - val_loss: 0.9045 - val_acc: 0.6818\n",
            "Epoch 127/300\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.8924 - acc: 0.5977 - val_loss: 0.9047 - val_acc: 0.6818\n",
            "Epoch 128/300\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.8922 - acc: 0.5977 - val_loss: 0.9057 - val_acc: 0.6818\n",
            "Epoch 129/300\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.8912 - acc: 0.5977 - val_loss: 0.9055 - val_acc: 0.6818\n",
            "Epoch 130/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8918 - acc: 0.5977 - val_loss: 0.9059 - val_acc: 0.6818\n",
            "Epoch 131/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8913 - acc: 0.5977 - val_loss: 0.9065 - val_acc: 0.6818\n",
            "Epoch 132/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8900 - acc: 0.5977 - val_loss: 0.9070 - val_acc: 0.6818\n",
            "Epoch 133/300\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8897 - acc: 0.5977 - val_loss: 0.9070 - val_acc: 0.6818\n",
            "Epoch 134/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8892 - acc: 0.5977 - val_loss: 0.9075 - val_acc: 0.6818\n",
            "Epoch 135/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8891 - acc: 0.5977 - val_loss: 0.9079 - val_acc: 0.6818\n",
            "Epoch 136/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8888 - acc: 0.5977 - val_loss: 0.9085 - val_acc: 0.6818\n",
            "Epoch 137/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8879 - acc: 0.5977 - val_loss: 0.9089 - val_acc: 0.6818\n",
            "Epoch 138/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8871 - acc: 0.5977 - val_loss: 0.9099 - val_acc: 0.6818\n",
            "Epoch 139/300\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.8870 - acc: 0.5977 - val_loss: 0.9099 - val_acc: 0.6818\n",
            "Epoch 140/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8867 - acc: 0.5977 - val_loss: 0.9103 - val_acc: 0.6818\n",
            "Epoch 141/300\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8857 - acc: 0.5977 - val_loss: 0.9109 - val_acc: 0.6818\n",
            "Epoch 142/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.8858 - acc: 0.5977 - val_loss: 0.9111 - val_acc: 0.6818\n",
            "Epoch 143/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8852 - acc: 0.5977 - val_loss: 0.9116 - val_acc: 0.6818\n",
            "Epoch 144/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8849 - acc: 0.5977 - val_loss: 0.9119 - val_acc: 0.6818\n",
            "Epoch 145/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8847 - acc: 0.5977 - val_loss: 0.9126 - val_acc: 0.6818\n",
            "Epoch 146/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8833 - acc: 0.5977 - val_loss: 0.9126 - val_acc: 0.6818\n",
            "Epoch 147/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8833 - acc: 0.5977 - val_loss: 0.9134 - val_acc: 0.6818\n",
            "Epoch 148/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8834 - acc: 0.5977 - val_loss: 0.9133 - val_acc: 0.6818\n",
            "Epoch 149/300\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.8833 - acc: 0.5977 - val_loss: 0.9131 - val_acc: 0.6818\n",
            "Epoch 150/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8822 - acc: 0.5977 - val_loss: 0.9139 - val_acc: 0.6818\n",
            "Epoch 151/300\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.8817 - acc: 0.5977 - val_loss: 0.9146 - val_acc: 0.6818\n",
            "Epoch 152/300\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.8826 - acc: 0.5977 - val_loss: 0.9144 - val_acc: 0.6818\n",
            "Epoch 153/300\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.8812 - acc: 0.5977 - val_loss: 0.9150 - val_acc: 0.6818\n",
            "Epoch 154/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8808 - acc: 0.5977 - val_loss: 0.9150 - val_acc: 0.7045\n",
            "Epoch 155/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8807 - acc: 0.5977 - val_loss: 0.9153 - val_acc: 0.6818\n",
            "Epoch 156/300\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8809 - acc: 0.5977 - val_loss: 0.9156 - val_acc: 0.6818\n",
            "Epoch 157/300\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8794 - acc: 0.5977 - val_loss: 0.9159 - val_acc: 0.7045\n",
            "Epoch 158/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8788 - acc: 0.5977 - val_loss: 0.9164 - val_acc: 0.7045\n",
            "Epoch 159/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8789 - acc: 0.5977 - val_loss: 0.9161 - val_acc: 0.6818\n",
            "Epoch 160/300\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8786 - acc: 0.5977 - val_loss: 0.9165 - val_acc: 0.6818\n",
            "Epoch 161/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8787 - acc: 0.5977 - val_loss: 0.9168 - val_acc: 0.7045\n",
            "Epoch 162/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8778 - acc: 0.5977 - val_loss: 0.9173 - val_acc: 0.7045\n",
            "Epoch 163/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8777 - acc: 0.5977 - val_loss: 0.9175 - val_acc: 0.7045\n",
            "Epoch 164/300\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.8769 - acc: 0.5977 - val_loss: 0.9179 - val_acc: 0.7045\n",
            "Epoch 165/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8767 - acc: 0.5977 - val_loss: 0.9179 - val_acc: 0.7045\n",
            "Epoch 166/300\n",
            "87/87 [==============================] - 0s 291us/step - loss: 0.8766 - acc: 0.5977 - val_loss: 0.9183 - val_acc: 0.7045\n",
            "Epoch 167/300\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.8763 - acc: 0.5977 - val_loss: 0.9181 - val_acc: 0.7045\n",
            "Epoch 168/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8754 - acc: 0.5977 - val_loss: 0.9182 - val_acc: 0.7045\n",
            "Epoch 169/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.8752 - acc: 0.5977 - val_loss: 0.9190 - val_acc: 0.7045\n",
            "Epoch 170/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.8749 - acc: 0.5977 - val_loss: 0.9191 - val_acc: 0.7045\n",
            "Epoch 171/300\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.8741 - acc: 0.5977 - val_loss: 0.9197 - val_acc: 0.7045\n",
            "Epoch 172/300\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.8737 - acc: 0.5977 - val_loss: 0.9198 - val_acc: 0.7045\n",
            "Epoch 173/300\n",
            "87/87 [==============================] - 0s 310us/step - loss: 0.8736 - acc: 0.5977 - val_loss: 0.9196 - val_acc: 0.7045\n",
            "Epoch 174/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8729 - acc: 0.5977 - val_loss: 0.9198 - val_acc: 0.6818\n",
            "Epoch 175/300\n",
            "87/87 [==============================] - 0s 362us/step - loss: 0.8734 - acc: 0.5977 - val_loss: 0.9202 - val_acc: 0.7045\n",
            "Epoch 176/300\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.8729 - acc: 0.5977 - val_loss: 0.9204 - val_acc: 0.6818\n",
            "Epoch 177/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.8723 - acc: 0.6092 - val_loss: 0.9201 - val_acc: 0.6818\n",
            "Epoch 178/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.8715 - acc: 0.5977 - val_loss: 0.9209 - val_acc: 0.6818\n",
            "Epoch 179/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.8723 - acc: 0.5977 - val_loss: 0.9215 - val_acc: 0.7045\n",
            "Epoch 180/300\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.8708 - acc: 0.5977 - val_loss: 0.9215 - val_acc: 0.6818\n",
            "Epoch 181/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8707 - acc: 0.5977 - val_loss: 0.9215 - val_acc: 0.6818\n",
            "Epoch 182/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.8700 - acc: 0.5977 - val_loss: 0.9220 - val_acc: 0.6818\n",
            "Epoch 183/300\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.8697 - acc: 0.5977 - val_loss: 0.9215 - val_acc: 0.6818\n",
            "Epoch 184/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8688 - acc: 0.5977 - val_loss: 0.9226 - val_acc: 0.6818\n",
            "Epoch 185/300\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.8696 - acc: 0.5977 - val_loss: 0.9227 - val_acc: 0.6818\n",
            "Epoch 186/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8682 - acc: 0.5977 - val_loss: 0.9231 - val_acc: 0.6591\n",
            "Epoch 187/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8688 - acc: 0.5977 - val_loss: 0.9231 - val_acc: 0.6818\n",
            "Epoch 188/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8674 - acc: 0.5977 - val_loss: 0.9240 - val_acc: 0.6818\n",
            "Epoch 189/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8671 - acc: 0.5977 - val_loss: 0.9241 - val_acc: 0.6591\n",
            "Epoch 190/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8673 - acc: 0.5977 - val_loss: 0.9246 - val_acc: 0.6818\n",
            "Epoch 191/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8669 - acc: 0.6092 - val_loss: 0.9242 - val_acc: 0.6818\n",
            "Epoch 192/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8664 - acc: 0.6092 - val_loss: 0.9253 - val_acc: 0.6818\n",
            "Epoch 193/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8660 - acc: 0.6092 - val_loss: 0.9256 - val_acc: 0.6818\n",
            "Epoch 194/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8657 - acc: 0.5977 - val_loss: 0.9252 - val_acc: 0.6818\n",
            "Epoch 195/300\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8654 - acc: 0.6092 - val_loss: 0.9253 - val_acc: 0.6818\n",
            "Epoch 196/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8651 - acc: 0.6092 - val_loss: 0.9262 - val_acc: 0.6818\n",
            "Epoch 197/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8641 - acc: 0.5977 - val_loss: 0.9263 - val_acc: 0.6818\n",
            "Epoch 198/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8645 - acc: 0.5977 - val_loss: 0.9266 - val_acc: 0.6818\n",
            "Epoch 199/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8642 - acc: 0.6092 - val_loss: 0.9263 - val_acc: 0.6591\n",
            "Epoch 200/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.8638 - acc: 0.6092 - val_loss: 0.9273 - val_acc: 0.6818\n",
            "Epoch 201/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8627 - acc: 0.5977 - val_loss: 0.9271 - val_acc: 0.6818\n",
            "Epoch 202/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8626 - acc: 0.6207 - val_loss: 0.9273 - val_acc: 0.6818\n",
            "Epoch 203/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.8627 - acc: 0.5977 - val_loss: 0.9278 - val_acc: 0.6818\n",
            "Epoch 204/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8612 - acc: 0.6092 - val_loss: 0.9282 - val_acc: 0.6818\n",
            "Epoch 205/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8613 - acc: 0.6092 - val_loss: 0.9290 - val_acc: 0.6818\n",
            "Epoch 206/300\n",
            "87/87 [==============================] - 0s 278us/step - loss: 0.8612 - acc: 0.6207 - val_loss: 0.9291 - val_acc: 0.6818\n",
            "Epoch 207/300\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.8612 - acc: 0.6092 - val_loss: 0.9298 - val_acc: 0.6591\n",
            "Epoch 208/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8605 - acc: 0.6207 - val_loss: 0.9296 - val_acc: 0.6818\n",
            "Epoch 209/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8607 - acc: 0.6207 - val_loss: 0.9303 - val_acc: 0.6818\n",
            "Epoch 210/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.8599 - acc: 0.6207 - val_loss: 0.9303 - val_acc: 0.6818\n",
            "Epoch 211/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8597 - acc: 0.6092 - val_loss: 0.9310 - val_acc: 0.6818\n",
            "Epoch 212/300\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.8594 - acc: 0.6207 - val_loss: 0.9315 - val_acc: 0.6818\n",
            "Epoch 213/300\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.8588 - acc: 0.6207 - val_loss: 0.9320 - val_acc: 0.6818\n",
            "Epoch 214/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8592 - acc: 0.6207 - val_loss: 0.9321 - val_acc: 0.6818\n",
            "Epoch 215/300\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.8584 - acc: 0.6207 - val_loss: 0.9324 - val_acc: 0.6818\n",
            "Epoch 216/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8583 - acc: 0.6207 - val_loss: 0.9324 - val_acc: 0.6818\n",
            "Epoch 217/300\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8573 - acc: 0.6207 - val_loss: 0.9337 - val_acc: 0.6818\n",
            "Epoch 218/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.8573 - acc: 0.6207 - val_loss: 0.9336 - val_acc: 0.6818\n",
            "Epoch 219/300\n",
            "87/87 [==============================] - 0s 327us/step - loss: 0.8571 - acc: 0.6207 - val_loss: 0.9336 - val_acc: 0.6818\n",
            "Epoch 220/300\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.8571 - acc: 0.6437 - val_loss: 0.9341 - val_acc: 0.6591\n",
            "Epoch 221/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8560 - acc: 0.6437 - val_loss: 0.9347 - val_acc: 0.6591\n",
            "Epoch 222/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8560 - acc: 0.6437 - val_loss: 0.9353 - val_acc: 0.6591\n",
            "Epoch 223/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8554 - acc: 0.6207 - val_loss: 0.9358 - val_acc: 0.6818\n",
            "Epoch 224/300\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.8549 - acc: 0.6437 - val_loss: 0.9359 - val_acc: 0.6818\n",
            "Epoch 225/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8544 - acc: 0.6437 - val_loss: 0.9359 - val_acc: 0.6818\n",
            "Epoch 226/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8551 - acc: 0.6437 - val_loss: 0.9368 - val_acc: 0.6818\n",
            "Epoch 227/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8544 - acc: 0.6437 - val_loss: 0.9365 - val_acc: 0.6591\n",
            "Epoch 228/300\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.8544 - acc: 0.6437 - val_loss: 0.9373 - val_acc: 0.6591\n",
            "Epoch 229/300\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.8540 - acc: 0.6437 - val_loss: 0.9380 - val_acc: 0.6591\n",
            "Epoch 230/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.8533 - acc: 0.6437 - val_loss: 0.9378 - val_acc: 0.6591\n",
            "Epoch 231/300\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8534 - acc: 0.6437 - val_loss: 0.9382 - val_acc: 0.6591\n",
            "Epoch 232/300\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.8527 - acc: 0.6437 - val_loss: 0.9385 - val_acc: 0.6591\n",
            "Epoch 233/300\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.8525 - acc: 0.6437 - val_loss: 0.9385 - val_acc: 0.6591\n",
            "Epoch 234/300\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.8520 - acc: 0.6437 - val_loss: 0.9394 - val_acc: 0.6364\n",
            "Epoch 235/300\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.8520 - acc: 0.6437 - val_loss: 0.9398 - val_acc: 0.6364\n",
            "Epoch 236/300\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.8516 - acc: 0.6437 - val_loss: 0.9402 - val_acc: 0.6364\n",
            "Epoch 237/300\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8516 - acc: 0.6437 - val_loss: 0.9406 - val_acc: 0.6364\n",
            "Epoch 238/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8513 - acc: 0.6437 - val_loss: 0.9406 - val_acc: 0.6364\n",
            "Epoch 239/300\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8507 - acc: 0.6437 - val_loss: 0.9413 - val_acc: 0.6364\n",
            "Epoch 240/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8512 - acc: 0.6437 - val_loss: 0.9416 - val_acc: 0.6364\n",
            "Epoch 241/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8507 - acc: 0.6437 - val_loss: 0.9417 - val_acc: 0.6364\n",
            "Epoch 242/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.8493 - acc: 0.6437 - val_loss: 0.9427 - val_acc: 0.6364\n",
            "Epoch 243/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8491 - acc: 0.6437 - val_loss: 0.9425 - val_acc: 0.6364\n",
            "Epoch 244/300\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8488 - acc: 0.6437 - val_loss: 0.9428 - val_acc: 0.6364\n",
            "Epoch 245/300\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.8488 - acc: 0.6437 - val_loss: 0.9434 - val_acc: 0.6364\n",
            "Epoch 246/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8485 - acc: 0.6437 - val_loss: 0.9440 - val_acc: 0.6364\n",
            "Epoch 247/300\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.8480 - acc: 0.6437 - val_loss: 0.9441 - val_acc: 0.6364\n",
            "Epoch 248/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.8483 - acc: 0.6437 - val_loss: 0.9453 - val_acc: 0.6364\n",
            "Epoch 249/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.8487 - acc: 0.6437 - val_loss: 0.9459 - val_acc: 0.6364\n",
            "Epoch 250/300\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.8474 - acc: 0.6437 - val_loss: 0.9460 - val_acc: 0.6364\n",
            "Epoch 251/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8470 - acc: 0.6437 - val_loss: 0.9463 - val_acc: 0.6364\n",
            "Epoch 252/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8468 - acc: 0.6437 - val_loss: 0.9468 - val_acc: 0.6364\n",
            "Epoch 253/300\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8466 - acc: 0.6437 - val_loss: 0.9473 - val_acc: 0.6364\n",
            "Epoch 254/300\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8467 - acc: 0.6437 - val_loss: 0.9484 - val_acc: 0.6364\n",
            "Epoch 255/300\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8458 - acc: 0.6437 - val_loss: 0.9484 - val_acc: 0.6364\n",
            "Epoch 256/300\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.8457 - acc: 0.6437 - val_loss: 0.9488 - val_acc: 0.6364\n",
            "Epoch 257/300\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8462 - acc: 0.6437 - val_loss: 0.9491 - val_acc: 0.6364\n",
            "Epoch 258/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8450 - acc: 0.6437 - val_loss: 0.9499 - val_acc: 0.6364\n",
            "Epoch 259/300\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8449 - acc: 0.6437 - val_loss: 0.9503 - val_acc: 0.6364\n",
            "Epoch 260/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8448 - acc: 0.6437 - val_loss: 0.9504 - val_acc: 0.6364\n",
            "Epoch 261/300\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8448 - acc: 0.6437 - val_loss: 0.9515 - val_acc: 0.6364\n",
            "Epoch 262/300\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.8449 - acc: 0.6437 - val_loss: 0.9522 - val_acc: 0.6364\n",
            "Epoch 263/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8443 - acc: 0.6437 - val_loss: 0.9521 - val_acc: 0.6364\n",
            "Epoch 264/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8440 - acc: 0.6437 - val_loss: 0.9531 - val_acc: 0.6364\n",
            "Epoch 265/300\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.8435 - acc: 0.6437 - val_loss: 0.9531 - val_acc: 0.6364\n",
            "Epoch 266/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8435 - acc: 0.6437 - val_loss: 0.9540 - val_acc: 0.6364\n",
            "Epoch 267/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8439 - acc: 0.6437 - val_loss: 0.9552 - val_acc: 0.6364\n",
            "Epoch 268/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8424 - acc: 0.6437 - val_loss: 0.9554 - val_acc: 0.6364\n",
            "Epoch 269/300\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.8426 - acc: 0.6437 - val_loss: 0.9557 - val_acc: 0.6364\n",
            "Epoch 270/300\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.8425 - acc: 0.6437 - val_loss: 0.9561 - val_acc: 0.6364\n",
            "Epoch 271/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8423 - acc: 0.6437 - val_loss: 0.9558 - val_acc: 0.6364\n",
            "Epoch 272/300\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8416 - acc: 0.6437 - val_loss: 0.9569 - val_acc: 0.6364\n",
            "Epoch 273/300\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8423 - acc: 0.6437 - val_loss: 0.9572 - val_acc: 0.6364\n",
            "Epoch 274/300\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8411 - acc: 0.6437 - val_loss: 0.9580 - val_acc: 0.6364\n",
            "Epoch 275/300\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8413 - acc: 0.6437 - val_loss: 0.9586 - val_acc: 0.6364\n",
            "Epoch 276/300\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8411 - acc: 0.6437 - val_loss: 0.9596 - val_acc: 0.6364\n",
            "Epoch 277/300\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.8418 - acc: 0.6437 - val_loss: 0.9597 - val_acc: 0.6364\n",
            "Epoch 278/300\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8400 - acc: 0.6437 - val_loss: 0.9608 - val_acc: 0.6364\n",
            "Epoch 279/300\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8402 - acc: 0.6437 - val_loss: 0.9612 - val_acc: 0.6364\n",
            "Epoch 280/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8405 - acc: 0.6437 - val_loss: 0.9614 - val_acc: 0.6364\n",
            "Epoch 281/300\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.8397 - acc: 0.6437 - val_loss: 0.9622 - val_acc: 0.6364\n",
            "Epoch 282/300\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.8400 - acc: 0.6437 - val_loss: 0.9637 - val_acc: 0.6364\n",
            "Epoch 283/300\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8409 - acc: 0.6437 - val_loss: 0.9646 - val_acc: 0.6364\n",
            "Epoch 284/300\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8397 - acc: 0.6437 - val_loss: 0.9642 - val_acc: 0.6364\n",
            "Epoch 285/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8387 - acc: 0.6437 - val_loss: 0.9653 - val_acc: 0.6364\n",
            "Epoch 286/300\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8387 - acc: 0.6437 - val_loss: 0.9663 - val_acc: 0.6364\n",
            "Epoch 287/300\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.8378 - acc: 0.6437 - val_loss: 0.9664 - val_acc: 0.6364\n",
            "Epoch 288/300\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.8378 - acc: 0.6437 - val_loss: 0.9674 - val_acc: 0.6364\n",
            "Epoch 289/300\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8384 - acc: 0.6437 - val_loss: 0.9680 - val_acc: 0.6364\n",
            "Epoch 290/300\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.8381 - acc: 0.6437 - val_loss: 0.9690 - val_acc: 0.6364\n",
            "Epoch 291/300\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8379 - acc: 0.6437 - val_loss: 0.9691 - val_acc: 0.6364\n",
            "Epoch 292/300\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.8366 - acc: 0.6437 - val_loss: 0.9703 - val_acc: 0.6364\n",
            "Epoch 293/300\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.8368 - acc: 0.6437 - val_loss: 0.9707 - val_acc: 0.6364\n",
            "Epoch 294/300\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8377 - acc: 0.6437 - val_loss: 0.9724 - val_acc: 0.6591\n",
            "Epoch 295/300\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.8357 - acc: 0.6437 - val_loss: 0.9718 - val_acc: 0.6364\n",
            "Epoch 296/300\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.8365 - acc: 0.6437 - val_loss: 0.9727 - val_acc: 0.6364\n",
            "Epoch 297/300\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8362 - acc: 0.6437 - val_loss: 0.9732 - val_acc: 0.6591\n",
            "Epoch 298/300\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8351 - acc: 0.6437 - val_loss: 0.9746 - val_acc: 0.6591\n",
            "Epoch 299/300\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.8361 - acc: 0.6437 - val_loss: 0.9745 - val_acc: 0.6364\n",
            "Epoch 300/300\n",
            "87/87 [==============================] - 0s 288us/step - loss: 0.8353 - acc: 0.6437 - val_loss: 0.9754 - val_acc: 0.6364\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/300\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 1.3834 - acc: 0.4432 - val_loss: 1.2738 - val_acc: 0.4419\n",
            "Epoch 2/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 1.3242 - acc: 0.4545 - val_loss: 1.2231 - val_acc: 0.4419\n",
            "Epoch 3/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.2712 - acc: 0.4545 - val_loss: 1.1824 - val_acc: 0.4419\n",
            "Epoch 4/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.2292 - acc: 0.4545 - val_loss: 1.1478 - val_acc: 0.4651\n",
            "Epoch 5/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 1.1922 - acc: 0.4545 - val_loss: 1.1192 - val_acc: 0.4884\n",
            "Epoch 6/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 1.1622 - acc: 0.4545 - val_loss: 1.0962 - val_acc: 0.4884\n",
            "Epoch 7/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.1357 - acc: 0.4545 - val_loss: 1.0761 - val_acc: 0.4884\n",
            "Epoch 8/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.1137 - acc: 0.4545 - val_loss: 1.0592 - val_acc: 0.4884\n",
            "Epoch 9/300\n",
            "88/88 [==============================] - 0s 300us/step - loss: 1.0948 - acc: 0.4659 - val_loss: 1.0449 - val_acc: 0.4651\n",
            "Epoch 10/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0783 - acc: 0.4773 - val_loss: 1.0338 - val_acc: 0.4419\n",
            "Epoch 11/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0644 - acc: 0.4545 - val_loss: 1.0235 - val_acc: 0.4419\n",
            "Epoch 12/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 1.0531 - acc: 0.4545 - val_loss: 1.0155 - val_acc: 0.4419\n",
            "Epoch 13/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.0419 - acc: 0.4432 - val_loss: 1.0078 - val_acc: 0.4186\n",
            "Epoch 14/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0327 - acc: 0.4773 - val_loss: 1.0018 - val_acc: 0.4186\n",
            "Epoch 15/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 1.0249 - acc: 0.4545 - val_loss: 0.9968 - val_acc: 0.4186\n",
            "Epoch 16/300\n",
            "88/88 [==============================] - 0s 191us/step - loss: 1.0183 - acc: 0.4773 - val_loss: 0.9923 - val_acc: 0.4419\n",
            "Epoch 17/300\n",
            "88/88 [==============================] - 0s 201us/step - loss: 1.0122 - acc: 0.4886 - val_loss: 0.9885 - val_acc: 0.4419\n",
            "Epoch 18/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.0069 - acc: 0.4886 - val_loss: 0.9847 - val_acc: 0.4419\n",
            "Epoch 19/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 1.0009 - acc: 0.4886 - val_loss: 0.9819 - val_acc: 0.4419\n",
            "Epoch 20/300\n",
            "88/88 [==============================] - 0s 264us/step - loss: 0.9962 - acc: 0.5000 - val_loss: 0.9796 - val_acc: 0.4419\n",
            "Epoch 21/300\n",
            "88/88 [==============================] - 0s 200us/step - loss: 0.9927 - acc: 0.5000 - val_loss: 0.9776 - val_acc: 0.4651\n",
            "Epoch 22/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.9886 - acc: 0.5000 - val_loss: 0.9755 - val_acc: 0.5116\n",
            "Epoch 23/300\n",
            "88/88 [==============================] - 0s 271us/step - loss: 0.9841 - acc: 0.5000 - val_loss: 0.9745 - val_acc: 0.5116\n",
            "Epoch 24/300\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.9809 - acc: 0.5114 - val_loss: 0.9727 - val_acc: 0.4884\n",
            "Epoch 25/300\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.9780 - acc: 0.5227 - val_loss: 0.9712 - val_acc: 0.4884\n",
            "Epoch 26/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.9746 - acc: 0.5341 - val_loss: 0.9698 - val_acc: 0.4884\n",
            "Epoch 27/300\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.9720 - acc: 0.5341 - val_loss: 0.9685 - val_acc: 0.4884\n",
            "Epoch 28/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.9692 - acc: 0.5455 - val_loss: 0.9674 - val_acc: 0.4651\n",
            "Epoch 29/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.9671 - acc: 0.5341 - val_loss: 0.9661 - val_acc: 0.4651\n",
            "Epoch 30/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9646 - acc: 0.5341 - val_loss: 0.9654 - val_acc: 0.4884\n",
            "Epoch 31/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.9625 - acc: 0.5455 - val_loss: 0.9645 - val_acc: 0.4884\n",
            "Epoch 32/300\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.9603 - acc: 0.5455 - val_loss: 0.9639 - val_acc: 0.5116\n",
            "Epoch 33/300\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.9580 - acc: 0.5455 - val_loss: 0.9632 - val_acc: 0.5349\n",
            "Epoch 34/300\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.9568 - acc: 0.5455 - val_loss: 0.9624 - val_acc: 0.5349\n",
            "Epoch 35/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.9541 - acc: 0.5455 - val_loss: 0.9622 - val_acc: 0.5349\n",
            "Epoch 36/300\n",
            "88/88 [==============================] - 0s 339us/step - loss: 0.9520 - acc: 0.5568 - val_loss: 0.9617 - val_acc: 0.5349\n",
            "Epoch 37/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.9509 - acc: 0.5568 - val_loss: 0.9614 - val_acc: 0.5349\n",
            "Epoch 38/300\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.9488 - acc: 0.5568 - val_loss: 0.9610 - val_acc: 0.5116\n",
            "Epoch 39/300\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.9467 - acc: 0.5568 - val_loss: 0.9606 - val_acc: 0.5116\n",
            "Epoch 40/300\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.9449 - acc: 0.5682 - val_loss: 0.9603 - val_acc: 0.5116\n",
            "Epoch 41/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.9436 - acc: 0.5682 - val_loss: 0.9599 - val_acc: 0.5349\n",
            "Epoch 42/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9425 - acc: 0.5682 - val_loss: 0.9598 - val_acc: 0.5349\n",
            "Epoch 43/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.9404 - acc: 0.5568 - val_loss: 0.9596 - val_acc: 0.5349\n",
            "Epoch 44/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.9388 - acc: 0.5682 - val_loss: 0.9595 - val_acc: 0.5349\n",
            "Epoch 45/300\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.9371 - acc: 0.5682 - val_loss: 0.9591 - val_acc: 0.5349\n",
            "Epoch 46/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.9359 - acc: 0.5455 - val_loss: 0.9590 - val_acc: 0.5349\n",
            "Epoch 47/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.9351 - acc: 0.5455 - val_loss: 0.9588 - val_acc: 0.5116\n",
            "Epoch 48/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9334 - acc: 0.5455 - val_loss: 0.9586 - val_acc: 0.5116\n",
            "Epoch 49/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.9320 - acc: 0.5341 - val_loss: 0.9586 - val_acc: 0.5116\n",
            "Epoch 50/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9304 - acc: 0.5455 - val_loss: 0.9584 - val_acc: 0.5116\n",
            "Epoch 51/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.9290 - acc: 0.5455 - val_loss: 0.9585 - val_acc: 0.5116\n",
            "Epoch 52/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.9278 - acc: 0.5455 - val_loss: 0.9585 - val_acc: 0.5116\n",
            "Epoch 53/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.9265 - acc: 0.5455 - val_loss: 0.9584 - val_acc: 0.5116\n",
            "Epoch 54/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9258 - acc: 0.5568 - val_loss: 0.9585 - val_acc: 0.5116\n",
            "Epoch 55/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.9245 - acc: 0.5455 - val_loss: 0.9585 - val_acc: 0.5116\n",
            "Epoch 56/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.9237 - acc: 0.5455 - val_loss: 0.9588 - val_acc: 0.5116\n",
            "Epoch 57/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.9229 - acc: 0.5455 - val_loss: 0.9588 - val_acc: 0.5349\n",
            "Epoch 58/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.9222 - acc: 0.5455 - val_loss: 0.9588 - val_acc: 0.5349\n",
            "Epoch 59/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.9208 - acc: 0.5568 - val_loss: 0.9589 - val_acc: 0.5349\n",
            "Epoch 60/300\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.9198 - acc: 0.5341 - val_loss: 0.9590 - val_acc: 0.5349\n",
            "Epoch 61/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.9193 - acc: 0.5568 - val_loss: 0.9594 - val_acc: 0.5349\n",
            "Epoch 62/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.9184 - acc: 0.5455 - val_loss: 0.9595 - val_acc: 0.5349\n",
            "Epoch 63/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.9174 - acc: 0.5455 - val_loss: 0.9595 - val_acc: 0.5349\n",
            "Epoch 64/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9171 - acc: 0.5455 - val_loss: 0.9599 - val_acc: 0.5349\n",
            "Epoch 65/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.9158 - acc: 0.5341 - val_loss: 0.9601 - val_acc: 0.5349\n",
            "Epoch 66/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9149 - acc: 0.5455 - val_loss: 0.9604 - val_acc: 0.5349\n",
            "Epoch 67/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.9141 - acc: 0.5341 - val_loss: 0.9608 - val_acc: 0.5349\n",
            "Epoch 68/300\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.9135 - acc: 0.5341 - val_loss: 0.9609 - val_acc: 0.5349\n",
            "Epoch 69/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.9129 - acc: 0.5341 - val_loss: 0.9612 - val_acc: 0.5349\n",
            "Epoch 70/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9119 - acc: 0.5341 - val_loss: 0.9614 - val_acc: 0.5349\n",
            "Epoch 71/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.9113 - acc: 0.5341 - val_loss: 0.9613 - val_acc: 0.5349\n",
            "Epoch 72/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.9107 - acc: 0.5341 - val_loss: 0.9614 - val_acc: 0.5349\n",
            "Epoch 73/300\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.9096 - acc: 0.5341 - val_loss: 0.9615 - val_acc: 0.5349\n",
            "Epoch 74/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9088 - acc: 0.5341 - val_loss: 0.9616 - val_acc: 0.5349\n",
            "Epoch 75/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.9082 - acc: 0.5341 - val_loss: 0.9617 - val_acc: 0.5349\n",
            "Epoch 76/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9075 - acc: 0.5341 - val_loss: 0.9616 - val_acc: 0.5349\n",
            "Epoch 77/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.9072 - acc: 0.5341 - val_loss: 0.9618 - val_acc: 0.5349\n",
            "Epoch 78/300\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9059 - acc: 0.5341 - val_loss: 0.9621 - val_acc: 0.5581\n",
            "Epoch 79/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.9060 - acc: 0.5341 - val_loss: 0.9621 - val_acc: 0.5581\n",
            "Epoch 80/300\n",
            "88/88 [==============================] - 0s 306us/step - loss: 0.9053 - acc: 0.5341 - val_loss: 0.9622 - val_acc: 0.5581\n",
            "Epoch 81/300\n",
            "88/88 [==============================] - 0s 295us/step - loss: 0.9041 - acc: 0.5341 - val_loss: 0.9627 - val_acc: 0.5581\n",
            "Epoch 82/300\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.9036 - acc: 0.5227 - val_loss: 0.9625 - val_acc: 0.5581\n",
            "Epoch 83/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9034 - acc: 0.5227 - val_loss: 0.9625 - val_acc: 0.5581\n",
            "Epoch 84/300\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.9021 - acc: 0.5227 - val_loss: 0.9626 - val_acc: 0.5581\n",
            "Epoch 85/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.9021 - acc: 0.5227 - val_loss: 0.9630 - val_acc: 0.5581\n",
            "Epoch 86/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.9009 - acc: 0.5227 - val_loss: 0.9630 - val_acc: 0.5581\n",
            "Epoch 87/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.9014 - acc: 0.5227 - val_loss: 0.9631 - val_acc: 0.5581\n",
            "Epoch 88/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8997 - acc: 0.5341 - val_loss: 0.9636 - val_acc: 0.5581\n",
            "Epoch 89/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8991 - acc: 0.5341 - val_loss: 0.9636 - val_acc: 0.5581\n",
            "Epoch 90/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8987 - acc: 0.5227 - val_loss: 0.9640 - val_acc: 0.5581\n",
            "Epoch 91/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8983 - acc: 0.5227 - val_loss: 0.9642 - val_acc: 0.5581\n",
            "Epoch 92/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8979 - acc: 0.5227 - val_loss: 0.9644 - val_acc: 0.5581\n",
            "Epoch 93/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8973 - acc: 0.5227 - val_loss: 0.9644 - val_acc: 0.5581\n",
            "Epoch 94/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8971 - acc: 0.5341 - val_loss: 0.9645 - val_acc: 0.5581\n",
            "Epoch 95/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8964 - acc: 0.5227 - val_loss: 0.9642 - val_acc: 0.5581\n",
            "Epoch 96/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8959 - acc: 0.5227 - val_loss: 0.9647 - val_acc: 0.5581\n",
            "Epoch 97/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8957 - acc: 0.5227 - val_loss: 0.9646 - val_acc: 0.5581\n",
            "Epoch 98/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.8947 - acc: 0.5227 - val_loss: 0.9650 - val_acc: 0.5581\n",
            "Epoch 99/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8947 - acc: 0.5341 - val_loss: 0.9650 - val_acc: 0.5581\n",
            "Epoch 100/300\n",
            "88/88 [==============================] - 0s 286us/step - loss: 0.8941 - acc: 0.5227 - val_loss: 0.9652 - val_acc: 0.5581\n",
            "Epoch 101/300\n",
            "88/88 [==============================] - 0s 283us/step - loss: 0.8945 - acc: 0.5227 - val_loss: 0.9650 - val_acc: 0.5581\n",
            "Epoch 102/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8930 - acc: 0.5341 - val_loss: 0.9653 - val_acc: 0.5581\n",
            "Epoch 103/300\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8928 - acc: 0.5341 - val_loss: 0.9653 - val_acc: 0.5581\n",
            "Epoch 104/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8919 - acc: 0.5341 - val_loss: 0.9656 - val_acc: 0.5581\n",
            "Epoch 105/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8920 - acc: 0.5227 - val_loss: 0.9658 - val_acc: 0.5581\n",
            "Epoch 106/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8919 - acc: 0.5227 - val_loss: 0.9657 - val_acc: 0.5581\n",
            "Epoch 107/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8907 - acc: 0.5341 - val_loss: 0.9660 - val_acc: 0.5581\n",
            "Epoch 108/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8905 - acc: 0.5341 - val_loss: 0.9661 - val_acc: 0.5581\n",
            "Epoch 109/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8908 - acc: 0.5227 - val_loss: 0.9661 - val_acc: 0.5349\n",
            "Epoch 110/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8901 - acc: 0.5227 - val_loss: 0.9664 - val_acc: 0.5581\n",
            "Epoch 111/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8897 - acc: 0.5227 - val_loss: 0.9666 - val_acc: 0.5349\n",
            "Epoch 112/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8897 - acc: 0.5341 - val_loss: 0.9666 - val_acc: 0.5581\n",
            "Epoch 113/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8887 - acc: 0.5227 - val_loss: 0.9666 - val_acc: 0.5349\n",
            "Epoch 114/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8891 - acc: 0.5227 - val_loss: 0.9669 - val_acc: 0.5349\n",
            "Epoch 115/300\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.8881 - acc: 0.5227 - val_loss: 0.9670 - val_acc: 0.5349\n",
            "Epoch 116/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8876 - acc: 0.5227 - val_loss: 0.9669 - val_acc: 0.5349\n",
            "Epoch 117/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8878 - acc: 0.5341 - val_loss: 0.9671 - val_acc: 0.5349\n",
            "Epoch 118/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8872 - acc: 0.5227 - val_loss: 0.9675 - val_acc: 0.5349\n",
            "Epoch 119/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8869 - acc: 0.5114 - val_loss: 0.9680 - val_acc: 0.5349\n",
            "Epoch 120/300\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.8860 - acc: 0.5227 - val_loss: 0.9679 - val_acc: 0.5349\n",
            "Epoch 121/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8858 - acc: 0.5114 - val_loss: 0.9684 - val_acc: 0.5349\n",
            "Epoch 122/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8849 - acc: 0.5227 - val_loss: 0.9682 - val_acc: 0.5349\n",
            "Epoch 123/300\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.8844 - acc: 0.5227 - val_loss: 0.9684 - val_acc: 0.5349\n",
            "Epoch 124/300\n",
            "88/88 [==============================] - 0s 284us/step - loss: 0.8845 - acc: 0.5227 - val_loss: 0.9684 - val_acc: 0.5349\n",
            "Epoch 125/300\n",
            "88/88 [==============================] - 0s 259us/step - loss: 0.8840 - acc: 0.5227 - val_loss: 0.9686 - val_acc: 0.5349\n",
            "Epoch 126/300\n",
            "88/88 [==============================] - 0s 311us/step - loss: 0.8835 - acc: 0.5227 - val_loss: 0.9690 - val_acc: 0.5349\n",
            "Epoch 127/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.8830 - acc: 0.5227 - val_loss: 0.9691 - val_acc: 0.5349\n",
            "Epoch 128/300\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.8825 - acc: 0.5227 - val_loss: 0.9695 - val_acc: 0.5116\n",
            "Epoch 129/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8825 - acc: 0.5114 - val_loss: 0.9697 - val_acc: 0.5349\n",
            "Epoch 130/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8818 - acc: 0.5227 - val_loss: 0.9697 - val_acc: 0.5349\n",
            "Epoch 131/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8810 - acc: 0.5227 - val_loss: 0.9698 - val_acc: 0.5349\n",
            "Epoch 132/300\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.8811 - acc: 0.5114 - val_loss: 0.9704 - val_acc: 0.5116\n",
            "Epoch 133/300\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.8801 - acc: 0.5341 - val_loss: 0.9701 - val_acc: 0.5349\n",
            "Epoch 134/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8791 - acc: 0.5341 - val_loss: 0.9707 - val_acc: 0.5349\n",
            "Epoch 135/300\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8788 - acc: 0.5341 - val_loss: 0.9708 - val_acc: 0.5349\n",
            "Epoch 136/300\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.8782 - acc: 0.5455 - val_loss: 0.9707 - val_acc: 0.5349\n",
            "Epoch 137/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8778 - acc: 0.5455 - val_loss: 0.9713 - val_acc: 0.5116\n",
            "Epoch 138/300\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.8774 - acc: 0.5227 - val_loss: 0.9715 - val_acc: 0.5116\n",
            "Epoch 139/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8765 - acc: 0.5455 - val_loss: 0.9716 - val_acc: 0.5116\n",
            "Epoch 140/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8762 - acc: 0.5227 - val_loss: 0.9719 - val_acc: 0.5116\n",
            "Epoch 141/300\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8757 - acc: 0.5341 - val_loss: 0.9722 - val_acc: 0.5116\n",
            "Epoch 142/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8747 - acc: 0.5341 - val_loss: 0.9723 - val_acc: 0.5116\n",
            "Epoch 143/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8747 - acc: 0.5341 - val_loss: 0.9727 - val_acc: 0.5116\n",
            "Epoch 144/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8738 - acc: 0.5341 - val_loss: 0.9730 - val_acc: 0.5116\n",
            "Epoch 145/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8735 - acc: 0.5341 - val_loss: 0.9732 - val_acc: 0.5349\n",
            "Epoch 146/300\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8731 - acc: 0.5227 - val_loss: 0.9736 - val_acc: 0.5116\n",
            "Epoch 147/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8725 - acc: 0.5227 - val_loss: 0.9741 - val_acc: 0.5116\n",
            "Epoch 148/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8728 - acc: 0.5341 - val_loss: 0.9745 - val_acc: 0.5116\n",
            "Epoch 149/300\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.8716 - acc: 0.5341 - val_loss: 0.9746 - val_acc: 0.5116\n",
            "Epoch 150/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8712 - acc: 0.5227 - val_loss: 0.9751 - val_acc: 0.5116\n",
            "Epoch 151/300\n",
            "88/88 [==============================] - 0s 319us/step - loss: 0.8710 - acc: 0.5227 - val_loss: 0.9755 - val_acc: 0.5116\n",
            "Epoch 152/300\n",
            "88/88 [==============================] - 0s 187us/step - loss: 0.8710 - acc: 0.5227 - val_loss: 0.9757 - val_acc: 0.5349\n",
            "Epoch 153/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8704 - acc: 0.5227 - val_loss: 0.9766 - val_acc: 0.5116\n",
            "Epoch 154/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8703 - acc: 0.5227 - val_loss: 0.9764 - val_acc: 0.5116\n",
            "Epoch 155/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8703 - acc: 0.5227 - val_loss: 0.9769 - val_acc: 0.5116\n",
            "Epoch 156/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8687 - acc: 0.5227 - val_loss: 0.9778 - val_acc: 0.5116\n",
            "Epoch 157/300\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.8690 - acc: 0.5227 - val_loss: 0.9783 - val_acc: 0.5116\n",
            "Epoch 158/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8687 - acc: 0.5341 - val_loss: 0.9785 - val_acc: 0.5116\n",
            "Epoch 159/300\n",
            "88/88 [==============================] - 0s 201us/step - loss: 0.8678 - acc: 0.5227 - val_loss: 0.9789 - val_acc: 0.5116\n",
            "Epoch 160/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8671 - acc: 0.5227 - val_loss: 0.9794 - val_acc: 0.5116\n",
            "Epoch 161/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8673 - acc: 0.5341 - val_loss: 0.9803 - val_acc: 0.5116\n",
            "Epoch 162/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8665 - acc: 0.5341 - val_loss: 0.9804 - val_acc: 0.5116\n",
            "Epoch 163/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8665 - acc: 0.5227 - val_loss: 0.9813 - val_acc: 0.5116\n",
            "Epoch 164/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8666 - acc: 0.5455 - val_loss: 0.9812 - val_acc: 0.5116\n",
            "Epoch 165/300\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.8657 - acc: 0.5227 - val_loss: 0.9817 - val_acc: 0.5116\n",
            "Epoch 166/300\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.8655 - acc: 0.5455 - val_loss: 0.9824 - val_acc: 0.5116\n",
            "Epoch 167/300\n",
            "88/88 [==============================] - 0s 303us/step - loss: 0.8651 - acc: 0.5568 - val_loss: 0.9827 - val_acc: 0.5116\n",
            "Epoch 168/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8647 - acc: 0.5455 - val_loss: 0.9834 - val_acc: 0.5116\n",
            "Epoch 169/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8645 - acc: 0.5455 - val_loss: 0.9838 - val_acc: 0.5116\n",
            "Epoch 170/300\n",
            "88/88 [==============================] - 0s 275us/step - loss: 0.8645 - acc: 0.5568 - val_loss: 0.9840 - val_acc: 0.5116\n",
            "Epoch 171/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8635 - acc: 0.5455 - val_loss: 0.9847 - val_acc: 0.5116\n",
            "Epoch 172/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8635 - acc: 0.5568 - val_loss: 0.9849 - val_acc: 0.5116\n",
            "Epoch 173/300\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8633 - acc: 0.5682 - val_loss: 0.9855 - val_acc: 0.5116\n",
            "Epoch 174/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8624 - acc: 0.5568 - val_loss: 0.9856 - val_acc: 0.5116\n",
            "Epoch 175/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8620 - acc: 0.5568 - val_loss: 0.9864 - val_acc: 0.5116\n",
            "Epoch 176/300\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8613 - acc: 0.5682 - val_loss: 0.9866 - val_acc: 0.5116\n",
            "Epoch 177/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8616 - acc: 0.5568 - val_loss: 0.9870 - val_acc: 0.5116\n",
            "Epoch 178/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8607 - acc: 0.5568 - val_loss: 0.9875 - val_acc: 0.5116\n",
            "Epoch 179/300\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.8606 - acc: 0.5455 - val_loss: 0.9882 - val_acc: 0.5116\n",
            "Epoch 180/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8604 - acc: 0.5682 - val_loss: 0.9886 - val_acc: 0.5116\n",
            "Epoch 181/300\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.8608 - acc: 0.5795 - val_loss: 0.9891 - val_acc: 0.5116\n",
            "Epoch 182/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8594 - acc: 0.5682 - val_loss: 0.9899 - val_acc: 0.5116\n",
            "Epoch 183/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8597 - acc: 0.5682 - val_loss: 0.9905 - val_acc: 0.5116\n",
            "Epoch 184/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8593 - acc: 0.5909 - val_loss: 0.9904 - val_acc: 0.5116\n",
            "Epoch 185/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8590 - acc: 0.5682 - val_loss: 0.9912 - val_acc: 0.5116\n",
            "Epoch 186/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8585 - acc: 0.5682 - val_loss: 0.9915 - val_acc: 0.5116\n",
            "Epoch 187/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8582 - acc: 0.5682 - val_loss: 0.9922 - val_acc: 0.5116\n",
            "Epoch 188/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8583 - acc: 0.5568 - val_loss: 0.9925 - val_acc: 0.5116\n",
            "Epoch 189/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8576 - acc: 0.5682 - val_loss: 0.9927 - val_acc: 0.5116\n",
            "Epoch 190/300\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.8569 - acc: 0.5795 - val_loss: 0.9932 - val_acc: 0.5116\n",
            "Epoch 191/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8568 - acc: 0.5682 - val_loss: 0.9937 - val_acc: 0.4884\n",
            "Epoch 192/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8568 - acc: 0.5682 - val_loss: 0.9939 - val_acc: 0.4884\n",
            "Epoch 193/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8564 - acc: 0.5795 - val_loss: 0.9943 - val_acc: 0.4884\n",
            "Epoch 194/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8559 - acc: 0.5795 - val_loss: 0.9951 - val_acc: 0.4884\n",
            "Epoch 195/300\n",
            "88/88 [==============================] - 0s 319us/step - loss: 0.8555 - acc: 0.5795 - val_loss: 0.9953 - val_acc: 0.5116\n",
            "Epoch 196/300\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.8556 - acc: 0.5795 - val_loss: 0.9951 - val_acc: 0.4884\n",
            "Epoch 197/300\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.8552 - acc: 0.5795 - val_loss: 0.9960 - val_acc: 0.5116\n",
            "Epoch 198/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8547 - acc: 0.5795 - val_loss: 0.9963 - val_acc: 0.5116\n",
            "Epoch 199/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8543 - acc: 0.5795 - val_loss: 0.9967 - val_acc: 0.5116\n",
            "Epoch 200/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8541 - acc: 0.5795 - val_loss: 0.9972 - val_acc: 0.5116\n",
            "Epoch 201/300\n",
            "88/88 [==============================] - 0s 196us/step - loss: 0.8540 - acc: 0.5795 - val_loss: 0.9974 - val_acc: 0.5116\n",
            "Epoch 202/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8536 - acc: 0.5795 - val_loss: 0.9977 - val_acc: 0.5116\n",
            "Epoch 203/300\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.8531 - acc: 0.5795 - val_loss: 0.9982 - val_acc: 0.5116\n",
            "Epoch 204/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8536 - acc: 0.5795 - val_loss: 0.9986 - val_acc: 0.5116\n",
            "Epoch 205/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8527 - acc: 0.5795 - val_loss: 0.9990 - val_acc: 0.5116\n",
            "Epoch 206/300\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.8532 - acc: 0.5795 - val_loss: 0.9994 - val_acc: 0.5116\n",
            "Epoch 207/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8522 - acc: 0.5795 - val_loss: 0.9996 - val_acc: 0.5116\n",
            "Epoch 208/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8517 - acc: 0.5795 - val_loss: 0.9997 - val_acc: 0.5116\n",
            "Epoch 209/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8516 - acc: 0.5682 - val_loss: 1.0005 - val_acc: 0.5116\n",
            "Epoch 210/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8516 - acc: 0.5795 - val_loss: 1.0006 - val_acc: 0.5116\n",
            "Epoch 211/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8510 - acc: 0.5795 - val_loss: 1.0011 - val_acc: 0.5116\n",
            "Epoch 212/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8513 - acc: 0.5909 - val_loss: 1.0013 - val_acc: 0.5116\n",
            "Epoch 213/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8500 - acc: 0.5795 - val_loss: 1.0018 - val_acc: 0.5116\n",
            "Epoch 214/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8503 - acc: 0.5795 - val_loss: 1.0027 - val_acc: 0.5116\n",
            "Epoch 215/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8502 - acc: 0.5795 - val_loss: 1.0024 - val_acc: 0.5116\n",
            "Epoch 216/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8492 - acc: 0.5682 - val_loss: 1.0033 - val_acc: 0.5116\n",
            "Epoch 217/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8495 - acc: 0.6023 - val_loss: 1.0033 - val_acc: 0.5116\n",
            "Epoch 218/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8502 - acc: 0.5909 - val_loss: 1.0039 - val_acc: 0.5349\n",
            "Epoch 219/300\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.8488 - acc: 0.5909 - val_loss: 1.0038 - val_acc: 0.5116\n",
            "Epoch 220/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8488 - acc: 0.5909 - val_loss: 1.0044 - val_acc: 0.5116\n",
            "Epoch 221/300\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8483 - acc: 0.5909 - val_loss: 1.0049 - val_acc: 0.5116\n",
            "Epoch 222/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8480 - acc: 0.5909 - val_loss: 1.0055 - val_acc: 0.5349\n",
            "Epoch 223/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8484 - acc: 0.6023 - val_loss: 1.0053 - val_acc: 0.5116\n",
            "Epoch 224/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8474 - acc: 0.6023 - val_loss: 1.0059 - val_acc: 0.5116\n",
            "Epoch 225/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8469 - acc: 0.5909 - val_loss: 1.0062 - val_acc: 0.5116\n",
            "Epoch 226/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8467 - acc: 0.5909 - val_loss: 1.0065 - val_acc: 0.5116\n",
            "Epoch 227/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8464 - acc: 0.5909 - val_loss: 1.0068 - val_acc: 0.5349\n",
            "Epoch 228/300\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8458 - acc: 0.5909 - val_loss: 1.0074 - val_acc: 0.5116\n",
            "Epoch 229/300\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8460 - acc: 0.5909 - val_loss: 1.0074 - val_acc: 0.5116\n",
            "Epoch 230/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8458 - acc: 0.5909 - val_loss: 1.0081 - val_acc: 0.5349\n",
            "Epoch 231/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8457 - acc: 0.5909 - val_loss: 1.0079 - val_acc: 0.5116\n",
            "Epoch 232/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8448 - acc: 0.5909 - val_loss: 1.0085 - val_acc: 0.5349\n",
            "Epoch 233/300\n",
            "88/88 [==============================] - 0s 296us/step - loss: 0.8439 - acc: 0.5909 - val_loss: 1.0088 - val_acc: 0.5349\n",
            "Epoch 234/300\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8438 - acc: 0.5909 - val_loss: 1.0095 - val_acc: 0.5349\n",
            "Epoch 235/300\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.8432 - acc: 0.5909 - val_loss: 1.0094 - val_acc: 0.5116\n",
            "Epoch 236/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.8439 - acc: 0.6023 - val_loss: 1.0103 - val_acc: 0.5349\n",
            "Epoch 237/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8424 - acc: 0.6023 - val_loss: 1.0101 - val_acc: 0.5116\n",
            "Epoch 238/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8419 - acc: 0.6023 - val_loss: 1.0101 - val_acc: 0.5349\n",
            "Epoch 239/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8417 - acc: 0.6136 - val_loss: 1.0109 - val_acc: 0.5349\n",
            "Epoch 240/300\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.8414 - acc: 0.6136 - val_loss: 1.0107 - val_acc: 0.5349\n",
            "Epoch 241/300\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.8407 - acc: 0.6023 - val_loss: 1.0114 - val_acc: 0.5349\n",
            "Epoch 242/300\n",
            "88/88 [==============================] - 0s 295us/step - loss: 0.8402 - acc: 0.6023 - val_loss: 1.0114 - val_acc: 0.5349\n",
            "Epoch 243/300\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.8394 - acc: 0.6136 - val_loss: 1.0116 - val_acc: 0.5349\n",
            "Epoch 244/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8402 - acc: 0.6136 - val_loss: 1.0119 - val_acc: 0.5349\n",
            "Epoch 245/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8386 - acc: 0.5909 - val_loss: 1.0128 - val_acc: 0.5349\n",
            "Epoch 246/300\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.8388 - acc: 0.6023 - val_loss: 1.0134 - val_acc: 0.5349\n",
            "Epoch 247/300\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8382 - acc: 0.6023 - val_loss: 1.0129 - val_acc: 0.5349\n",
            "Epoch 248/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8380 - acc: 0.6023 - val_loss: 1.0135 - val_acc: 0.5349\n",
            "Epoch 249/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8368 - acc: 0.5909 - val_loss: 1.0141 - val_acc: 0.5349\n",
            "Epoch 250/300\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.8364 - acc: 0.6136 - val_loss: 1.0146 - val_acc: 0.5349\n",
            "Epoch 251/300\n",
            "88/88 [==============================] - 0s 184us/step - loss: 0.8376 - acc: 0.5909 - val_loss: 1.0144 - val_acc: 0.5349\n",
            "Epoch 252/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8357 - acc: 0.6023 - val_loss: 1.0148 - val_acc: 0.5349\n",
            "Epoch 253/300\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8355 - acc: 0.6136 - val_loss: 1.0152 - val_acc: 0.5349\n",
            "Epoch 254/300\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8356 - acc: 0.6023 - val_loss: 1.0162 - val_acc: 0.5349\n",
            "Epoch 255/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8344 - acc: 0.6023 - val_loss: 1.0166 - val_acc: 0.5349\n",
            "Epoch 256/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8341 - acc: 0.6023 - val_loss: 1.0163 - val_acc: 0.5349\n",
            "Epoch 257/300\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8337 - acc: 0.6136 - val_loss: 1.0173 - val_acc: 0.5349\n",
            "Epoch 258/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8339 - acc: 0.6136 - val_loss: 1.0178 - val_acc: 0.5349\n",
            "Epoch 259/300\n",
            "88/88 [==============================] - 0s 202us/step - loss: 0.8332 - acc: 0.6023 - val_loss: 1.0177 - val_acc: 0.5349\n",
            "Epoch 260/300\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8326 - acc: 0.6136 - val_loss: 1.0185 - val_acc: 0.5349\n",
            "Epoch 261/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8321 - acc: 0.6023 - val_loss: 1.0184 - val_acc: 0.5349\n",
            "Epoch 262/300\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8312 - acc: 0.6136 - val_loss: 1.0190 - val_acc: 0.5349\n",
            "Epoch 263/300\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.8317 - acc: 0.6136 - val_loss: 1.0196 - val_acc: 0.5349\n",
            "Epoch 264/300\n",
            "88/88 [==============================] - 0s 290us/step - loss: 0.8310 - acc: 0.6023 - val_loss: 1.0201 - val_acc: 0.5349\n",
            "Epoch 265/300\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.8302 - acc: 0.6136 - val_loss: 1.0207 - val_acc: 0.5349\n",
            "Epoch 266/300\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.8297 - acc: 0.6136 - val_loss: 1.0210 - val_acc: 0.5349\n",
            "Epoch 267/300\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8294 - acc: 0.6136 - val_loss: 1.0218 - val_acc: 0.5349\n",
            "Epoch 268/300\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.8298 - acc: 0.6136 - val_loss: 1.0225 - val_acc: 0.5349\n",
            "Epoch 269/300\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.8295 - acc: 0.6023 - val_loss: 1.0225 - val_acc: 0.5349\n",
            "Epoch 270/300\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8282 - acc: 0.6136 - val_loss: 1.0234 - val_acc: 0.5349\n",
            "Epoch 271/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8284 - acc: 0.6023 - val_loss: 1.0239 - val_acc: 0.5349\n",
            "Epoch 272/300\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8274 - acc: 0.6136 - val_loss: 1.0240 - val_acc: 0.5349\n",
            "Epoch 273/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8275 - acc: 0.6136 - val_loss: 1.0239 - val_acc: 0.5349\n",
            "Epoch 274/300\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.8269 - acc: 0.6023 - val_loss: 1.0251 - val_acc: 0.5349\n",
            "Epoch 275/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8263 - acc: 0.6250 - val_loss: 1.0254 - val_acc: 0.5349\n",
            "Epoch 276/300\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.8266 - acc: 0.6136 - val_loss: 1.0259 - val_acc: 0.5349\n",
            "Epoch 277/300\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8259 - acc: 0.6136 - val_loss: 1.0262 - val_acc: 0.5349\n",
            "Epoch 278/300\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8255 - acc: 0.6136 - val_loss: 1.0271 - val_acc: 0.5349\n",
            "Epoch 279/300\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8253 - acc: 0.6136 - val_loss: 1.0280 - val_acc: 0.5349\n",
            "Epoch 280/300\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.8247 - acc: 0.6136 - val_loss: 1.0283 - val_acc: 0.5349\n",
            "Epoch 281/300\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.8243 - acc: 0.6136 - val_loss: 1.0287 - val_acc: 0.5349\n",
            "Epoch 282/300\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8242 - acc: 0.6136 - val_loss: 1.0295 - val_acc: 0.5349\n",
            "Epoch 283/300\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8242 - acc: 0.6136 - val_loss: 1.0298 - val_acc: 0.5349\n",
            "Epoch 284/300\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.8227 - acc: 0.6136 - val_loss: 1.0302 - val_acc: 0.5349\n",
            "Epoch 285/300\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8228 - acc: 0.6136 - val_loss: 1.0306 - val_acc: 0.5349\n",
            "Epoch 286/300\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.8221 - acc: 0.6136 - val_loss: 1.0311 - val_acc: 0.5349\n",
            "Epoch 287/300\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.8223 - acc: 0.6136 - val_loss: 1.0318 - val_acc: 0.5349\n",
            "Epoch 288/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8220 - acc: 0.6136 - val_loss: 1.0325 - val_acc: 0.5349\n",
            "Epoch 289/300\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8215 - acc: 0.6136 - val_loss: 1.0321 - val_acc: 0.5349\n",
            "Epoch 290/300\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8203 - acc: 0.6136 - val_loss: 1.0329 - val_acc: 0.5349\n",
            "Epoch 291/300\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8204 - acc: 0.6136 - val_loss: 1.0336 - val_acc: 0.5349\n",
            "Epoch 292/300\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.8198 - acc: 0.6136 - val_loss: 1.0339 - val_acc: 0.5349\n",
            "Epoch 293/300\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.8198 - acc: 0.6136 - val_loss: 1.0341 - val_acc: 0.5349\n",
            "Epoch 294/300\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.8195 - acc: 0.6136 - val_loss: 1.0349 - val_acc: 0.5349\n",
            "Epoch 295/300\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.8186 - acc: 0.6136 - val_loss: 1.0358 - val_acc: 0.5349\n",
            "Epoch 296/300\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.8187 - acc: 0.6250 - val_loss: 1.0365 - val_acc: 0.5349\n",
            "Epoch 297/300\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8187 - acc: 0.6136 - val_loss: 1.0366 - val_acc: 0.5349\n",
            "Epoch 298/300\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.8180 - acc: 0.6136 - val_loss: 1.0371 - val_acc: 0.5349\n",
            "Epoch 299/300\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.8170 - acc: 0.6250 - val_loss: 1.0377 - val_acc: 0.5349\n",
            "Epoch 300/300\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.8175 - acc: 0.6023 - val_loss: 1.0381 - val_acc: 0.5349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2eeOHoYbina",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zDN2PrRc36l",
        "colab_type": "code",
        "outputId": "f3872be4-611e-46c2-e619-77130a390fdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tss7vRUEgAcz",
        "colab_type": "code",
        "outputId": "f18eda24-5458-441e-a067-976cff41084b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpKE3iTJBHzO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "average_acc_history = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "average_val_acc_history = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "average_val_loss_history = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQrkCEMUD2RI",
        "colab_type": "code",
        "outputId": "50a3c0cf-7aa2-4d19-a26d-347c0b6df2dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9UhSxIaHtuO",
        "colab_type": "text"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq6zsienD5ct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJizyjnaIPhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfEHEYLgIQUQ",
        "colab_type": "code",
        "outputId": "dc5e25f3-fe0f-464d-e427-13bb5275653e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa985db1ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJwLnnEtwngiccy7BeSJwzrkE9/8B4kGDNx4hxFcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aoc4wMjfI97j",
        "colab_type": "text"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZi7VzbFIbtJ",
        "colab_type": "code",
        "outputId": "223ff954-7b56-474f-fccb-f7f31068b410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fa9859f9f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgN-E84SUPUq",
        "colab_type": "text"
      },
      "source": [
        "#Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VokWFUNPJOsJ",
        "colab_type": "code",
        "outputId": "8bec803c-0ddc-48bd-f0a1-f57a8ef4879f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_pca, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_pca, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.2654 - acc: 0.3130\n",
            "Epoch 2/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.2582 - acc: 0.3206\n",
            "Epoch 3/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.2512 - acc: 0.3282\n",
            "Epoch 4/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.2441 - acc: 0.3511\n",
            "Epoch 5/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2377 - acc: 0.3588\n",
            "Epoch 6/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.2312 - acc: 0.3817\n",
            "Epoch 7/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.2248 - acc: 0.3817\n",
            "Epoch 8/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.2190 - acc: 0.3893\n",
            "Epoch 9/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.2133 - acc: 0.3893\n",
            "Epoch 10/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.2078 - acc: 0.3893\n",
            "Epoch 11/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.2026 - acc: 0.3969\n",
            "Epoch 12/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1976 - acc: 0.3969\n",
            "Epoch 13/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.1924 - acc: 0.3969\n",
            "Epoch 14/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1874 - acc: 0.4046\n",
            "Epoch 15/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1828 - acc: 0.4198\n",
            "Epoch 16/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 1.1780 - acc: 0.4198\n",
            "Epoch 17/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.1734 - acc: 0.4275\n",
            "Epoch 18/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.1692 - acc: 0.4351\n",
            "Epoch 19/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1649 - acc: 0.4198\n",
            "Epoch 20/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.1608 - acc: 0.4198\n",
            "Epoch 21/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.1572 - acc: 0.4275\n",
            "Epoch 22/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 1.1535 - acc: 0.4275\n",
            "Epoch 23/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.1498 - acc: 0.4275\n",
            "Epoch 24/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.1461 - acc: 0.4351\n",
            "Epoch 25/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.1426 - acc: 0.4351\n",
            "Epoch 26/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.1394 - acc: 0.4351\n",
            "Epoch 27/1000\n",
            "131/131 [==============================] - 0s 225us/step - loss: 1.1360 - acc: 0.4351\n",
            "Epoch 28/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.1329 - acc: 0.4504\n",
            "Epoch 29/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1299 - acc: 0.4504\n",
            "Epoch 30/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 1.1269 - acc: 0.4504\n",
            "Epoch 31/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 1.1237 - acc: 0.4504\n",
            "Epoch 32/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 1.1209 - acc: 0.4504\n",
            "Epoch 33/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 1.1182 - acc: 0.4580\n",
            "Epoch 34/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.1154 - acc: 0.4580\n",
            "Epoch 35/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 1.1123 - acc: 0.4580\n",
            "Epoch 36/1000\n",
            "131/131 [==============================] - 0s 208us/step - loss: 1.1099 - acc: 0.4580\n",
            "Epoch 37/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.1074 - acc: 0.4656\n",
            "Epoch 38/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1051 - acc: 0.4656\n",
            "Epoch 39/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.1026 - acc: 0.4733\n",
            "Epoch 40/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.1002 - acc: 0.4733\n",
            "Epoch 41/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.0979 - acc: 0.4809\n",
            "Epoch 42/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0960 - acc: 0.4809\n",
            "Epoch 43/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0938 - acc: 0.4809\n",
            "Epoch 44/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0914 - acc: 0.4809\n",
            "Epoch 45/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0893 - acc: 0.4809\n",
            "Epoch 46/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0873 - acc: 0.4809\n",
            "Epoch 47/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0852 - acc: 0.4809\n",
            "Epoch 48/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0832 - acc: 0.4885\n",
            "Epoch 49/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0815 - acc: 0.4962\n",
            "Epoch 50/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 1.0797 - acc: 0.4962\n",
            "Epoch 51/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 1.0779 - acc: 0.4962\n",
            "Epoch 52/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0761 - acc: 0.4962\n",
            "Epoch 53/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 1.0746 - acc: 0.4962\n",
            "Epoch 54/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0726 - acc: 0.4962\n",
            "Epoch 55/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 1.0708 - acc: 0.4962\n",
            "Epoch 56/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0691 - acc: 0.5038\n",
            "Epoch 57/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0675 - acc: 0.5038\n",
            "Epoch 58/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0657 - acc: 0.5038\n",
            "Epoch 59/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0642 - acc: 0.5038\n",
            "Epoch 60/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.0625 - acc: 0.4962\n",
            "Epoch 61/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0610 - acc: 0.4962\n",
            "Epoch 62/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0594 - acc: 0.4962\n",
            "Epoch 63/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0579 - acc: 0.4962\n",
            "Epoch 64/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0564 - acc: 0.4962\n",
            "Epoch 65/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 1.0551 - acc: 0.4962\n",
            "Epoch 66/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 1.0535 - acc: 0.4962\n",
            "Epoch 67/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 1.0521 - acc: 0.4962\n",
            "Epoch 68/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 1.0506 - acc: 0.4962\n",
            "Epoch 69/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 1.0491 - acc: 0.4962\n",
            "Epoch 70/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0478 - acc: 0.4962\n",
            "Epoch 71/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0464 - acc: 0.4962\n",
            "Epoch 72/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 1.0452 - acc: 0.4962\n",
            "Epoch 73/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0438 - acc: 0.4962\n",
            "Epoch 74/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0426 - acc: 0.4962\n",
            "Epoch 75/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 1.0413 - acc: 0.4962\n",
            "Epoch 76/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0401 - acc: 0.4962\n",
            "Epoch 77/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 1.0388 - acc: 0.4962\n",
            "Epoch 78/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0375 - acc: 0.4962\n",
            "Epoch 79/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 1.0365 - acc: 0.4962\n",
            "Epoch 80/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.0355 - acc: 0.4962\n",
            "Epoch 81/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 1.0343 - acc: 0.4962\n",
            "Epoch 82/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0332 - acc: 0.4962\n",
            "Epoch 83/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 1.0322 - acc: 0.4885\n",
            "Epoch 84/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 1.0311 - acc: 0.4885\n",
            "Epoch 85/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 1.0302 - acc: 0.4885\n",
            "Epoch 86/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 1.0291 - acc: 0.4885\n",
            "Epoch 87/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 1.0281 - acc: 0.4885\n",
            "Epoch 88/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0271 - acc: 0.4885\n",
            "Epoch 89/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0262 - acc: 0.4885\n",
            "Epoch 90/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0250 - acc: 0.4885\n",
            "Epoch 91/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 1.0240 - acc: 0.4809\n",
            "Epoch 92/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 1.0232 - acc: 0.4733\n",
            "Epoch 93/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0222 - acc: 0.4733\n",
            "Epoch 94/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0213 - acc: 0.4733\n",
            "Epoch 95/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0203 - acc: 0.4809\n",
            "Epoch 96/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0195 - acc: 0.4733\n",
            "Epoch 97/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 1.0186 - acc: 0.4733\n",
            "Epoch 98/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 1.0176 - acc: 0.4809\n",
            "Epoch 99/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 1.0168 - acc: 0.4809\n",
            "Epoch 100/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 1.0161 - acc: 0.4809\n",
            "Epoch 101/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 1.0151 - acc: 0.4809\n",
            "Epoch 102/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0143 - acc: 0.4809\n",
            "Epoch 103/1000\n",
            "131/131 [==============================] - 0s 196us/step - loss: 1.0136 - acc: 0.4809\n",
            "Epoch 104/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0128 - acc: 0.4809\n",
            "Epoch 105/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0121 - acc: 0.4809\n",
            "Epoch 106/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 1.0115 - acc: 0.4809\n",
            "Epoch 107/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 1.0107 - acc: 0.4809\n",
            "Epoch 108/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 1.0101 - acc: 0.4809\n",
            "Epoch 109/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0094 - acc: 0.4809\n",
            "Epoch 110/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 1.0087 - acc: 0.4885\n",
            "Epoch 111/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 1.0081 - acc: 0.4885\n",
            "Epoch 112/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 1.0074 - acc: 0.4885\n",
            "Epoch 113/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0068 - acc: 0.4885\n",
            "Epoch 114/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0061 - acc: 0.4885\n",
            "Epoch 115/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0055 - acc: 0.4885\n",
            "Epoch 116/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 1.0049 - acc: 0.4885\n",
            "Epoch 117/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 1.0043 - acc: 0.4885\n",
            "Epoch 118/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 1.0037 - acc: 0.4885\n",
            "Epoch 119/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 1.0031 - acc: 0.4885\n",
            "Epoch 120/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 1.0024 - acc: 0.4885\n",
            "Epoch 121/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 1.0020 - acc: 0.4885\n",
            "Epoch 122/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 1.0013 - acc: 0.4885\n",
            "Epoch 123/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 1.0008 - acc: 0.4885\n",
            "Epoch 124/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 1.0002 - acc: 0.4885\n",
            "Epoch 125/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9995 - acc: 0.4885\n",
            "Epoch 126/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9989 - acc: 0.4885\n",
            "Epoch 127/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9983 - acc: 0.4885\n",
            "Epoch 128/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9978 - acc: 0.4885\n",
            "Epoch 129/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9972 - acc: 0.4885\n",
            "Epoch 130/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9966 - acc: 0.4885\n",
            "Epoch 131/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9960 - acc: 0.4885\n",
            "Epoch 132/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9954 - acc: 0.4885\n",
            "Epoch 133/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9948 - acc: 0.4885\n",
            "Epoch 134/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9943 - acc: 0.4885\n",
            "Epoch 135/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9937 - acc: 0.4885\n",
            "Epoch 136/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9930 - acc: 0.4962\n",
            "Epoch 137/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9923 - acc: 0.4962\n",
            "Epoch 138/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9918 - acc: 0.4962\n",
            "Epoch 139/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9913 - acc: 0.4962\n",
            "Epoch 140/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9907 - acc: 0.4962\n",
            "Epoch 141/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9900 - acc: 0.5038\n",
            "Epoch 142/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9894 - acc: 0.5038\n",
            "Epoch 143/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9890 - acc: 0.5038\n",
            "Epoch 144/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9884 - acc: 0.5115\n",
            "Epoch 145/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9880 - acc: 0.5038\n",
            "Epoch 146/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9874 - acc: 0.5115\n",
            "Epoch 147/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9869 - acc: 0.5115\n",
            "Epoch 148/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9864 - acc: 0.5115\n",
            "Epoch 149/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9859 - acc: 0.5115\n",
            "Epoch 150/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9854 - acc: 0.5191\n",
            "Epoch 151/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9849 - acc: 0.5191\n",
            "Epoch 152/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9845 - acc: 0.5191\n",
            "Epoch 153/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9840 - acc: 0.5191\n",
            "Epoch 154/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9836 - acc: 0.5191\n",
            "Epoch 155/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9831 - acc: 0.5191\n",
            "Epoch 156/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9826 - acc: 0.5191\n",
            "Epoch 157/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9821 - acc: 0.5267\n",
            "Epoch 158/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9817 - acc: 0.5420\n",
            "Epoch 159/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9812 - acc: 0.5420\n",
            "Epoch 160/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9807 - acc: 0.5420\n",
            "Epoch 161/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9802 - acc: 0.5344\n",
            "Epoch 162/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9797 - acc: 0.5420\n",
            "Epoch 163/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9793 - acc: 0.5420\n",
            "Epoch 164/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9788 - acc: 0.5420\n",
            "Epoch 165/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9783 - acc: 0.5420\n",
            "Epoch 166/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9778 - acc: 0.5420\n",
            "Epoch 167/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9773 - acc: 0.5420\n",
            "Epoch 168/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9768 - acc: 0.5420\n",
            "Epoch 169/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9765 - acc: 0.5420\n",
            "Epoch 170/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9760 - acc: 0.5420\n",
            "Epoch 171/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9755 - acc: 0.5420\n",
            "Epoch 172/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9750 - acc: 0.5420\n",
            "Epoch 173/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9746 - acc: 0.5420\n",
            "Epoch 174/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9743 - acc: 0.5420\n",
            "Epoch 175/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9738 - acc: 0.5420\n",
            "Epoch 176/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9733 - acc: 0.5420\n",
            "Epoch 177/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9727 - acc: 0.5420\n",
            "Epoch 178/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9725 - acc: 0.5420\n",
            "Epoch 179/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9719 - acc: 0.5344\n",
            "Epoch 180/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9715 - acc: 0.5344\n",
            "Epoch 181/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9711 - acc: 0.5344\n",
            "Epoch 182/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9707 - acc: 0.5344\n",
            "Epoch 183/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9702 - acc: 0.5344\n",
            "Epoch 184/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9699 - acc: 0.5344\n",
            "Epoch 185/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9693 - acc: 0.5344\n",
            "Epoch 186/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.9690 - acc: 0.5344\n",
            "Epoch 187/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9686 - acc: 0.5344\n",
            "Epoch 188/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9682 - acc: 0.5344\n",
            "Epoch 189/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9679 - acc: 0.5344\n",
            "Epoch 190/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9676 - acc: 0.5344\n",
            "Epoch 191/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9670 - acc: 0.5344\n",
            "Epoch 192/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9666 - acc: 0.5344\n",
            "Epoch 193/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9662 - acc: 0.5344\n",
            "Epoch 194/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9658 - acc: 0.5344\n",
            "Epoch 195/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9655 - acc: 0.5344\n",
            "Epoch 196/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9651 - acc: 0.5344\n",
            "Epoch 197/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9648 - acc: 0.5344\n",
            "Epoch 198/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9645 - acc: 0.5344\n",
            "Epoch 199/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9641 - acc: 0.5344\n",
            "Epoch 200/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9638 - acc: 0.5344\n",
            "Epoch 201/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9635 - acc: 0.5344\n",
            "Epoch 202/1000\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.9631 - acc: 0.5344\n",
            "Epoch 203/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9628 - acc: 0.5344\n",
            "Epoch 204/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9625 - acc: 0.5344\n",
            "Epoch 205/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9621 - acc: 0.5344\n",
            "Epoch 206/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9619 - acc: 0.5344\n",
            "Epoch 207/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9617 - acc: 0.5344\n",
            "Epoch 208/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9613 - acc: 0.5344\n",
            "Epoch 209/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9610 - acc: 0.5344\n",
            "Epoch 210/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9607 - acc: 0.5344\n",
            "Epoch 211/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9603 - acc: 0.5344\n",
            "Epoch 212/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9600 - acc: 0.5344\n",
            "Epoch 213/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9598 - acc: 0.5344\n",
            "Epoch 214/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9595 - acc: 0.5344\n",
            "Epoch 215/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9591 - acc: 0.5344\n",
            "Epoch 216/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9589 - acc: 0.5344\n",
            "Epoch 217/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9585 - acc: 0.5420\n",
            "Epoch 218/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9584 - acc: 0.5344\n",
            "Epoch 219/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9580 - acc: 0.5420\n",
            "Epoch 220/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9577 - acc: 0.5420\n",
            "Epoch 221/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9575 - acc: 0.5420\n",
            "Epoch 222/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9571 - acc: 0.5496\n",
            "Epoch 223/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9568 - acc: 0.5420\n",
            "Epoch 224/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9565 - acc: 0.5496\n",
            "Epoch 225/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9563 - acc: 0.5496\n",
            "Epoch 226/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9561 - acc: 0.5496\n",
            "Epoch 227/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9557 - acc: 0.5496\n",
            "Epoch 228/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9554 - acc: 0.5496\n",
            "Epoch 229/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9551 - acc: 0.5496\n",
            "Epoch 230/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9548 - acc: 0.5496\n",
            "Epoch 231/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9546 - acc: 0.5496\n",
            "Epoch 232/1000\n",
            "131/131 [==============================] - 0s 206us/step - loss: 0.9544 - acc: 0.5496\n",
            "Epoch 233/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9541 - acc: 0.5496\n",
            "Epoch 234/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9540 - acc: 0.5344\n",
            "Epoch 235/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9536 - acc: 0.5420\n",
            "Epoch 236/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9535 - acc: 0.5344\n",
            "Epoch 237/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9532 - acc: 0.5344\n",
            "Epoch 238/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9529 - acc: 0.5420\n",
            "Epoch 239/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9526 - acc: 0.5420\n",
            "Epoch 240/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9523 - acc: 0.5420\n",
            "Epoch 241/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9522 - acc: 0.5420\n",
            "Epoch 242/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9519 - acc: 0.5420\n",
            "Epoch 243/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9517 - acc: 0.5420\n",
            "Epoch 244/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9514 - acc: 0.5420\n",
            "Epoch 245/1000\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.9512 - acc: 0.5344\n",
            "Epoch 246/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9509 - acc: 0.5344\n",
            "Epoch 247/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9506 - acc: 0.5344\n",
            "Epoch 248/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9505 - acc: 0.5344\n",
            "Epoch 249/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9501 - acc: 0.5420\n",
            "Epoch 250/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9499 - acc: 0.5420\n",
            "Epoch 251/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9496 - acc: 0.5344\n",
            "Epoch 252/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9494 - acc: 0.5344\n",
            "Epoch 253/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9492 - acc: 0.5344\n",
            "Epoch 254/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9489 - acc: 0.5420\n",
            "Epoch 255/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9488 - acc: 0.5420\n",
            "Epoch 256/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9485 - acc: 0.5420\n",
            "Epoch 257/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9483 - acc: 0.5420\n",
            "Epoch 258/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9480 - acc: 0.5420\n",
            "Epoch 259/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9478 - acc: 0.5344\n",
            "Epoch 260/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9476 - acc: 0.5420\n",
            "Epoch 261/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9475 - acc: 0.5420\n",
            "Epoch 262/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9472 - acc: 0.5420\n",
            "Epoch 263/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9470 - acc: 0.5420\n",
            "Epoch 264/1000\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.9467 - acc: 0.5496\n",
            "Epoch 265/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9466 - acc: 0.5496\n",
            "Epoch 266/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9463 - acc: 0.5496\n",
            "Epoch 267/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9461 - acc: 0.5496\n",
            "Epoch 268/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9459 - acc: 0.5496\n",
            "Epoch 269/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9457 - acc: 0.5573\n",
            "Epoch 270/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9456 - acc: 0.5573\n",
            "Epoch 271/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9453 - acc: 0.5649\n",
            "Epoch 272/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9451 - acc: 0.5649\n",
            "Epoch 273/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9449 - acc: 0.5573\n",
            "Epoch 274/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9447 - acc: 0.5573\n",
            "Epoch 275/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9445 - acc: 0.5649\n",
            "Epoch 276/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9443 - acc: 0.5649\n",
            "Epoch 277/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9441 - acc: 0.5725\n",
            "Epoch 278/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9439 - acc: 0.5649\n",
            "Epoch 279/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9437 - acc: 0.5725\n",
            "Epoch 280/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9435 - acc: 0.5725\n",
            "Epoch 281/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9435 - acc: 0.5725\n",
            "Epoch 282/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9432 - acc: 0.5725\n",
            "Epoch 283/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9430 - acc: 0.5725\n",
            "Epoch 284/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9428 - acc: 0.5725\n",
            "Epoch 285/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9426 - acc: 0.5725\n",
            "Epoch 286/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9423 - acc: 0.5725\n",
            "Epoch 287/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9422 - acc: 0.5725\n",
            "Epoch 288/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9420 - acc: 0.5725\n",
            "Epoch 289/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9418 - acc: 0.5725\n",
            "Epoch 290/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9417 - acc: 0.5725\n",
            "Epoch 291/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9414 - acc: 0.5725\n",
            "Epoch 292/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9413 - acc: 0.5725\n",
            "Epoch 293/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9411 - acc: 0.5725\n",
            "Epoch 294/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9409 - acc: 0.5725\n",
            "Epoch 295/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9408 - acc: 0.5725\n",
            "Epoch 296/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9405 - acc: 0.5725\n",
            "Epoch 297/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9404 - acc: 0.5725\n",
            "Epoch 298/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9402 - acc: 0.5725\n",
            "Epoch 299/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9401 - acc: 0.5725\n",
            "Epoch 300/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9399 - acc: 0.5725\n",
            "Epoch 301/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.9398 - acc: 0.5725\n",
            "Epoch 302/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9397 - acc: 0.5725\n",
            "Epoch 303/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9395 - acc: 0.5725\n",
            "Epoch 304/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9392 - acc: 0.5725\n",
            "Epoch 305/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9391 - acc: 0.5725\n",
            "Epoch 306/1000\n",
            "131/131 [==============================] - 0s 137us/step - loss: 0.9390 - acc: 0.5725\n",
            "Epoch 307/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9388 - acc: 0.5725\n",
            "Epoch 308/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9386 - acc: 0.5725\n",
            "Epoch 309/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9386 - acc: 0.5725\n",
            "Epoch 310/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9384 - acc: 0.5725\n",
            "Epoch 311/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9381 - acc: 0.5725\n",
            "Epoch 312/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9379 - acc: 0.5725\n",
            "Epoch 313/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9378 - acc: 0.5725\n",
            "Epoch 314/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9376 - acc: 0.5725\n",
            "Epoch 315/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9375 - acc: 0.5725\n",
            "Epoch 316/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9375 - acc: 0.5725\n",
            "Epoch 317/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9372 - acc: 0.5725\n",
            "Epoch 318/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9370 - acc: 0.5725\n",
            "Epoch 319/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9368 - acc: 0.5725\n",
            "Epoch 320/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9368 - acc: 0.5725\n",
            "Epoch 321/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9365 - acc: 0.5725\n",
            "Epoch 322/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9363 - acc: 0.5725\n",
            "Epoch 323/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.9364 - acc: 0.5725\n",
            "Epoch 324/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9360 - acc: 0.5725\n",
            "Epoch 325/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9358 - acc: 0.5725\n",
            "Epoch 326/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9357 - acc: 0.5725\n",
            "Epoch 327/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9355 - acc: 0.5725\n",
            "Epoch 328/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9354 - acc: 0.5725\n",
            "Epoch 329/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9352 - acc: 0.5725\n",
            "Epoch 330/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9351 - acc: 0.5725\n",
            "Epoch 331/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9351 - acc: 0.5725\n",
            "Epoch 332/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9349 - acc: 0.5725\n",
            "Epoch 333/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.9346 - acc: 0.5802\n",
            "Epoch 334/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9344 - acc: 0.5802\n",
            "Epoch 335/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9343 - acc: 0.5802\n",
            "Epoch 336/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9342 - acc: 0.5802\n",
            "Epoch 337/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9341 - acc: 0.5802\n",
            "Epoch 338/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9339 - acc: 0.5802\n",
            "Epoch 339/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9337 - acc: 0.5802\n",
            "Epoch 340/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9335 - acc: 0.5802\n",
            "Epoch 341/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9334 - acc: 0.5802\n",
            "Epoch 342/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9333 - acc: 0.5802\n",
            "Epoch 343/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9331 - acc: 0.5802\n",
            "Epoch 344/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 345/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9329 - acc: 0.5802\n",
            "Epoch 346/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9326 - acc: 0.5802\n",
            "Epoch 347/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9324 - acc: 0.5802\n",
            "Epoch 348/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9324 - acc: 0.5802\n",
            "Epoch 349/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9322 - acc: 0.5878\n",
            "Epoch 350/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9320 - acc: 0.5878\n",
            "Epoch 351/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9319 - acc: 0.5878\n",
            "Epoch 352/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9318 - acc: 0.5878\n",
            "Epoch 353/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9316 - acc: 0.5954\n",
            "Epoch 354/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9315 - acc: 0.5878\n",
            "Epoch 355/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9313 - acc: 0.5954\n",
            "Epoch 356/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9312 - acc: 0.5954\n",
            "Epoch 357/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9310 - acc: 0.5954\n",
            "Epoch 358/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9309 - acc: 0.5954\n",
            "Epoch 359/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9307 - acc: 0.5954\n",
            "Epoch 360/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9306 - acc: 0.5954\n",
            "Epoch 361/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9304 - acc: 0.5954\n",
            "Epoch 362/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9303 - acc: 0.5954\n",
            "Epoch 363/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9301 - acc: 0.5954\n",
            "Epoch 364/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9299 - acc: 0.5954\n",
            "Epoch 365/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9299 - acc: 0.5954\n",
            "Epoch 366/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9298 - acc: 0.5954\n",
            "Epoch 367/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9296 - acc: 0.5954\n",
            "Epoch 368/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9294 - acc: 0.5954\n",
            "Epoch 369/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9293 - acc: 0.5954\n",
            "Epoch 370/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9293 - acc: 0.5954\n",
            "Epoch 371/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9292 - acc: 0.5954\n",
            "Epoch 372/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9290 - acc: 0.5878\n",
            "Epoch 373/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9289 - acc: 0.5878\n",
            "Epoch 374/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9286 - acc: 0.5878\n",
            "Epoch 375/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9286 - acc: 0.5878\n",
            "Epoch 376/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9284 - acc: 0.5878\n",
            "Epoch 377/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9283 - acc: 0.5878\n",
            "Epoch 378/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9282 - acc: 0.5878\n",
            "Epoch 379/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9280 - acc: 0.5878\n",
            "Epoch 380/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9279 - acc: 0.5878\n",
            "Epoch 381/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9278 - acc: 0.5878\n",
            "Epoch 382/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9277 - acc: 0.5878\n",
            "Epoch 383/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9275 - acc: 0.5878\n",
            "Epoch 384/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9275 - acc: 0.5878\n",
            "Epoch 385/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9273 - acc: 0.5878\n",
            "Epoch 386/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9273 - acc: 0.5878\n",
            "Epoch 387/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9272 - acc: 0.5878\n",
            "Epoch 388/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9270 - acc: 0.5878\n",
            "Epoch 389/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.9269 - acc: 0.5878\n",
            "Epoch 390/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9267 - acc: 0.5878\n",
            "Epoch 391/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9267 - acc: 0.5878\n",
            "Epoch 392/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9266 - acc: 0.5878\n",
            "Epoch 393/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9263 - acc: 0.5878\n",
            "Epoch 394/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9263 - acc: 0.5954\n",
            "Epoch 395/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9261 - acc: 0.5878\n",
            "Epoch 396/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9259 - acc: 0.5878\n",
            "Epoch 397/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9258 - acc: 0.5954\n",
            "Epoch 398/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9257 - acc: 0.5878\n",
            "Epoch 399/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9257 - acc: 0.5954\n",
            "Epoch 400/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9256 - acc: 0.5954\n",
            "Epoch 401/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9253 - acc: 0.5954\n",
            "Epoch 402/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9252 - acc: 0.5954\n",
            "Epoch 403/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9251 - acc: 0.5954\n",
            "Epoch 404/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9250 - acc: 0.5954\n",
            "Epoch 405/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9249 - acc: 0.5954\n",
            "Epoch 406/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9247 - acc: 0.5954\n",
            "Epoch 407/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9249 - acc: 0.5954\n",
            "Epoch 408/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9245 - acc: 0.5954\n",
            "Epoch 409/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9245 - acc: 0.5954\n",
            "Epoch 410/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9243 - acc: 0.5954\n",
            "Epoch 411/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9241 - acc: 0.5954\n",
            "Epoch 412/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9239 - acc: 0.5954\n",
            "Epoch 413/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9239 - acc: 0.5954\n",
            "Epoch 414/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9237 - acc: 0.5954\n",
            "Epoch 415/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9235 - acc: 0.5954\n",
            "Epoch 416/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9235 - acc: 0.5954\n",
            "Epoch 417/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9234 - acc: 0.5954\n",
            "Epoch 418/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9231 - acc: 0.5954\n",
            "Epoch 419/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9230 - acc: 0.5954\n",
            "Epoch 420/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9229 - acc: 0.5954\n",
            "Epoch 421/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9229 - acc: 0.5954\n",
            "Epoch 422/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9227 - acc: 0.5954\n",
            "Epoch 423/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9226 - acc: 0.5954\n",
            "Epoch 424/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9224 - acc: 0.5954\n",
            "Epoch 425/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9224 - acc: 0.5954\n",
            "Epoch 426/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9222 - acc: 0.5954\n",
            "Epoch 427/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9220 - acc: 0.5954\n",
            "Epoch 428/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9219 - acc: 0.5954\n",
            "Epoch 429/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9219 - acc: 0.5954\n",
            "Epoch 430/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9216 - acc: 0.5954\n",
            "Epoch 431/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9215 - acc: 0.5954\n",
            "Epoch 432/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9215 - acc: 0.5954\n",
            "Epoch 433/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9214 - acc: 0.5954\n",
            "Epoch 434/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9213 - acc: 0.5954\n",
            "Epoch 435/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9210 - acc: 0.5954\n",
            "Epoch 436/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9210 - acc: 0.5954\n",
            "Epoch 437/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9209 - acc: 0.5954\n",
            "Epoch 438/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9207 - acc: 0.6031\n",
            "Epoch 439/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9206 - acc: 0.6031\n",
            "Epoch 440/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9204 - acc: 0.6031\n",
            "Epoch 441/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9204 - acc: 0.6031\n",
            "Epoch 442/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9203 - acc: 0.6031\n",
            "Epoch 443/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9201 - acc: 0.6107\n",
            "Epoch 444/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9200 - acc: 0.6107\n",
            "Epoch 445/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.9199 - acc: 0.6107\n",
            "Epoch 446/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9198 - acc: 0.6031\n",
            "Epoch 447/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9198 - acc: 0.6107\n",
            "Epoch 448/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9196 - acc: 0.6031\n",
            "Epoch 449/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9194 - acc: 0.6107\n",
            "Epoch 450/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9193 - acc: 0.6031\n",
            "Epoch 451/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9191 - acc: 0.6031\n",
            "Epoch 452/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9191 - acc: 0.6031\n",
            "Epoch 453/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9188 - acc: 0.5954\n",
            "Epoch 454/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9187 - acc: 0.5954\n",
            "Epoch 455/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9186 - acc: 0.6031\n",
            "Epoch 456/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9185 - acc: 0.5954\n",
            "Epoch 457/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9184 - acc: 0.6031\n",
            "Epoch 458/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9183 - acc: 0.6031\n",
            "Epoch 459/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9181 - acc: 0.6031\n",
            "Epoch 460/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9182 - acc: 0.5954\n",
            "Epoch 461/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9180 - acc: 0.6031\n",
            "Epoch 462/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9179 - acc: 0.5954\n",
            "Epoch 463/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9178 - acc: 0.6031\n",
            "Epoch 464/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9176 - acc: 0.5954\n",
            "Epoch 465/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9175 - acc: 0.5954\n",
            "Epoch 466/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9174 - acc: 0.6031\n",
            "Epoch 467/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9173 - acc: 0.5954\n",
            "Epoch 468/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9172 - acc: 0.5954\n",
            "Epoch 469/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.9171 - acc: 0.5954\n",
            "Epoch 470/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9169 - acc: 0.6031\n",
            "Epoch 471/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9168 - acc: 0.6031\n",
            "Epoch 472/1000\n",
            "131/131 [==============================] - 0s 211us/step - loss: 0.9167 - acc: 0.6031\n",
            "Epoch 473/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9165 - acc: 0.6031\n",
            "Epoch 474/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9164 - acc: 0.6031\n",
            "Epoch 475/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9164 - acc: 0.5954\n",
            "Epoch 476/1000\n",
            "131/131 [==============================] - 0s 204us/step - loss: 0.9164 - acc: 0.5954\n",
            "Epoch 477/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9161 - acc: 0.6031\n",
            "Epoch 478/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.9159 - acc: 0.6031\n",
            "Epoch 479/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9159 - acc: 0.6031\n",
            "Epoch 480/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9157 - acc: 0.6031\n",
            "Epoch 481/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9157 - acc: 0.6031\n",
            "Epoch 482/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9155 - acc: 0.6031\n",
            "Epoch 483/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9154 - acc: 0.6031\n",
            "Epoch 484/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9154 - acc: 0.6031\n",
            "Epoch 485/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9151 - acc: 0.6031\n",
            "Epoch 486/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9151 - acc: 0.5954\n",
            "Epoch 487/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9149 - acc: 0.6031\n",
            "Epoch 488/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9148 - acc: 0.6031\n",
            "Epoch 489/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9146 - acc: 0.6031\n",
            "Epoch 490/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9145 - acc: 0.6031\n",
            "Epoch 491/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9144 - acc: 0.6031\n",
            "Epoch 492/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9142 - acc: 0.5954\n",
            "Epoch 493/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9141 - acc: 0.5954\n",
            "Epoch 494/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9140 - acc: 0.5954\n",
            "Epoch 495/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.9141 - acc: 0.6031\n",
            "Epoch 496/1000\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.9138 - acc: 0.5954\n",
            "Epoch 497/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9137 - acc: 0.6031\n",
            "Epoch 498/1000\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.9135 - acc: 0.5954\n",
            "Epoch 499/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9135 - acc: 0.5954\n",
            "Epoch 500/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.9134 - acc: 0.5954\n",
            "Epoch 501/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.9132 - acc: 0.5954\n",
            "Epoch 502/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.9132 - acc: 0.5954\n",
            "Epoch 503/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9130 - acc: 0.6031\n",
            "Epoch 504/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9128 - acc: 0.6031\n",
            "Epoch 505/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9128 - acc: 0.6031\n",
            "Epoch 506/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9127 - acc: 0.6031\n",
            "Epoch 507/1000\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.9126 - acc: 0.6031\n",
            "Epoch 508/1000\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.9125 - acc: 0.6031\n",
            "Epoch 509/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.9123 - acc: 0.6031\n",
            "Epoch 510/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9122 - acc: 0.6031\n",
            "Epoch 511/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9121 - acc: 0.6031\n",
            "Epoch 512/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.9119 - acc: 0.6031\n",
            "Epoch 513/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9120 - acc: 0.6031\n",
            "Epoch 514/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.9118 - acc: 0.5954\n",
            "Epoch 515/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9116 - acc: 0.5954\n",
            "Epoch 516/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9115 - acc: 0.5954\n",
            "Epoch 517/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9114 - acc: 0.5954\n",
            "Epoch 518/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9113 - acc: 0.5954\n",
            "Epoch 519/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9112 - acc: 0.5954\n",
            "Epoch 520/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9112 - acc: 0.5954\n",
            "Epoch 521/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9109 - acc: 0.6031\n",
            "Epoch 522/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9109 - acc: 0.5954\n",
            "Epoch 523/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9107 - acc: 0.6031\n",
            "Epoch 524/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.9106 - acc: 0.6031\n",
            "Epoch 525/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.9106 - acc: 0.5954\n",
            "Epoch 526/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9105 - acc: 0.5954\n",
            "Epoch 527/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9103 - acc: 0.5954\n",
            "Epoch 528/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9103 - acc: 0.5954\n",
            "Epoch 529/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9101 - acc: 0.5954\n",
            "Epoch 530/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.9100 - acc: 0.5954\n",
            "Epoch 531/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9099 - acc: 0.6031\n",
            "Epoch 532/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9097 - acc: 0.5954\n",
            "Epoch 533/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9096 - acc: 0.6031\n",
            "Epoch 534/1000\n",
            "131/131 [==============================] - 0s 240us/step - loss: 0.9096 - acc: 0.6031\n",
            "Epoch 535/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9095 - acc: 0.6031\n",
            "Epoch 536/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9094 - acc: 0.6031\n",
            "Epoch 537/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9093 - acc: 0.5954\n",
            "Epoch 538/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.9091 - acc: 0.6031\n",
            "Epoch 539/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.9092 - acc: 0.6031\n",
            "Epoch 540/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9089 - acc: 0.6031\n",
            "Epoch 541/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9089 - acc: 0.5954\n",
            "Epoch 542/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9087 - acc: 0.5954\n",
            "Epoch 543/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9086 - acc: 0.6031\n",
            "Epoch 544/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9085 - acc: 0.5954\n",
            "Epoch 545/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9086 - acc: 0.6031\n",
            "Epoch 546/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9083 - acc: 0.6031\n",
            "Epoch 547/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9083 - acc: 0.5954\n",
            "Epoch 548/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9082 - acc: 0.6031\n",
            "Epoch 549/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9080 - acc: 0.5954\n",
            "Epoch 550/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9080 - acc: 0.6031\n",
            "Epoch 551/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9079 - acc: 0.5954\n",
            "Epoch 552/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9079 - acc: 0.5954\n",
            "Epoch 553/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9077 - acc: 0.5954\n",
            "Epoch 554/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9077 - acc: 0.6031\n",
            "Epoch 555/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9074 - acc: 0.6031\n",
            "Epoch 556/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9074 - acc: 0.5954\n",
            "Epoch 557/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9074 - acc: 0.5954\n",
            "Epoch 558/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.9072 - acc: 0.5954\n",
            "Epoch 559/1000\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.9071 - acc: 0.6031\n",
            "Epoch 560/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.9070 - acc: 0.6031\n",
            "Epoch 561/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9070 - acc: 0.5954\n",
            "Epoch 562/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9069 - acc: 0.5954\n",
            "Epoch 563/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9069 - acc: 0.5954\n",
            "Epoch 564/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.9067 - acc: 0.6031\n",
            "Epoch 565/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.9066 - acc: 0.5954\n",
            "Epoch 566/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9065 - acc: 0.5954\n",
            "Epoch 567/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9063 - acc: 0.5954\n",
            "Epoch 568/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.9063 - acc: 0.5954\n",
            "Epoch 569/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9062 - acc: 0.5954\n",
            "Epoch 570/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9062 - acc: 0.5954\n",
            "Epoch 571/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9061 - acc: 0.5954\n",
            "Epoch 572/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9060 - acc: 0.6031\n",
            "Epoch 573/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9060 - acc: 0.6031\n",
            "Epoch 574/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9058 - acc: 0.6031\n",
            "Epoch 575/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9057 - acc: 0.6031\n",
            "Epoch 576/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9056 - acc: 0.6031\n",
            "Epoch 577/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9055 - acc: 0.6031\n",
            "Epoch 578/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9055 - acc: 0.6031\n",
            "Epoch 579/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9052 - acc: 0.6031\n",
            "Epoch 580/1000\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.9052 - acc: 0.6031\n",
            "Epoch 581/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.9051 - acc: 0.6031\n",
            "Epoch 582/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9049 - acc: 0.6031\n",
            "Epoch 583/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9051 - acc: 0.5954\n",
            "Epoch 584/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9048 - acc: 0.6031\n",
            "Epoch 585/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9047 - acc: 0.6031\n",
            "Epoch 586/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9046 - acc: 0.6031\n",
            "Epoch 587/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9045 - acc: 0.6031\n",
            "Epoch 588/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.9043 - acc: 0.6031\n",
            "Epoch 589/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.9045 - acc: 0.6031\n",
            "Epoch 590/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9043 - acc: 0.6031\n",
            "Epoch 591/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.9042 - acc: 0.6031\n",
            "Epoch 592/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9041 - acc: 0.6031\n",
            "Epoch 593/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9041 - acc: 0.6031\n",
            "Epoch 594/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.9038 - acc: 0.5954\n",
            "Epoch 595/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9038 - acc: 0.5954\n",
            "Epoch 596/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9037 - acc: 0.5954\n",
            "Epoch 597/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 598/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 599/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9035 - acc: 0.5954\n",
            "Epoch 600/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 601/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 602/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.9033 - acc: 0.5954\n",
            "Epoch 603/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9030 - acc: 0.5954\n",
            "Epoch 604/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.9030 - acc: 0.5954\n",
            "Epoch 605/1000\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.9029 - acc: 0.5954\n",
            "Epoch 606/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.9028 - acc: 0.5954\n",
            "Epoch 607/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9027 - acc: 0.5954\n",
            "Epoch 608/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.9026 - acc: 0.5954\n",
            "Epoch 609/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9026 - acc: 0.6031\n",
            "Epoch 610/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9024 - acc: 0.5954\n",
            "Epoch 611/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.9023 - acc: 0.5954\n",
            "Epoch 612/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9023 - acc: 0.5954\n",
            "Epoch 613/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.9022 - acc: 0.5954\n",
            "Epoch 614/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.9021 - acc: 0.5954\n",
            "Epoch 615/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9020 - acc: 0.5954\n",
            "Epoch 616/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9019 - acc: 0.5954\n",
            "Epoch 617/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9017 - acc: 0.5954\n",
            "Epoch 618/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9017 - acc: 0.5954\n",
            "Epoch 619/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9016 - acc: 0.5954\n",
            "Epoch 620/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.9016 - acc: 0.6031\n",
            "Epoch 621/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.9015 - acc: 0.5954\n",
            "Epoch 622/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.9012 - acc: 0.6031\n",
            "Epoch 623/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.9012 - acc: 0.5954\n",
            "Epoch 624/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.9011 - acc: 0.6031\n",
            "Epoch 625/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9012 - acc: 0.6031\n",
            "Epoch 626/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.9009 - acc: 0.6031\n",
            "Epoch 627/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.9008 - acc: 0.6031\n",
            "Epoch 628/1000\n",
            "131/131 [==============================] - 0s 225us/step - loss: 0.9008 - acc: 0.6031\n",
            "Epoch 629/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.9007 - acc: 0.6031\n",
            "Epoch 630/1000\n",
            "131/131 [==============================] - 0s 200us/step - loss: 0.9006 - acc: 0.6031\n",
            "Epoch 631/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9005 - acc: 0.6031\n",
            "Epoch 632/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9005 - acc: 0.6031\n",
            "Epoch 633/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.9004 - acc: 0.5954\n",
            "Epoch 634/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.9003 - acc: 0.6031\n",
            "Epoch 635/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.9002 - acc: 0.6031\n",
            "Epoch 636/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.9002 - acc: 0.6031\n",
            "Epoch 637/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.9001 - acc: 0.6031\n",
            "Epoch 638/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.9001 - acc: 0.6031\n",
            "Epoch 639/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8998 - acc: 0.6031\n",
            "Epoch 640/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8999 - acc: 0.6031\n",
            "Epoch 641/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8996 - acc: 0.6031\n",
            "Epoch 642/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8996 - acc: 0.5954\n",
            "Epoch 643/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8995 - acc: 0.6031\n",
            "Epoch 644/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8994 - acc: 0.6031\n",
            "Epoch 645/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8994 - acc: 0.6031\n",
            "Epoch 646/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8993 - acc: 0.5954\n",
            "Epoch 647/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8991 - acc: 0.6031\n",
            "Epoch 648/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8991 - acc: 0.6107\n",
            "Epoch 649/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8990 - acc: 0.6107\n",
            "Epoch 650/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8989 - acc: 0.6107\n",
            "Epoch 651/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8989 - acc: 0.5954\n",
            "Epoch 652/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8987 - acc: 0.6031\n",
            "Epoch 653/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8987 - acc: 0.6107\n",
            "Epoch 654/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8985 - acc: 0.6107\n",
            "Epoch 655/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8985 - acc: 0.6031\n",
            "Epoch 656/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8984 - acc: 0.6031\n",
            "Epoch 657/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 658/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 659/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8983 - acc: 0.6031\n",
            "Epoch 660/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8981 - acc: 0.6031\n",
            "Epoch 661/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 662/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8980 - acc: 0.6031\n",
            "Epoch 663/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 664/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8979 - acc: 0.6031\n",
            "Epoch 665/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8976 - acc: 0.6031\n",
            "Epoch 666/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8974 - acc: 0.6031\n",
            "Epoch 667/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8975 - acc: 0.6107\n",
            "Epoch 668/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.8973 - acc: 0.6031\n",
            "Epoch 669/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8973 - acc: 0.6031\n",
            "Epoch 670/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8972 - acc: 0.6031\n",
            "Epoch 671/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8971 - acc: 0.6031\n",
            "Epoch 672/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8970 - acc: 0.6031\n",
            "Epoch 673/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8969 - acc: 0.6031\n",
            "Epoch 674/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8968 - acc: 0.6031\n",
            "Epoch 675/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8967 - acc: 0.6031\n",
            "Epoch 676/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8966 - acc: 0.6031\n",
            "Epoch 677/1000\n",
            "131/131 [==============================] - 0s 231us/step - loss: 0.8966 - acc: 0.6031\n",
            "Epoch 678/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8965 - acc: 0.6107\n",
            "Epoch 679/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8964 - acc: 0.6107\n",
            "Epoch 680/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8963 - acc: 0.5954\n",
            "Epoch 681/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8962 - acc: 0.6031\n",
            "Epoch 682/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8962 - acc: 0.6031\n",
            "Epoch 683/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8960 - acc: 0.6031\n",
            "Epoch 684/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8960 - acc: 0.6107\n",
            "Epoch 685/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8958 - acc: 0.6031\n",
            "Epoch 686/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8958 - acc: 0.6031\n",
            "Epoch 687/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8959 - acc: 0.6107\n",
            "Epoch 688/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8956 - acc: 0.6031\n",
            "Epoch 689/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8957 - acc: 0.5954\n",
            "Epoch 690/1000\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.8955 - acc: 0.6031\n",
            "Epoch 691/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8953 - acc: 0.5954\n",
            "Epoch 692/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8954 - acc: 0.5954\n",
            "Epoch 693/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8952 - acc: 0.6031\n",
            "Epoch 694/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8952 - acc: 0.5954\n",
            "Epoch 695/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8952 - acc: 0.6031\n",
            "Epoch 696/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8951 - acc: 0.6031\n",
            "Epoch 697/1000\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 698/1000\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 699/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8949 - acc: 0.5954\n",
            "Epoch 700/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8947 - acc: 0.6031\n",
            "Epoch 701/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8945 - acc: 0.6031\n",
            "Epoch 702/1000\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.8946 - acc: 0.5954\n",
            "Epoch 703/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8944 - acc: 0.5954\n",
            "Epoch 704/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8944 - acc: 0.5954\n",
            "Epoch 705/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8946 - acc: 0.5954\n",
            "Epoch 706/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8942 - acc: 0.6031\n",
            "Epoch 707/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8941 - acc: 0.6031\n",
            "Epoch 708/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8941 - acc: 0.6031\n",
            "Epoch 709/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8940 - acc: 0.6031\n",
            "Epoch 710/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8939 - acc: 0.6031\n",
            "Epoch 711/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8939 - acc: 0.6031\n",
            "Epoch 712/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8938 - acc: 0.6031\n",
            "Epoch 713/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8938 - acc: 0.6031\n",
            "Epoch 714/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8937 - acc: 0.6031\n",
            "Epoch 715/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8936 - acc: 0.6031\n",
            "Epoch 716/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8935 - acc: 0.6031\n",
            "Epoch 717/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8933 - acc: 0.6031\n",
            "Epoch 718/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8934 - acc: 0.6031\n",
            "Epoch 719/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8933 - acc: 0.6031\n",
            "Epoch 720/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8931 - acc: 0.6031\n",
            "Epoch 721/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8931 - acc: 0.6031\n",
            "Epoch 722/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8930 - acc: 0.6031\n",
            "Epoch 723/1000\n",
            "131/131 [==============================] - 0s 214us/step - loss: 0.8929 - acc: 0.6031\n",
            "Epoch 724/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8929 - acc: 0.6031\n",
            "Epoch 725/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8928 - acc: 0.6107\n",
            "Epoch 726/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8927 - acc: 0.6107\n",
            "Epoch 727/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8927 - acc: 0.6031\n",
            "Epoch 728/1000\n",
            "131/131 [==============================] - 0s 212us/step - loss: 0.8926 - acc: 0.6031\n",
            "Epoch 729/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8924 - acc: 0.6031\n",
            "Epoch 730/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8924 - acc: 0.6031\n",
            "Epoch 731/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8923 - acc: 0.6031\n",
            "Epoch 732/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8922 - acc: 0.6031\n",
            "Epoch 733/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8923 - acc: 0.6031\n",
            "Epoch 734/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8921 - acc: 0.6031\n",
            "Epoch 735/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8920 - acc: 0.6031\n",
            "Epoch 736/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8919 - acc: 0.6031\n",
            "Epoch 737/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8918 - acc: 0.6031\n",
            "Epoch 738/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8917 - acc: 0.6031\n",
            "Epoch 739/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8917 - acc: 0.6107\n",
            "Epoch 740/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8916 - acc: 0.6031\n",
            "Epoch 741/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8915 - acc: 0.6031\n",
            "Epoch 742/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8915 - acc: 0.6031\n",
            "Epoch 743/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8913 - acc: 0.6107\n",
            "Epoch 744/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8912 - acc: 0.6031\n",
            "Epoch 745/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8911 - acc: 0.6031\n",
            "Epoch 746/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8911 - acc: 0.6107\n",
            "Epoch 747/1000\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.8910 - acc: 0.6031\n",
            "Epoch 748/1000\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.8910 - acc: 0.6107\n",
            "Epoch 749/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8909 - acc: 0.6107\n",
            "Epoch 750/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8908 - acc: 0.6107\n",
            "Epoch 751/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8906 - acc: 0.6107\n",
            "Epoch 752/1000\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.8905 - acc: 0.6031\n",
            "Epoch 753/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8905 - acc: 0.6031\n",
            "Epoch 754/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8904 - acc: 0.6031\n",
            "Epoch 755/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8902 - acc: 0.6031\n",
            "Epoch 756/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8903 - acc: 0.6031\n",
            "Epoch 757/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8902 - acc: 0.6031\n",
            "Epoch 758/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8903 - acc: 0.6031\n",
            "Epoch 759/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8900 - acc: 0.6031\n",
            "Epoch 760/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8900 - acc: 0.6031\n",
            "Epoch 761/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8899 - acc: 0.6031\n",
            "Epoch 762/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8898 - acc: 0.6031\n",
            "Epoch 763/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8897 - acc: 0.6031\n",
            "Epoch 764/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8897 - acc: 0.6031\n",
            "Epoch 765/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8896 - acc: 0.6031\n",
            "Epoch 766/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8895 - acc: 0.6031\n",
            "Epoch 767/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8893 - acc: 0.6031\n",
            "Epoch 768/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8894 - acc: 0.6031\n",
            "Epoch 769/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8892 - acc: 0.6031\n",
            "Epoch 770/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8891 - acc: 0.6031\n",
            "Epoch 771/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8892 - acc: 0.6031\n",
            "Epoch 772/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8889 - acc: 0.6031\n",
            "Epoch 773/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8889 - acc: 0.6031\n",
            "Epoch 774/1000\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.8888 - acc: 0.6031\n",
            "Epoch 775/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8886 - acc: 0.6031\n",
            "Epoch 776/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8887 - acc: 0.6031\n",
            "Epoch 777/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8885 - acc: 0.6031\n",
            "Epoch 778/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8886 - acc: 0.6031\n",
            "Epoch 779/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8885 - acc: 0.6031\n",
            "Epoch 780/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8884 - acc: 0.6031\n",
            "Epoch 781/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 782/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 783/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8882 - acc: 0.6031\n",
            "Epoch 784/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8881 - acc: 0.6031\n",
            "Epoch 785/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8880 - acc: 0.6031\n",
            "Epoch 786/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8879 - acc: 0.6031\n",
            "Epoch 787/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8877 - acc: 0.6031\n",
            "Epoch 788/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 789/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 790/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8876 - acc: 0.6031\n",
            "Epoch 791/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8874 - acc: 0.6031\n",
            "Epoch 792/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8873 - acc: 0.6031\n",
            "Epoch 793/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8872 - acc: 0.6031\n",
            "Epoch 794/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8873 - acc: 0.6031\n",
            "Epoch 795/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8870 - acc: 0.6031\n",
            "Epoch 796/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8870 - acc: 0.6031\n",
            "Epoch 797/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8869 - acc: 0.6031\n",
            "Epoch 798/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8868 - acc: 0.6031\n",
            "Epoch 799/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8867 - acc: 0.6031\n",
            "Epoch 800/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 801/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 802/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8866 - acc: 0.6031\n",
            "Epoch 803/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8865 - acc: 0.6031\n",
            "Epoch 804/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 805/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 806/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8863 - acc: 0.6031\n",
            "Epoch 807/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8862 - acc: 0.6031\n",
            "Epoch 808/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8861 - acc: 0.6031\n",
            "Epoch 809/1000\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.8859 - acc: 0.5954\n",
            "Epoch 810/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8859 - acc: 0.6031\n",
            "Epoch 811/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8858 - acc: 0.6031\n",
            "Epoch 812/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8858 - acc: 0.5954\n",
            "Epoch 813/1000\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.8856 - acc: 0.6031\n",
            "Epoch 814/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8855 - acc: 0.6031\n",
            "Epoch 815/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8854 - acc: 0.6031\n",
            "Epoch 816/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8855 - acc: 0.6031\n",
            "Epoch 817/1000\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.8853 - acc: 0.6031\n",
            "Epoch 818/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8852 - acc: 0.6031\n",
            "Epoch 819/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8851 - acc: 0.6031\n",
            "Epoch 820/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8851 - acc: 0.6031\n",
            "Epoch 821/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8850 - acc: 0.6031\n",
            "Epoch 822/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8849 - acc: 0.6031\n",
            "Epoch 823/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8847 - acc: 0.6031\n",
            "Epoch 824/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8848 - acc: 0.6031\n",
            "Epoch 825/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8847 - acc: 0.6031\n",
            "Epoch 826/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8845 - acc: 0.6031\n",
            "Epoch 827/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8845 - acc: 0.6031\n",
            "Epoch 828/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8844 - acc: 0.6031\n",
            "Epoch 829/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8843 - acc: 0.6031\n",
            "Epoch 830/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 831/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 832/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8840 - acc: 0.6031\n",
            "Epoch 833/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8842 - acc: 0.6031\n",
            "Epoch 834/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8839 - acc: 0.6031\n",
            "Epoch 835/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8840 - acc: 0.6031\n",
            "Epoch 836/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8838 - acc: 0.6031\n",
            "Epoch 837/1000\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.8837 - acc: 0.6031\n",
            "Epoch 838/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 839/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 840/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8836 - acc: 0.6031\n",
            "Epoch 841/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 842/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 843/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8834 - acc: 0.6031\n",
            "Epoch 844/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8832 - acc: 0.6031\n",
            "Epoch 845/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8832 - acc: 0.6031\n",
            "Epoch 846/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8831 - acc: 0.6031\n",
            "Epoch 847/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 848/1000\n",
            "131/131 [==============================] - 0s 199us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 849/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8829 - acc: 0.6031\n",
            "Epoch 850/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8827 - acc: 0.6031\n",
            "Epoch 851/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8828 - acc: 0.6031\n",
            "Epoch 852/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 853/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 854/1000\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.8826 - acc: 0.6031\n",
            "Epoch 855/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8824 - acc: 0.6031\n",
            "Epoch 856/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8823 - acc: 0.6031\n",
            "Epoch 857/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8823 - acc: 0.6031\n",
            "Epoch 858/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8822 - acc: 0.6031\n",
            "Epoch 859/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8822 - acc: 0.6031\n",
            "Epoch 860/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8820 - acc: 0.6031\n",
            "Epoch 861/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 862/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 863/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8818 - acc: 0.6031\n",
            "Epoch 864/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8818 - acc: 0.6031\n",
            "Epoch 865/1000\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8819 - acc: 0.6031\n",
            "Epoch 866/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8817 - acc: 0.6031\n",
            "Epoch 867/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8816 - acc: 0.6031\n",
            "Epoch 868/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8816 - acc: 0.6031\n",
            "Epoch 869/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8814 - acc: 0.6031\n",
            "Epoch 870/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8814 - acc: 0.6031\n",
            "Epoch 871/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 872/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 873/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8812 - acc: 0.6031\n",
            "Epoch 874/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8811 - acc: 0.6031\n",
            "Epoch 875/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8810 - acc: 0.6031\n",
            "Epoch 876/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 877/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 878/1000\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.8808 - acc: 0.6031\n",
            "Epoch 879/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8809 - acc: 0.6031\n",
            "Epoch 880/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8805 - acc: 0.6031\n",
            "Epoch 881/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8806 - acc: 0.6031\n",
            "Epoch 882/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8806 - acc: 0.6031\n",
            "Epoch 883/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8804 - acc: 0.6031\n",
            "Epoch 884/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8803 - acc: 0.6031\n",
            "Epoch 885/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8803 - acc: 0.6031\n",
            "Epoch 886/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8802 - acc: 0.6031\n",
            "Epoch 887/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8802 - acc: 0.6031\n",
            "Epoch 888/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 889/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8801 - acc: 0.6031\n",
            "Epoch 890/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8799 - acc: 0.6031\n",
            "Epoch 891/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 892/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8800 - acc: 0.6031\n",
            "Epoch 893/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8797 - acc: 0.6031\n",
            "Epoch 894/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8796 - acc: 0.6031\n",
            "Epoch 895/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.8795 - acc: 0.6031\n",
            "Epoch 896/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8795 - acc: 0.6031\n",
            "Epoch 897/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8794 - acc: 0.6031\n",
            "Epoch 898/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8794 - acc: 0.6031\n",
            "Epoch 899/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8793 - acc: 0.6031\n",
            "Epoch 900/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 901/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8793 - acc: 0.6031\n",
            "Epoch 902/1000\n",
            "131/131 [==============================] - 0s 219us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 903/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8791 - acc: 0.6031\n",
            "Epoch 904/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8789 - acc: 0.6031\n",
            "Epoch 905/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8789 - acc: 0.6031\n",
            "Epoch 906/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8790 - acc: 0.6031\n",
            "Epoch 907/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 908/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 909/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8786 - acc: 0.6031\n",
            "Epoch 910/1000\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.8785 - acc: 0.6031\n",
            "Epoch 911/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8784 - acc: 0.6031\n",
            "Epoch 912/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8783 - acc: 0.6031\n",
            "Epoch 913/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8782 - acc: 0.6031\n",
            "Epoch 914/1000\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8781 - acc: 0.6031\n",
            "Epoch 915/1000\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.8781 - acc: 0.6031\n",
            "Epoch 916/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8780 - acc: 0.6031\n",
            "Epoch 917/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8780 - acc: 0.6031\n",
            "Epoch 918/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8779 - acc: 0.6031\n",
            "Epoch 919/1000\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 920/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 921/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8778 - acc: 0.6031\n",
            "Epoch 922/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8776 - acc: 0.6031\n",
            "Epoch 923/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8776 - acc: 0.6031\n",
            "Epoch 924/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8776 - acc: 0.5954\n",
            "Epoch 925/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8775 - acc: 0.5954\n",
            "Epoch 926/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8774 - acc: 0.6031\n",
            "Epoch 927/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8773 - acc: 0.6031\n",
            "Epoch 928/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8772 - acc: 0.6031\n",
            "Epoch 929/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8771 - acc: 0.5954\n",
            "Epoch 930/1000\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.8770 - acc: 0.6031\n",
            "Epoch 931/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8769 - acc: 0.6031\n",
            "Epoch 932/1000\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.8769 - acc: 0.5954\n",
            "Epoch 933/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 934/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 935/1000\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.8768 - acc: 0.6031\n",
            "Epoch 936/1000\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8767 - acc: 0.5954\n",
            "Epoch 937/1000\n",
            "131/131 [==============================] - 0s 207us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 938/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 939/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 940/1000\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8763 - acc: 0.6031\n",
            "Epoch 941/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8763 - acc: 0.6031\n",
            "Epoch 942/1000\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.8762 - acc: 0.6031\n",
            "Epoch 943/1000\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 944/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8761 - acc: 0.6031\n",
            "Epoch 945/1000\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.8765 - acc: 0.6031\n",
            "Epoch 946/1000\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 947/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 948/1000\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.8760 - acc: 0.6031\n",
            "Epoch 949/1000\n",
            "131/131 [==============================] - 0s 223us/step - loss: 0.8758 - acc: 0.6031\n",
            "Epoch 950/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8759 - acc: 0.6031\n",
            "Epoch 951/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8755 - acc: 0.6031\n",
            "Epoch 952/1000\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.8756 - acc: 0.5954\n",
            "Epoch 953/1000\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.8754 - acc: 0.6031\n",
            "Epoch 954/1000\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.8753 - acc: 0.6031\n",
            "Epoch 955/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8754 - acc: 0.5954\n",
            "Epoch 956/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8753 - acc: 0.5954\n",
            "Epoch 957/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8754 - acc: 0.5954\n",
            "Epoch 958/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8752 - acc: 0.5954\n",
            "Epoch 959/1000\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.8751 - acc: 0.5954\n",
            "Epoch 960/1000\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8750 - acc: 0.5954\n",
            "Epoch 961/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8750 - acc: 0.5954\n",
            "Epoch 962/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8749 - acc: 0.5954\n",
            "Epoch 963/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8749 - acc: 0.5954\n",
            "Epoch 964/1000\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.8747 - acc: 0.5954\n",
            "Epoch 965/1000\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 966/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 967/1000\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.8746 - acc: 0.5954\n",
            "Epoch 968/1000\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.8745 - acc: 0.5954\n",
            "Epoch 969/1000\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.8744 - acc: 0.5954\n",
            "Epoch 970/1000\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.8742 - acc: 0.5954\n",
            "Epoch 971/1000\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.8744 - acc: 0.5954\n",
            "Epoch 972/1000\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8743 - acc: 0.5954\n",
            "Epoch 973/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8742 - acc: 0.5954\n",
            "Epoch 974/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8741 - acc: 0.5954\n",
            "Epoch 975/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8740 - acc: 0.5954\n",
            "Epoch 976/1000\n",
            "131/131 [==============================] - 0s 143us/step - loss: 0.8740 - acc: 0.5954\n",
            "Epoch 977/1000\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.8741 - acc: 0.5954\n",
            "Epoch 978/1000\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.8739 - acc: 0.5954\n",
            "Epoch 979/1000\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.8739 - acc: 0.5954\n",
            "Epoch 980/1000\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8738 - acc: 0.5954\n",
            "Epoch 981/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8737 - acc: 0.5954\n",
            "Epoch 982/1000\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.8737 - acc: 0.5954\n",
            "Epoch 983/1000\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8736 - acc: 0.5954\n",
            "Epoch 984/1000\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.8735 - acc: 0.5954\n",
            "Epoch 985/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8734 - acc: 0.5954\n",
            "Epoch 986/1000\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.8734 - acc: 0.5954\n",
            "Epoch 987/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 988/1000\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 989/1000\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8731 - acc: 0.5954\n",
            "Epoch 990/1000\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8732 - acc: 0.5954\n",
            "Epoch 991/1000\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.8733 - acc: 0.5954\n",
            "Epoch 992/1000\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 993/1000\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 994/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8730 - acc: 0.5954\n",
            "Epoch 995/1000\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.8729 - acc: 0.5954\n",
            "Epoch 996/1000\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8728 - acc: 0.5954\n",
            "Epoch 997/1000\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8728 - acc: 0.5954\n",
            "Epoch 998/1000\n",
            "131/131 [==============================] - 0s 145us/step - loss: 0.8727 - acc: 0.5954\n",
            "Epoch 999/1000\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.8726 - acc: 0.5954\n",
            "Epoch 1000/1000\n",
            "131/131 [==============================] - 0s 138us/step - loss: 0.8726 - acc: 0.5954\n",
            "34/34 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-tXqN8teV_6",
        "colab_type": "code",
        "outputId": "bfcdc956-e7d5-4402-ae3d-eaa3c774f1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqboU_IeYu-",
        "colab_type": "code",
        "outputId": "7054a667-3ec5-44b5-a16b-f12c892bba8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20588235294117646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOK9WqumiKV2",
        "colab_type": "text"
      },
      "source": [
        "#Prova con LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvXfgKi2uWa",
        "colab_type": "text"
      },
      "source": [
        "##LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYTGbSu22w34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMCI-ALj23N1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda = LinearDiscriminantAnalysis(n_components=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqcJWjWj3rvS",
        "colab_type": "code",
        "outputId": "773331b8-d789-48d3-c8e5-1902868bc321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "train_data_stand_lda = lda.fit(train_data_stand, train_labels_dec).transform(train_data_stand)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:463: ChangedBehaviorWarning: n_components cannot be larger than min(n_features, n_classes - 1). Using min(n_features, n_classes - 1) = min(107, 3 - 1) = 2 components.\n",
            "  ChangedBehaviorWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/discriminant_analysis.py:469: FutureWarning: In version 0.23, setting n_components > min(n_features, n_classes - 1) will raise a ValueError. You should set n_components to None (default), or a value smaller or equal to min(n_features, n_classes - 1).\n",
            "  warnings.warn(future_msg, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3opFZvVx45TC",
        "colab_type": "code",
        "outputId": "a8b2e761-e182-494c-a650-ce7dbcdf6e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_lda.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjpMHM6iqbbC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = lda.transform(test_data_stand)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JAOqZLBxkXFs"
      },
      "source": [
        "###Z score dei dati dopo PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DisVOwPBkXF8",
        "colab": {}
      },
      "source": [
        "mean = train_data_stand_lda.mean(axis=0)\n",
        "std = train_data_stand_lda.std(axis=0)\n",
        "train_data_stand_lda = train_data_stand_lda - mean\n",
        "train_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d6vzVp7KkXGU",
        "colab": {}
      },
      "source": [
        "test_data_stand_lda = test_data_stand_lda - mean\n",
        "test_data_stand_lda /= std"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5r_lV6xwmmPi"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vOCMUAeUmmP0",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "from keras.preprocessing import sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OjW-IrxommQE",
        "colab": {}
      },
      "source": [
        "from keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-AuSahCmmQO",
        "colab": {}
      },
      "source": [
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CUw7ZwNwmmQV",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PfAdcrAmmmQc",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(4, activation='relu', input_shape=(2,)))\n",
        "  #model.add(layers.Dense(7, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.001, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m_uMkq9TkKEc"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nbGEaTxNkKEo"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZZWkHQXkKEw",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6fb8ef44-36c0-425a-8eb8-b90dbaea27b7",
        "id": "79E9JOcukKE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_lda, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1dac3719-8dd0-4eae-b561-b06fff110250",
        "id": "RwE734fFkKFF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a7fbc5c8-d088-4912-fa5b-858f4e75f059",
        "id": "DjbzRWoekKFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeM5283okKFT",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EqbBo3ogkKFY",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lCSQeyEDkKFe"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Vx0gV_BkKFg",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b7a797e7-f2a9-41df-85d0-a4cdc008c1f7",
        "id": "I8eztKAtkKFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 200\n",
        "all_acc_histories_lda = []\n",
        "all_loss_histories_lda = []\n",
        "all_val_acc_histories_lda = []\n",
        "all_val_loss_histories_lda = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_lda, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_lda[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_lda[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history = history.history['acc']\n",
        "  all_acc_histories_lda.append(acc_history)\n",
        "\n",
        "  loss_history = history.history['loss']\n",
        "  all_loss_histories_lda.append(loss_history)\n",
        "\n",
        "  acc_val_history = history.history['val_acc']\n",
        "  all_val_acc_histories_lda.append(acc_val_history)\n",
        "\n",
        "  loss_val_history = history.history['val_loss']\n",
        "  all_val_loss_histories_lda.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 3ms/step - loss: 1.0956 - acc: 0.2069 - val_loss: 1.0482 - val_acc: 0.3182\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 1.0795 - acc: 0.3333 - val_loss: 1.0327 - val_acc: 0.3636\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 1.0640 - acc: 0.3793 - val_loss: 1.0181 - val_acc: 0.4091\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 1.0490 - acc: 0.4483 - val_loss: 1.0036 - val_acc: 0.4773\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 1.0348 - acc: 0.4828 - val_loss: 0.9903 - val_acc: 0.5455\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 203us/step - loss: 1.0214 - acc: 0.5172 - val_loss: 0.9776 - val_acc: 0.5455\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 201us/step - loss: 1.0085 - acc: 0.5287 - val_loss: 0.9656 - val_acc: 0.5682\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9964 - acc: 0.5747 - val_loss: 0.9538 - val_acc: 0.5682\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.9849 - acc: 0.5747 - val_loss: 0.9429 - val_acc: 0.5909\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.9740 - acc: 0.5977 - val_loss: 0.9324 - val_acc: 0.6136\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.9634 - acc: 0.6092 - val_loss: 0.9223 - val_acc: 0.6136\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9535 - acc: 0.6322 - val_loss: 0.9127 - val_acc: 0.6136\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.9437 - acc: 0.6437 - val_loss: 0.9032 - val_acc: 0.6818\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.9345 - acc: 0.6667 - val_loss: 0.8946 - val_acc: 0.7045\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.9255 - acc: 0.6667 - val_loss: 0.8859 - val_acc: 0.7045\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9168 - acc: 0.6667 - val_loss: 0.8775 - val_acc: 0.6818\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.9084 - acc: 0.6667 - val_loss: 0.8694 - val_acc: 0.7273\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9002 - acc: 0.6667 - val_loss: 0.8617 - val_acc: 0.7273\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8921 - acc: 0.6782 - val_loss: 0.8537 - val_acc: 0.7727\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8843 - acc: 0.6897 - val_loss: 0.8463 - val_acc: 0.7727\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8768 - acc: 0.7011 - val_loss: 0.8389 - val_acc: 0.7955\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8693 - acc: 0.7356 - val_loss: 0.8317 - val_acc: 0.7955\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.8621 - acc: 0.7356 - val_loss: 0.8247 - val_acc: 0.7955\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.8551 - acc: 0.7356 - val_loss: 0.8180 - val_acc: 0.7955\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.8483 - acc: 0.7471 - val_loss: 0.8113 - val_acc: 0.7955\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8416 - acc: 0.7586 - val_loss: 0.8048 - val_acc: 0.8182\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 360us/step - loss: 0.8350 - acc: 0.7586 - val_loss: 0.7985 - val_acc: 0.8182\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8287 - acc: 0.7931 - val_loss: 0.7924 - val_acc: 0.8409\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8225 - acc: 0.8046 - val_loss: 0.7863 - val_acc: 0.8409\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8164 - acc: 0.8046 - val_loss: 0.7803 - val_acc: 0.8409\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.8104 - acc: 0.8276 - val_loss: 0.7746 - val_acc: 0.8636\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8045 - acc: 0.8506 - val_loss: 0.7688 - val_acc: 0.8636\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7988 - acc: 0.8621 - val_loss: 0.7632 - val_acc: 0.8636\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 338us/step - loss: 0.7930 - acc: 0.8736 - val_loss: 0.7578 - val_acc: 0.8636\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7875 - acc: 0.8736 - val_loss: 0.7523 - val_acc: 0.8636\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7820 - acc: 0.8851 - val_loss: 0.7471 - val_acc: 0.8864\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.7767 - acc: 0.8851 - val_loss: 0.7419 - val_acc: 0.8864\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7715 - acc: 0.8851 - val_loss: 0.7368 - val_acc: 0.9091\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.7662 - acc: 0.8851 - val_loss: 0.7318 - val_acc: 0.9091\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 201us/step - loss: 0.7611 - acc: 0.8966 - val_loss: 0.7269 - val_acc: 0.9318\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7561 - acc: 0.8966 - val_loss: 0.7220 - val_acc: 0.9318\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.7512 - acc: 0.9080 - val_loss: 0.7172 - val_acc: 0.9318\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7464 - acc: 0.9080 - val_loss: 0.7125 - val_acc: 0.9318\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.7417 - acc: 0.9080 - val_loss: 0.7079 - val_acc: 0.9318\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7370 - acc: 0.9080 - val_loss: 0.7033 - val_acc: 0.9318\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.7326 - acc: 0.9080 - val_loss: 0.6988 - val_acc: 0.9773\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7279 - acc: 0.9080 - val_loss: 0.6944 - val_acc: 0.9773\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7234 - acc: 0.9080 - val_loss: 0.6900 - val_acc: 0.9773\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7191 - acc: 0.9195 - val_loss: 0.6857 - val_acc: 0.9773\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.7147 - acc: 0.9195 - val_loss: 0.6815 - val_acc: 0.9773\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.7105 - acc: 0.9310 - val_loss: 0.6773 - val_acc: 0.9773\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7062 - acc: 0.9310 - val_loss: 0.6732 - val_acc: 0.9773\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.7021 - acc: 0.9310 - val_loss: 0.6692 - val_acc: 0.9773\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.6980 - acc: 0.9425 - val_loss: 0.6651 - val_acc: 1.0000\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6939 - acc: 0.9425 - val_loss: 0.6611 - val_acc: 1.0000\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6899 - acc: 0.9425 - val_loss: 0.6572 - val_acc: 1.0000\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.6859 - acc: 0.9425 - val_loss: 0.6533 - val_acc: 1.0000\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6820 - acc: 0.9540 - val_loss: 0.6495 - val_acc: 1.0000\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.6780 - acc: 0.9540 - val_loss: 0.6457 - val_acc: 1.0000\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.6742 - acc: 0.9540 - val_loss: 0.6420 - val_acc: 1.0000\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6705 - acc: 0.9540 - val_loss: 0.6383 - val_acc: 1.0000\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.6667 - acc: 0.9540 - val_loss: 0.6346 - val_acc: 1.0000\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.6630 - acc: 0.9540 - val_loss: 0.6309 - val_acc: 1.0000\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.6594 - acc: 0.9540 - val_loss: 0.6274 - val_acc: 1.0000\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.6558 - acc: 0.9540 - val_loss: 0.6239 - val_acc: 1.0000\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.6523 - acc: 0.9540 - val_loss: 0.6204 - val_acc: 1.0000\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.6487 - acc: 0.9540 - val_loss: 0.6170 - val_acc: 1.0000\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6453 - acc: 0.9540 - val_loss: 0.6135 - val_acc: 1.0000\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.6418 - acc: 0.9540 - val_loss: 0.6101 - val_acc: 1.0000\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6384 - acc: 0.9540 - val_loss: 0.6068 - val_acc: 1.0000\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6351 - acc: 0.9540 - val_loss: 0.6035 - val_acc: 1.0000\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6317 - acc: 0.9540 - val_loss: 0.6002 - val_acc: 1.0000\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.6284 - acc: 0.9540 - val_loss: 0.5969 - val_acc: 1.0000\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.6251 - acc: 0.9540 - val_loss: 0.5937 - val_acc: 1.0000\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.6219 - acc: 0.9540 - val_loss: 0.5905 - val_acc: 1.0000\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6186 - acc: 0.9540 - val_loss: 0.5873 - val_acc: 1.0000\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6155 - acc: 0.9540 - val_loss: 0.5842 - val_acc: 1.0000\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6123 - acc: 0.9540 - val_loss: 0.5811 - val_acc: 1.0000\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6092 - acc: 0.9540 - val_loss: 0.5780 - val_acc: 1.0000\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 324us/step - loss: 0.6061 - acc: 0.9540 - val_loss: 0.5749 - val_acc: 1.0000\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.6030 - acc: 0.9540 - val_loss: 0.5719 - val_acc: 1.0000\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5999 - acc: 0.9540 - val_loss: 0.5689 - val_acc: 1.0000\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5969 - acc: 0.9540 - val_loss: 0.5659 - val_acc: 1.0000\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.5939 - acc: 0.9540 - val_loss: 0.5629 - val_acc: 1.0000\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.5909 - acc: 0.9540 - val_loss: 0.5600 - val_acc: 1.0000\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.5880 - acc: 0.9540 - val_loss: 0.5571 - val_acc: 1.0000\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.5850 - acc: 0.9540 - val_loss: 0.5542 - val_acc: 1.0000\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5821 - acc: 0.9540 - val_loss: 0.5513 - val_acc: 1.0000\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5792 - acc: 0.9540 - val_loss: 0.5485 - val_acc: 1.0000\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5763 - acc: 0.9540 - val_loss: 0.5456 - val_acc: 1.0000\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 197us/step - loss: 0.5735 - acc: 0.9540 - val_loss: 0.5428 - val_acc: 1.0000\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 202us/step - loss: 0.5707 - acc: 0.9540 - val_loss: 0.5401 - val_acc: 1.0000\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5679 - acc: 0.9540 - val_loss: 0.5373 - val_acc: 1.0000\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5652 - acc: 0.9540 - val_loss: 0.5346 - val_acc: 1.0000\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 187us/step - loss: 0.5624 - acc: 0.9540 - val_loss: 0.5319 - val_acc: 1.0000\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5597 - acc: 0.9540 - val_loss: 0.5292 - val_acc: 1.0000\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.5569 - acc: 0.9655 - val_loss: 0.5266 - val_acc: 1.0000\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.5543 - acc: 0.9655 - val_loss: 0.5239 - val_acc: 1.0000\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5516 - acc: 0.9655 - val_loss: 0.5213 - val_acc: 1.0000\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5489 - acc: 0.9655 - val_loss: 0.5187 - val_acc: 1.0000\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5463 - acc: 0.9655 - val_loss: 0.5161 - val_acc: 1.0000\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5437 - acc: 0.9655 - val_loss: 0.5136 - val_acc: 1.0000\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5411 - acc: 0.9655 - val_loss: 0.5110 - val_acc: 1.0000\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5385 - acc: 0.9655 - val_loss: 0.5085 - val_acc: 1.0000\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 200us/step - loss: 0.5359 - acc: 0.9655 - val_loss: 0.5060 - val_acc: 1.0000\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 199us/step - loss: 0.5334 - acc: 0.9655 - val_loss: 0.5035 - val_acc: 1.0000\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5309 - acc: 0.9655 - val_loss: 0.5011 - val_acc: 1.0000\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5284 - acc: 0.9655 - val_loss: 0.4986 - val_acc: 1.0000\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 196us/step - loss: 0.5259 - acc: 0.9655 - val_loss: 0.4962 - val_acc: 1.0000\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 203us/step - loss: 0.5234 - acc: 0.9655 - val_loss: 0.4938 - val_acc: 1.0000\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5210 - acc: 0.9655 - val_loss: 0.4914 - val_acc: 1.0000\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5186 - acc: 0.9655 - val_loss: 0.4890 - val_acc: 1.0000\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5162 - acc: 0.9655 - val_loss: 0.4867 - val_acc: 1.0000\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.5138 - acc: 0.9655 - val_loss: 0.4844 - val_acc: 1.0000\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5114 - acc: 0.9655 - val_loss: 0.4821 - val_acc: 1.0000\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.5090 - acc: 0.9655 - val_loss: 0.4798 - val_acc: 1.0000\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.5067 - acc: 0.9770 - val_loss: 0.4775 - val_acc: 1.0000\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5044 - acc: 0.9770 - val_loss: 0.4752 - val_acc: 1.0000\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.5021 - acc: 0.9770 - val_loss: 0.4730 - val_acc: 1.0000\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.4998 - acc: 0.9770 - val_loss: 0.4707 - val_acc: 1.0000\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4974 - acc: 0.9770 - val_loss: 0.4685 - val_acc: 1.0000\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4952 - acc: 0.9770 - val_loss: 0.4663 - val_acc: 1.0000\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.4929 - acc: 0.9770 - val_loss: 0.4642 - val_acc: 1.0000\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.4907 - acc: 0.9770 - val_loss: 0.4620 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.4885 - acc: 0.9770 - val_loss: 0.4598 - val_acc: 1.0000\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4863 - acc: 0.9770 - val_loss: 0.4577 - val_acc: 1.0000\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4840 - acc: 0.9770 - val_loss: 0.4556 - val_acc: 1.0000\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.4819 - acc: 0.9770 - val_loss: 0.4535 - val_acc: 1.0000\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 204us/step - loss: 0.4797 - acc: 0.9770 - val_loss: 0.4514 - val_acc: 1.0000\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.4775 - acc: 0.9770 - val_loss: 0.4493 - val_acc: 1.0000\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4754 - acc: 0.9770 - val_loss: 0.4473 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4733 - acc: 0.9770 - val_loss: 0.4452 - val_acc: 1.0000\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.4712 - acc: 0.9770 - val_loss: 0.4432 - val_acc: 1.0000\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.4691 - acc: 0.9770 - val_loss: 0.4412 - val_acc: 1.0000\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.4670 - acc: 0.9770 - val_loss: 0.4392 - val_acc: 1.0000\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4650 - acc: 0.9770 - val_loss: 0.4372 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4629 - acc: 0.9770 - val_loss: 0.4353 - val_acc: 1.0000\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4609 - acc: 0.9770 - val_loss: 0.4333 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4588 - acc: 0.9770 - val_loss: 0.4314 - val_acc: 1.0000\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.4567 - acc: 0.9770 - val_loss: 0.4295 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4548 - acc: 0.9770 - val_loss: 0.4276 - val_acc: 1.0000\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.4528 - acc: 0.9770 - val_loss: 0.4257 - val_acc: 1.0000\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4508 - acc: 0.9770 - val_loss: 0.4238 - val_acc: 1.0000\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4488 - acc: 0.9770 - val_loss: 0.4219 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4469 - acc: 0.9770 - val_loss: 0.4201 - val_acc: 1.0000\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4449 - acc: 0.9770 - val_loss: 0.4182 - val_acc: 1.0000\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4430 - acc: 0.9770 - val_loss: 0.4164 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.4411 - acc: 0.9770 - val_loss: 0.4146 - val_acc: 1.0000\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.4392 - acc: 0.9770 - val_loss: 0.4128 - val_acc: 1.0000\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4373 - acc: 0.9770 - val_loss: 0.4110 - val_acc: 1.0000\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4354 - acc: 0.9770 - val_loss: 0.4092 - val_acc: 1.0000\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4336 - acc: 0.9885 - val_loss: 0.4075 - val_acc: 1.0000\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.4317 - acc: 0.9885 - val_loss: 0.4057 - val_acc: 1.0000\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.4299 - acc: 0.9885 - val_loss: 0.4040 - val_acc: 1.0000\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4281 - acc: 0.9885 - val_loss: 0.4023 - val_acc: 1.0000\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4263 - acc: 0.9885 - val_loss: 0.4005 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4245 - acc: 0.9885 - val_loss: 0.3988 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4227 - acc: 0.9885 - val_loss: 0.3972 - val_acc: 1.0000\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.4209 - acc: 0.9885 - val_loss: 0.3955 - val_acc: 1.0000\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4191 - acc: 0.9885 - val_loss: 0.3938 - val_acc: 1.0000\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4174 - acc: 0.9885 - val_loss: 0.3922 - val_acc: 1.0000\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4156 - acc: 0.9885 - val_loss: 0.3905 - val_acc: 1.0000\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4139 - acc: 0.9885 - val_loss: 0.3889 - val_acc: 1.0000\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4122 - acc: 0.9885 - val_loss: 0.3873 - val_acc: 1.0000\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4105 - acc: 0.9885 - val_loss: 0.3856 - val_acc: 1.0000\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4088 - acc: 0.9885 - val_loss: 0.3840 - val_acc: 1.0000\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4071 - acc: 0.9885 - val_loss: 0.3825 - val_acc: 1.0000\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4054 - acc: 0.9885 - val_loss: 0.3809 - val_acc: 1.0000\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4038 - acc: 0.9885 - val_loss: 0.3793 - val_acc: 1.0000\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.4021 - acc: 0.9885 - val_loss: 0.3778 - val_acc: 1.0000\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.4005 - acc: 0.9885 - val_loss: 0.3762 - val_acc: 1.0000\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3988 - acc: 0.9885 - val_loss: 0.3747 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3972 - acc: 0.9885 - val_loss: 0.3731 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.3956 - acc: 0.9885 - val_loss: 0.3716 - val_acc: 1.0000\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.3940 - acc: 0.9885 - val_loss: 0.3701 - val_acc: 1.0000\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.3924 - acc: 0.9885 - val_loss: 0.3686 - val_acc: 1.0000\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3908 - acc: 0.9885 - val_loss: 0.3671 - val_acc: 1.0000\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.3892 - acc: 0.9885 - val_loss: 0.3657 - val_acc: 1.0000\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.3877 - acc: 0.9885 - val_loss: 0.3642 - val_acc: 1.0000\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.3861 - acc: 0.9885 - val_loss: 0.3627 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.3846 - acc: 0.9885 - val_loss: 0.3613 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.3830 - acc: 0.9885 - val_loss: 0.3598 - val_acc: 1.0000\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.3815 - acc: 0.9885 - val_loss: 0.3584 - val_acc: 1.0000\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.3800 - acc: 0.9885 - val_loss: 0.3570 - val_acc: 1.0000\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.3785 - acc: 0.9885 - val_loss: 0.3556 - val_acc: 1.0000\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.3770 - acc: 0.9885 - val_loss: 0.3542 - val_acc: 1.0000\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 314us/step - loss: 0.3755 - acc: 0.9885 - val_loss: 0.3528 - val_acc: 1.0000\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3740 - acc: 0.9885 - val_loss: 0.3514 - val_acc: 1.0000\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3725 - acc: 0.9885 - val_loss: 0.3500 - val_acc: 1.0000\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.3711 - acc: 0.9885 - val_loss: 0.3487 - val_acc: 1.0000\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3696 - acc: 0.9885 - val_loss: 0.3473 - val_acc: 1.0000\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.3682 - acc: 0.9885 - val_loss: 0.3460 - val_acc: 1.0000\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.3667 - acc: 0.9885 - val_loss: 0.3446 - val_acc: 1.0000\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.3653 - acc: 1.0000 - val_loss: 0.3433 - val_acc: 1.0000\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.3638 - acc: 1.0000 - val_loss: 0.3420 - val_acc: 1.0000\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.3624 - acc: 1.0000 - val_loss: 0.3407 - val_acc: 1.0000\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.3611 - acc: 1.0000 - val_loss: 0.3394 - val_acc: 1.0000\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 269us/step - loss: 0.3597 - acc: 1.0000 - val_loss: 0.3381 - val_acc: 1.0000\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.3583 - acc: 1.0000 - val_loss: 0.3368 - val_acc: 1.0000\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.3569 - acc: 1.0000 - val_loss: 0.3355 - val_acc: 1.0000\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 4ms/step - loss: 1.5660 - acc: 0.0000e+00 - val_loss: 1.4870 - val_acc: 0.0909\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.5505 - acc: 0.0000e+00 - val_loss: 1.4727 - val_acc: 0.0909\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 1.5344 - acc: 0.0115 - val_loss: 1.4585 - val_acc: 0.0909\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.5193 - acc: 0.0345 - val_loss: 1.4445 - val_acc: 0.0909\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.5045 - acc: 0.0460 - val_loss: 1.4311 - val_acc: 0.1136\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.4901 - acc: 0.0460 - val_loss: 1.4176 - val_acc: 0.1136\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 1.4755 - acc: 0.0690 - val_loss: 1.4047 - val_acc: 0.1364\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.4616 - acc: 0.0920 - val_loss: 1.3920 - val_acc: 0.1591\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 1.4479 - acc: 0.1149 - val_loss: 1.3796 - val_acc: 0.1591\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 1.4346 - acc: 0.1264 - val_loss: 1.3675 - val_acc: 0.2273\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 204us/step - loss: 1.4217 - acc: 0.1839 - val_loss: 1.3556 - val_acc: 0.2273\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 1.4094 - acc: 0.1954 - val_loss: 1.3442 - val_acc: 0.2955\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 1.3973 - acc: 0.2184 - val_loss: 1.3329 - val_acc: 0.3409\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.3856 - acc: 0.2529 - val_loss: 1.3223 - val_acc: 0.3636\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.3742 - acc: 0.2644 - val_loss: 1.3115 - val_acc: 0.3636\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 1.3631 - acc: 0.2759 - val_loss: 1.3010 - val_acc: 0.3636\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 275us/step - loss: 1.3520 - acc: 0.2989 - val_loss: 1.2910 - val_acc: 0.3636\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 1.3414 - acc: 0.3103 - val_loss: 1.2810 - val_acc: 0.3864\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 1.3310 - acc: 0.3448 - val_loss: 1.2710 - val_acc: 0.3864\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.3207 - acc: 0.3678 - val_loss: 1.2615 - val_acc: 0.3864\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 1.3107 - acc: 0.3908 - val_loss: 1.2519 - val_acc: 0.4318\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 1.3009 - acc: 0.3908 - val_loss: 1.2426 - val_acc: 0.4318\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 1.2913 - acc: 0.3908 - val_loss: 1.2338 - val_acc: 0.4318\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.2820 - acc: 0.3908 - val_loss: 1.2247 - val_acc: 0.4545\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.2724 - acc: 0.4138 - val_loss: 1.2158 - val_acc: 0.4773\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 1.2628 - acc: 0.4138 - val_loss: 1.2071 - val_acc: 0.5000\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 1.2537 - acc: 0.4253 - val_loss: 1.1982 - val_acc: 0.5000\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 267us/step - loss: 1.2443 - acc: 0.4598 - val_loss: 1.1896 - val_acc: 0.5000\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.2347 - acc: 0.4943 - val_loss: 1.1809 - val_acc: 0.5227\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 1.2252 - acc: 0.5172 - val_loss: 1.1721 - val_acc: 0.5455\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.2154 - acc: 0.5632 - val_loss: 1.1633 - val_acc: 0.5682\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 1.2056 - acc: 0.5747 - val_loss: 1.1547 - val_acc: 0.5682\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 1.1960 - acc: 0.6092 - val_loss: 1.1465 - val_acc: 0.5909\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.1867 - acc: 0.6322 - val_loss: 1.1385 - val_acc: 0.6136\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 1.1773 - acc: 0.6437 - val_loss: 1.1304 - val_acc: 0.6136\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.1681 - acc: 0.6667 - val_loss: 1.1225 - val_acc: 0.6136\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 1.1589 - acc: 0.6667 - val_loss: 1.1148 - val_acc: 0.6136\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 1.1497 - acc: 0.6782 - val_loss: 1.1070 - val_acc: 0.6364\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 1.1404 - acc: 0.6782 - val_loss: 1.0996 - val_acc: 0.6364\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 1.1315 - acc: 0.7011 - val_loss: 1.0922 - val_acc: 0.6364\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.1227 - acc: 0.7241 - val_loss: 1.0848 - val_acc: 0.6591\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 1.1142 - acc: 0.7356 - val_loss: 1.0771 - val_acc: 0.6818\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 1.1055 - acc: 0.7356 - val_loss: 1.0697 - val_acc: 0.6818\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.0973 - acc: 0.7471 - val_loss: 1.0622 - val_acc: 0.6818\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 1.0886 - acc: 0.7701 - val_loss: 1.0548 - val_acc: 0.6818\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 1.0802 - acc: 0.7701 - val_loss: 1.0473 - val_acc: 0.7045\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 1.0716 - acc: 0.7701 - val_loss: 1.0402 - val_acc: 0.7045\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 1.0632 - acc: 0.7816 - val_loss: 1.0330 - val_acc: 0.7500\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 283us/step - loss: 1.0548 - acc: 0.7816 - val_loss: 1.0258 - val_acc: 0.7727\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 1.0468 - acc: 0.7816 - val_loss: 1.0184 - val_acc: 0.7727\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 1.0384 - acc: 0.7931 - val_loss: 1.0113 - val_acc: 0.7727\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 1.0305 - acc: 0.8046 - val_loss: 1.0041 - val_acc: 0.7727\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.0226 - acc: 0.8276 - val_loss: 0.9971 - val_acc: 0.7727\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.0148 - acc: 0.8276 - val_loss: 0.9901 - val_acc: 0.7727\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 1.0070 - acc: 0.8276 - val_loss: 0.9835 - val_acc: 0.7727\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.9994 - acc: 0.8276 - val_loss: 0.9763 - val_acc: 0.7727\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.9921 - acc: 0.8276 - val_loss: 0.9693 - val_acc: 0.8182\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.9847 - acc: 0.8276 - val_loss: 0.9625 - val_acc: 0.8182\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.9775 - acc: 0.8276 - val_loss: 0.9557 - val_acc: 0.8182\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9703 - acc: 0.8276 - val_loss: 0.9492 - val_acc: 0.8182\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.9633 - acc: 0.8276 - val_loss: 0.9426 - val_acc: 0.8409\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.9563 - acc: 0.8276 - val_loss: 0.9362 - val_acc: 0.8409\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9495 - acc: 0.8276 - val_loss: 0.9296 - val_acc: 0.8636\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.9424 - acc: 0.8276 - val_loss: 0.9234 - val_acc: 0.8636\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9354 - acc: 0.8276 - val_loss: 0.9168 - val_acc: 0.8636\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.9282 - acc: 0.8276 - val_loss: 0.9104 - val_acc: 0.8636\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.9211 - acc: 0.8276 - val_loss: 0.9041 - val_acc: 0.8636\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9142 - acc: 0.8276 - val_loss: 0.8979 - val_acc: 0.8636\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.9074 - acc: 0.8276 - val_loss: 0.8918 - val_acc: 0.8636\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9006 - acc: 0.8276 - val_loss: 0.8857 - val_acc: 0.8636\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.8940 - acc: 0.8391 - val_loss: 0.8797 - val_acc: 0.8636\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.8873 - acc: 0.8506 - val_loss: 0.8738 - val_acc: 0.8636\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8806 - acc: 0.8506 - val_loss: 0.8679 - val_acc: 0.8636\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8740 - acc: 0.8621 - val_loss: 0.8621 - val_acc: 0.8636\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8675 - acc: 0.8621 - val_loss: 0.8563 - val_acc: 0.8636\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.8611 - acc: 0.8621 - val_loss: 0.8507 - val_acc: 0.8636\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8548 - acc: 0.8621 - val_loss: 0.8450 - val_acc: 0.8636\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8486 - acc: 0.8621 - val_loss: 0.8395 - val_acc: 0.8636\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 343us/step - loss: 0.8424 - acc: 0.8621 - val_loss: 0.8340 - val_acc: 0.8636\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.8363 - acc: 0.8621 - val_loss: 0.8286 - val_acc: 0.8636\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8302 - acc: 0.8621 - val_loss: 0.8232 - val_acc: 0.8636\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8243 - acc: 0.8621 - val_loss: 0.8180 - val_acc: 0.8636\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8184 - acc: 0.8621 - val_loss: 0.8127 - val_acc: 0.8636\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8125 - acc: 0.8621 - val_loss: 0.8075 - val_acc: 0.8636\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.8068 - acc: 0.8621 - val_loss: 0.8024 - val_acc: 0.8636\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.8011 - acc: 0.8621 - val_loss: 0.7973 - val_acc: 0.8636\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.7955 - acc: 0.8621 - val_loss: 0.7923 - val_acc: 0.8636\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7898 - acc: 0.8621 - val_loss: 0.7874 - val_acc: 0.8636\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.7842 - acc: 0.8621 - val_loss: 0.7825 - val_acc: 0.8636\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.7788 - acc: 0.8621 - val_loss: 0.7776 - val_acc: 0.8636\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.7734 - acc: 0.8621 - val_loss: 0.7728 - val_acc: 0.8636\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7679 - acc: 0.8621 - val_loss: 0.7681 - val_acc: 0.8636\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7627 - acc: 0.8621 - val_loss: 0.7635 - val_acc: 0.8636\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.7575 - acc: 0.8621 - val_loss: 0.7589 - val_acc: 0.8636\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.7523 - acc: 0.8621 - val_loss: 0.7544 - val_acc: 0.8636\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7471 - acc: 0.8621 - val_loss: 0.7500 - val_acc: 0.8636\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7421 - acc: 0.8621 - val_loss: 0.7457 - val_acc: 0.8636\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.7372 - acc: 0.8621 - val_loss: 0.7414 - val_acc: 0.8636\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7323 - acc: 0.8621 - val_loss: 0.7372 - val_acc: 0.8636\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.7274 - acc: 0.8621 - val_loss: 0.7331 - val_acc: 0.8636\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7228 - acc: 0.8621 - val_loss: 0.7290 - val_acc: 0.8636\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.7180 - acc: 0.8621 - val_loss: 0.7250 - val_acc: 0.8636\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.7134 - acc: 0.8621 - val_loss: 0.7210 - val_acc: 0.8636\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7089 - acc: 0.8621 - val_loss: 0.7171 - val_acc: 0.8636\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.7044 - acc: 0.8621 - val_loss: 0.7133 - val_acc: 0.8636\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7000 - acc: 0.8621 - val_loss: 0.7096 - val_acc: 0.8636\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6956 - acc: 0.8621 - val_loss: 0.7062 - val_acc: 0.8636\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6913 - acc: 0.8736 - val_loss: 0.7029 - val_acc: 0.8636\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.6872 - acc: 0.8736 - val_loss: 0.6997 - val_acc: 0.8636\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6832 - acc: 0.8736 - val_loss: 0.6964 - val_acc: 0.8636\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6792 - acc: 0.8736 - val_loss: 0.6933 - val_acc: 0.8636\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.6751 - acc: 0.8736 - val_loss: 0.6901 - val_acc: 0.8636\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6713 - acc: 0.8736 - val_loss: 0.6870 - val_acc: 0.8636\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6675 - acc: 0.8736 - val_loss: 0.6839 - val_acc: 0.8636\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.6638 - acc: 0.8736 - val_loss: 0.6809 - val_acc: 0.8636\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.6603 - acc: 0.8736 - val_loss: 0.6780 - val_acc: 0.8636\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.6567 - acc: 0.8736 - val_loss: 0.6751 - val_acc: 0.8636\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6533 - acc: 0.8736 - val_loss: 0.6722 - val_acc: 0.8636\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.6500 - acc: 0.8736 - val_loss: 0.6694 - val_acc: 0.8636\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6467 - acc: 0.8736 - val_loss: 0.6666 - val_acc: 0.8636\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6436 - acc: 0.8736 - val_loss: 0.6638 - val_acc: 0.8636\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6404 - acc: 0.8736 - val_loss: 0.6611 - val_acc: 0.8636\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6372 - acc: 0.8736 - val_loss: 0.6583 - val_acc: 0.8636\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6341 - acc: 0.8736 - val_loss: 0.6555 - val_acc: 0.8636\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6311 - acc: 0.8736 - val_loss: 0.6529 - val_acc: 0.8636\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.6279 - acc: 0.8736 - val_loss: 0.6502 - val_acc: 0.8636\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.6249 - acc: 0.8736 - val_loss: 0.6476 - val_acc: 0.8636\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6219 - acc: 0.8736 - val_loss: 0.6449 - val_acc: 0.8636\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 267us/step - loss: 0.6190 - acc: 0.8736 - val_loss: 0.6423 - val_acc: 0.8636\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6161 - acc: 0.8736 - val_loss: 0.6397 - val_acc: 0.8636\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.6133 - acc: 0.8736 - val_loss: 0.6372 - val_acc: 0.8636\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.6105 - acc: 0.8736 - val_loss: 0.6346 - val_acc: 0.8636\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.6078 - acc: 0.8736 - val_loss: 0.6320 - val_acc: 0.8636\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.6050 - acc: 0.8736 - val_loss: 0.6295 - val_acc: 0.8636\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6023 - acc: 0.8736 - val_loss: 0.6270 - val_acc: 0.8636\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5995 - acc: 0.8736 - val_loss: 0.6245 - val_acc: 0.8636\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5969 - acc: 0.8736 - val_loss: 0.6221 - val_acc: 0.8636\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5941 - acc: 0.8736 - val_loss: 0.6195 - val_acc: 0.8636\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5915 - acc: 0.8736 - val_loss: 0.6171 - val_acc: 0.8636\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5888 - acc: 0.8736 - val_loss: 0.6147 - val_acc: 0.8636\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5862 - acc: 0.8736 - val_loss: 0.6122 - val_acc: 0.8636\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.5836 - acc: 0.8736 - val_loss: 0.6098 - val_acc: 0.8636\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 275us/step - loss: 0.5809 - acc: 0.8736 - val_loss: 0.6074 - val_acc: 0.8636\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5784 - acc: 0.8736 - val_loss: 0.6050 - val_acc: 0.8636\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5757 - acc: 0.8736 - val_loss: 0.6026 - val_acc: 0.8636\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5733 - acc: 0.8736 - val_loss: 0.6003 - val_acc: 0.8636\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5707 - acc: 0.8736 - val_loss: 0.5980 - val_acc: 0.8636\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.5681 - acc: 0.8736 - val_loss: 0.5957 - val_acc: 0.8636\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5656 - acc: 0.8736 - val_loss: 0.5933 - val_acc: 0.8636\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5631 - acc: 0.8736 - val_loss: 0.5910 - val_acc: 0.8636\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.5607 - acc: 0.8736 - val_loss: 0.5888 - val_acc: 0.8636\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 299us/step - loss: 0.5582 - acc: 0.8736 - val_loss: 0.5865 - val_acc: 0.8636\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5558 - acc: 0.8736 - val_loss: 0.5844 - val_acc: 0.8636\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.5534 - acc: 0.8736 - val_loss: 0.5822 - val_acc: 0.8636\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5510 - acc: 0.8736 - val_loss: 0.5799 - val_acc: 0.8636\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.5486 - acc: 0.8736 - val_loss: 0.5778 - val_acc: 0.8636\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5463 - acc: 0.8736 - val_loss: 0.5756 - val_acc: 0.8636\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5439 - acc: 0.8736 - val_loss: 0.5735 - val_acc: 0.8636\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.5416 - acc: 0.8736 - val_loss: 0.5714 - val_acc: 0.8636\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.5393 - acc: 0.8736 - val_loss: 0.5693 - val_acc: 0.8636\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.5370 - acc: 0.8736 - val_loss: 0.5672 - val_acc: 0.8636\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.5347 - acc: 0.8736 - val_loss: 0.5651 - val_acc: 0.8636\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.5324 - acc: 0.8736 - val_loss: 0.5630 - val_acc: 0.8636\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.5301 - acc: 0.8736 - val_loss: 0.5609 - val_acc: 0.8636\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5279 - acc: 0.8736 - val_loss: 0.5589 - val_acc: 0.8636\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.5258 - acc: 0.8736 - val_loss: 0.5569 - val_acc: 0.8636\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.5236 - acc: 0.8736 - val_loss: 0.5548 - val_acc: 0.8636\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.5214 - acc: 0.8736 - val_loss: 0.5529 - val_acc: 0.8636\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.5193 - acc: 0.8736 - val_loss: 0.5509 - val_acc: 0.8636\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5172 - acc: 0.8736 - val_loss: 0.5489 - val_acc: 0.8636\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5151 - acc: 0.8736 - val_loss: 0.5470 - val_acc: 0.8636\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.5130 - acc: 0.8736 - val_loss: 0.5450 - val_acc: 0.8636\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.5109 - acc: 0.8736 - val_loss: 0.5431 - val_acc: 0.8636\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.5089 - acc: 0.8736 - val_loss: 0.5412 - val_acc: 0.8636\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.5069 - acc: 0.8736 - val_loss: 0.5392 - val_acc: 0.8636\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.5048 - acc: 0.8736 - val_loss: 0.5373 - val_acc: 0.8636\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.5028 - acc: 0.8736 - val_loss: 0.5355 - val_acc: 0.8636\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5008 - acc: 0.8736 - val_loss: 0.5336 - val_acc: 0.8636\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4988 - acc: 0.8736 - val_loss: 0.5318 - val_acc: 0.8636\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4968 - acc: 0.8736 - val_loss: 0.5299 - val_acc: 0.8636\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4949 - acc: 0.8736 - val_loss: 0.5281 - val_acc: 0.8636\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.4929 - acc: 0.8736 - val_loss: 0.5262 - val_acc: 0.8636\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.4910 - acc: 0.8736 - val_loss: 0.5245 - val_acc: 0.8636\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4891 - acc: 0.8736 - val_loss: 0.5226 - val_acc: 0.8636\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.4871 - acc: 0.8736 - val_loss: 0.5209 - val_acc: 0.8636\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 248us/step - loss: 0.4852 - acc: 0.8736 - val_loss: 0.5191 - val_acc: 0.8636\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.4834 - acc: 0.8736 - val_loss: 0.5173 - val_acc: 0.8636\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.4815 - acc: 0.8736 - val_loss: 0.5156 - val_acc: 0.8636\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.4797 - acc: 0.8736 - val_loss: 0.5138 - val_acc: 0.8636\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4778 - acc: 0.8736 - val_loss: 0.5121 - val_acc: 0.8636\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.4759 - acc: 0.8736 - val_loss: 0.5104 - val_acc: 0.8636\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.4741 - acc: 0.8736 - val_loss: 0.5087 - val_acc: 0.8636\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4723 - acc: 0.8736 - val_loss: 0.5070 - val_acc: 0.8636\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4705 - acc: 0.8736 - val_loss: 0.5053 - val_acc: 0.8636\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4688 - acc: 0.8736 - val_loss: 0.5036 - val_acc: 0.8636\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.4670 - acc: 0.8736 - val_loss: 0.5019 - val_acc: 0.8636\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.4652 - acc: 0.8736 - val_loss: 0.5002 - val_acc: 0.8636\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4635 - acc: 0.8736 - val_loss: 0.4986 - val_acc: 0.8636\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.4617 - acc: 0.8736 - val_loss: 0.4969 - val_acc: 0.8636\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.4600 - acc: 0.8736 - val_loss: 0.4953 - val_acc: 0.8636\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/200\n",
            "88/88 [==============================] - 0s 4ms/step - loss: 1.4273 - acc: 0.3523 - val_loss: 1.5551 - val_acc: 0.3488\n",
            "Epoch 2/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.4094 - acc: 0.3523 - val_loss: 1.5319 - val_acc: 0.3488\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 1.3922 - acc: 0.3523 - val_loss: 1.5100 - val_acc: 0.3488\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 1.3759 - acc: 0.3523 - val_loss: 1.4889 - val_acc: 0.3488\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 0s 209us/step - loss: 1.3601 - acc: 0.3523 - val_loss: 1.4671 - val_acc: 0.3488\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.3446 - acc: 0.3523 - val_loss: 1.4479 - val_acc: 0.3488\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.3301 - acc: 0.3636 - val_loss: 1.4290 - val_acc: 0.3488\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.3160 - acc: 0.3636 - val_loss: 1.4101 - val_acc: 0.3488\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.3023 - acc: 0.3636 - val_loss: 1.3929 - val_acc: 0.3488\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.2893 - acc: 0.3636 - val_loss: 1.3758 - val_acc: 0.3488\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 1.2769 - acc: 0.3636 - val_loss: 1.3588 - val_acc: 0.3488\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 0s 264us/step - loss: 1.2646 - acc: 0.3636 - val_loss: 1.3434 - val_acc: 0.3488\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.2532 - acc: 0.3523 - val_loss: 1.3275 - val_acc: 0.3256\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 0s 259us/step - loss: 1.2419 - acc: 0.3523 - val_loss: 1.3127 - val_acc: 0.3256\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 0s 266us/step - loss: 1.2309 - acc: 0.3523 - val_loss: 1.2988 - val_acc: 0.3256\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 1.2202 - acc: 0.3523 - val_loss: 1.2846 - val_acc: 0.3256\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 0s 267us/step - loss: 1.2103 - acc: 0.3409 - val_loss: 1.2706 - val_acc: 0.3256\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 1.2000 - acc: 0.3523 - val_loss: 1.2579 - val_acc: 0.3256\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.1902 - acc: 0.3523 - val_loss: 1.2453 - val_acc: 0.3256\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 1.1810 - acc: 0.3523 - val_loss: 1.2328 - val_acc: 0.3256\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 0s 203us/step - loss: 1.1718 - acc: 0.3523 - val_loss: 1.2215 - val_acc: 0.3256\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.1630 - acc: 0.3523 - val_loss: 1.2097 - val_acc: 0.3256\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.1544 - acc: 0.3523 - val_loss: 1.1988 - val_acc: 0.3256\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 1.1460 - acc: 0.3523 - val_loss: 1.1876 - val_acc: 0.3256\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 1.1378 - acc: 0.3636 - val_loss: 1.1771 - val_acc: 0.3256\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 1.1299 - acc: 0.3636 - val_loss: 1.1668 - val_acc: 0.3256\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 1.1222 - acc: 0.3750 - val_loss: 1.1567 - val_acc: 0.3256\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 1.1147 - acc: 0.3750 - val_loss: 1.1470 - val_acc: 0.3256\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.1073 - acc: 0.3750 - val_loss: 1.1371 - val_acc: 0.3256\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 1.1001 - acc: 0.3750 - val_loss: 1.1281 - val_acc: 0.3488\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 1.0932 - acc: 0.3864 - val_loss: 1.1186 - val_acc: 0.3488\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 0s 256us/step - loss: 1.0863 - acc: 0.3864 - val_loss: 1.1098 - val_acc: 0.3488\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0796 - acc: 0.3864 - val_loss: 1.1015 - val_acc: 0.3488\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0730 - acc: 0.4205 - val_loss: 1.0930 - val_acc: 0.3488\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 1.0667 - acc: 0.4205 - val_loss: 1.0845 - val_acc: 0.3488\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 1.0603 - acc: 0.4205 - val_loss: 1.0763 - val_acc: 0.3488\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 1.0543 - acc: 0.4205 - val_loss: 1.0681 - val_acc: 0.3488\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 1.0481 - acc: 0.4318 - val_loss: 1.0606 - val_acc: 0.3488\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 1.0421 - acc: 0.4432 - val_loss: 1.0529 - val_acc: 0.3488\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 1.0363 - acc: 0.4432 - val_loss: 1.0454 - val_acc: 0.3488\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 0s 258us/step - loss: 1.0305 - acc: 0.4432 - val_loss: 1.0379 - val_acc: 0.3488\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 0s 350us/step - loss: 1.0249 - acc: 0.4545 - val_loss: 1.0306 - val_acc: 0.3721\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0193 - acc: 0.4545 - val_loss: 1.0232 - val_acc: 0.3721\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 1.0137 - acc: 0.4659 - val_loss: 1.0161 - val_acc: 0.3721\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 1.0084 - acc: 0.4659 - val_loss: 1.0095 - val_acc: 0.3953\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0027 - acc: 0.4886 - val_loss: 1.0025 - val_acc: 0.3953\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9975 - acc: 0.4886 - val_loss: 0.9955 - val_acc: 0.3953\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.9921 - acc: 0.4886 - val_loss: 0.9891 - val_acc: 0.3953\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 0s 321us/step - loss: 0.9869 - acc: 0.5000 - val_loss: 0.9825 - val_acc: 0.3953\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.9817 - acc: 0.5114 - val_loss: 0.9760 - val_acc: 0.3953\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 0s 320us/step - loss: 0.9766 - acc: 0.5227 - val_loss: 0.9694 - val_acc: 0.4419\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.9715 - acc: 0.5227 - val_loss: 0.9633 - val_acc: 0.4651\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.9666 - acc: 0.5455 - val_loss: 0.9569 - val_acc: 0.4884\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 0s 267us/step - loss: 0.9618 - acc: 0.5682 - val_loss: 0.9507 - val_acc: 0.5116\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.9567 - acc: 0.5909 - val_loss: 0.9445 - val_acc: 0.5349\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.9517 - acc: 0.6136 - val_loss: 0.9384 - val_acc: 0.5349\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.9469 - acc: 0.6136 - val_loss: 0.9322 - val_acc: 0.5349\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.9420 - acc: 0.6136 - val_loss: 0.9262 - val_acc: 0.5581\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.9373 - acc: 0.6250 - val_loss: 0.9201 - val_acc: 0.5814\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.9324 - acc: 0.6250 - val_loss: 0.9143 - val_acc: 0.6047\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.9276 - acc: 0.6364 - val_loss: 0.9087 - val_acc: 0.6279\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 0s 265us/step - loss: 0.9230 - acc: 0.6705 - val_loss: 0.9028 - val_acc: 0.6279\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9182 - acc: 0.6932 - val_loss: 0.8970 - val_acc: 0.6279\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.9136 - acc: 0.7045 - val_loss: 0.8912 - val_acc: 0.6977\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.9090 - acc: 0.7045 - val_loss: 0.8856 - val_acc: 0.7442\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.9044 - acc: 0.7045 - val_loss: 0.8801 - val_acc: 0.7442\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8999 - acc: 0.7045 - val_loss: 0.8745 - val_acc: 0.7442\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.8953 - acc: 0.7045 - val_loss: 0.8690 - val_acc: 0.7442\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 0s 297us/step - loss: 0.8909 - acc: 0.7159 - val_loss: 0.8634 - val_acc: 0.7674\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.8863 - acc: 0.7273 - val_loss: 0.8580 - val_acc: 0.7674\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 0s 295us/step - loss: 0.8819 - acc: 0.7386 - val_loss: 0.8525 - val_acc: 0.8140\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8775 - acc: 0.7386 - val_loss: 0.8473 - val_acc: 0.8140\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.8731 - acc: 0.7500 - val_loss: 0.8420 - val_acc: 0.8140\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 0s 205us/step - loss: 0.8686 - acc: 0.7614 - val_loss: 0.8367 - val_acc: 0.8140\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8642 - acc: 0.7614 - val_loss: 0.8314 - val_acc: 0.8140\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8599 - acc: 0.7614 - val_loss: 0.8261 - val_acc: 0.8140\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8555 - acc: 0.7727 - val_loss: 0.8209 - val_acc: 0.8140\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.8512 - acc: 0.7727 - val_loss: 0.8157 - val_acc: 0.8140\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.8468 - acc: 0.7727 - val_loss: 0.8106 - val_acc: 0.8140\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.8425 - acc: 0.7841 - val_loss: 0.8055 - val_acc: 0.8140\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.8382 - acc: 0.7955 - val_loss: 0.8004 - val_acc: 0.8140\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.8340 - acc: 0.7955 - val_loss: 0.7953 - val_acc: 0.8372\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.8298 - acc: 0.7955 - val_loss: 0.7901 - val_acc: 0.8372\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.8255 - acc: 0.7955 - val_loss: 0.7851 - val_acc: 0.8372\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.8213 - acc: 0.8068 - val_loss: 0.7802 - val_acc: 0.8605\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.8171 - acc: 0.8068 - val_loss: 0.7752 - val_acc: 0.8837\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.8130 - acc: 0.8068 - val_loss: 0.7703 - val_acc: 0.8837\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.8087 - acc: 0.8182 - val_loss: 0.7653 - val_acc: 0.8837\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8046 - acc: 0.8182 - val_loss: 0.7605 - val_acc: 0.8837\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.8005 - acc: 0.8182 - val_loss: 0.7556 - val_acc: 0.8837\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7964 - acc: 0.8182 - val_loss: 0.7508 - val_acc: 0.8837\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.7924 - acc: 0.8182 - val_loss: 0.7459 - val_acc: 0.8837\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.7883 - acc: 0.8182 - val_loss: 0.7411 - val_acc: 0.8837\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.7842 - acc: 0.8182 - val_loss: 0.7364 - val_acc: 0.8837\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7802 - acc: 0.8182 - val_loss: 0.7315 - val_acc: 0.8837\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 0s 359us/step - loss: 0.7762 - acc: 0.8182 - val_loss: 0.7267 - val_acc: 0.8837\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.7722 - acc: 0.8182 - val_loss: 0.7220 - val_acc: 0.8837\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.7682 - acc: 0.8182 - val_loss: 0.7174 - val_acc: 0.8837\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7642 - acc: 0.8182 - val_loss: 0.7127 - val_acc: 0.8837\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7602 - acc: 0.8182 - val_loss: 0.7081 - val_acc: 0.8837\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 0s 294us/step - loss: 0.7562 - acc: 0.8182 - val_loss: 0.7034 - val_acc: 0.8837\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.7523 - acc: 0.8182 - val_loss: 0.6989 - val_acc: 0.8837\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 0s 270us/step - loss: 0.7484 - acc: 0.8295 - val_loss: 0.6943 - val_acc: 0.8837\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.7444 - acc: 0.8295 - val_loss: 0.6898 - val_acc: 0.8837\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.7405 - acc: 0.8295 - val_loss: 0.6852 - val_acc: 0.8837\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.7367 - acc: 0.8295 - val_loss: 0.6807 - val_acc: 0.8837\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.7328 - acc: 0.8295 - val_loss: 0.6763 - val_acc: 0.8837\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.7291 - acc: 0.8295 - val_loss: 0.6718 - val_acc: 0.8837\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.7253 - acc: 0.8295 - val_loss: 0.6674 - val_acc: 0.8837\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7214 - acc: 0.8295 - val_loss: 0.6631 - val_acc: 0.8837\n",
            "Epoch 111/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7176 - acc: 0.8295 - val_loss: 0.6587 - val_acc: 0.8837\n",
            "Epoch 112/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7139 - acc: 0.8295 - val_loss: 0.6544 - val_acc: 0.8837\n",
            "Epoch 113/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.7101 - acc: 0.8295 - val_loss: 0.6500 - val_acc: 0.8837\n",
            "Epoch 114/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.7064 - acc: 0.8409 - val_loss: 0.6458 - val_acc: 0.8837\n",
            "Epoch 115/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.7028 - acc: 0.8409 - val_loss: 0.6415 - val_acc: 0.8837\n",
            "Epoch 116/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.6990 - acc: 0.8409 - val_loss: 0.6372 - val_acc: 0.8837\n",
            "Epoch 117/200\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.6953 - acc: 0.8523 - val_loss: 0.6330 - val_acc: 0.8837\n",
            "Epoch 118/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.6917 - acc: 0.8523 - val_loss: 0.6289 - val_acc: 0.8837\n",
            "Epoch 119/200\n",
            "88/88 [==============================] - 0s 281us/step - loss: 0.6881 - acc: 0.8523 - val_loss: 0.6248 - val_acc: 0.8837\n",
            "Epoch 120/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6845 - acc: 0.8523 - val_loss: 0.6206 - val_acc: 0.8837\n",
            "Epoch 121/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6808 - acc: 0.8523 - val_loss: 0.6166 - val_acc: 0.8837\n",
            "Epoch 122/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.6772 - acc: 0.8523 - val_loss: 0.6125 - val_acc: 0.8837\n",
            "Epoch 123/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.6737 - acc: 0.8523 - val_loss: 0.6085 - val_acc: 0.8837\n",
            "Epoch 124/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.6702 - acc: 0.8523 - val_loss: 0.6045 - val_acc: 0.8837\n",
            "Epoch 125/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6667 - acc: 0.8523 - val_loss: 0.6005 - val_acc: 0.8837\n",
            "Epoch 126/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.6632 - acc: 0.8523 - val_loss: 0.5965 - val_acc: 0.8837\n",
            "Epoch 127/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.6597 - acc: 0.8523 - val_loss: 0.5926 - val_acc: 0.8837\n",
            "Epoch 128/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.6563 - acc: 0.8523 - val_loss: 0.5887 - val_acc: 0.8837\n",
            "Epoch 129/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.6529 - acc: 0.8523 - val_loss: 0.5849 - val_acc: 0.8837\n",
            "Epoch 130/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6495 - acc: 0.8523 - val_loss: 0.5810 - val_acc: 0.8837\n",
            "Epoch 131/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6461 - acc: 0.8523 - val_loss: 0.5772 - val_acc: 0.8837\n",
            "Epoch 132/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.6427 - acc: 0.8523 - val_loss: 0.5735 - val_acc: 0.8837\n",
            "Epoch 133/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6394 - acc: 0.8523 - val_loss: 0.5697 - val_acc: 0.8837\n",
            "Epoch 134/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6360 - acc: 0.8523 - val_loss: 0.5660 - val_acc: 0.8837\n",
            "Epoch 135/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6327 - acc: 0.8636 - val_loss: 0.5623 - val_acc: 0.8837\n",
            "Epoch 136/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.6295 - acc: 0.8636 - val_loss: 0.5586 - val_acc: 0.8837\n",
            "Epoch 137/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.6262 - acc: 0.8636 - val_loss: 0.5550 - val_acc: 0.8837\n",
            "Epoch 138/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.6230 - acc: 0.8636 - val_loss: 0.5514 - val_acc: 0.8837\n",
            "Epoch 139/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.6198 - acc: 0.8636 - val_loss: 0.5479 - val_acc: 0.8837\n",
            "Epoch 140/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.6166 - acc: 0.8636 - val_loss: 0.5444 - val_acc: 0.8837\n",
            "Epoch 141/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.6134 - acc: 0.8636 - val_loss: 0.5408 - val_acc: 0.8837\n",
            "Epoch 142/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.6103 - acc: 0.8636 - val_loss: 0.5374 - val_acc: 0.8837\n",
            "Epoch 143/200\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.6072 - acc: 0.8636 - val_loss: 0.5340 - val_acc: 0.8837\n",
            "Epoch 144/200\n",
            "88/88 [==============================] - 0s 278us/step - loss: 0.6041 - acc: 0.8636 - val_loss: 0.5306 - val_acc: 0.8837\n",
            "Epoch 145/200\n",
            "88/88 [==============================] - 0s 308us/step - loss: 0.6010 - acc: 0.8636 - val_loss: 0.5272 - val_acc: 0.8837\n",
            "Epoch 146/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.5980 - acc: 0.8636 - val_loss: 0.5238 - val_acc: 0.8837\n",
            "Epoch 147/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.5950 - acc: 0.8636 - val_loss: 0.5205 - val_acc: 0.8837\n",
            "Epoch 148/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5920 - acc: 0.8636 - val_loss: 0.5172 - val_acc: 0.8837\n",
            "Epoch 149/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.5890 - acc: 0.8636 - val_loss: 0.5140 - val_acc: 0.8837\n",
            "Epoch 150/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5860 - acc: 0.8636 - val_loss: 0.5107 - val_acc: 0.8837\n",
            "Epoch 151/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.5830 - acc: 0.8636 - val_loss: 0.5075 - val_acc: 0.8837\n",
            "Epoch 152/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5802 - acc: 0.8636 - val_loss: 0.5043 - val_acc: 0.8837\n",
            "Epoch 153/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5772 - acc: 0.8636 - val_loss: 0.5012 - val_acc: 0.8837\n",
            "Epoch 154/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5744 - acc: 0.8636 - val_loss: 0.4981 - val_acc: 0.8837\n",
            "Epoch 155/200\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.5715 - acc: 0.8636 - val_loss: 0.4950 - val_acc: 0.8837\n",
            "Epoch 156/200\n",
            "88/88 [==============================] - 0s 274us/step - loss: 0.5687 - acc: 0.8636 - val_loss: 0.4920 - val_acc: 0.8837\n",
            "Epoch 157/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.5659 - acc: 0.8636 - val_loss: 0.4889 - val_acc: 0.8837\n",
            "Epoch 158/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.5631 - acc: 0.8636 - val_loss: 0.4859 - val_acc: 0.8837\n",
            "Epoch 159/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.5604 - acc: 0.8636 - val_loss: 0.4829 - val_acc: 0.8837\n",
            "Epoch 160/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.5576 - acc: 0.8636 - val_loss: 0.4800 - val_acc: 0.8837\n",
            "Epoch 161/200\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.5549 - acc: 0.8636 - val_loss: 0.4771 - val_acc: 0.8837\n",
            "Epoch 162/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5523 - acc: 0.8636 - val_loss: 0.4741 - val_acc: 0.8837\n",
            "Epoch 163/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5495 - acc: 0.8636 - val_loss: 0.4714 - val_acc: 0.8837\n",
            "Epoch 164/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5468 - acc: 0.8636 - val_loss: 0.4685 - val_acc: 0.8837\n",
            "Epoch 165/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.5442 - acc: 0.8636 - val_loss: 0.4657 - val_acc: 0.8837\n",
            "Epoch 166/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5416 - acc: 0.8636 - val_loss: 0.4630 - val_acc: 0.8837\n",
            "Epoch 167/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5390 - acc: 0.8636 - val_loss: 0.4602 - val_acc: 0.8837\n",
            "Epoch 168/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.5364 - acc: 0.8636 - val_loss: 0.4575 - val_acc: 0.8837\n",
            "Epoch 169/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.5339 - acc: 0.8636 - val_loss: 0.4548 - val_acc: 0.8837\n",
            "Epoch 170/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5313 - acc: 0.8636 - val_loss: 0.4522 - val_acc: 0.8837\n",
            "Epoch 171/200\n",
            "88/88 [==============================] - 0s 261us/step - loss: 0.5289 - acc: 0.8636 - val_loss: 0.4495 - val_acc: 0.8837\n",
            "Epoch 172/200\n",
            "88/88 [==============================] - 0s 268us/step - loss: 0.5263 - acc: 0.8636 - val_loss: 0.4469 - val_acc: 0.8837\n",
            "Epoch 173/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5239 - acc: 0.8636 - val_loss: 0.4443 - val_acc: 0.8837\n",
            "Epoch 174/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.5215 - acc: 0.8750 - val_loss: 0.4418 - val_acc: 0.8837\n",
            "Epoch 175/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.5190 - acc: 0.8750 - val_loss: 0.4392 - val_acc: 0.8837\n",
            "Epoch 176/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5166 - acc: 0.8750 - val_loss: 0.4367 - val_acc: 0.8837\n",
            "Epoch 177/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.5143 - acc: 0.8750 - val_loss: 0.4342 - val_acc: 0.8837\n",
            "Epoch 178/200\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.5119 - acc: 0.8750 - val_loss: 0.4318 - val_acc: 0.8837\n",
            "Epoch 179/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.5096 - acc: 0.8750 - val_loss: 0.4294 - val_acc: 0.8837\n",
            "Epoch 180/200\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.5073 - acc: 0.8750 - val_loss: 0.4270 - val_acc: 0.8837\n",
            "Epoch 181/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5050 - acc: 0.8750 - val_loss: 0.4246 - val_acc: 0.8837\n",
            "Epoch 182/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.5028 - acc: 0.8750 - val_loss: 0.4222 - val_acc: 0.8837\n",
            "Epoch 183/200\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.5005 - acc: 0.8750 - val_loss: 0.4199 - val_acc: 0.8837\n",
            "Epoch 184/200\n",
            "88/88 [==============================] - 0s 302us/step - loss: 0.4983 - acc: 0.8750 - val_loss: 0.4176 - val_acc: 0.8837\n",
            "Epoch 185/200\n",
            "88/88 [==============================] - 0s 277us/step - loss: 0.4961 - acc: 0.8750 - val_loss: 0.4154 - val_acc: 0.8837\n",
            "Epoch 186/200\n",
            "88/88 [==============================] - 0s 276us/step - loss: 0.4939 - acc: 0.8750 - val_loss: 0.4131 - val_acc: 0.8837\n",
            "Epoch 187/200\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.4918 - acc: 0.8750 - val_loss: 0.4109 - val_acc: 0.8837\n",
            "Epoch 188/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.4896 - acc: 0.8750 - val_loss: 0.4086 - val_acc: 0.8837\n",
            "Epoch 189/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4875 - acc: 0.8750 - val_loss: 0.4065 - val_acc: 0.8837\n",
            "Epoch 190/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.4854 - acc: 0.8750 - val_loss: 0.4043 - val_acc: 0.8837\n",
            "Epoch 191/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.4832 - acc: 0.8750 - val_loss: 0.4022 - val_acc: 0.8837\n",
            "Epoch 192/200\n",
            "88/88 [==============================] - 0s 273us/step - loss: 0.4812 - acc: 0.8750 - val_loss: 0.4000 - val_acc: 0.8837\n",
            "Epoch 193/200\n",
            "88/88 [==============================] - 0s 284us/step - loss: 0.4792 - acc: 0.8750 - val_loss: 0.3980 - val_acc: 0.8837\n",
            "Epoch 194/200\n",
            "88/88 [==============================] - 0s 291us/step - loss: 0.4771 - acc: 0.8750 - val_loss: 0.3959 - val_acc: 0.8837\n",
            "Epoch 195/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.4751 - acc: 0.8750 - val_loss: 0.3938 - val_acc: 0.8837\n",
            "Epoch 196/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4731 - acc: 0.8750 - val_loss: 0.3918 - val_acc: 0.8837\n",
            "Epoch 197/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4711 - acc: 0.8750 - val_loss: 0.3898 - val_acc: 0.8837\n",
            "Epoch 198/200\n",
            "88/88 [==============================] - 0s 262us/step - loss: 0.4691 - acc: 0.8750 - val_loss: 0.3878 - val_acc: 0.8837\n",
            "Epoch 199/200\n",
            "88/88 [==============================] - 0s 312us/step - loss: 0.4672 - acc: 0.8750 - val_loss: 0.3858 - val_acc: 0.8837\n",
            "Epoch 200/200\n",
            "88/88 [==============================] - 0s 290us/step - loss: 0.4653 - acc: 0.8750 - val_loss: 0.3839 - val_acc: 0.8837\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weMNsKxZnVSt",
        "colab_type": "code",
        "outputId": "ac0aad6e-4f0c-4911-d5aa-8d9a93d72ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 764
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.05997689,  1.78480965],\n",
              "       [-1.70377632,  2.62178086],\n",
              "       [-0.88024938,  1.10677205],\n",
              "       [-1.6221035 ,  2.53797672],\n",
              "       [-1.20438162,  1.42505216],\n",
              "       [-0.72831295, -0.54994201],\n",
              "       [-0.71544696, -1.05313817],\n",
              "       [-0.78016566, -0.87184816],\n",
              "       [-0.66616332, -1.04551976],\n",
              "       [-0.80814457, -1.1534213 ],\n",
              "       [-0.18117691, -0.5752502 ],\n",
              "       [-0.5797643 , -0.90332856],\n",
              "       [-0.75946851, -0.80650801],\n",
              "       [-0.47381325, -0.94095818],\n",
              "       [-0.51651934, -0.3343159 ],\n",
              "       [-0.9078223 , -0.51343018],\n",
              "       [-0.83040609, -0.52280685],\n",
              "       [-0.8364583 , -1.1044959 ],\n",
              "       [-1.06491561, -0.23177315],\n",
              "       [-1.09349754, -1.34108765],\n",
              "       [-1.44273181, -1.36811384],\n",
              "       [-0.39121396, -1.41060042],\n",
              "       [-0.96660223, -1.52141472],\n",
              "       [-0.29851281, -1.2365264 ],\n",
              "       [-0.66466281, -0.89831149],\n",
              "       [ 1.12079905,  0.10919806],\n",
              "       [ 0.41839628,  0.46087105],\n",
              "       [ 0.81585568,  0.3499919 ],\n",
              "       [ 0.6952246 , -0.22961609],\n",
              "       [ 0.78712685,  0.6056401 ],\n",
              "       [ 0.98738836,  0.40085179],\n",
              "       [ 1.18764219,  0.36694321],\n",
              "       [ 1.70546993,  0.15254344],\n",
              "       [ 1.84248269, -0.39151261],\n",
              "       [ 1.42391516,  0.0626175 ],\n",
              "       [ 1.3927998 ,  0.12226364],\n",
              "       [ 1.6269367 ,  0.7386622 ],\n",
              "       [ 1.87133538,  1.64617693],\n",
              "       [ 1.27102475, -0.06582761],\n",
              "       [ 0.9978794 ,  0.55048648],\n",
              "       [ 0.59853173,  0.27469505],\n",
              "       [ 1.40628344,  0.21564038],\n",
              "       [ 1.07460331,  0.42032253]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60PpnMXrkKFq",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "73e12cde-c14b-450b-98eb-7b041eeeba54",
        "id": "nCDzc10dkKFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "bb4c53c5-3df6-4198-fcd6-734693824ef7",
        "id": "Y8cLzq3AkKF2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q7PBWUP9kKF6",
        "colab": {}
      },
      "source": [
        "average_acc_history_lda = [np.mean([x[i] for x in all_acc_histories_lda]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_lda = [np.mean([x[i] for x in all_loss_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_acc_history_lda = [np.mean([x[i] for x in all_val_acc_histories_lda]) for i in range(num_epochs)]\n",
        "average_val_loss_history_lda = [np.mean([x[i] for x in all_val_loss_histories_lda]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8af43eaa-9c12-4e93-fa53-098df0cbb569",
        "id": "GXZaeLG7kKF_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0-h4UV1kKGD"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ep2R0tm9kKGF",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dYcufxLHkKGK",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "83f56f43-f5d1-499a-8832-b8c6c6395adf",
        "id": "XLBO05_pkKGQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_lda, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_lda, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a931790f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBM45F+U8ETjnXJT7f8xlZ/fFF3xDAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XV3hLoOykKGV"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c032b20b-9858-4289-c487-5a939cc9d178",
        "id": "ttjl5eApkKGV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_lda, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_lda, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a930e8e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ckvqkyTYqE7B"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "52db6399-3cfc-4e22-9cce-c68582a9bc57",
        "id": "kmiW5yZ0qE7Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_lda, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_lda, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "131/131 [==============================] - 0s 3ms/step - loss: 1.1954 - acc: 0.5420\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 1.1574 - acc: 0.5725\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.1221 - acc: 0.5649\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 1.0884 - acc: 0.5649\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 1.0563 - acc: 0.5649\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 1.0275 - acc: 0.5649\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 1.0002 - acc: 0.5649\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.9746 - acc: 0.5573\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 0s 190us/step - loss: 0.9505 - acc: 0.5573\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.9285 - acc: 0.5649\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 0s 213us/step - loss: 0.9079 - acc: 0.5649\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.8879 - acc: 0.5802\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8689 - acc: 0.5878\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.8518 - acc: 0.6565\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.8362 - acc: 0.7099\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.8214 - acc: 0.7786\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.8075 - acc: 0.8168\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7944 - acc: 0.8397\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.7817 - acc: 0.8779\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.7695 - acc: 0.9008\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.7577 - acc: 0.9160\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.7466 - acc: 0.9237\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.7363 - acc: 0.9237\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.7254 - acc: 0.9237\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 0s 240us/step - loss: 0.7154 - acc: 0.9237\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7059 - acc: 0.9237\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.6967 - acc: 0.9313\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.6875 - acc: 0.9313\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6786 - acc: 0.9313\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.6699 - acc: 0.9313\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 0s 227us/step - loss: 0.6617 - acc: 0.9313\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6535 - acc: 0.9313\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.6454 - acc: 0.9313\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.6376 - acc: 0.9313\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6300 - acc: 0.9313\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6227 - acc: 0.9313\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.6156 - acc: 0.9313\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.6086 - acc: 0.9313\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6018 - acc: 0.9313\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5950 - acc: 0.9313\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5884 - acc: 0.9313\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.5820 - acc: 0.9313\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5756 - acc: 0.9313\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.5692 - acc: 0.9313\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5631 - acc: 0.9313\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5570 - acc: 0.9313\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5510 - acc: 0.9237\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.5451 - acc: 0.9313\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.5394 - acc: 0.9313\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.5337 - acc: 0.9237\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.5282 - acc: 0.9237\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5227 - acc: 0.9237\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.5174 - acc: 0.9237\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.5121 - acc: 0.9313\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.5068 - acc: 0.9237\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5018 - acc: 0.9313\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.4966 - acc: 0.9313\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4917 - acc: 0.9313\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4866 - acc: 0.9313\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4818 - acc: 0.9313\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4768 - acc: 0.9389\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.4721 - acc: 0.9389\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.4674 - acc: 0.9389\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.4627 - acc: 0.9389\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.4581 - acc: 0.9389\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4536 - acc: 0.9389\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4491 - acc: 0.9389\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.4447 - acc: 0.9389\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.4403 - acc: 0.9389\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.4361 - acc: 0.9389\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4319 - acc: 0.9389\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4277 - acc: 0.9389\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4236 - acc: 0.9389\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4195 - acc: 0.9389\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.4154 - acc: 0.9389\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.4114 - acc: 0.9389\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4075 - acc: 0.9389\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.4036 - acc: 0.9389\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3997 - acc: 0.9389\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.3959 - acc: 0.9389\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.3922 - acc: 0.9389\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3885 - acc: 0.9389\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.3849 - acc: 0.9389\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3812 - acc: 0.9389\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.3776 - acc: 0.9389\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.3742 - acc: 0.9389\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.3707 - acc: 0.9389\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.3673 - acc: 0.9389\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.3639 - acc: 0.9389\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.3606 - acc: 0.9389\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.3573 - acc: 0.9389\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.3540 - acc: 0.9389\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.3508 - acc: 0.9389\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3476 - acc: 0.9466\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.3445 - acc: 0.9466\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.3414 - acc: 0.9466\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3384 - acc: 0.9466\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.3353 - acc: 0.9466\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 0s 140us/step - loss: 0.3323 - acc: 0.9542\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3294 - acc: 0.9542\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.3264 - acc: 0.9542\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3235 - acc: 0.9542\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3206 - acc: 0.9542\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.3178 - acc: 0.9542\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3150 - acc: 0.9618\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3123 - acc: 0.9618\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.3096 - acc: 0.9618\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.3069 - acc: 0.9618\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.3042 - acc: 0.9618\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.3016 - acc: 0.9618\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.2989 - acc: 0.9618\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.2964 - acc: 0.9618\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.2937 - acc: 0.9618\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2912 - acc: 0.9618\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.2887 - acc: 0.9618\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2862 - acc: 0.9618\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.2838 - acc: 0.9618\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2814 - acc: 0.9618\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.2791 - acc: 0.9618\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2767 - acc: 0.9618\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.2744 - acc: 0.9618\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.2721 - acc: 0.9618\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2698 - acc: 0.9618\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2675 - acc: 0.9618\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2653 - acc: 0.9618\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.2630 - acc: 0.9618\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.2607 - acc: 0.9618\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.2586 - acc: 0.9618\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.2565 - acc: 0.9618\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.2543 - acc: 0.9618\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.2522 - acc: 0.9618\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2502 - acc: 0.9618\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.2481 - acc: 0.9618\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2461 - acc: 0.9618\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2441 - acc: 0.9618\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.2421 - acc: 0.9618\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.2401 - acc: 0.9618\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2382 - acc: 0.9618\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.2364 - acc: 0.9618\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.2345 - acc: 0.9618\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.2326 - acc: 0.9618\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2308 - acc: 0.9618\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.2289 - acc: 0.9618\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.2271 - acc: 0.9618\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.2253 - acc: 0.9618\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2236 - acc: 0.9618\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.2218 - acc: 0.9618\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.2201 - acc: 0.9618\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.2184 - acc: 0.9618\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2167 - acc: 0.9618\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.2150 - acc: 0.9695\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2133 - acc: 0.9695\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 0.2117 - acc: 0.9695\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.2100 - acc: 0.9695\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.2084 - acc: 0.9695\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.2069 - acc: 0.9695\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.2053 - acc: 0.9695\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.2038 - acc: 0.9771\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.2023 - acc: 0.9771\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.2007 - acc: 0.9771\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.1992 - acc: 0.9771\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1978 - acc: 0.9771\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.1964 - acc: 0.9771\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.1950 - acc: 0.9771\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.1936 - acc: 0.9771\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.1922 - acc: 0.9771\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.1908 - acc: 0.9771\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.1895 - acc: 0.9771\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.1882 - acc: 0.9771\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1869 - acc: 0.9847\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.1855 - acc: 0.9847\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.1842 - acc: 0.9847\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.1829 - acc: 0.9847\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1817 - acc: 0.9847\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.1804 - acc: 0.9847\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1791 - acc: 0.9847\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1779 - acc: 0.9847\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.1767 - acc: 0.9847\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1755 - acc: 0.9847\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.1743 - acc: 0.9847\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.1731 - acc: 0.9847\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1720 - acc: 0.9847\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.1708 - acc: 0.9847\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.1697 - acc: 0.9847\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 0s 141us/step - loss: 0.1686 - acc: 0.9847\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.1675 - acc: 0.9847\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 0s 146us/step - loss: 0.1665 - acc: 0.9847\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.1654 - acc: 0.9847\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1643 - acc: 0.9847\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.1633 - acc: 0.9847\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 0s 142us/step - loss: 0.1623 - acc: 0.9847\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.1612 - acc: 0.9847\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.1602 - acc: 0.9847\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.1592 - acc: 0.9847\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.1582 - acc: 0.9847\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.1572 - acc: 0.9847\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 0s 139us/step - loss: 0.1563 - acc: 0.9847\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.1553 - acc: 0.9847\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 0s 150us/step - loss: 0.1543 - acc: 0.9847\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.1534 - acc: 0.9847\n",
            "34/34 [==============================] - 0s 3ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "35e1976b-f813-4182-8842-6b839d99bf30",
        "id": "lCz1ZOSIqE7i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "67bd4a44-79b4-40ba-b65a-5b751506666b",
        "id": "GbroOBzLqE7v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11764705882352941"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX_hXso7rd39",
        "colab_type": "text"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlrMqdh1w2bs",
        "colab_type": "text"
      },
      "source": [
        "Remove correlated features manually"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZN9I7JNNUyS",
        "colab_type": "text"
      },
      "source": [
        "# Prova remove correlated features with treshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wACff-R4Ib",
        "colab_type": "text"
      },
      "source": [
        "https://campus.datacamp.com/courses/dimensionality-reduction-in-python/feature-selection-i-selecting-for-feature-information?ex=14"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNufM8B3NYs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a positive correlation matrix\n",
        "corr_df = train_data_stand.corr().abs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-W8ShRUvN63m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create and apply mask\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DgfSECpOwPu",
        "colab_type": "code",
        "outputId": "5d4050b2-1b27-4008-fda7-8db4bf4affda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "mask"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True, ...,  True,  True,  True],\n",
              "       [False,  True,  True, ...,  True,  True,  True],\n",
              "       [False, False,  True, ...,  True,  True,  True],\n",
              "       ...,\n",
              "       [False, False, False, ...,  True,  True,  True],\n",
              "       [False, False, False, ..., False,  True,  True],\n",
              "       [False, False, False, ..., False, False,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5xQLptVOw6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tri_df = corr_df.mask(mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeDvePEPO8mn",
        "colab_type": "code",
        "outputId": "d101c796-70b3-4395-cf5d-b017c85ab044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "source": [
        "tri_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>VoxelVolume</th>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <th>MeshVolume</th>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <th>Sphericity</th>\n",
              "      <th>LeastAxisLength</th>\n",
              "      <th>Elongation</th>\n",
              "      <th>SurfaceVolumeRatio</th>\n",
              "      <th>Maximum2DDiameterSlice</th>\n",
              "      <th>Flatness</th>\n",
              "      <th>SurfaceArea</th>\n",
              "      <th>MinorAxisLength</th>\n",
              "      <th>Maximum2DDiameterColumn</th>\n",
              "      <th>Maximum2DDiameterRow</th>\n",
              "      <th>GrayLevelVariance</th>\n",
              "      <th>HighGrayLevelEmphasis</th>\n",
              "      <th>DependenceEntropy</th>\n",
              "      <th>DependenceNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity</th>\n",
              "      <th>SmallDependenceEmphasis</th>\n",
              "      <th>SmallDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>DependenceNonUniformityNormalized</th>\n",
              "      <th>LargeDependenceEmphasis</th>\n",
              "      <th>LargeDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>DependenceVariance</th>\n",
              "      <th>LargeDependenceHighGrayLevelEmphasis</th>\n",
              "      <th>SmallDependenceLowGrayLevelEmphasis</th>\n",
              "      <th>LowGrayLevelEmphasis</th>\n",
              "      <th>JointAverage</th>\n",
              "      <th>SumAverage</th>\n",
              "      <th>JointEntropy</th>\n",
              "      <th>ClusterShade</th>\n",
              "      <th>MaximumProbability</th>\n",
              "      <th>Idmn</th>\n",
              "      <th>JointEnergy</th>\n",
              "      <th>Contrast</th>\n",
              "      <th>DifferenceEntropy</th>\n",
              "      <th>InverseVariance</th>\n",
              "      <th>DifferenceVariance</th>\n",
              "      <th>Idn</th>\n",
              "      <th>...</th>\n",
              "      <th>10Percentile</th>\n",
              "      <th>Kurtosis</th>\n",
              "      <th>Mean</th>\n",
              "      <th>ShortRunLowGrayLevelEmphasis</th>\n",
              "      <th>GrayLevelVariance.1</th>\n",
              "      <th>LowGrayLevelRunEmphasis</th>\n",
              "      <th>GrayLevelNonUniformityNormalized</th>\n",
              "      <th>RunVariance</th>\n",
              "      <th>GrayLevelNonUniformity.1</th>\n",
              "      <th>LongRunEmphasis</th>\n",
              "      <th>ShortRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunLengthNonUniformity</th>\n",
              "      <th>ShortRunEmphasis</th>\n",
              "      <th>LongRunHighGrayLevelEmphasis</th>\n",
              "      <th>RunPercentage</th>\n",
              "      <th>LongRunLowGrayLevelEmphasis</th>\n",
              "      <th>RunEntropy</th>\n",
              "      <th>HighGrayLevelRunEmphasis</th>\n",
              "      <th>RunLengthNonUniformityNormalized</th>\n",
              "      <th>GrayLevelVariance.2</th>\n",
              "      <th>ZoneVariance</th>\n",
              "      <th>GrayLevelNonUniformityNormalized.1</th>\n",
              "      <th>SizeZoneNonUniformityNormalized</th>\n",
              "      <th>SizeZoneNonUniformity</th>\n",
              "      <th>GrayLevelNonUniformity.2</th>\n",
              "      <th>LargeAreaEmphasis</th>\n",
              "      <th>SmallAreaHighGrayLevelEmphasis</th>\n",
              "      <th>ZonePercentage</th>\n",
              "      <th>LargeAreaLowGrayLevelEmphasis</th>\n",
              "      <th>LargeAreaHighGrayLevelEmphasis</th>\n",
              "      <th>HighGrayLevelZoneEmphasis</th>\n",
              "      <th>SmallAreaEmphasis</th>\n",
              "      <th>LowGrayLevelZoneEmphasis</th>\n",
              "      <th>ZoneEntropy</th>\n",
              "      <th>SmallAreaLowGrayLevelEmphasis</th>\n",
              "      <th>Coarseness</th>\n",
              "      <th>Complexity</th>\n",
              "      <th>Strength</th>\n",
              "      <th>Contrast.1</th>\n",
              "      <th>Busyness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>VoxelVolume</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Maximum3DDiameter</th>\n",
              "      <td>0.821982</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MeshVolume</th>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.821800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MajorAxisLength</th>\n",
              "      <td>0.785606</td>\n",
              "      <td>0.964760</td>\n",
              "      <td>0.785457</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Sphericity</th>\n",
              "      <td>0.329751</td>\n",
              "      <td>0.678628</td>\n",
              "      <td>0.329524</td>\n",
              "      <td>0.694906</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Coarseness</th>\n",
              "      <td>0.401634</td>\n",
              "      <td>0.586992</td>\n",
              "      <td>0.401370</td>\n",
              "      <td>0.546184</td>\n",
              "      <td>0.356708</td>\n",
              "      <td>0.569919</td>\n",
              "      <td>0.048602</td>\n",
              "      <td>0.813595</td>\n",
              "      <td>0.614989</td>\n",
              "      <td>0.094676</td>\n",
              "      <td>0.471734</td>\n",
              "      <td>0.598815</td>\n",
              "      <td>0.567029</td>\n",
              "      <td>0.554227</td>\n",
              "      <td>0.192052</td>\n",
              "      <td>0.605729</td>\n",
              "      <td>0.246122</td>\n",
              "      <td>0.406714</td>\n",
              "      <td>0.351606</td>\n",
              "      <td>0.669357</td>\n",
              "      <td>0.183179</td>\n",
              "      <td>0.731964</td>\n",
              "      <td>0.417524</td>\n",
              "      <td>0.077465</td>\n",
              "      <td>0.379163</td>\n",
              "      <td>0.437111</td>\n",
              "      <td>0.787425</td>\n",
              "      <td>0.182358</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.624801</td>\n",
              "      <td>0.317491</td>\n",
              "      <td>0.119708</td>\n",
              "      <td>0.324753</td>\n",
              "      <td>0.661086</td>\n",
              "      <td>0.326758</td>\n",
              "      <td>0.468498</td>\n",
              "      <td>0.481677</td>\n",
              "      <td>0.578282</td>\n",
              "      <td>0.319663</td>\n",
              "      <td>0.667850</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350082</td>\n",
              "      <td>0.292163</td>\n",
              "      <td>0.539925</td>\n",
              "      <td>0.345026</td>\n",
              "      <td>0.148147</td>\n",
              "      <td>0.251303</td>\n",
              "      <td>0.399211</td>\n",
              "      <td>0.336092</td>\n",
              "      <td>0.360524</td>\n",
              "      <td>0.381037</td>\n",
              "      <td>0.556410</td>\n",
              "      <td>0.412333</td>\n",
              "      <td>0.521415</td>\n",
              "      <td>0.480184</td>\n",
              "      <td>0.512788</td>\n",
              "      <td>0.008499</td>\n",
              "      <td>0.194296</td>\n",
              "      <td>0.595126</td>\n",
              "      <td>0.552550</td>\n",
              "      <td>0.201612</td>\n",
              "      <td>0.272599</td>\n",
              "      <td>0.029599</td>\n",
              "      <td>0.374723</td>\n",
              "      <td>0.426885</td>\n",
              "      <td>0.417651</td>\n",
              "      <td>0.272696</td>\n",
              "      <td>0.501061</td>\n",
              "      <td>0.667822</td>\n",
              "      <td>0.156196</td>\n",
              "      <td>0.163082</td>\n",
              "      <td>0.511486</td>\n",
              "      <td>0.366279</td>\n",
              "      <td>0.704958</td>\n",
              "      <td>0.523589</td>\n",
              "      <td>0.754391</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Complexity</th>\n",
              "      <td>0.277302</td>\n",
              "      <td>0.257297</td>\n",
              "      <td>0.277025</td>\n",
              "      <td>0.229141</td>\n",
              "      <td>0.182615</td>\n",
              "      <td>0.268450</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.214902</td>\n",
              "      <td>0.256226</td>\n",
              "      <td>0.019975</td>\n",
              "      <td>0.283738</td>\n",
              "      <td>0.284716</td>\n",
              "      <td>0.259748</td>\n",
              "      <td>0.265833</td>\n",
              "      <td>0.441669</td>\n",
              "      <td>0.213055</td>\n",
              "      <td>0.423394</td>\n",
              "      <td>0.392267</td>\n",
              "      <td>0.103854</td>\n",
              "      <td>0.068935</td>\n",
              "      <td>0.422629</td>\n",
              "      <td>0.058114</td>\n",
              "      <td>0.198407</td>\n",
              "      <td>0.214767</td>\n",
              "      <td>0.177538</td>\n",
              "      <td>0.125140</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.275899</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.148703</td>\n",
              "      <td>0.322120</td>\n",
              "      <td>0.410309</td>\n",
              "      <td>0.276311</td>\n",
              "      <td>0.079483</td>\n",
              "      <td>0.275068</td>\n",
              "      <td>0.221594</td>\n",
              "      <td>0.217025</td>\n",
              "      <td>0.125930</td>\n",
              "      <td>0.298911</td>\n",
              "      <td>0.099449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.267658</td>\n",
              "      <td>0.198372</td>\n",
              "      <td>0.116352</td>\n",
              "      <td>0.255976</td>\n",
              "      <td>0.464523</td>\n",
              "      <td>0.267090</td>\n",
              "      <td>0.228132</td>\n",
              "      <td>0.195523</td>\n",
              "      <td>0.139835</td>\n",
              "      <td>0.198482</td>\n",
              "      <td>0.296969</td>\n",
              "      <td>0.359280</td>\n",
              "      <td>0.166626</td>\n",
              "      <td>0.054576</td>\n",
              "      <td>0.168824</td>\n",
              "      <td>0.228273</td>\n",
              "      <td>0.355694</td>\n",
              "      <td>0.220089</td>\n",
              "      <td>0.151630</td>\n",
              "      <td>0.687837</td>\n",
              "      <td>0.011797</td>\n",
              "      <td>0.469854</td>\n",
              "      <td>0.036549</td>\n",
              "      <td>0.453017</td>\n",
              "      <td>0.274863</td>\n",
              "      <td>0.011927</td>\n",
              "      <td>0.351102</td>\n",
              "      <td>0.063147</td>\n",
              "      <td>0.152285</td>\n",
              "      <td>0.074162</td>\n",
              "      <td>0.313890</td>\n",
              "      <td>0.037902</td>\n",
              "      <td>0.048116</td>\n",
              "      <td>0.609656</td>\n",
              "      <td>0.083326</td>\n",
              "      <td>0.166795</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Strength</th>\n",
              "      <td>0.510155</td>\n",
              "      <td>0.642559</td>\n",
              "      <td>0.509976</td>\n",
              "      <td>0.607597</td>\n",
              "      <td>0.339595</td>\n",
              "      <td>0.645544</td>\n",
              "      <td>0.021965</td>\n",
              "      <td>0.758610</td>\n",
              "      <td>0.670268</td>\n",
              "      <td>0.005149</td>\n",
              "      <td>0.562243</td>\n",
              "      <td>0.659201</td>\n",
              "      <td>0.624228</td>\n",
              "      <td>0.620794</td>\n",
              "      <td>0.064819</td>\n",
              "      <td>0.388883</td>\n",
              "      <td>0.341710</td>\n",
              "      <td>0.538344</td>\n",
              "      <td>0.425689</td>\n",
              "      <td>0.458791</td>\n",
              "      <td>0.226763</td>\n",
              "      <td>0.486770</td>\n",
              "      <td>0.290047</td>\n",
              "      <td>0.085559</td>\n",
              "      <td>0.272032</td>\n",
              "      <td>0.325476</td>\n",
              "      <td>0.564383</td>\n",
              "      <td>0.104481</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.387344</td>\n",
              "      <td>0.099738</td>\n",
              "      <td>0.184149</td>\n",
              "      <td>0.141523</td>\n",
              "      <td>0.432332</td>\n",
              "      <td>0.139576</td>\n",
              "      <td>0.296993</td>\n",
              "      <td>0.271914</td>\n",
              "      <td>0.332061</td>\n",
              "      <td>0.195143</td>\n",
              "      <td>0.422246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.126145</td>\n",
              "      <td>0.021895</td>\n",
              "      <td>0.283099</td>\n",
              "      <td>0.224183</td>\n",
              "      <td>0.030685</td>\n",
              "      <td>0.155122</td>\n",
              "      <td>0.160531</td>\n",
              "      <td>0.240018</td>\n",
              "      <td>0.438144</td>\n",
              "      <td>0.264077</td>\n",
              "      <td>0.333647</td>\n",
              "      <td>0.530074</td>\n",
              "      <td>0.343866</td>\n",
              "      <td>0.337177</td>\n",
              "      <td>0.347487</td>\n",
              "      <td>0.027798</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.372899</td>\n",
              "      <td>0.365016</td>\n",
              "      <td>0.083608</td>\n",
              "      <td>0.329544</td>\n",
              "      <td>0.113899</td>\n",
              "      <td>0.390861</td>\n",
              "      <td>0.567592</td>\n",
              "      <td>0.552634</td>\n",
              "      <td>0.329531</td>\n",
              "      <td>0.225342</td>\n",
              "      <td>0.451164</td>\n",
              "      <td>0.206294</td>\n",
              "      <td>0.221538</td>\n",
              "      <td>0.253885</td>\n",
              "      <td>0.383804</td>\n",
              "      <td>0.556837</td>\n",
              "      <td>0.544355</td>\n",
              "      <td>0.611316</td>\n",
              "      <td>0.746738</td>\n",
              "      <td>0.074615</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Contrast.1</th>\n",
              "      <td>0.458069</td>\n",
              "      <td>0.592542</td>\n",
              "      <td>0.457835</td>\n",
              "      <td>0.555373</td>\n",
              "      <td>0.380133</td>\n",
              "      <td>0.588236</td>\n",
              "      <td>0.022289</td>\n",
              "      <td>0.639843</td>\n",
              "      <td>0.626229</td>\n",
              "      <td>0.065998</td>\n",
              "      <td>0.515229</td>\n",
              "      <td>0.623406</td>\n",
              "      <td>0.576338</td>\n",
              "      <td>0.557039</td>\n",
              "      <td>0.699380</td>\n",
              "      <td>0.654218</td>\n",
              "      <td>0.165224</td>\n",
              "      <td>0.399133</td>\n",
              "      <td>0.462206</td>\n",
              "      <td>0.847990</td>\n",
              "      <td>0.404501</td>\n",
              "      <td>0.807251</td>\n",
              "      <td>0.503620</td>\n",
              "      <td>0.040094</td>\n",
              "      <td>0.336053</td>\n",
              "      <td>0.507856</td>\n",
              "      <td>0.811825</td>\n",
              "      <td>0.372830</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.678354</td>\n",
              "      <td>0.674231</td>\n",
              "      <td>0.184830</td>\n",
              "      <td>0.427215</td>\n",
              "      <td>0.958897</td>\n",
              "      <td>0.507007</td>\n",
              "      <td>0.941239</td>\n",
              "      <td>0.822280</td>\n",
              "      <td>0.836388</td>\n",
              "      <td>0.858476</td>\n",
              "      <td>0.916872</td>\n",
              "      <td>...</td>\n",
              "      <td>0.777488</td>\n",
              "      <td>0.592929</td>\n",
              "      <td>0.882098</td>\n",
              "      <td>0.561351</td>\n",
              "      <td>0.652744</td>\n",
              "      <td>0.457706</td>\n",
              "      <td>0.693751</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>0.473433</td>\n",
              "      <td>0.477352</td>\n",
              "      <td>0.601669</td>\n",
              "      <td>0.442302</td>\n",
              "      <td>0.679507</td>\n",
              "      <td>0.566054</td>\n",
              "      <td>0.635554</td>\n",
              "      <td>0.154862</td>\n",
              "      <td>0.617903</td>\n",
              "      <td>0.665488</td>\n",
              "      <td>0.705727</td>\n",
              "      <td>0.010631</td>\n",
              "      <td>0.390378</td>\n",
              "      <td>0.398000</td>\n",
              "      <td>0.502327</td>\n",
              "      <td>0.354174</td>\n",
              "      <td>0.437122</td>\n",
              "      <td>0.390572</td>\n",
              "      <td>0.610009</td>\n",
              "      <td>0.852276</td>\n",
              "      <td>0.135793</td>\n",
              "      <td>0.242269</td>\n",
              "      <td>0.640887</td>\n",
              "      <td>0.494642</td>\n",
              "      <td>0.794082</td>\n",
              "      <td>0.291511</td>\n",
              "      <td>0.759051</td>\n",
              "      <td>0.551508</td>\n",
              "      <td>0.030764</td>\n",
              "      <td>0.374697</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Busyness</th>\n",
              "      <td>0.937894</td>\n",
              "      <td>0.810919</td>\n",
              "      <td>0.937881</td>\n",
              "      <td>0.773599</td>\n",
              "      <td>0.339451</td>\n",
              "      <td>0.870370</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.682675</td>\n",
              "      <td>0.843581</td>\n",
              "      <td>0.059322</td>\n",
              "      <td>0.913619</td>\n",
              "      <td>0.867644</td>\n",
              "      <td>0.805965</td>\n",
              "      <td>0.828142</td>\n",
              "      <td>0.176933</td>\n",
              "      <td>0.469093</td>\n",
              "      <td>0.023465</td>\n",
              "      <td>0.890974</td>\n",
              "      <td>0.884836</td>\n",
              "      <td>0.556691</td>\n",
              "      <td>0.356571</td>\n",
              "      <td>0.485873</td>\n",
              "      <td>0.386115</td>\n",
              "      <td>0.155181</td>\n",
              "      <td>0.253053</td>\n",
              "      <td>0.425932</td>\n",
              "      <td>0.366938</td>\n",
              "      <td>0.011316</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.441339</td>\n",
              "      <td>0.364459</td>\n",
              "      <td>0.187027</td>\n",
              "      <td>0.262478</td>\n",
              "      <td>0.474823</td>\n",
              "      <td>0.313668</td>\n",
              "      <td>0.411803</td>\n",
              "      <td>0.469366</td>\n",
              "      <td>0.490211</td>\n",
              "      <td>0.354087</td>\n",
              "      <td>0.516006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.294360</td>\n",
              "      <td>0.307158</td>\n",
              "      <td>0.382443</td>\n",
              "      <td>0.108961</td>\n",
              "      <td>0.147768</td>\n",
              "      <td>0.048395</td>\n",
              "      <td>0.428333</td>\n",
              "      <td>0.320272</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.365808</td>\n",
              "      <td>0.400702</td>\n",
              "      <td>0.915079</td>\n",
              "      <td>0.486168</td>\n",
              "      <td>0.448260</td>\n",
              "      <td>0.462777</td>\n",
              "      <td>0.126498</td>\n",
              "      <td>0.276150</td>\n",
              "      <td>0.466663</td>\n",
              "      <td>0.497241</td>\n",
              "      <td>0.130804</td>\n",
              "      <td>0.770513</td>\n",
              "      <td>0.135920</td>\n",
              "      <td>0.372307</td>\n",
              "      <td>0.858147</td>\n",
              "      <td>0.912460</td>\n",
              "      <td>0.770458</td>\n",
              "      <td>0.381146</td>\n",
              "      <td>0.557089</td>\n",
              "      <td>0.516059</td>\n",
              "      <td>0.592938</td>\n",
              "      <td>0.403923</td>\n",
              "      <td>0.370737</td>\n",
              "      <td>0.363137</td>\n",
              "      <td>0.403091</td>\n",
              "      <td>0.382465</td>\n",
              "      <td>0.434561</td>\n",
              "      <td>0.093528</td>\n",
              "      <td>0.602417</td>\n",
              "      <td>0.418697</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>107 rows × 107 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   VoxelVolume  Maximum3DDiameter  ...  Contrast.1  Busyness\n",
              "VoxelVolume                NaN                NaN  ...         NaN       NaN\n",
              "Maximum3DDiameter     0.821982                NaN  ...         NaN       NaN\n",
              "MeshVolume            0.999999           0.821800  ...         NaN       NaN\n",
              "MajorAxisLength       0.785606           0.964760  ...         NaN       NaN\n",
              "Sphericity            0.329751           0.678628  ...         NaN       NaN\n",
              "...                        ...                ...  ...         ...       ...\n",
              "Coarseness            0.401634           0.586992  ...         NaN       NaN\n",
              "Complexity            0.277302           0.257297  ...         NaN       NaN\n",
              "Strength              0.510155           0.642559  ...         NaN       NaN\n",
              "Contrast.1            0.458069           0.592542  ...         NaN       NaN\n",
              "Busyness              0.937894           0.810919  ...    0.418697       NaN\n",
              "\n",
              "[107 rows x 107 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQ1MkWP2O9hU",
        "colab_type": "code",
        "outputId": "72fe37f4-26ad-4a6d-9ca2-5732a582c13f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Find columns that meet threshold\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c]>0.70)]\n",
        "to_drop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['VoxelVolume',\n",
              " 'Maximum3DDiameter',\n",
              " 'MeshVolume',\n",
              " 'MajorAxisLength',\n",
              " 'LeastAxisLength',\n",
              " 'Elongation',\n",
              " 'SurfaceVolumeRatio',\n",
              " 'Maximum2DDiameterSlice',\n",
              " 'SurfaceArea',\n",
              " 'MinorAxisLength',\n",
              " 'Maximum2DDiameterColumn',\n",
              " 'Maximum2DDiameterRow',\n",
              " 'GrayLevelVariance',\n",
              " 'HighGrayLevelEmphasis',\n",
              " 'DependenceEntropy',\n",
              " 'DependenceNonUniformity',\n",
              " 'GrayLevelNonUniformity',\n",
              " 'SmallDependenceEmphasis',\n",
              " 'SmallDependenceHighGrayLevelEmphasis',\n",
              " 'DependenceNonUniformityNormalized',\n",
              " 'LargeDependenceEmphasis',\n",
              " 'LargeDependenceLowGrayLevelEmphasis',\n",
              " 'DependenceVariance',\n",
              " 'LargeDependenceHighGrayLevelEmphasis',\n",
              " 'SmallDependenceLowGrayLevelEmphasis',\n",
              " 'LowGrayLevelEmphasis',\n",
              " 'JointAverage',\n",
              " 'SumAverage',\n",
              " 'JointEntropy',\n",
              " 'ClusterShade',\n",
              " 'MaximumProbability',\n",
              " 'Idmn',\n",
              " 'JointEnergy',\n",
              " 'Contrast',\n",
              " 'DifferenceEntropy',\n",
              " 'InverseVariance',\n",
              " 'DifferenceVariance',\n",
              " 'Idn',\n",
              " 'Idm',\n",
              " 'Correlation',\n",
              " 'Autocorrelation',\n",
              " 'SumEntropy',\n",
              " 'SumSquares',\n",
              " 'ClusterProminence',\n",
              " 'Imc2',\n",
              " 'DifferenceAverage',\n",
              " 'Id',\n",
              " 'ClusterTendency',\n",
              " 'InterquartileRange',\n",
              " 'Skewness',\n",
              " 'Uniformity',\n",
              " 'Median',\n",
              " 'Energy',\n",
              " 'RobustMeanAbsoluteDeviation',\n",
              " 'MeanAbsoluteDeviation',\n",
              " 'Maximum',\n",
              " 'RootMeanSquared',\n",
              " 'Minimum',\n",
              " 'Entropy',\n",
              " 'Range',\n",
              " 'Variance',\n",
              " '10Percentile',\n",
              " 'Kurtosis',\n",
              " 'Mean',\n",
              " 'ShortRunLowGrayLevelEmphasis',\n",
              " 'GrayLevelVariance.1',\n",
              " 'LowGrayLevelRunEmphasis',\n",
              " 'GrayLevelNonUniformityNormalized',\n",
              " 'RunVariance',\n",
              " 'GrayLevelNonUniformity.1',\n",
              " 'LongRunEmphasis',\n",
              " 'ShortRunHighGrayLevelEmphasis',\n",
              " 'RunLengthNonUniformity',\n",
              " 'ShortRunEmphasis',\n",
              " 'LongRunHighGrayLevelEmphasis',\n",
              " 'RunPercentage',\n",
              " 'LongRunLowGrayLevelEmphasis',\n",
              " 'RunEntropy',\n",
              " 'HighGrayLevelRunEmphasis',\n",
              " 'RunLengthNonUniformityNormalized',\n",
              " 'ZoneVariance',\n",
              " 'SizeZoneNonUniformityNormalized',\n",
              " 'SizeZoneNonUniformity',\n",
              " 'GrayLevelNonUniformity.2',\n",
              " 'LargeAreaEmphasis',\n",
              " 'SmallAreaHighGrayLevelEmphasis',\n",
              " 'ZonePercentage',\n",
              " 'LowGrayLevelZoneEmphasis',\n",
              " 'SmallAreaLowGrayLevelEmphasis',\n",
              " 'Coarseness']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvKeereQoci",
        "colab_type": "text"
      },
      "source": [
        "The reason we used the mask to set half of the matrix to NA value is that we ewnt to avoid removing both features when thay have a strong correlation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32R-tG-WPNXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "train_data_stand_reduced = train_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnNUiUoZ0Vdv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Drop those columns\n",
        "test_data_stand_reduced = test_data_stand.drop(to_drop, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn-uuBifRO8G",
        "colab_type": "code",
        "outputId": "36f2591d-c481-48e2-e198-92c5a6a30732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3wuGcLV0acQ",
        "colab_type": "code",
        "outputId": "7fdee798-f561-4033-8759-7d1a9a0271b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1qR3re7QwA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_stand_reduced = train_data_stand_reduced.to_numpy()\n",
        "test_data_stand_reduced = test_data_stand_reduced.to_numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUG3a0o5W1O",
        "colab_type": "code",
        "outputId": "36e4e0cc-9e5a-4c06-ef84-3ed448f89222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR8wkFES71GJ",
        "colab_type": "code",
        "outputId": "3adf95c8-cd56-46ee-a538-db6b90ac3f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_data_stand_reduced.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(131, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ljMaQeuSHe1",
        "colab_type": "text"
      },
      "source": [
        "funziona bene, però bisogna stare attenti a basarsi unicamente sul coefficiente di correlazione. Se y = x^2, x e y risulteranno scorrelate secondo il coeffiente di correlazione di Pearson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxuH4u741mLR",
        "colab_type": "text"
      },
      "source": [
        "##Building Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "02fg1RRV6SkV",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "  model = keras.models.Sequential()\n",
        "  model.add(layers.Dense(10, activation='relu', input_shape=(17,)))\n",
        "  #model.add(layers.Dense(5, activation='relu'))\n",
        "  #model.add(layers.Dropout(rate=0.2))\n",
        "\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  sgd = SGD(lr=0.01, momentum=0.5)\n",
        "  \n",
        "  model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjmeGHBV1xs_",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V5hBUph6149E"
      },
      "source": [
        "##Stratified k-fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H7NwNpEO149J"
      },
      "source": [
        "This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0MD1VB0S149O",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c2f96d60-6bca-4438-8038-8fccfc0543b4",
        "id": "VkpRnrey149Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "skf.get_n_splits(train_data_stand_reduced, train_labels_dec)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "eeb620be-fd15-4291-ffd7-bd7d831c79b9",
        "id": "F3zK4E6o149g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "for train_index, test_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAIN: [  1   2   4   5   8  10  11  12  13  14  17  20  21  22  23  24  25  26\n",
            "  27  29  30  31  33  34  37  38  39  40  41  42  43  46  47  48  49  50\n",
            "  55  58  60  61  62  63  64  65  67  69  70  71  73  75  76  77  79  81\n",
            "  82  83  84  85  87  88  89  91  92  94  96  97  98  99 100 101 103 106\n",
            " 107 108 110 115 116 117 118 119 121 122 124 126 127 129 130] TEST: [  0   3   6   7   9  15  16  18  19  28  32  35  36  44  45  51  52  53\n",
            "  54  56  57  59  66  68  72  74  78  80  86  90  93  95 102 104 105 109\n",
            " 111 112 113 114 120 123 125 128]\n",
            "TRAIN: [  0   1   3   6   7   8   9  10  13  14  15  16  17  18  19  20  23  24\n",
            "  25  28  30  31  32  33  35  36  37  39  40  44  45  47  49  51  52  53\n",
            "  54  55  56  57  58  59  63  64  66  67  68  71  72  73  74  78  80  81\n",
            "  82  84  85  86  88  89  90  92  93  94  95  98  99 101 102 104 105 106\n",
            " 109 110 111 112 113 114 115 117 118 120 122 123 125 128 129] TEST: [  2   4   5  11  12  21  22  26  27  29  34  38  41  42  43  46  48  50\n",
            "  60  61  62  65  69  70  75  76  77  79  83  87  91  96  97 100 103 107\n",
            " 108 116 119 121 124 126 127 130]\n",
            "TRAIN: [  0   2   3   4   5   6   7   9  11  12  15  16  18  19  21  22  26  27\n",
            "  28  29  32  34  35  36  38  41  42  43  44  45  46  48  50  51  52  53\n",
            "  54  56  57  59  60  61  62  65  66  68  69  70  72  74  75  76  77  78\n",
            "  79  80  83  86  87  90  91  93  95  96  97 100 102 103 104 105 107 108\n",
            " 109 111 112 113 114 116 119 120 121 123 124 125 126 127 128 130] TEST: [  1   8  10  13  14  17  20  23  24  25  30  31  33  37  39  40  47  49\n",
            "  55  58  63  64  67  71  73  81  82  84  85  88  89  92  94  98  99 101\n",
            " 106 110 115 117 118 122 129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "e4b020b9-ff66-4fb1-dad2-a5dc8dea05bd",
        "id": "BDX_0MCd149o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels_dec[125]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9QlfChU149v",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bvCKZfjj1490",
        "colab": {}
      },
      "source": [
        "#for train_index, val_index in skf.split(train_data_stand_pca, train_labels_dec):\n",
        " \n",
        "#  partial_train_data = np.array([train_data_stand_pca[i] for i in train_index])\n",
        "#  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "\n",
        "#  val_data = np.array([train_data_stand_pca[i] for i in val_index])\n",
        "#  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "#  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "#  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        "#  model = build_model()\n",
        "#  model.fit(partial_train_data, one_hot_partial_train_targets, epochs = num_epochs, batch_size=1)\n",
        "\n",
        "#  val_loss, val_accuracy = model.evaluate(val_data, one_hot_val_targets)\n",
        "#  all_scores.append(val_accuracy)\n",
        "#I parametri per la valutazione vengono calcolati una volta per ogni k-fold, per ogni set di validazione, quindi k volte"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3bjrjhWo1497"
      },
      "source": [
        "C'è un problema: keras.utils.to_categorical produces a one-hot encoded class vector, i.e. the multilabel-indicator mentioned in the error message. StratifiedKFold is not designed to work with such input; i.e. your y must be a 1-D array of your class labels.\n",
        "Essentially, what you have to do is simply to invert the order of the operations: split first (using your intial y_train), and convert to_categorical afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Du7DFfm1498",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Qsz_jnVasri",
        "colab_type": "code",
        "outputId": "494ce790-54ba-476c-f67d-5019cb3908b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(train_data_stand_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "55ded20c-5846-48ad-8583-1d1aa85ab9b7",
        "id": "kHNpJxBL14-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "num_epochs = 200\n",
        "all_acc_histories_reduced = []\n",
        "all_loss_histories_reduced = []\n",
        "all_val_acc_histories_reduced = []\n",
        "all_val_loss_histories_reduced = []\n",
        "\n",
        "for train_index, val_index in skf.split(train_data_stand_reduced, train_labels_dec):\n",
        " \n",
        "  partial_train_data = np.array([train_data_stand_reduced[i] for i in train_index])\n",
        "  partial_train_targets = np.array([train_labels_dec[i] for i in train_index])\n",
        "  \n",
        "  val_data = np.array([train_data_stand_reduced[i] for i in val_index])\n",
        "  val_targets = np.array([train_labels_dec[i] for i in val_index])\n",
        "\n",
        "  one_hot_partial_train_targets = to_categorical(partial_train_targets)\n",
        "  one_hot_val_targets = to_categorical(val_targets)\n",
        "\n",
        " \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, one_hot_partial_train_targets, validation_data=(val_data, one_hot_val_targets), \n",
        "                      epochs=num_epochs, batch_size=8)\n",
        "  \n",
        "  acc_history_reduced = history.history['acc']\n",
        "  all_acc_histories_reduced.append(acc_history)\n",
        "\n",
        "  loss_history_reduced = history.history['loss']\n",
        "  all_loss_histories_reduced.append(loss_history)\n",
        "\n",
        "  acc_val_history_reduced = history.history['val_acc']\n",
        "  all_val_acc_histories_reduced.append(acc_val_history)\n",
        "\n",
        "  loss_val_history_reduced = history.history['val_loss']\n",
        "  all_val_loss_histories_reduced.append(loss_val_history)\n",
        "  \n",
        "\n",
        "#I parametri per la valutazione vengono calcolati per ogni epoca, quindi num_epochs volte. \n",
        "#Il tutto viene ripetuto un numero di volte pari a n_splits.\n",
        "#Si ottiene una lista con n_splits elementi ciascuno dei quali è una lista lunga num_epochs,\n",
        "#ogni elemento può essere uno fra questi: dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 5ms/step - loss: 1.2852 - acc: 0.3678 - val_loss: 1.2351 - val_acc: 0.4545\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.1662 - acc: 0.4023 - val_loss: 1.1834 - val_acc: 0.5227\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.0859 - acc: 0.4368 - val_loss: 1.1589 - val_acc: 0.5227\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 1.0316 - acc: 0.4483 - val_loss: 1.1529 - val_acc: 0.5455\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.9955 - acc: 0.4828 - val_loss: 1.1452 - val_acc: 0.5455\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.9708 - acc: 0.5287 - val_loss: 1.1419 - val_acc: 0.5682\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.9524 - acc: 0.5517 - val_loss: 1.1426 - val_acc: 0.5682\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.9334 - acc: 0.5747 - val_loss: 1.1414 - val_acc: 0.5682\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.9187 - acc: 0.5862 - val_loss: 1.1395 - val_acc: 0.5909\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.9059 - acc: 0.5862 - val_loss: 1.1375 - val_acc: 0.5909\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 319us/step - loss: 0.8948 - acc: 0.6092 - val_loss: 1.1398 - val_acc: 0.5909\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.8847 - acc: 0.5862 - val_loss: 1.1395 - val_acc: 0.5909\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 350us/step - loss: 0.8806 - acc: 0.6092 - val_loss: 1.1422 - val_acc: 0.5909\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.8729 - acc: 0.5977 - val_loss: 1.1426 - val_acc: 0.5682\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.8609 - acc: 0.6207 - val_loss: 1.1404 - val_acc: 0.5682\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.8524 - acc: 0.6092 - val_loss: 1.1423 - val_acc: 0.5455\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8468 - acc: 0.6322 - val_loss: 1.1468 - val_acc: 0.5682\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8386 - acc: 0.6667 - val_loss: 1.1507 - val_acc: 0.5455\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8320 - acc: 0.6552 - val_loss: 1.1508 - val_acc: 0.5227\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8293 - acc: 0.6667 - val_loss: 1.1537 - val_acc: 0.5227\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.8191 - acc: 0.6782 - val_loss: 1.1541 - val_acc: 0.5227\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.8154 - acc: 0.6782 - val_loss: 1.1575 - val_acc: 0.5227\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8105 - acc: 0.6782 - val_loss: 1.1618 - val_acc: 0.5227\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.8055 - acc: 0.6897 - val_loss: 1.1643 - val_acc: 0.5455\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.8026 - acc: 0.6782 - val_loss: 1.1670 - val_acc: 0.5227\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7971 - acc: 0.6897 - val_loss: 1.1696 - val_acc: 0.5000\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.7941 - acc: 0.6897 - val_loss: 1.1748 - val_acc: 0.5227\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.7906 - acc: 0.6782 - val_loss: 1.1758 - val_acc: 0.5000\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.7821 - acc: 0.6782 - val_loss: 1.1796 - val_acc: 0.5000\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7801 - acc: 0.6782 - val_loss: 1.1828 - val_acc: 0.5227\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7759 - acc: 0.6782 - val_loss: 1.1871 - val_acc: 0.5455\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7705 - acc: 0.6897 - val_loss: 1.1921 - val_acc: 0.5455\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.7658 - acc: 0.6897 - val_loss: 1.1936 - val_acc: 0.5455\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.7591 - acc: 0.6897 - val_loss: 1.1986 - val_acc: 0.5455\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7556 - acc: 0.6897 - val_loss: 1.2031 - val_acc: 0.5455\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.7535 - acc: 0.6897 - val_loss: 1.2042 - val_acc: 0.5227\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.7472 - acc: 0.6897 - val_loss: 1.2088 - val_acc: 0.5227\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.7437 - acc: 0.6897 - val_loss: 1.2130 - val_acc: 0.5455\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.7393 - acc: 0.6897 - val_loss: 1.2175 - val_acc: 0.5227\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.7356 - acc: 0.6897 - val_loss: 1.2215 - val_acc: 0.5455\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.7326 - acc: 0.6897 - val_loss: 1.2247 - val_acc: 0.5455\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.7283 - acc: 0.6897 - val_loss: 1.2298 - val_acc: 0.5455\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.7254 - acc: 0.6897 - val_loss: 1.2320 - val_acc: 0.5455\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7204 - acc: 0.6897 - val_loss: 1.2381 - val_acc: 0.5455\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7167 - acc: 0.6897 - val_loss: 1.2447 - val_acc: 0.5227\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.7167 - acc: 0.6782 - val_loss: 1.2478 - val_acc: 0.5682\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 354us/step - loss: 0.7118 - acc: 0.6897 - val_loss: 1.2515 - val_acc: 0.5682\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.7088 - acc: 0.6782 - val_loss: 1.2548 - val_acc: 0.5000\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.7032 - acc: 0.7011 - val_loss: 1.2600 - val_acc: 0.5227\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7013 - acc: 0.6897 - val_loss: 1.2643 - val_acc: 0.5227\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6984 - acc: 0.7011 - val_loss: 1.2695 - val_acc: 0.5227\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6943 - acc: 0.6897 - val_loss: 1.2741 - val_acc: 0.5000\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.6898 - acc: 0.7011 - val_loss: 1.2781 - val_acc: 0.5227\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6870 - acc: 0.6782 - val_loss: 1.2838 - val_acc: 0.5227\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.6836 - acc: 0.7011 - val_loss: 1.2850 - val_acc: 0.5000\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6793 - acc: 0.7011 - val_loss: 1.2907 - val_acc: 0.5227\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6762 - acc: 0.7011 - val_loss: 1.2959 - val_acc: 0.5000\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 286us/step - loss: 0.6736 - acc: 0.7011 - val_loss: 1.2994 - val_acc: 0.5227\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.6734 - acc: 0.7126 - val_loss: 1.3060 - val_acc: 0.5000\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.6695 - acc: 0.7011 - val_loss: 1.3105 - val_acc: 0.5000\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.6640 - acc: 0.7011 - val_loss: 1.3141 - val_acc: 0.5000\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6627 - acc: 0.7011 - val_loss: 1.3171 - val_acc: 0.5000\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6602 - acc: 0.7126 - val_loss: 1.3223 - val_acc: 0.5000\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6566 - acc: 0.6897 - val_loss: 1.3270 - val_acc: 0.5000\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 210us/step - loss: 0.6556 - acc: 0.7011 - val_loss: 1.3292 - val_acc: 0.5227\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 435us/step - loss: 0.6525 - acc: 0.7126 - val_loss: 1.3346 - val_acc: 0.5227\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 300us/step - loss: 0.6485 - acc: 0.6897 - val_loss: 1.3414 - val_acc: 0.5000\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.6471 - acc: 0.6897 - val_loss: 1.3460 - val_acc: 0.5000\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6427 - acc: 0.7011 - val_loss: 1.3519 - val_acc: 0.5000\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.6426 - acc: 0.7011 - val_loss: 1.3567 - val_acc: 0.5000\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6366 - acc: 0.7011 - val_loss: 1.3579 - val_acc: 0.5227\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.6389 - acc: 0.6897 - val_loss: 1.3624 - val_acc: 0.5227\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6354 - acc: 0.7126 - val_loss: 1.3673 - val_acc: 0.5227\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6312 - acc: 0.6897 - val_loss: 1.3718 - val_acc: 0.5227\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.6298 - acc: 0.7126 - val_loss: 1.3782 - val_acc: 0.5227\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6267 - acc: 0.7126 - val_loss: 1.3805 - val_acc: 0.5227\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6236 - acc: 0.7126 - val_loss: 1.3856 - val_acc: 0.5227\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.6202 - acc: 0.7126 - val_loss: 1.3911 - val_acc: 0.5000\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.6168 - acc: 0.7011 - val_loss: 1.3968 - val_acc: 0.5000\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.6140 - acc: 0.7241 - val_loss: 1.4029 - val_acc: 0.5000\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.6146 - acc: 0.7126 - val_loss: 1.4089 - val_acc: 0.5227\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 207us/step - loss: 0.6136 - acc: 0.7126 - val_loss: 1.4136 - val_acc: 0.5000\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6127 - acc: 0.7126 - val_loss: 1.4191 - val_acc: 0.5000\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.6043 - acc: 0.7356 - val_loss: 1.4240 - val_acc: 0.5000\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.6043 - acc: 0.7241 - val_loss: 1.4288 - val_acc: 0.5227\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.6032 - acc: 0.7011 - val_loss: 1.4382 - val_acc: 0.5000\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.6019 - acc: 0.7011 - val_loss: 1.4391 - val_acc: 0.5000\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5974 - acc: 0.6897 - val_loss: 1.4462 - val_acc: 0.5000\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5922 - acc: 0.7241 - val_loss: 1.4491 - val_acc: 0.5227\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.5959 - acc: 0.7241 - val_loss: 1.4546 - val_acc: 0.5227\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5877 - acc: 0.7356 - val_loss: 1.4607 - val_acc: 0.5000\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5878 - acc: 0.7241 - val_loss: 1.4668 - val_acc: 0.5000\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.5827 - acc: 0.7126 - val_loss: 1.4735 - val_acc: 0.5000\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.5806 - acc: 0.7471 - val_loss: 1.4760 - val_acc: 0.5227\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.5797 - acc: 0.7241 - val_loss: 1.4811 - val_acc: 0.5227\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.5776 - acc: 0.7241 - val_loss: 1.4890 - val_acc: 0.4773\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5775 - acc: 0.7471 - val_loss: 1.4900 - val_acc: 0.5227\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.5720 - acc: 0.7701 - val_loss: 1.4963 - val_acc: 0.5000\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 205us/step - loss: 0.5727 - acc: 0.7356 - val_loss: 1.5022 - val_acc: 0.5000\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5669 - acc: 0.7471 - val_loss: 1.5046 - val_acc: 0.5227\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5673 - acc: 0.7701 - val_loss: 1.5094 - val_acc: 0.5000\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 206us/step - loss: 0.5640 - acc: 0.7586 - val_loss: 1.5146 - val_acc: 0.5227\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5606 - acc: 0.7586 - val_loss: 1.5225 - val_acc: 0.5227\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 208us/step - loss: 0.5609 - acc: 0.7471 - val_loss: 1.5253 - val_acc: 0.5227\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5609 - acc: 0.7471 - val_loss: 1.5361 - val_acc: 0.5000\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5549 - acc: 0.7586 - val_loss: 1.5429 - val_acc: 0.5227\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.5532 - acc: 0.7586 - val_loss: 1.5498 - val_acc: 0.5000\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5529 - acc: 0.7471 - val_loss: 1.5495 - val_acc: 0.5000\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5498 - acc: 0.7586 - val_loss: 1.5544 - val_acc: 0.5000\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.5450 - acc: 0.7586 - val_loss: 1.5627 - val_acc: 0.5000\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5421 - acc: 0.7701 - val_loss: 1.5691 - val_acc: 0.5227\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.5394 - acc: 0.7586 - val_loss: 1.5742 - val_acc: 0.5000\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.5366 - acc: 0.7701 - val_loss: 1.5791 - val_acc: 0.5000\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5365 - acc: 0.7701 - val_loss: 1.5822 - val_acc: 0.5000\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.5346 - acc: 0.7586 - val_loss: 1.5890 - val_acc: 0.5000\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.5314 - acc: 0.7471 - val_loss: 1.5960 - val_acc: 0.5000\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5281 - acc: 0.7471 - val_loss: 1.5998 - val_acc: 0.5000\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5250 - acc: 0.7816 - val_loss: 1.6097 - val_acc: 0.5000\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5238 - acc: 0.7586 - val_loss: 1.6090 - val_acc: 0.5000\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5231 - acc: 0.7471 - val_loss: 1.6153 - val_acc: 0.5000\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.5183 - acc: 0.7816 - val_loss: 1.6247 - val_acc: 0.5000\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5171 - acc: 0.7816 - val_loss: 1.6328 - val_acc: 0.5000\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.5175 - acc: 0.7816 - val_loss: 1.6342 - val_acc: 0.5000\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.5111 - acc: 0.7816 - val_loss: 1.6380 - val_acc: 0.5000\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5093 - acc: 0.7701 - val_loss: 1.6438 - val_acc: 0.5000\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.5069 - acc: 0.7816 - val_loss: 1.6508 - val_acc: 0.5000\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.5040 - acc: 0.7701 - val_loss: 1.6628 - val_acc: 0.5000\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5046 - acc: 0.7586 - val_loss: 1.6669 - val_acc: 0.5000\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4967 - acc: 0.7701 - val_loss: 1.6673 - val_acc: 0.5000\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4973 - acc: 0.7586 - val_loss: 1.6751 - val_acc: 0.5000\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 270us/step - loss: 0.4957 - acc: 0.7931 - val_loss: 1.6898 - val_acc: 0.5000\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.4927 - acc: 0.7816 - val_loss: 1.6929 - val_acc: 0.5000\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 283us/step - loss: 0.4911 - acc: 0.7816 - val_loss: 1.6955 - val_acc: 0.5000\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4912 - acc: 0.8161 - val_loss: 1.7018 - val_acc: 0.5000\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.4885 - acc: 0.7931 - val_loss: 1.7053 - val_acc: 0.4773\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 273us/step - loss: 0.4852 - acc: 0.7816 - val_loss: 1.7091 - val_acc: 0.4773\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.4836 - acc: 0.7931 - val_loss: 1.7150 - val_acc: 0.4773\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4789 - acc: 0.8046 - val_loss: 1.7188 - val_acc: 0.4773\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 296us/step - loss: 0.4783 - acc: 0.8161 - val_loss: 1.7220 - val_acc: 0.4773\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4761 - acc: 0.8046 - val_loss: 1.7274 - val_acc: 0.4773\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.4751 - acc: 0.8161 - val_loss: 1.7338 - val_acc: 0.4773\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4727 - acc: 0.8046 - val_loss: 1.7341 - val_acc: 0.4773\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 211us/step - loss: 0.4692 - acc: 0.8046 - val_loss: 1.7400 - val_acc: 0.4773\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.4657 - acc: 0.8161 - val_loss: 1.7462 - val_acc: 0.4773\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.4661 - acc: 0.7931 - val_loss: 1.7467 - val_acc: 0.4773\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.4639 - acc: 0.8161 - val_loss: 1.7533 - val_acc: 0.4773\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.4599 - acc: 0.8276 - val_loss: 1.7564 - val_acc: 0.4773\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 317us/step - loss: 0.4599 - acc: 0.8161 - val_loss: 1.7620 - val_acc: 0.4773\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.4554 - acc: 0.8161 - val_loss: 1.7669 - val_acc: 0.4773\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.4555 - acc: 0.8276 - val_loss: 1.7693 - val_acc: 0.4773\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.4529 - acc: 0.8161 - val_loss: 1.7745 - val_acc: 0.4773\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.4509 - acc: 0.8161 - val_loss: 1.7808 - val_acc: 0.4545\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 325us/step - loss: 0.4502 - acc: 0.8276 - val_loss: 1.7847 - val_acc: 0.4545\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.4471 - acc: 0.8276 - val_loss: 1.7890 - val_acc: 0.4545\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4417 - acc: 0.8391 - val_loss: 1.7954 - val_acc: 0.4545\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 232us/step - loss: 0.4421 - acc: 0.8391 - val_loss: 1.8015 - val_acc: 0.4545\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.4391 - acc: 0.8276 - val_loss: 1.8032 - val_acc: 0.4545\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.4406 - acc: 0.8276 - val_loss: 1.8101 - val_acc: 0.4545\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.4346 - acc: 0.8391 - val_loss: 1.8134 - val_acc: 0.4545\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.4349 - acc: 0.8276 - val_loss: 1.8193 - val_acc: 0.4318\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.4301 - acc: 0.8276 - val_loss: 1.8265 - val_acc: 0.4318\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.4314 - acc: 0.8276 - val_loss: 1.8272 - val_acc: 0.4318\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4269 - acc: 0.8276 - val_loss: 1.8318 - val_acc: 0.4318\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.4280 - acc: 0.8276 - val_loss: 1.8345 - val_acc: 0.4318\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.4251 - acc: 0.8391 - val_loss: 1.8417 - val_acc: 0.4318\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.4219 - acc: 0.8276 - val_loss: 1.8468 - val_acc: 0.4091\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.4187 - acc: 0.8506 - val_loss: 1.8524 - val_acc: 0.4091\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4203 - acc: 0.8276 - val_loss: 1.8572 - val_acc: 0.4091\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.4169 - acc: 0.8276 - val_loss: 1.8606 - val_acc: 0.4091\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.4152 - acc: 0.8276 - val_loss: 1.8673 - val_acc: 0.4091\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.4128 - acc: 0.8391 - val_loss: 1.8713 - val_acc: 0.4091\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.4112 - acc: 0.8506 - val_loss: 1.8731 - val_acc: 0.4091\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.4092 - acc: 0.8506 - val_loss: 1.8799 - val_acc: 0.4091\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.4050 - acc: 0.8506 - val_loss: 1.8861 - val_acc: 0.4318\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.4070 - acc: 0.8506 - val_loss: 1.8917 - val_acc: 0.4091\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.4018 - acc: 0.8621 - val_loss: 1.8976 - val_acc: 0.4091\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.4006 - acc: 0.8621 - val_loss: 1.9015 - val_acc: 0.4091\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.4007 - acc: 0.8391 - val_loss: 1.9086 - val_acc: 0.4091\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.3977 - acc: 0.8621 - val_loss: 1.9139 - val_acc: 0.4091\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3929 - acc: 0.8736 - val_loss: 1.9200 - val_acc: 0.4091\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.3915 - acc: 0.8736 - val_loss: 1.9242 - val_acc: 0.4091\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.3933 - acc: 0.8851 - val_loss: 1.9273 - val_acc: 0.4091\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.3918 - acc: 0.8621 - val_loss: 1.9345 - val_acc: 0.4091\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.3875 - acc: 0.8736 - val_loss: 1.9433 - val_acc: 0.4091\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.3843 - acc: 0.8851 - val_loss: 1.9450 - val_acc: 0.4091\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.3844 - acc: 0.8736 - val_loss: 1.9488 - val_acc: 0.4091\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.3817 - acc: 0.8851 - val_loss: 1.9566 - val_acc: 0.4091\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 260us/step - loss: 0.3796 - acc: 0.8851 - val_loss: 1.9627 - val_acc: 0.4091\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.3783 - acc: 0.8736 - val_loss: 1.9663 - val_acc: 0.4091\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.3756 - acc: 0.8966 - val_loss: 1.9742 - val_acc: 0.4091\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.3739 - acc: 0.8851 - val_loss: 1.9784 - val_acc: 0.4091\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.3703 - acc: 0.8851 - val_loss: 1.9831 - val_acc: 0.4091\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 351us/step - loss: 0.3719 - acc: 0.9080 - val_loss: 1.9862 - val_acc: 0.4091\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 363us/step - loss: 0.3687 - acc: 0.8966 - val_loss: 1.9909 - val_acc: 0.4318\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 292us/step - loss: 0.3652 - acc: 0.8851 - val_loss: 1.9966 - val_acc: 0.4091\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.3663 - acc: 0.8851 - val_loss: 2.0003 - val_acc: 0.4091\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 0.3628 - acc: 0.9080 - val_loss: 2.0061 - val_acc: 0.4091\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.3617 - acc: 0.8966 - val_loss: 2.0133 - val_acc: 0.4091\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.3604 - acc: 0.9080 - val_loss: 2.0188 - val_acc: 0.4091\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 242us/step - loss: 0.3604 - acc: 0.9195 - val_loss: 2.0222 - val_acc: 0.4091\n",
            "Train on 87 samples, validate on 44 samples\n",
            "Epoch 1/200\n",
            "87/87 [==============================] - 0s 6ms/step - loss: 1.5228 - acc: 0.4138 - val_loss: 1.1639 - val_acc: 0.4318\n",
            "Epoch 2/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 1.3265 - acc: 0.4138 - val_loss: 1.0803 - val_acc: 0.4318\n",
            "Epoch 3/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 1.2118 - acc: 0.3563 - val_loss: 1.0293 - val_acc: 0.4318\n",
            "Epoch 4/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.1403 - acc: 0.3908 - val_loss: 1.0045 - val_acc: 0.4545\n",
            "Epoch 5/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 1.0966 - acc: 0.4023 - val_loss: 0.9879 - val_acc: 0.5000\n",
            "Epoch 6/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 1.0656 - acc: 0.4483 - val_loss: 0.9780 - val_acc: 0.5227\n",
            "Epoch 7/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 1.0503 - acc: 0.4368 - val_loss: 0.9718 - val_acc: 0.5455\n",
            "Epoch 8/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 1.0302 - acc: 0.4598 - val_loss: 0.9677 - val_acc: 0.5000\n",
            "Epoch 9/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 1.0172 - acc: 0.4598 - val_loss: 0.9663 - val_acc: 0.5000\n",
            "Epoch 10/200\n",
            "87/87 [==============================] - 0s 226us/step - loss: 1.0018 - acc: 0.4828 - val_loss: 0.9659 - val_acc: 0.5227\n",
            "Epoch 11/200\n",
            "87/87 [==============================] - 0s 339us/step - loss: 0.9942 - acc: 0.4828 - val_loss: 0.9648 - val_acc: 0.4773\n",
            "Epoch 12/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.9835 - acc: 0.5172 - val_loss: 0.9652 - val_acc: 0.4773\n",
            "Epoch 13/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.9783 - acc: 0.5287 - val_loss: 0.9652 - val_acc: 0.4545\n",
            "Epoch 14/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.9707 - acc: 0.5172 - val_loss: 0.9664 - val_acc: 0.4773\n",
            "Epoch 15/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.9632 - acc: 0.5172 - val_loss: 0.9674 - val_acc: 0.4318\n",
            "Epoch 16/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9595 - acc: 0.5402 - val_loss: 0.9687 - val_acc: 0.4318\n",
            "Epoch 17/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9510 - acc: 0.5287 - val_loss: 0.9701 - val_acc: 0.4091\n",
            "Epoch 18/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9482 - acc: 0.5402 - val_loss: 0.9710 - val_acc: 0.4091\n",
            "Epoch 19/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.9432 - acc: 0.5172 - val_loss: 0.9737 - val_acc: 0.4091\n",
            "Epoch 20/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.9368 - acc: 0.5172 - val_loss: 0.9763 - val_acc: 0.4091\n",
            "Epoch 21/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.9331 - acc: 0.5287 - val_loss: 0.9768 - val_acc: 0.4091\n",
            "Epoch 22/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.9279 - acc: 0.5402 - val_loss: 0.9799 - val_acc: 0.4091\n",
            "Epoch 23/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.9257 - acc: 0.5287 - val_loss: 0.9808 - val_acc: 0.4318\n",
            "Epoch 24/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.9221 - acc: 0.5402 - val_loss: 0.9831 - val_acc: 0.4545\n",
            "Epoch 25/200\n",
            "87/87 [==============================] - 0s 276us/step - loss: 0.9189 - acc: 0.5402 - val_loss: 0.9841 - val_acc: 0.4545\n",
            "Epoch 26/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.9152 - acc: 0.5517 - val_loss: 0.9875 - val_acc: 0.4545\n",
            "Epoch 27/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.9109 - acc: 0.5517 - val_loss: 0.9892 - val_acc: 0.4545\n",
            "Epoch 28/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.9081 - acc: 0.5402 - val_loss: 0.9913 - val_acc: 0.4318\n",
            "Epoch 29/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.9053 - acc: 0.5517 - val_loss: 0.9939 - val_acc: 0.4318\n",
            "Epoch 30/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.9040 - acc: 0.5517 - val_loss: 0.9959 - val_acc: 0.4318\n",
            "Epoch 31/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.8995 - acc: 0.5402 - val_loss: 0.9986 - val_acc: 0.4318\n",
            "Epoch 32/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.8971 - acc: 0.5517 - val_loss: 1.0012 - val_acc: 0.4318\n",
            "Epoch 33/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8937 - acc: 0.5517 - val_loss: 1.0018 - val_acc: 0.4318\n",
            "Epoch 34/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8915 - acc: 0.5517 - val_loss: 1.0038 - val_acc: 0.4318\n",
            "Epoch 35/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8887 - acc: 0.5747 - val_loss: 1.0073 - val_acc: 0.4318\n",
            "Epoch 36/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8868 - acc: 0.5517 - val_loss: 1.0078 - val_acc: 0.4318\n",
            "Epoch 37/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8844 - acc: 0.5747 - val_loss: 1.0125 - val_acc: 0.4318\n",
            "Epoch 38/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.8835 - acc: 0.5632 - val_loss: 1.0129 - val_acc: 0.4318\n",
            "Epoch 39/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.8793 - acc: 0.5747 - val_loss: 1.0156 - val_acc: 0.4318\n",
            "Epoch 40/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8783 - acc: 0.5632 - val_loss: 1.0170 - val_acc: 0.4318\n",
            "Epoch 41/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8748 - acc: 0.5862 - val_loss: 1.0192 - val_acc: 0.4318\n",
            "Epoch 42/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.8719 - acc: 0.5862 - val_loss: 1.0212 - val_acc: 0.4545\n",
            "Epoch 43/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8685 - acc: 0.5747 - val_loss: 1.0240 - val_acc: 0.4545\n",
            "Epoch 44/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.8671 - acc: 0.5747 - val_loss: 1.0254 - val_acc: 0.4545\n",
            "Epoch 45/200\n",
            "87/87 [==============================] - 0s 282us/step - loss: 0.8646 - acc: 0.5862 - val_loss: 1.0279 - val_acc: 0.4545\n",
            "Epoch 46/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.8627 - acc: 0.5977 - val_loss: 1.0289 - val_acc: 0.4545\n",
            "Epoch 47/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.8600 - acc: 0.5747 - val_loss: 1.0311 - val_acc: 0.4545\n",
            "Epoch 48/200\n",
            "87/87 [==============================] - 0s 279us/step - loss: 0.8577 - acc: 0.5747 - val_loss: 1.0326 - val_acc: 0.4545\n",
            "Epoch 49/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.8560 - acc: 0.5862 - val_loss: 1.0351 - val_acc: 0.4545\n",
            "Epoch 50/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.8546 - acc: 0.5862 - val_loss: 1.0376 - val_acc: 0.4545\n",
            "Epoch 51/200\n",
            "87/87 [==============================] - 0s 287us/step - loss: 0.8538 - acc: 0.5862 - val_loss: 1.0382 - val_acc: 0.4545\n",
            "Epoch 52/200\n",
            "87/87 [==============================] - 0s 212us/step - loss: 0.8498 - acc: 0.5862 - val_loss: 1.0431 - val_acc: 0.4545\n",
            "Epoch 53/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.8469 - acc: 0.5977 - val_loss: 1.0439 - val_acc: 0.4545\n",
            "Epoch 54/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8442 - acc: 0.6092 - val_loss: 1.0455 - val_acc: 0.4545\n",
            "Epoch 55/200\n",
            "87/87 [==============================] - 0s 252us/step - loss: 0.8423 - acc: 0.6092 - val_loss: 1.0492 - val_acc: 0.4545\n",
            "Epoch 56/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8413 - acc: 0.5977 - val_loss: 1.0495 - val_acc: 0.4545\n",
            "Epoch 57/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.8380 - acc: 0.6092 - val_loss: 1.0526 - val_acc: 0.4545\n",
            "Epoch 58/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.8359 - acc: 0.5977 - val_loss: 1.0534 - val_acc: 0.4545\n",
            "Epoch 59/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.8342 - acc: 0.6092 - val_loss: 1.0571 - val_acc: 0.4545\n",
            "Epoch 60/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.8305 - acc: 0.6092 - val_loss: 1.0587 - val_acc: 0.4545\n",
            "Epoch 61/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.8295 - acc: 0.6207 - val_loss: 1.0609 - val_acc: 0.4545\n",
            "Epoch 62/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.8261 - acc: 0.6207 - val_loss: 1.0636 - val_acc: 0.4545\n",
            "Epoch 63/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.8259 - acc: 0.6207 - val_loss: 1.0655 - val_acc: 0.4545\n",
            "Epoch 64/200\n",
            "87/87 [==============================] - 0s 268us/step - loss: 0.8221 - acc: 0.6207 - val_loss: 1.0692 - val_acc: 0.4545\n",
            "Epoch 65/200\n",
            "87/87 [==============================] - 0s 281us/step - loss: 0.8214 - acc: 0.6207 - val_loss: 1.0714 - val_acc: 0.4545\n",
            "Epoch 66/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.8205 - acc: 0.6207 - val_loss: 1.0731 - val_acc: 0.4545\n",
            "Epoch 67/200\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.8180 - acc: 0.6207 - val_loss: 1.0753 - val_acc: 0.4773\n",
            "Epoch 68/200\n",
            "87/87 [==============================] - 0s 335us/step - loss: 0.8152 - acc: 0.5977 - val_loss: 1.0788 - val_acc: 0.4773\n",
            "Epoch 69/200\n",
            "87/87 [==============================] - 0s 306us/step - loss: 0.8140 - acc: 0.6322 - val_loss: 1.0789 - val_acc: 0.4773\n",
            "Epoch 70/200\n",
            "87/87 [==============================] - 0s 271us/step - loss: 0.8113 - acc: 0.6322 - val_loss: 1.0810 - val_acc: 0.4773\n",
            "Epoch 71/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.8096 - acc: 0.6322 - val_loss: 1.0848 - val_acc: 0.4773\n",
            "Epoch 72/200\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.8067 - acc: 0.6322 - val_loss: 1.0867 - val_acc: 0.4773\n",
            "Epoch 73/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.8044 - acc: 0.6322 - val_loss: 1.0892 - val_acc: 0.4773\n",
            "Epoch 74/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.8042 - acc: 0.6322 - val_loss: 1.0914 - val_acc: 0.4318\n",
            "Epoch 75/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.8034 - acc: 0.6437 - val_loss: 1.0935 - val_acc: 0.4318\n",
            "Epoch 76/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.8005 - acc: 0.6322 - val_loss: 1.0972 - val_acc: 0.4318\n",
            "Epoch 77/200\n",
            "87/87 [==============================] - 0s 233us/step - loss: 0.7980 - acc: 0.6437 - val_loss: 1.0988 - val_acc: 0.4318\n",
            "Epoch 78/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7950 - acc: 0.6437 - val_loss: 1.1013 - val_acc: 0.4318\n",
            "Epoch 79/200\n",
            "87/87 [==============================] - 0s 240us/step - loss: 0.7941 - acc: 0.6322 - val_loss: 1.1053 - val_acc: 0.4318\n",
            "Epoch 80/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7910 - acc: 0.6437 - val_loss: 1.1059 - val_acc: 0.4091\n",
            "Epoch 81/200\n",
            "87/87 [==============================] - 0s 253us/step - loss: 0.7907 - acc: 0.6437 - val_loss: 1.1090 - val_acc: 0.4091\n",
            "Epoch 82/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7879 - acc: 0.6437 - val_loss: 1.1118 - val_acc: 0.4091\n",
            "Epoch 83/200\n",
            "87/87 [==============================] - 0s 257us/step - loss: 0.7871 - acc: 0.6437 - val_loss: 1.1151 - val_acc: 0.4091\n",
            "Epoch 84/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7830 - acc: 0.6322 - val_loss: 1.1180 - val_acc: 0.4091\n",
            "Epoch 85/200\n",
            "87/87 [==============================] - 0s 277us/step - loss: 0.7826 - acc: 0.6437 - val_loss: 1.1217 - val_acc: 0.3864\n",
            "Epoch 86/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7793 - acc: 0.6437 - val_loss: 1.1208 - val_acc: 0.3864\n",
            "Epoch 87/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.7784 - acc: 0.6437 - val_loss: 1.1254 - val_acc: 0.3864\n",
            "Epoch 88/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7755 - acc: 0.6437 - val_loss: 1.1266 - val_acc: 0.3864\n",
            "Epoch 89/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.7742 - acc: 0.6552 - val_loss: 1.1282 - val_acc: 0.3864\n",
            "Epoch 90/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7720 - acc: 0.6552 - val_loss: 1.1304 - val_acc: 0.3864\n",
            "Epoch 91/200\n",
            "87/87 [==============================] - 0s 256us/step - loss: 0.7705 - acc: 0.6552 - val_loss: 1.1344 - val_acc: 0.3864\n",
            "Epoch 92/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.7692 - acc: 0.6667 - val_loss: 1.1378 - val_acc: 0.3864\n",
            "Epoch 93/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7657 - acc: 0.6667 - val_loss: 1.1392 - val_acc: 0.3864\n",
            "Epoch 94/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.7632 - acc: 0.6667 - val_loss: 1.1410 - val_acc: 0.3864\n",
            "Epoch 95/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.7632 - acc: 0.6667 - val_loss: 1.1428 - val_acc: 0.3864\n",
            "Epoch 96/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7595 - acc: 0.6667 - val_loss: 1.1481 - val_acc: 0.3864\n",
            "Epoch 97/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.7568 - acc: 0.6667 - val_loss: 1.1492 - val_acc: 0.3864\n",
            "Epoch 98/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7548 - acc: 0.6667 - val_loss: 1.1533 - val_acc: 0.3636\n",
            "Epoch 99/200\n",
            "87/87 [==============================] - 0s 266us/step - loss: 0.7544 - acc: 0.6667 - val_loss: 1.1543 - val_acc: 0.3636\n",
            "Epoch 100/200\n",
            "87/87 [==============================] - 0s 223us/step - loss: 0.7504 - acc: 0.6782 - val_loss: 1.1578 - val_acc: 0.3636\n",
            "Epoch 101/200\n",
            "87/87 [==============================] - 0s 331us/step - loss: 0.7498 - acc: 0.6782 - val_loss: 1.1636 - val_acc: 0.3864\n",
            "Epoch 102/200\n",
            "87/87 [==============================] - 0s 297us/step - loss: 0.7475 - acc: 0.6782 - val_loss: 1.1638 - val_acc: 0.3864\n",
            "Epoch 103/200\n",
            "87/87 [==============================] - 0s 272us/step - loss: 0.7464 - acc: 0.6782 - val_loss: 1.1685 - val_acc: 0.3864\n",
            "Epoch 104/200\n",
            "87/87 [==============================] - 0s 314us/step - loss: 0.7436 - acc: 0.6782 - val_loss: 1.1706 - val_acc: 0.3864\n",
            "Epoch 105/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.7422 - acc: 0.6667 - val_loss: 1.1704 - val_acc: 0.3636\n",
            "Epoch 106/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.7372 - acc: 0.6782 - val_loss: 1.1747 - val_acc: 0.3864\n",
            "Epoch 107/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.7360 - acc: 0.6782 - val_loss: 1.1771 - val_acc: 0.3864\n",
            "Epoch 108/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.7337 - acc: 0.6782 - val_loss: 1.1839 - val_acc: 0.3864\n",
            "Epoch 109/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7333 - acc: 0.6782 - val_loss: 1.1839 - val_acc: 0.3864\n",
            "Epoch 110/200\n",
            "87/87 [==============================] - 0s 310us/step - loss: 0.7324 - acc: 0.6667 - val_loss: 1.1867 - val_acc: 0.3864\n",
            "Epoch 111/200\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.7285 - acc: 0.6667 - val_loss: 1.1934 - val_acc: 0.3864\n",
            "Epoch 112/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.7256 - acc: 0.6667 - val_loss: 1.1982 - val_acc: 0.4091\n",
            "Epoch 113/200\n",
            "87/87 [==============================] - 0s 307us/step - loss: 0.7247 - acc: 0.6667 - val_loss: 1.2019 - val_acc: 0.3864\n",
            "Epoch 114/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.7216 - acc: 0.6552 - val_loss: 1.2065 - val_acc: 0.3864\n",
            "Epoch 115/200\n",
            "87/87 [==============================] - 0s 221us/step - loss: 0.7190 - acc: 0.6667 - val_loss: 1.2103 - val_acc: 0.3864\n",
            "Epoch 116/200\n",
            "87/87 [==============================] - 0s 289us/step - loss: 0.7198 - acc: 0.6552 - val_loss: 1.2129 - val_acc: 0.3864\n",
            "Epoch 117/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7149 - acc: 0.6552 - val_loss: 1.2186 - val_acc: 0.3864\n",
            "Epoch 118/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.7154 - acc: 0.6552 - val_loss: 1.2237 - val_acc: 0.3864\n",
            "Epoch 119/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.7126 - acc: 0.6552 - val_loss: 1.2254 - val_acc: 0.3864\n",
            "Epoch 120/200\n",
            "87/87 [==============================] - 0s 209us/step - loss: 0.7107 - acc: 0.6552 - val_loss: 1.2297 - val_acc: 0.4091\n",
            "Epoch 121/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.7089 - acc: 0.6667 - val_loss: 1.2317 - val_acc: 0.3636\n",
            "Epoch 122/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.7061 - acc: 0.6667 - val_loss: 1.2347 - val_acc: 0.3864\n",
            "Epoch 123/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.7040 - acc: 0.6667 - val_loss: 1.2362 - val_acc: 0.3636\n",
            "Epoch 124/200\n",
            "87/87 [==============================] - 0s 255us/step - loss: 0.7022 - acc: 0.6667 - val_loss: 1.2395 - val_acc: 0.3636\n",
            "Epoch 125/200\n",
            "87/87 [==============================] - 0s 250us/step - loss: 0.7014 - acc: 0.6667 - val_loss: 1.2418 - val_acc: 0.3636\n",
            "Epoch 126/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6984 - acc: 0.6782 - val_loss: 1.2445 - val_acc: 0.3864\n",
            "Epoch 127/200\n",
            "87/87 [==============================] - 0s 290us/step - loss: 0.6950 - acc: 0.6782 - val_loss: 1.2462 - val_acc: 0.4091\n",
            "Epoch 128/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6929 - acc: 0.6782 - val_loss: 1.2520 - val_acc: 0.3864\n",
            "Epoch 129/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.6934 - acc: 0.6782 - val_loss: 1.2545 - val_acc: 0.3864\n",
            "Epoch 130/200\n",
            "87/87 [==============================] - 0s 244us/step - loss: 0.6903 - acc: 0.6782 - val_loss: 1.2615 - val_acc: 0.4091\n",
            "Epoch 131/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6880 - acc: 0.6782 - val_loss: 1.2642 - val_acc: 0.3864\n",
            "Epoch 132/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6861 - acc: 0.6782 - val_loss: 1.2633 - val_acc: 0.4091\n",
            "Epoch 133/200\n",
            "87/87 [==============================] - 0s 230us/step - loss: 0.6837 - acc: 0.6782 - val_loss: 1.2664 - val_acc: 0.4091\n",
            "Epoch 134/200\n",
            "87/87 [==============================] - 0s 254us/step - loss: 0.6830 - acc: 0.6782 - val_loss: 1.2725 - val_acc: 0.4318\n",
            "Epoch 135/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6804 - acc: 0.6782 - val_loss: 1.2729 - val_acc: 0.4318\n",
            "Epoch 136/200\n",
            "87/87 [==============================] - 0s 216us/step - loss: 0.6784 - acc: 0.6782 - val_loss: 1.2769 - val_acc: 0.4318\n",
            "Epoch 137/200\n",
            "87/87 [==============================] - 0s 335us/step - loss: 0.6760 - acc: 0.6782 - val_loss: 1.2804 - val_acc: 0.3864\n",
            "Epoch 138/200\n",
            "87/87 [==============================] - 0s 262us/step - loss: 0.6745 - acc: 0.6782 - val_loss: 1.2844 - val_acc: 0.4318\n",
            "Epoch 139/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6728 - acc: 0.6782 - val_loss: 1.2886 - val_acc: 0.4318\n",
            "Epoch 140/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.6726 - acc: 0.6897 - val_loss: 1.2881 - val_acc: 0.4318\n",
            "Epoch 141/200\n",
            "87/87 [==============================] - 0s 238us/step - loss: 0.6690 - acc: 0.6897 - val_loss: 1.2912 - val_acc: 0.4318\n",
            "Epoch 142/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6670 - acc: 0.6897 - val_loss: 1.2942 - val_acc: 0.4318\n",
            "Epoch 143/200\n",
            "87/87 [==============================] - 0s 213us/step - loss: 0.6644 - acc: 0.6897 - val_loss: 1.2955 - val_acc: 0.4318\n",
            "Epoch 144/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6623 - acc: 0.6897 - val_loss: 1.2957 - val_acc: 0.4318\n",
            "Epoch 145/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6609 - acc: 0.6897 - val_loss: 1.3028 - val_acc: 0.4318\n",
            "Epoch 146/200\n",
            "87/87 [==============================] - 0s 285us/step - loss: 0.6599 - acc: 0.6897 - val_loss: 1.3034 - val_acc: 0.4318\n",
            "Epoch 147/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6574 - acc: 0.6897 - val_loss: 1.3068 - val_acc: 0.4318\n",
            "Epoch 148/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.6562 - acc: 0.6897 - val_loss: 1.3105 - val_acc: 0.4318\n",
            "Epoch 149/200\n",
            "87/87 [==============================] - 0s 229us/step - loss: 0.6536 - acc: 0.6897 - val_loss: 1.3136 - val_acc: 0.4318\n",
            "Epoch 150/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6521 - acc: 0.6897 - val_loss: 1.3147 - val_acc: 0.4318\n",
            "Epoch 151/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6496 - acc: 0.6897 - val_loss: 1.3175 - val_acc: 0.4318\n",
            "Epoch 152/200\n",
            "87/87 [==============================] - 0s 235us/step - loss: 0.6463 - acc: 0.6897 - val_loss: 1.3160 - val_acc: 0.4545\n",
            "Epoch 153/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6460 - acc: 0.6897 - val_loss: 1.3205 - val_acc: 0.4545\n",
            "Epoch 154/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6434 - acc: 0.6897 - val_loss: 1.3220 - val_acc: 0.4545\n",
            "Epoch 155/200\n",
            "87/87 [==============================] - 0s 231us/step - loss: 0.6414 - acc: 0.7011 - val_loss: 1.3222 - val_acc: 0.4545\n",
            "Epoch 156/200\n",
            "87/87 [==============================] - 0s 249us/step - loss: 0.6392 - acc: 0.6897 - val_loss: 1.3225 - val_acc: 0.4545\n",
            "Epoch 157/200\n",
            "87/87 [==============================] - 0s 246us/step - loss: 0.6384 - acc: 0.6897 - val_loss: 1.3308 - val_acc: 0.4545\n",
            "Epoch 158/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6345 - acc: 0.6897 - val_loss: 1.3329 - val_acc: 0.4545\n",
            "Epoch 159/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6328 - acc: 0.6897 - val_loss: 1.3320 - val_acc: 0.4545\n",
            "Epoch 160/200\n",
            "87/87 [==============================] - 0s 259us/step - loss: 0.6317 - acc: 0.7011 - val_loss: 1.3398 - val_acc: 0.4773\n",
            "Epoch 161/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.6288 - acc: 0.7011 - val_loss: 1.3392 - val_acc: 0.4545\n",
            "Epoch 162/200\n",
            "87/87 [==============================] - 0s 224us/step - loss: 0.6273 - acc: 0.7011 - val_loss: 1.3436 - val_acc: 0.4773\n",
            "Epoch 163/200\n",
            "87/87 [==============================] - 0s 241us/step - loss: 0.6263 - acc: 0.7011 - val_loss: 1.3463 - val_acc: 0.4773\n",
            "Epoch 164/200\n",
            "87/87 [==============================] - 0s 218us/step - loss: 0.6238 - acc: 0.7126 - val_loss: 1.3512 - val_acc: 0.4773\n",
            "Epoch 165/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.6231 - acc: 0.7126 - val_loss: 1.3534 - val_acc: 0.4773\n",
            "Epoch 166/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6196 - acc: 0.7126 - val_loss: 1.3576 - val_acc: 0.4773\n",
            "Epoch 167/200\n",
            "87/87 [==============================] - 0s 274us/step - loss: 0.6192 - acc: 0.7011 - val_loss: 1.3601 - val_acc: 0.4773\n",
            "Epoch 168/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.6172 - acc: 0.7011 - val_loss: 1.3637 - val_acc: 0.4773\n",
            "Epoch 169/200\n",
            "87/87 [==============================] - 0s 284us/step - loss: 0.6134 - acc: 0.7126 - val_loss: 1.3615 - val_acc: 0.4773\n",
            "Epoch 170/200\n",
            "87/87 [==============================] - 0s 247us/step - loss: 0.6112 - acc: 0.7126 - val_loss: 1.3613 - val_acc: 0.4773\n",
            "Epoch 171/200\n",
            "87/87 [==============================] - 0s 280us/step - loss: 0.6090 - acc: 0.7126 - val_loss: 1.3633 - val_acc: 0.4773\n",
            "Epoch 172/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6080 - acc: 0.7126 - val_loss: 1.3635 - val_acc: 0.4773\n",
            "Epoch 173/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.6057 - acc: 0.7356 - val_loss: 1.3683 - val_acc: 0.4773\n",
            "Epoch 174/200\n",
            "87/87 [==============================] - 0s 243us/step - loss: 0.6046 - acc: 0.7241 - val_loss: 1.3769 - val_acc: 0.4773\n",
            "Epoch 175/200\n",
            "87/87 [==============================] - 0s 263us/step - loss: 0.6008 - acc: 0.7241 - val_loss: 1.3813 - val_acc: 0.4773\n",
            "Epoch 176/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.6005 - acc: 0.7126 - val_loss: 1.3809 - val_acc: 0.4773\n",
            "Epoch 177/200\n",
            "87/87 [==============================] - 0s 220us/step - loss: 0.5998 - acc: 0.7241 - val_loss: 1.3883 - val_acc: 0.4773\n",
            "Epoch 178/200\n",
            "87/87 [==============================] - 0s 264us/step - loss: 0.5957 - acc: 0.7356 - val_loss: 1.3887 - val_acc: 0.4773\n",
            "Epoch 179/200\n",
            "87/87 [==============================] - 0s 261us/step - loss: 0.5963 - acc: 0.7356 - val_loss: 1.3994 - val_acc: 0.4773\n",
            "Epoch 180/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5911 - acc: 0.7471 - val_loss: 1.3952 - val_acc: 0.4773\n",
            "Epoch 181/200\n",
            "87/87 [==============================] - 0s 227us/step - loss: 0.5911 - acc: 0.7356 - val_loss: 1.3997 - val_acc: 0.4773\n",
            "Epoch 182/200\n",
            "87/87 [==============================] - 0s 215us/step - loss: 0.5902 - acc: 0.7356 - val_loss: 1.4039 - val_acc: 0.4773\n",
            "Epoch 183/200\n",
            "87/87 [==============================] - 0s 214us/step - loss: 0.5866 - acc: 0.7356 - val_loss: 1.4070 - val_acc: 0.5000\n",
            "Epoch 184/200\n",
            "87/87 [==============================] - 0s 237us/step - loss: 0.5872 - acc: 0.7471 - val_loss: 1.4027 - val_acc: 0.5000\n",
            "Epoch 185/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.5830 - acc: 0.7471 - val_loss: 1.4096 - val_acc: 0.5000\n",
            "Epoch 186/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5819 - acc: 0.7471 - val_loss: 1.4158 - val_acc: 0.5000\n",
            "Epoch 187/200\n",
            "87/87 [==============================] - 0s 222us/step - loss: 0.5801 - acc: 0.7356 - val_loss: 1.4200 - val_acc: 0.5000\n",
            "Epoch 188/200\n",
            "87/87 [==============================] - 0s 245us/step - loss: 0.5779 - acc: 0.7471 - val_loss: 1.4229 - val_acc: 0.5000\n",
            "Epoch 189/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5768 - acc: 0.7471 - val_loss: 1.4266 - val_acc: 0.5000\n",
            "Epoch 190/200\n",
            "87/87 [==============================] - 0s 251us/step - loss: 0.5742 - acc: 0.7471 - val_loss: 1.4330 - val_acc: 0.5000\n",
            "Epoch 191/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5722 - acc: 0.7471 - val_loss: 1.4312 - val_acc: 0.5000\n",
            "Epoch 192/200\n",
            "87/87 [==============================] - 0s 225us/step - loss: 0.5701 - acc: 0.7471 - val_loss: 1.4382 - val_acc: 0.5000\n",
            "Epoch 193/200\n",
            "87/87 [==============================] - 0s 219us/step - loss: 0.5689 - acc: 0.7471 - val_loss: 1.4430 - val_acc: 0.5000\n",
            "Epoch 194/200\n",
            "87/87 [==============================] - 0s 236us/step - loss: 0.5677 - acc: 0.7471 - val_loss: 1.4486 - val_acc: 0.5000\n",
            "Epoch 195/200\n",
            "87/87 [==============================] - 0s 239us/step - loss: 0.5677 - acc: 0.7471 - val_loss: 1.4504 - val_acc: 0.5000\n",
            "Epoch 196/200\n",
            "87/87 [==============================] - 0s 258us/step - loss: 0.5631 - acc: 0.7471 - val_loss: 1.4505 - val_acc: 0.5000\n",
            "Epoch 197/200\n",
            "87/87 [==============================] - 0s 228us/step - loss: 0.5632 - acc: 0.7471 - val_loss: 1.4563 - val_acc: 0.5000\n",
            "Epoch 198/200\n",
            "87/87 [==============================] - 0s 265us/step - loss: 0.5598 - acc: 0.7586 - val_loss: 1.4544 - val_acc: 0.5000\n",
            "Epoch 199/200\n",
            "87/87 [==============================] - 0s 234us/step - loss: 0.5589 - acc: 0.7586 - val_loss: 1.4595 - val_acc: 0.5000\n",
            "Epoch 200/200\n",
            "87/87 [==============================] - 0s 217us/step - loss: 0.5601 - acc: 0.7471 - val_loss: 1.4646 - val_acc: 0.5000\n",
            "Train on 88 samples, validate on 43 samples\n",
            "Epoch 1/200\n",
            "88/88 [==============================] - 1s 7ms/step - loss: 1.2554 - acc: 0.3636 - val_loss: 1.0964 - val_acc: 0.4186\n",
            "Epoch 2/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 1.1828 - acc: 0.3409 - val_loss: 1.0426 - val_acc: 0.4186\n",
            "Epoch 3/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.1204 - acc: 0.3750 - val_loss: 1.0148 - val_acc: 0.4419\n",
            "Epoch 4/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 1.0827 - acc: 0.3750 - val_loss: 0.9974 - val_acc: 0.4651\n",
            "Epoch 5/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 1.0487 - acc: 0.4091 - val_loss: 0.9894 - val_acc: 0.4651\n",
            "Epoch 6/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 1.0203 - acc: 0.4432 - val_loss: 0.9847 - val_acc: 0.4651\n",
            "Epoch 7/200\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.9983 - acc: 0.4545 - val_loss: 0.9827 - val_acc: 0.4651\n",
            "Epoch 8/200\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.9792 - acc: 0.4659 - val_loss: 0.9812 - val_acc: 0.4419\n",
            "Epoch 9/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.9631 - acc: 0.5114 - val_loss: 0.9834 - val_acc: 0.4419\n",
            "Epoch 10/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.9486 - acc: 0.5455 - val_loss: 0.9835 - val_acc: 0.4651\n",
            "Epoch 11/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.9330 - acc: 0.5341 - val_loss: 0.9880 - val_acc: 0.4651\n",
            "Epoch 12/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.9218 - acc: 0.5568 - val_loss: 0.9895 - val_acc: 0.4884\n",
            "Epoch 13/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.9112 - acc: 0.5568 - val_loss: 0.9933 - val_acc: 0.4884\n",
            "Epoch 14/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.8981 - acc: 0.5568 - val_loss: 0.9987 - val_acc: 0.5116\n",
            "Epoch 15/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.8886 - acc: 0.5455 - val_loss: 1.0026 - val_acc: 0.5349\n",
            "Epoch 16/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.8819 - acc: 0.5568 - val_loss: 1.0068 - val_acc: 0.5349\n",
            "Epoch 17/200\n",
            "88/88 [==============================] - 0s 295us/step - loss: 0.8696 - acc: 0.5795 - val_loss: 1.0105 - val_acc: 0.5814\n",
            "Epoch 18/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.8633 - acc: 0.6023 - val_loss: 1.0133 - val_acc: 0.5814\n",
            "Epoch 19/200\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.8554 - acc: 0.6023 - val_loss: 1.0156 - val_acc: 0.5814\n",
            "Epoch 20/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.8473 - acc: 0.6136 - val_loss: 1.0206 - val_acc: 0.5814\n",
            "Epoch 21/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.8416 - acc: 0.6136 - val_loss: 1.0255 - val_acc: 0.5814\n",
            "Epoch 22/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.8347 - acc: 0.6136 - val_loss: 1.0288 - val_acc: 0.5814\n",
            "Epoch 23/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.8272 - acc: 0.6250 - val_loss: 1.0323 - val_acc: 0.6047\n",
            "Epoch 24/200\n",
            "88/88 [==============================] - 0s 266us/step - loss: 0.8211 - acc: 0.6250 - val_loss: 1.0375 - val_acc: 0.5814\n",
            "Epoch 25/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.8150 - acc: 0.6250 - val_loss: 1.0389 - val_acc: 0.5814\n",
            "Epoch 26/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.8100 - acc: 0.6364 - val_loss: 1.0423 - val_acc: 0.5814\n",
            "Epoch 27/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.8028 - acc: 0.6364 - val_loss: 1.0475 - val_acc: 0.6047\n",
            "Epoch 28/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.7993 - acc: 0.6364 - val_loss: 1.0514 - val_acc: 0.6047\n",
            "Epoch 29/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.7937 - acc: 0.6591 - val_loss: 1.0547 - val_acc: 0.5814\n",
            "Epoch 30/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.7878 - acc: 0.6591 - val_loss: 1.0584 - val_acc: 0.5814\n",
            "Epoch 31/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7810 - acc: 0.6818 - val_loss: 1.0617 - val_acc: 0.5814\n",
            "Epoch 32/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.7780 - acc: 0.6705 - val_loss: 1.0657 - val_acc: 0.6047\n",
            "Epoch 33/200\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.7718 - acc: 0.6818 - val_loss: 1.0687 - val_acc: 0.5814\n",
            "Epoch 34/200\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.7658 - acc: 0.6818 - val_loss: 1.0733 - val_acc: 0.5814\n",
            "Epoch 35/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.7614 - acc: 0.6818 - val_loss: 1.0769 - val_acc: 0.5814\n",
            "Epoch 36/200\n",
            "88/88 [==============================] - 0s 250us/step - loss: 0.7560 - acc: 0.6932 - val_loss: 1.0792 - val_acc: 0.5581\n",
            "Epoch 37/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7519 - acc: 0.6932 - val_loss: 1.0835 - val_acc: 0.5814\n",
            "Epoch 38/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7472 - acc: 0.6932 - val_loss: 1.0876 - val_acc: 0.5814\n",
            "Epoch 39/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7428 - acc: 0.7045 - val_loss: 1.0885 - val_acc: 0.5814\n",
            "Epoch 40/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.7412 - acc: 0.7045 - val_loss: 1.0943 - val_acc: 0.5814\n",
            "Epoch 41/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.7329 - acc: 0.7273 - val_loss: 1.0952 - val_acc: 0.5814\n",
            "Epoch 42/200\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.7297 - acc: 0.7159 - val_loss: 1.0960 - val_acc: 0.5814\n",
            "Epoch 43/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.7244 - acc: 0.7159 - val_loss: 1.1015 - val_acc: 0.5814\n",
            "Epoch 44/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.7195 - acc: 0.7159 - val_loss: 1.1031 - val_acc: 0.5814\n",
            "Epoch 45/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.7164 - acc: 0.7159 - val_loss: 1.1066 - val_acc: 0.5814\n",
            "Epoch 46/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7131 - acc: 0.7159 - val_loss: 1.1079 - val_acc: 0.5814\n",
            "Epoch 47/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.7075 - acc: 0.7159 - val_loss: 1.1142 - val_acc: 0.5581\n",
            "Epoch 48/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.7029 - acc: 0.7273 - val_loss: 1.1164 - val_acc: 0.5814\n",
            "Epoch 49/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.6982 - acc: 0.7159 - val_loss: 1.1179 - val_acc: 0.5581\n",
            "Epoch 50/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6963 - acc: 0.7159 - val_loss: 1.1236 - val_acc: 0.5581\n",
            "Epoch 51/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6913 - acc: 0.7159 - val_loss: 1.1239 - val_acc: 0.5814\n",
            "Epoch 52/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.6857 - acc: 0.7159 - val_loss: 1.1278 - val_acc: 0.5581\n",
            "Epoch 53/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.6818 - acc: 0.7273 - val_loss: 1.1345 - val_acc: 0.5581\n",
            "Epoch 54/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6790 - acc: 0.7159 - val_loss: 1.1380 - val_acc: 0.5814\n",
            "Epoch 55/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6747 - acc: 0.7159 - val_loss: 1.1403 - val_acc: 0.5814\n",
            "Epoch 56/200\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.6712 - acc: 0.7159 - val_loss: 1.1425 - val_acc: 0.5581\n",
            "Epoch 57/200\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.6674 - acc: 0.7159 - val_loss: 1.1478 - val_acc: 0.5581\n",
            "Epoch 58/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.6643 - acc: 0.7159 - val_loss: 1.1497 - val_acc: 0.5581\n",
            "Epoch 59/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.6600 - acc: 0.7045 - val_loss: 1.1528 - val_acc: 0.5581\n",
            "Epoch 60/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.6568 - acc: 0.7159 - val_loss: 1.1553 - val_acc: 0.5581\n",
            "Epoch 61/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6547 - acc: 0.7386 - val_loss: 1.1633 - val_acc: 0.5581\n",
            "Epoch 62/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.6494 - acc: 0.7273 - val_loss: 1.1626 - val_acc: 0.5581\n",
            "Epoch 63/200\n",
            "88/88 [==============================] - 0s 235us/step - loss: 0.6447 - acc: 0.7386 - val_loss: 1.1693 - val_acc: 0.5814\n",
            "Epoch 64/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.6403 - acc: 0.7273 - val_loss: 1.1703 - val_acc: 0.5581\n",
            "Epoch 65/200\n",
            "88/88 [==============================] - 0s 212us/step - loss: 0.6358 - acc: 0.7614 - val_loss: 1.1748 - val_acc: 0.5581\n",
            "Epoch 66/200\n",
            "88/88 [==============================] - 0s 272us/step - loss: 0.6330 - acc: 0.7500 - val_loss: 1.1765 - val_acc: 0.5581\n",
            "Epoch 67/200\n",
            "88/88 [==============================] - 0s 300us/step - loss: 0.6285 - acc: 0.7273 - val_loss: 1.1795 - val_acc: 0.5581\n",
            "Epoch 68/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.6242 - acc: 0.7500 - val_loss: 1.1858 - val_acc: 0.5581\n",
            "Epoch 69/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.6206 - acc: 0.7386 - val_loss: 1.1888 - val_acc: 0.5581\n",
            "Epoch 70/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.6166 - acc: 0.7614 - val_loss: 1.1918 - val_acc: 0.5581\n",
            "Epoch 71/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.6132 - acc: 0.7273 - val_loss: 1.1965 - val_acc: 0.5581\n",
            "Epoch 72/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.6079 - acc: 0.7614 - val_loss: 1.1978 - val_acc: 0.5581\n",
            "Epoch 73/200\n",
            "88/88 [==============================] - 0s 251us/step - loss: 0.6063 - acc: 0.7614 - val_loss: 1.2027 - val_acc: 0.5581\n",
            "Epoch 74/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.6003 - acc: 0.7727 - val_loss: 1.2055 - val_acc: 0.5814\n",
            "Epoch 75/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.5966 - acc: 0.7614 - val_loss: 1.2108 - val_acc: 0.5814\n",
            "Epoch 76/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.5930 - acc: 0.7727 - val_loss: 1.2138 - val_acc: 0.5814\n",
            "Epoch 77/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5903 - acc: 0.7614 - val_loss: 1.2170 - val_acc: 0.5581\n",
            "Epoch 78/200\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.5843 - acc: 0.7727 - val_loss: 1.2186 - val_acc: 0.5814\n",
            "Epoch 79/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.5819 - acc: 0.7727 - val_loss: 1.2236 - val_acc: 0.5814\n",
            "Epoch 80/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5788 - acc: 0.7614 - val_loss: 1.2246 - val_acc: 0.5814\n",
            "Epoch 81/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.5735 - acc: 0.7841 - val_loss: 1.2281 - val_acc: 0.6047\n",
            "Epoch 82/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.5708 - acc: 0.7727 - val_loss: 1.2309 - val_acc: 0.5814\n",
            "Epoch 83/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.5670 - acc: 0.7841 - val_loss: 1.2340 - val_acc: 0.5814\n",
            "Epoch 84/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.5639 - acc: 0.7727 - val_loss: 1.2352 - val_acc: 0.5814\n",
            "Epoch 85/200\n",
            "88/88 [==============================] - 0s 249us/step - loss: 0.5598 - acc: 0.7955 - val_loss: 1.2420 - val_acc: 0.5814\n",
            "Epoch 86/200\n",
            "88/88 [==============================] - 0s 340us/step - loss: 0.5550 - acc: 0.7841 - val_loss: 1.2422 - val_acc: 0.5814\n",
            "Epoch 87/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.5521 - acc: 0.7727 - val_loss: 1.2446 - val_acc: 0.5814\n",
            "Epoch 88/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.5502 - acc: 0.7955 - val_loss: 1.2549 - val_acc: 0.5814\n",
            "Epoch 89/200\n",
            "88/88 [==============================] - 0s 260us/step - loss: 0.5458 - acc: 0.7841 - val_loss: 1.2595 - val_acc: 0.5814\n",
            "Epoch 90/200\n",
            "88/88 [==============================] - 0s 241us/step - loss: 0.5441 - acc: 0.7614 - val_loss: 1.2596 - val_acc: 0.5814\n",
            "Epoch 91/200\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.5408 - acc: 0.7955 - val_loss: 1.2713 - val_acc: 0.5814\n",
            "Epoch 92/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.5367 - acc: 0.7955 - val_loss: 1.2708 - val_acc: 0.5814\n",
            "Epoch 93/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.5327 - acc: 0.7955 - val_loss: 1.2738 - val_acc: 0.5814\n",
            "Epoch 94/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.5292 - acc: 0.7955 - val_loss: 1.2764 - val_acc: 0.5814\n",
            "Epoch 95/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.5271 - acc: 0.7955 - val_loss: 1.2811 - val_acc: 0.5814\n",
            "Epoch 96/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.5225 - acc: 0.7955 - val_loss: 1.2854 - val_acc: 0.5814\n",
            "Epoch 97/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.5207 - acc: 0.7955 - val_loss: 1.2899 - val_acc: 0.5814\n",
            "Epoch 98/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.5159 - acc: 0.8068 - val_loss: 1.2966 - val_acc: 0.5814\n",
            "Epoch 99/200\n",
            "88/88 [==============================] - 0s 228us/step - loss: 0.5138 - acc: 0.8068 - val_loss: 1.3000 - val_acc: 0.5814\n",
            "Epoch 100/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.5106 - acc: 0.8068 - val_loss: 1.3027 - val_acc: 0.5814\n",
            "Epoch 101/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.5080 - acc: 0.8068 - val_loss: 1.3082 - val_acc: 0.5814\n",
            "Epoch 102/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.5059 - acc: 0.8068 - val_loss: 1.3114 - val_acc: 0.5814\n",
            "Epoch 103/200\n",
            "88/88 [==============================] - 0s 353us/step - loss: 0.5016 - acc: 0.7955 - val_loss: 1.3149 - val_acc: 0.5814\n",
            "Epoch 104/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4979 - acc: 0.8068 - val_loss: 1.3179 - val_acc: 0.5814\n",
            "Epoch 105/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.4952 - acc: 0.8295 - val_loss: 1.3214 - val_acc: 0.5814\n",
            "Epoch 106/200\n",
            "88/88 [==============================] - 0s 213us/step - loss: 0.4925 - acc: 0.8068 - val_loss: 1.3293 - val_acc: 0.5581\n",
            "Epoch 107/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.4915 - acc: 0.8182 - val_loss: 1.3360 - val_acc: 0.5581\n",
            "Epoch 108/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.4880 - acc: 0.8182 - val_loss: 1.3347 - val_acc: 0.5581\n",
            "Epoch 109/200\n",
            "88/88 [==============================] - 0s 258us/step - loss: 0.4830 - acc: 0.8409 - val_loss: 1.3461 - val_acc: 0.5581\n",
            "Epoch 110/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4807 - acc: 0.8409 - val_loss: 1.3501 - val_acc: 0.5581\n",
            "Epoch 111/200\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.4789 - acc: 0.8409 - val_loss: 1.3541 - val_acc: 0.5581\n",
            "Epoch 112/200\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.4750 - acc: 0.8523 - val_loss: 1.3603 - val_acc: 0.5581\n",
            "Epoch 113/200\n",
            "88/88 [==============================] - 0s 214us/step - loss: 0.4728 - acc: 0.8295 - val_loss: 1.3649 - val_acc: 0.5581\n",
            "Epoch 114/200\n",
            "88/88 [==============================] - 0s 210us/step - loss: 0.4706 - acc: 0.8409 - val_loss: 1.3666 - val_acc: 0.5581\n",
            "Epoch 115/200\n",
            "88/88 [==============================] - 0s 209us/step - loss: 0.4691 - acc: 0.8409 - val_loss: 1.3753 - val_acc: 0.5581\n",
            "Epoch 116/200\n",
            "88/88 [==============================] - 0s 280us/step - loss: 0.4658 - acc: 0.8409 - val_loss: 1.3822 - val_acc: 0.5581\n",
            "Epoch 117/200\n",
            "88/88 [==============================] - 0s 224us/step - loss: 0.4626 - acc: 0.8409 - val_loss: 1.3855 - val_acc: 0.5349\n",
            "Epoch 118/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.4599 - acc: 0.8409 - val_loss: 1.3899 - val_acc: 0.5349\n",
            "Epoch 119/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.4579 - acc: 0.8295 - val_loss: 1.3917 - val_acc: 0.5349\n",
            "Epoch 120/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.4556 - acc: 0.8295 - val_loss: 1.4005 - val_acc: 0.5349\n",
            "Epoch 121/200\n",
            "88/88 [==============================] - 0s 293us/step - loss: 0.4526 - acc: 0.8409 - val_loss: 1.4059 - val_acc: 0.5349\n",
            "Epoch 122/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4512 - acc: 0.8409 - val_loss: 1.4089 - val_acc: 0.5349\n",
            "Epoch 123/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.4483 - acc: 0.8409 - val_loss: 1.4118 - val_acc: 0.5349\n",
            "Epoch 124/200\n",
            "88/88 [==============================] - 0s 229us/step - loss: 0.4457 - acc: 0.8409 - val_loss: 1.4137 - val_acc: 0.5349\n",
            "Epoch 125/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.4443 - acc: 0.8295 - val_loss: 1.4233 - val_acc: 0.5349\n",
            "Epoch 126/200\n",
            "88/88 [==============================] - 0s 242us/step - loss: 0.4418 - acc: 0.8523 - val_loss: 1.4302 - val_acc: 0.5349\n",
            "Epoch 127/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.4387 - acc: 0.8636 - val_loss: 1.4301 - val_acc: 0.5349\n",
            "Epoch 128/200\n",
            "88/88 [==============================] - 0s 240us/step - loss: 0.4368 - acc: 0.8636 - val_loss: 1.4416 - val_acc: 0.5349\n",
            "Epoch 129/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.4348 - acc: 0.8409 - val_loss: 1.4421 - val_acc: 0.5349\n",
            "Epoch 130/200\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.4326 - acc: 0.8409 - val_loss: 1.4483 - val_acc: 0.5349\n",
            "Epoch 131/200\n",
            "88/88 [==============================] - 0s 234us/step - loss: 0.4302 - acc: 0.8636 - val_loss: 1.4528 - val_acc: 0.5349\n",
            "Epoch 132/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.4289 - acc: 0.8523 - val_loss: 1.4549 - val_acc: 0.5349\n",
            "Epoch 133/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.4263 - acc: 0.8636 - val_loss: 1.4625 - val_acc: 0.5349\n",
            "Epoch 134/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4246 - acc: 0.8636 - val_loss: 1.4655 - val_acc: 0.5581\n",
            "Epoch 135/200\n",
            "88/88 [==============================] - 0s 207us/step - loss: 0.4211 - acc: 0.8636 - val_loss: 1.4713 - val_acc: 0.5581\n",
            "Epoch 136/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.4197 - acc: 0.8636 - val_loss: 1.4784 - val_acc: 0.5581\n",
            "Epoch 137/200\n",
            "88/88 [==============================] - 0s 253us/step - loss: 0.4193 - acc: 0.8523 - val_loss: 1.4762 - val_acc: 0.5581\n",
            "Epoch 138/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.4159 - acc: 0.8523 - val_loss: 1.4922 - val_acc: 0.5581\n",
            "Epoch 139/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.4151 - acc: 0.8523 - val_loss: 1.4898 - val_acc: 0.5581\n",
            "Epoch 140/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.4134 - acc: 0.8523 - val_loss: 1.4909 - val_acc: 0.5581\n",
            "Epoch 141/200\n",
            "88/88 [==============================] - 0s 232us/step - loss: 0.4103 - acc: 0.8636 - val_loss: 1.4964 - val_acc: 0.5581\n",
            "Epoch 142/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.4085 - acc: 0.8636 - val_loss: 1.5042 - val_acc: 0.5581\n",
            "Epoch 143/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.4063 - acc: 0.8636 - val_loss: 1.5099 - val_acc: 0.5581\n",
            "Epoch 144/200\n",
            "88/88 [==============================] - 0s 248us/step - loss: 0.4047 - acc: 0.8636 - val_loss: 1.5119 - val_acc: 0.5581\n",
            "Epoch 145/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.4033 - acc: 0.8636 - val_loss: 1.5161 - val_acc: 0.5581\n",
            "Epoch 146/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.4008 - acc: 0.8636 - val_loss: 1.5174 - val_acc: 0.5581\n",
            "Epoch 147/200\n",
            "88/88 [==============================] - 0s 215us/step - loss: 0.3985 - acc: 0.8636 - val_loss: 1.5249 - val_acc: 0.5581\n",
            "Epoch 148/200\n",
            "88/88 [==============================] - 0s 227us/step - loss: 0.3979 - acc: 0.8636 - val_loss: 1.5271 - val_acc: 0.5581\n",
            "Epoch 149/200\n",
            "88/88 [==============================] - 0s 257us/step - loss: 0.3959 - acc: 0.8636 - val_loss: 1.5270 - val_acc: 0.5581\n",
            "Epoch 150/200\n",
            "88/88 [==============================] - 0s 204us/step - loss: 0.3929 - acc: 0.8636 - val_loss: 1.5375 - val_acc: 0.5581\n",
            "Epoch 151/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.3921 - acc: 0.8636 - val_loss: 1.5352 - val_acc: 0.5814\n",
            "Epoch 152/200\n",
            "88/88 [==============================] - 0s 239us/step - loss: 0.3890 - acc: 0.8636 - val_loss: 1.5429 - val_acc: 0.5581\n",
            "Epoch 153/200\n",
            "88/88 [==============================] - 0s 208us/step - loss: 0.3877 - acc: 0.8636 - val_loss: 1.5459 - val_acc: 0.5814\n",
            "Epoch 154/200\n",
            "88/88 [==============================] - 0s 245us/step - loss: 0.3853 - acc: 0.8636 - val_loss: 1.5501 - val_acc: 0.5581\n",
            "Epoch 155/200\n",
            "88/88 [==============================] - 0s 203us/step - loss: 0.3833 - acc: 0.8636 - val_loss: 1.5559 - val_acc: 0.5581\n",
            "Epoch 156/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3812 - acc: 0.8636 - val_loss: 1.5591 - val_acc: 0.5581\n",
            "Epoch 157/200\n",
            "88/88 [==============================] - 0s 256us/step - loss: 0.3803 - acc: 0.8636 - val_loss: 1.5628 - val_acc: 0.5814\n",
            "Epoch 158/200\n",
            "88/88 [==============================] - 0s 247us/step - loss: 0.3780 - acc: 0.8636 - val_loss: 1.5708 - val_acc: 0.5814\n",
            "Epoch 159/200\n",
            "88/88 [==============================] - 0s 206us/step - loss: 0.3746 - acc: 0.8750 - val_loss: 1.5757 - val_acc: 0.5814\n",
            "Epoch 160/200\n",
            "88/88 [==============================] - 0s 237us/step - loss: 0.3728 - acc: 0.8864 - val_loss: 1.5785 - val_acc: 0.5581\n",
            "Epoch 161/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.3709 - acc: 0.8864 - val_loss: 1.5751 - val_acc: 0.5814\n",
            "Epoch 162/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3701 - acc: 0.8864 - val_loss: 1.5879 - val_acc: 0.5581\n",
            "Epoch 163/200\n",
            "88/88 [==============================] - 0s 254us/step - loss: 0.3670 - acc: 0.8864 - val_loss: 1.5868 - val_acc: 0.5581\n",
            "Epoch 164/200\n",
            "88/88 [==============================] - 0s 217us/step - loss: 0.3647 - acc: 0.8864 - val_loss: 1.5882 - val_acc: 0.5581\n",
            "Epoch 165/200\n",
            "88/88 [==============================] - 0s 219us/step - loss: 0.3640 - acc: 0.8864 - val_loss: 1.5947 - val_acc: 0.5581\n",
            "Epoch 166/200\n",
            "88/88 [==============================] - 0s 238us/step - loss: 0.3611 - acc: 0.8864 - val_loss: 1.5963 - val_acc: 0.5581\n",
            "Epoch 167/200\n",
            "88/88 [==============================] - 0s 218us/step - loss: 0.3603 - acc: 0.8864 - val_loss: 1.6023 - val_acc: 0.5581\n",
            "Epoch 168/200\n",
            "88/88 [==============================] - 0s 220us/step - loss: 0.3585 - acc: 0.8977 - val_loss: 1.6044 - val_acc: 0.5581\n",
            "Epoch 169/200\n",
            "88/88 [==============================] - 0s 285us/step - loss: 0.3564 - acc: 0.8977 - val_loss: 1.6145 - val_acc: 0.5581\n",
            "Epoch 170/200\n",
            "88/88 [==============================] - 0s 287us/step - loss: 0.3543 - acc: 0.8977 - val_loss: 1.6102 - val_acc: 0.5349\n",
            "Epoch 171/200\n",
            "88/88 [==============================] - 0s 269us/step - loss: 0.3538 - acc: 0.8977 - val_loss: 1.6127 - val_acc: 0.5349\n",
            "Epoch 172/200\n",
            "88/88 [==============================] - 0s 252us/step - loss: 0.3513 - acc: 0.8864 - val_loss: 1.6177 - val_acc: 0.5349\n",
            "Epoch 173/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.3489 - acc: 0.8977 - val_loss: 1.6243 - val_acc: 0.5349\n",
            "Epoch 174/200\n",
            "88/88 [==============================] - 0s 223us/step - loss: 0.3480 - acc: 0.8977 - val_loss: 1.6256 - val_acc: 0.5349\n",
            "Epoch 175/200\n",
            "88/88 [==============================] - 0s 230us/step - loss: 0.3462 - acc: 0.8977 - val_loss: 1.6315 - val_acc: 0.5349\n",
            "Epoch 176/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3444 - acc: 0.8977 - val_loss: 1.6377 - val_acc: 0.5349\n",
            "Epoch 177/200\n",
            "88/88 [==============================] - 0s 231us/step - loss: 0.3424 - acc: 0.8977 - val_loss: 1.6422 - val_acc: 0.5349\n",
            "Epoch 178/200\n",
            "88/88 [==============================] - 0s 221us/step - loss: 0.3409 - acc: 0.8977 - val_loss: 1.6400 - val_acc: 0.5349\n",
            "Epoch 179/200\n",
            "88/88 [==============================] - 0s 279us/step - loss: 0.3389 - acc: 0.8977 - val_loss: 1.6467 - val_acc: 0.5349\n",
            "Epoch 180/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3380 - acc: 0.8977 - val_loss: 1.6522 - val_acc: 0.5349\n",
            "Epoch 181/200\n",
            "88/88 [==============================] - 0s 334us/step - loss: 0.3354 - acc: 0.8977 - val_loss: 1.6585 - val_acc: 0.5349\n",
            "Epoch 182/200\n",
            "88/88 [==============================] - 0s 246us/step - loss: 0.3347 - acc: 0.8977 - val_loss: 1.6556 - val_acc: 0.5349\n",
            "Epoch 183/200\n",
            "88/88 [==============================] - 0s 233us/step - loss: 0.3330 - acc: 0.8977 - val_loss: 1.6601 - val_acc: 0.5349\n",
            "Epoch 184/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3312 - acc: 0.8977 - val_loss: 1.6634 - val_acc: 0.5349\n",
            "Epoch 185/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.3301 - acc: 0.8977 - val_loss: 1.6698 - val_acc: 0.5349\n",
            "Epoch 186/200\n",
            "88/88 [==============================] - 0s 335us/step - loss: 0.3285 - acc: 0.8977 - val_loss: 1.6673 - val_acc: 0.5349\n",
            "Epoch 187/200\n",
            "88/88 [==============================] - 0s 255us/step - loss: 0.3266 - acc: 0.8977 - val_loss: 1.6702 - val_acc: 0.5349\n",
            "Epoch 188/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3252 - acc: 0.8977 - val_loss: 1.6749 - val_acc: 0.5349\n",
            "Epoch 189/200\n",
            "88/88 [==============================] - 0s 236us/step - loss: 0.3244 - acc: 0.8977 - val_loss: 1.6734 - val_acc: 0.5349\n",
            "Epoch 190/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.3229 - acc: 0.8977 - val_loss: 1.6745 - val_acc: 0.5349\n",
            "Epoch 191/200\n",
            "88/88 [==============================] - 0s 244us/step - loss: 0.3205 - acc: 0.8977 - val_loss: 1.6794 - val_acc: 0.5349\n",
            "Epoch 192/200\n",
            "88/88 [==============================] - 0s 216us/step - loss: 0.3187 - acc: 0.8977 - val_loss: 1.6824 - val_acc: 0.5349\n",
            "Epoch 193/200\n",
            "88/88 [==============================] - 0s 211us/step - loss: 0.3182 - acc: 0.8977 - val_loss: 1.6854 - val_acc: 0.5349\n",
            "Epoch 194/200\n",
            "88/88 [==============================] - 0s 222us/step - loss: 0.3162 - acc: 0.8977 - val_loss: 1.6904 - val_acc: 0.5349\n",
            "Epoch 195/200\n",
            "88/88 [==============================] - 0s 226us/step - loss: 0.3155 - acc: 0.8977 - val_loss: 1.6929 - val_acc: 0.5349\n",
            "Epoch 196/200\n",
            "88/88 [==============================] - 0s 263us/step - loss: 0.3145 - acc: 0.8977 - val_loss: 1.6933 - val_acc: 0.5349\n",
            "Epoch 197/200\n",
            "88/88 [==============================] - 0s 225us/step - loss: 0.3119 - acc: 0.8977 - val_loss: 1.6972 - val_acc: 0.5349\n",
            "Epoch 198/200\n",
            "88/88 [==============================] - 0s 284us/step - loss: 0.3113 - acc: 0.8977 - val_loss: 1.6979 - val_acc: 0.5349\n",
            "Epoch 199/200\n",
            "88/88 [==============================] - 0s 243us/step - loss: 0.3089 - acc: 0.8977 - val_loss: 1.7020 - val_acc: 0.5349\n",
            "Epoch 200/200\n",
            "88/88 [==============================] - 0s 282us/step - loss: 0.3083 - acc: 0.8977 - val_loss: 1.7028 - val_acc: 0.5349\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2d1f19ec-b4e7-436e-ea6d-b332a351d7a9",
        "id": "6OH7qa6a14-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "val_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.09052215e-01, -1.30570140e+00, -1.50987053e+00,\n",
              "         4.78413407e-01, -5.48688420e-01, -2.19260239e-01,\n",
              "        -3.87514118e-01, -2.35668681e-01, -3.27761894e-01,\n",
              "        -3.30961271e-01, -8.29963414e-01,  1.09070485e+00,\n",
              "        -4.68298254e-01, -2.54403631e-01,  2.05362316e-01,\n",
              "        -1.58492380e-01, -8.76044573e-01],\n",
              "       [ 1.93706034e+00,  2.47792697e-01, -6.14324644e-01,\n",
              "        -1.43358516e+00,  1.74280631e-01, -1.01385115e+00,\n",
              "        -5.98180597e-01,  7.79397065e-02, -3.34625115e-01,\n",
              "        -3.30973016e-01, -1.31203202e+00,  4.46338623e-02,\n",
              "        -1.30913346e+00, -5.65265481e-01,  2.19195593e+00,\n",
              "         9.70100653e-01, -1.05149009e+00],\n",
              "       [ 7.34257050e-01,  9.68988268e-02,  3.85769839e-01,\n",
              "         1.61200050e-01, -5.40961150e-01, -3.83658358e-01,\n",
              "        -1.13750969e-01, -2.28591476e-01, -1.15055286e-01,\n",
              "        -3.30299955e-01,  1.35982249e-01,  3.71533408e-01,\n",
              "        -1.21254356e-01, -4.84378805e-01, -5.30458542e-01,\n",
              "        -4.87994766e-01,  1.88481002e-01],\n",
              "       [ 7.68510240e-01, -1.82042583e-01, -3.33326380e-01,\n",
              "         6.50849331e-02, -5.40195473e-01, -8.49453029e-01,\n",
              "         1.35406834e-01, -8.66383484e-01, -3.31690446e-01,\n",
              "        -3.30963175e-01, -7.37179436e-01,  4.04397633e-01,\n",
              "         3.35949635e-01,  3.71018266e-01, -4.24856494e-01,\n",
              "         1.10650982e+00, -5.76281334e-01],\n",
              "       [-2.67444175e-01,  2.78483283e-01,  5.18248153e-01,\n",
              "         1.05778670e+00, -5.44927981e-01, -2.74059612e-01,\n",
              "        -3.14211181e-01,  1.39098834e+00,  3.52354987e-01,\n",
              "         1.74307426e+00,  1.16373157e+00, -1.00481406e+00,\n",
              "        -2.31777499e-01, -7.50286880e-01, -4.86354308e-01,\n",
              "        -7.66223485e-01,  4.69101139e-01],\n",
              "       [ 1.54957687e+00,  1.90539890e+00, -2.22052320e-01,\n",
              "         3.87307146e-01, -5.45340485e-01, -1.01385115e+00,\n",
              "        -2.73626628e-01, -6.67618363e-01, -3.33713506e-01,\n",
              "        -3.30970545e-01, -1.05552128e+00, -1.16554466e+00,\n",
              "         7.92773556e-01, -6.04756035e-03, -1.36103435e-01,\n",
              "         2.84618093e-01, -7.68000271e-01],\n",
              "       [ 1.87054098e+00,  1.34674226e+00, -1.02135859e+00,\n",
              "        -6.88962901e-01,  1.41898628e+00, -7.67253969e-01,\n",
              "        -2.64847015e-01, -5.26713015e-01, -3.29415595e-01,\n",
              "        -3.30965430e-01, -1.76967570e+00,  1.59330175e+00,\n",
              "        -1.24929440e+00, -4.09476984e-01,  2.95159541e-01,\n",
              "         2.24992687e+00, -7.63153541e-01],\n",
              "       [ 1.40230428e+00,  4.89008981e-01, -1.59088213e+00,\n",
              "        -7.77936324e-01,  6.37226628e-01, -8.76852715e-01,\n",
              "        -2.27818116e-01, -3.15371130e-01, -3.33772401e-01,\n",
              "        -3.30969539e-01, -9.21826930e-01,  2.84393304e+00,\n",
              "        -1.70848419e+00, -1.46089724e-01,  1.24885248e+00,\n",
              "         1.45114145e+00, -9.68926187e-01],\n",
              "       [ 1.49637582e+00,  1.54770319e+00, -6.67501968e-01,\n",
              "        -1.28154597e+00, -5.45678923e-01, -1.06865052e+00,\n",
              "         5.59983970e-01, -8.66709024e-01, -3.31610893e-01,\n",
              "        -3.30972296e-01, -1.64687168e+00, -5.07742521e-01,\n",
              "         1.05796673e-01,  6.81620219e-01,  1.91473680e-01,\n",
              "         5.11821617e+00, -6.81890737e-01],\n",
              "       [ 1.00150735e+00, -8.58201787e-01,  3.11646964e-01,\n",
              "        -1.43442573e+00, -5.49929348e-01, -9.04252402e-01,\n",
              "        -5.28211743e-01, -1.20002808e-01, -3.35769175e-01,\n",
              "        -3.30973760e-01, -4.42456637e-01,  2.35041057e+00,\n",
              "        -1.82929991e+00,  2.68994980e-01,  2.63286133e+00,\n",
              "         1.29973450e+00, -1.07303619e+00],\n",
              "       [-6.27982153e-01,  1.82083041e+00,  6.89179021e-01,\n",
              "         1.26719711e+00, -5.43758936e-01, -1.64460866e-01,\n",
              "        -6.68394416e-02,  9.78698647e-01,  6.80756634e-01,\n",
              "         2.73073776e+00,  1.92168550e+00, -1.03524748e+00,\n",
              "         5.49325368e-01,  4.74349821e-01, -4.82848243e-01,\n",
              "        -8.08082795e-01,  9.01514579e-01],\n",
              "       [ 5.22267532e-01, -1.06875098e-01, -6.47157410e-01,\n",
              "        -2.81002626e-02, -5.47216346e-01,  5.47366256e-02,\n",
              "        -9.22810177e-01,  2.97018333e-01, -1.85535435e-01,\n",
              "        -3.30751288e-01, -1.01051841e+00,  5.33572761e-01,\n",
              "        -8.41591880e-01, -1.03863818e+00, -5.91370564e-01,\n",
              "        -4.78529164e-01,  7.65523623e-02],\n",
              "       [ 2.31051081e-01, -3.20246036e-01,  3.33606515e-01,\n",
              "        -1.92448965e+00, -5.47077623e-01, -1.39470679e+00,\n",
              "         1.71658188e-01, -8.49813810e-01, -3.35016786e-01,\n",
              "        -3.30973339e-01, -1.13057908e+00, -3.96647241e-02,\n",
              "        -7.76402925e-02, -1.46431122e-01,  1.57365298e+00,\n",
              "         2.06583169e+00, -9.92270817e-01],\n",
              "       [-6.44453936e-01, -8.78723589e-03, -1.04418142e-01,\n",
              "        -5.22952367e-01,  9.09313099e-01,  1.69871781e+00,\n",
              "        -1.21474566e+00,  1.75610763e+00, -9.44599440e-02,\n",
              "        -3.30471019e-01,  8.05783791e-01, -1.11896137e+00,\n",
              "        -5.71419421e-01, -1.13904175e+00,  2.36554102e-01,\n",
              "        -7.98738149e-01, -5.48945706e-01],\n",
              "       [-3.48340628e-01, -2.04547197e+00, -1.03467134e+00,\n",
              "        -1.55390505e-02, -5.46627799e-01, -1.15084958e+00,\n",
              "        -6.69297508e-02, -7.37879216e-01, -3.34679763e-01,\n",
              "        -3.30971699e-01, -1.03184069e+00,  1.50423112e+00,\n",
              "        -6.09364543e-01,  1.28156402e-01,  5.77786575e-02,\n",
              "         1.87632640e+00, -7.99203206e-01],\n",
              "       [ 1.31428729e+00,  8.33731470e-01, -2.06133494e+00,\n",
              "        -4.94935111e-01, -1.48787803e-01, -4.93257104e-01,\n",
              "        -1.11052146e+00,  2.96631799e-01, -3.34734762e-01,\n",
              "        -3.30973644e-01, -1.88126819e+00,  8.14095169e-01,\n",
              "        -1.86868564e+00, -9.97315097e-01,  1.51500876e+00,\n",
              "         6.33359739e-01, -1.04880671e+00],\n",
              "       [-7.78919476e-02, -8.97044120e-03,  6.51523027e-01,\n",
              "         7.49297997e-01, -5.27596185e-01, -5.20656791e-01,\n",
              "        -4.28916598e-02,  4.08624768e-01,  1.37001203e+00,\n",
              "         4.65521584e+00,  7.28554740e-01, -6.10275581e-01,\n",
              "         4.10570346e-01,  3.16088294e-01, -7.78599488e-01,\n",
              "        -7.78277706e-01,  2.46846217e+00],\n",
              "       [-1.07862168e+00, -9.07261519e-01, -1.76684217e+00,\n",
              "         4.54004496e-01, -5.45792945e-01, -8.22053342e-01,\n",
              "        -1.15501373e-01, -6.66255332e-01, -3.34572715e-01,\n",
              "        -3.30971879e-01, -1.27247672e+00,  1.63479878e+00,\n",
              "        -7.17735476e-01,  3.80034857e-01, -3.52185277e-01,\n",
              "         2.09857952e+00, -5.85517130e-01],\n",
              "       [-4.10778722e-01, -1.47731624e+00,  2.03066111e-02,\n",
              "         5.14359698e-01, -5.42388460e-01,  9.04126907e-01,\n",
              "         1.61247582e-01, -6.87811940e-01, -3.23281141e-01,\n",
              "        -3.30938728e-01, -8.53130201e-03, -2.23899349e-02,\n",
              "         4.65265450e-01, -1.25718098e-01, -5.02362406e-01,\n",
              "         3.93224819e-01, -3.54460457e-01],\n",
              "       [-2.67691522e+00, -7.83949702e-01, -4.35930764e-01,\n",
              "         7.82858235e-01, -5.43538425e-01, -3.56258672e-01,\n",
              "        -2.29288143e-01, -4.04234755e-01, -3.23171479e-01,\n",
              "        -3.30952033e-01, -6.84554256e-01,  1.72157557e-02,\n",
              "         1.25295199e-01, -5.44093976e-01, -6.52698092e-01,\n",
              "         2.90087082e-01, -1.25754879e-01],\n",
              "       [ 8.66821394e-01, -3.81661127e-01, -9.31529767e-01,\n",
              "        -1.08735653e-01,  2.68419262e+00, -8.22053342e-01,\n",
              "        -1.16075046e+00,  4.28848179e-01, -2.98252121e-01,\n",
              "        -3.30895891e-01, -5.72195095e-01,  2.93311022e-01,\n",
              "        -9.78331074e-01, -7.76096481e-01, -3.14273460e-01,\n",
              "        -3.22049443e-01, -5.37955363e-01],\n",
              "       [ 7.23052291e-02,  1.04327058e+00, -6.85581854e-01,\n",
              "         3.47382948e-01,  2.82555274e+00, -8.22053342e-01,\n",
              "        -8.49521291e-01, -1.35127259e-01, -3.32460424e-01,\n",
              "        -3.30967487e-01, -9.50662606e-01, -1.34840990e-01,\n",
              "        -4.28277955e-01, -5.22481418e-01, -6.28998767e-02,\n",
              "         3.20908629e-01, -7.80744615e-01],\n",
              "       [ 1.20793562e+00, -1.03223723e+00, -1.62396721e+00,\n",
              "         3.11100130e-01,  1.49808593e+00, -9.86451461e-01,\n",
              "        -4.48263664e-01, -2.91367343e-01, -3.31854994e-01,\n",
              "        -3.30968638e-01, -1.24663188e+00,  2.52799032e-01,\n",
              "        -6.54119703e-01, -4.47802506e-01,  1.69383467e-01,\n",
              "         1.04489784e+00, -8.04946099e-01],\n",
              "       [-6.87842207e-01,  1.72331016e+00,  9.61258968e-01,\n",
              "         5.87565237e-01, -5.46222606e-01, -1.64460866e-01,\n",
              "         7.23101620e-01,  7.19138408e-01, -1.35740067e-01,\n",
              "        -3.30621865e-01,  9.72960680e-01,  5.02376472e-01,\n",
              "        -3.50762360e-01, -6.61316119e-01,  2.15088506e-01,\n",
              "        -5.02291020e-01, -5.16875211e-01],\n",
              "       [-7.06431308e-01, -9.89748699e-03,  1.32801751e+00,\n",
              "        -1.37618450e+00, -5.39625628e-01,  6.60326170e+00,\n",
              "         3.83648904e+00, -1.69002919e+00, -2.97148140e-01,\n",
              "        -3.30858604e-01,  2.98396859e+00, -8.38653288e-01,\n",
              "         2.67323855e+00,  1.97331940e+00,  4.59022522e-01,\n",
              "        -5.16599725e-01, -7.19356677e-01],\n",
              "       [ 2.06934501e+00,  1.21207305e+00, -8.02529761e-01,\n",
              "        -3.23665850e+00,  3.52435672e-01, -1.01385115e+00,\n",
              "         5.82901521e-02, -7.18648977e-01, -3.35432838e-01,\n",
              "        -3.30974284e-01, -1.66414662e+00,  2.66798240e+00,\n",
              "        -1.79401751e+00,  3.67356718e-01,  2.40487403e+00,\n",
              "         4.02913212e+00, -1.04661333e+00],\n",
              "       [-4.81158272e-02, -2.32919315e-01,  1.22501708e+00,\n",
              "        -8.12476213e-01, -4.97813037e-01,  1.01372565e+00,\n",
              "         1.33700890e+00, -1.28646438e+00, -2.95623368e-01,\n",
              "        -3.30844390e-01, -3.13005653e-01, -3.33040465e-01,\n",
              "         1.52391557e+00,  1.11048723e+00, -7.64915655e-01,\n",
              "         4.28233562e-01,  4.28892001e-01],\n",
              "       [-9.82392012e-01, -1.50981574e-01,  4.83117627e-01,\n",
              "         2.22916324e-01, -5.37588700e-01, -5.48621204e-02,\n",
              "        -3.23635973e-01,  2.80315082e-01,  1.13406449e-01,\n",
              "        -3.29875677e-01,  8.18070701e-01, -1.43941597e+00,\n",
              "         1.04414111e+00,  1.63780813e-01, -7.28423285e-01,\n",
              "        -7.65482987e-01,  8.05402450e-01],\n",
              "       [ 6.97724794e-01,  1.63023754e-01, -1.75187371e+00,\n",
              "         1.14426911e+00, -5.48805576e-01,  8.21363121e-02,\n",
              "        -1.28708397e+00,  8.40567159e-01, -2.89064255e-01,\n",
              "        -3.30913130e-01, -1.78303534e-01, -1.94079236e+00,\n",
              "         1.47162088e-01, -7.67067466e-01, -4.77411270e-01,\n",
              "        -6.59369742e-01, -4.29973239e-01],\n",
              "       [ 1.47188959e+00,  5.62595026e-01, -1.49271155e+00,\n",
              "         1.02886770e+00,  6.05581723e-01, -3.56258672e-01,\n",
              "        -9.01440816e-01,  4.88482086e-01, -3.20638346e-01,\n",
              "        -3.30944186e-01, -3.05015407e-01,  4.24598068e-01,\n",
              "        -1.03575902e+00, -6.13459514e-01,  7.86519903e-01,\n",
              "        -3.65165869e-01, -8.72051850e-01],\n",
              "       [ 1.78305420e+00,  1.37711411e+00,  1.49916132e-01,\n",
              "        -5.15432158e-01, -5.46664013e-01, -1.17824927e+00,\n",
              "        -7.14148531e-01, -3.26597763e-01, -3.31802117e-01,\n",
              "        -3.30964935e-01, -1.10153207e+00, -3.25076660e-01,\n",
              "        -2.76899548e-01, -4.81278689e-01,  1.63951096e-01,\n",
              "         7.64810335e-01, -8.13461099e-01],\n",
              "       [ 1.33916856e+00,  1.74729366e+00,  9.32576683e-01,\n",
              "        -3.92248540e-01, -5.42436939e-01, -5.48056477e-01,\n",
              "         5.42853861e-01, -3.37319011e-01,  1.32217300e-01,\n",
              "        -3.30650739e-01,  9.93278185e-02,  2.18568209e+00,\n",
              "        -7.52734919e-01, -5.58159325e-01, -2.94368551e-01,\n",
              "         3.00978986e-04, -1.51329794e-01],\n",
              "       [-4.34428134e-01,  3.02004017e-01,  1.01503799e-01,\n",
              "        -7.25101197e-01, -5.28519067e-01,  8.21927847e-01,\n",
              "         1.45567081e+00, -7.68295455e-01,  4.14797781e-01,\n",
              "         2.25528874e+00,  5.75813151e-01,  6.77986198e-01,\n",
              "         7.49259558e-01, -2.17678153e-01, -7.81576726e-01,\n",
              "        -6.63814520e-01,  1.22155484e+00],\n",
              "       [-1.31924424e+00, -2.13873130e+00, -1.64219371e+00,\n",
              "         5.92161706e-01,  3.05030254e-01, -1.09661493e-01,\n",
              "        -8.13304283e-01,  4.59418668e-01, -2.47396709e-01,\n",
              "        -3.30904876e-01, -6.48364725e-01,  2.66347433e-01,\n",
              "        -5.20524304e-01, -8.21052616e-01,  4.23155858e-01,\n",
              "        -7.44445162e-01, -7.34242330e-01],\n",
              "       [ 4.98694935e-01,  7.35612793e-01,  6.35586964e-01,\n",
              "         6.54250049e-01,  3.16485533e-02, -1.64460866e-01,\n",
              "        -3.77661612e-01,  2.09671116e+00, -2.80318175e-01,\n",
              "        -3.30830305e-01,  6.43998628e-01,  5.19144703e-01,\n",
              "        -1.77373780e+00, -1.05512656e+00,  3.58224772e+00,\n",
              "        -7.01535374e-01, -9.40195936e-01],\n",
              "       [-5.53790101e-01,  1.03819108e+00,  1.37825865e+00,\n",
              "        -5.27827103e-01, -4.81328114e-01,  9.58926280e-01,\n",
              "         1.12437799e+00, -1.09948055e+00, -1.42972054e-01,\n",
              "        -3.30380716e-01,  1.64834270e-01, -1.08663847e+00,\n",
              "         1.87716536e+00,  3.95541488e+00, -6.54081800e-01,\n",
              "        -5.41593407e-01,  9.48322491e-01],\n",
              "       [-1.26926479e+00, -6.36680117e-01,  1.01970124e+00,\n",
              "        -5.33978697e-01, -5.33738171e-01,  9.86325966e-01,\n",
              "         1.25522324e+00, -6.93947595e-01, -1.94759811e-01,\n",
              "        -3.30667183e-01,  1.30284625e+00, -2.03162186e+00,\n",
              "         2.27840513e+00,  4.24096956e+00,  6.55272926e-01,\n",
              "        -7.58507969e-01, -2.33948455e-01],\n",
              "       [-3.20926151e-02, -6.03988430e-01, -7.78588951e-01,\n",
              "         6.46497882e-01,  1.92052260e+00,  5.75330669e-01,\n",
              "         1.94163568e-01,  1.55176070e-01, -2.84214659e-01,\n",
              "        -3.30810446e-01,  1.22361023e+00, -4.23498024e-02,\n",
              "         1.00554220e-01, -3.74807906e-01,  4.69542622e-01,\n",
              "        -6.39069582e-01, -7.50674863e-01],\n",
              "       [-1.04351660e+00, -1.81410010e+00,  3.47721555e-01,\n",
              "        -1.29945878e-02, -5.39025604e-01, -4.11058045e-01,\n",
              "        -3.10854978e-01, -4.68380791e-01, -2.99799844e-01,\n",
              "        -3.30884312e-01, -6.04430429e-01,  3.95954001e-01,\n",
              "        -2.75961036e-02, -3.55011848e-01, -6.40261061e-01,\n",
              "         1.25434692e-01,  1.69221243e-03],\n",
              "       [-1.70779933e+00, -1.82177506e+00,  6.30653686e-01,\n",
              "         7.72815870e-02, -5.41667522e-01, -7.94653656e-01,\n",
              "        -2.36091860e-01, -5.49058455e-01, -3.32324336e-01,\n",
              "        -3.30964704e-01, -2.78998649e-01,  7.31967045e-01,\n",
              "        -5.20673201e-02,  1.47627164e+00, -2.60634812e-01,\n",
              "         1.91408748e-01, -6.63483974e-01],\n",
              "       [-2.61293931e-01, -8.16561598e-01,  1.04338706e+00,\n",
              "        -1.15650705e+00, -5.41614180e-01,  2.60290747e+00,\n",
              "         1.30285401e+00, -7.54923071e-01, -2.82469107e-01,\n",
              "        -3.30793928e-01,  5.28076978e-01, -1.69820568e-01,\n",
              "         8.39421745e-01, -4.61928261e-01, -2.21762703e-01,\n",
              "        -2.37855717e-01, -5.14552169e-01],\n",
              "       [ 8.62558431e-01,  1.62239571e+00,  3.56990345e-01,\n",
              "        -2.80327055e-01, -5.41526545e-01,  9.04126907e-01,\n",
              "         7.81323904e-01, -7.68937274e-01, -1.92343026e-01,\n",
              "        -3.30479367e-01,  3.40655060e-01,  9.32793053e-01,\n",
              "         3.24131351e-01, -4.79406591e-01, -4.16664194e-01,\n",
              "        -3.06035404e-01, -1.41367216e-01],\n",
              "       [ 1.09128720e-01, -1.44962712e+00, -1.13746835e+00,\n",
              "        -1.59265282e-02,  2.15276655e+00, -2.19260239e-01,\n",
              "        -2.57819423e-01, -6.92733551e-01, -3.35814248e-01,\n",
              "        -3.30974439e-01, -1.20398783e+00,  9.16355465e-01,\n",
              "        -4.47333051e-01,  1.32357085e-01,  2.86341634e-01,\n",
              "         1.47032779e+00, -8.80778234e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2WoqA4rT14-Q",
        "colab": {}
      },
      "source": [
        "history_dict = history.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5b913cd5-2fc3-430d-d0b0-21b1599efc13",
        "id": "YEyjhsg414-W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "history_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "2109e582-22ac-4bd3-b3d7-de653f9cb0d9",
        "id": "9WylvzXG14-a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_acc_histories[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qw9GTwCC14-f",
        "colab": {}
      },
      "source": [
        "average_acc_history_reduced = [np.mean([x[i] for x in all_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "#media per epoca degli score ottenuti per tutte le k-fold\n",
        "#per ogni k-fold di fanno num_epoch epoche, la media viene fatta prendendo gli score di tutti i k-fold relativi ad una data epoca,\n",
        "#e si fa questo per tutte le epoche\n",
        "average_loss_history_reduced = [np.mean([x[i] for x in all_loss_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_acc_history_reduced = [np.mean([x[i] for x in all_val_acc_histories_reduced]) for i in range(num_epochs)]\n",
        "average_val_loss_history_reduced = [np.mean([x[i] for x in all_val_loss_histories_reduced]) for i in range(num_epochs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "9dfc4d2d-9870-48a6-d0f6-7e595ef1771b",
        "id": "MdMC3doS14-i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(average_val_acc_history_reduced)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evLl4sb8DOX",
        "colab_type": "code",
        "outputId": "6e488e75-1309-4117-8240-c3c32d0def6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_21 (Dense)             (None, 10)                180       \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 213\n",
            "Trainable params: 213\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wrcg4Bx625fC"
      },
      "source": [
        "##Plotting training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x9_gKwYk25fK",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCcaAgTi25fZ",
        "colab": {}
      },
      "source": [
        "epochs = range(1, num_epochs+1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "15e601a1-0eec-4799-a15c-2978e611e486",
        "id": "6H8nOl_X25fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_loss_history_reduced, 'b', label='training loss')\n",
        "plt.plot(epochs, average_val_loss_history_reduced, 'r', label='validation loss')\n",
        "plt.title('Training and validation loss lda')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a92d7db70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3dd3hUZdPA4d8QQpEuRelFivQWiqJS\nBAREEAuC8CoKYi+vimBDxV5e9MOOIioqiDRRUWwgoIB0KaJUBVRApAoqZb4/5gRCTEISsiXZua9r\nL3bPnt0zuxt29jxlHlFVnHPOxa5ckQ7AOedcZHkicM65GOeJwDnnYpwnAueci3GeCJxzLsZ5InDO\nuRjnicAdk4jEicgeEamQlftGkohUFZEsHzstIm1FZH2S2z+IyJnp2TcTx3pVRO7K7OPTeN6HROT1\nrH7eVI6V5nsgIm+JyP3hiCWW5Y50AC7ricieJDdPAP4GDga3r1bVtzPyfKp6ECiY1fvGAlWtkRXP\nIyL9gN6q2irJc/fLiud2zhNBDqSqh7+Ig19b/VT189T2F5HcqnogHLE556KPNw3FoODU/10RGS0i\nu4HeInKaiMwRkR0i8quIDBOR+GD/3CKiIlIpuP1WcP/HIrJbRGaLSOWM7hvc31FEfhSRnSLyrIh8\nLSJ9Uok7PTFeLSKrRWS7iAxL8tg4EXlaRLaJyFqgQxrvz90iMibZtudFZGhwvZ+IfB+8njXBr/XU\nnmujiLQKrp8gIqOC2JYDjZPte4+IrA2ed7mIdAm21wWeA84Mmt1+T/Le3p/k8dcEr32biEwSkdLp\neW+ORUS6BfHsEJEvRaRGkvvuEpFfRGSXiKxM8lqbi8jCYPtmEXkyncdqLCKLg/dgNJA3yX3FRWSK\niGwNXsMHIlI2va/DpUFV/ZKDL8B6oG2ybQ8B/wDnYT8G8gNNgGbYWWIV4EfghmD/3IAClYLbbwG/\nAwlAPPAu8FYm9i0F7Aa6BvfdCuwH+qTyWtIT4/tAEaAS8EfiawduAJYD5YDiwAz780/xOFWAPUCB\nJM+9BUgIbp8X7CNAG2AfUC+4ry2wPslzbQRaBdefAqYDxYCKwIpk+3YHSgefyaVBDCcF9/UDpieL\n8y3g/uB6+yDGBkA+4AXgy/S8Nym8/oeA14PrNYM42gSf0V3AD8H12sBPwMnBvpWBKsH1eUDP4Hoh\noFkqxzr8fmFf+huBm4Ln7xH8PSS+xpJAN+zvtTAwARgX6f9jOeHiZwSxa5aqfqCqh1R1n6rOU9W5\nqnpAVdcCw4GWaTx+nKrOV9X9wNvYF1BG9+0MLFbV94P7nsaSRorSGeOjqrpTVddjX7qJx+oOPK2q\nG1V1G/BYGsdZCyzDEhRAO2C7qs4P7v9AVdeq+RL4AkixQziZ7sBDqrpdVX/CfuUnPe5YVf01+Eze\nwZJ4QjqeF6AX8KqqLlbVv4BBQEsRKZdkn9Tem7T0ACar6pfBZ/QYlkyaAQewpFM7aF5cF7x3YF/g\n1USkuKruVtW56ThWCyxhPauq+1V1DLAo8U5V3aqqE4O/113AI6T9N+rSyRNB7NqQ9IaInCoiH4nI\nbyKyCxgClEjj8b8lub6XtDuIU9u3TNI4VFWxX4QpSmeM6ToW9ks2Le8APYPrlwa3E+PoLCJzReQP\nEdmB/RpP671KVDqtGESkj4gsCZpgdgCnpvN5wV7f4ecLvii3A0mbTjLymaX2vIewz6isqv4A3IZ9\nDluCpsaTg12vAGoBP4jItyLSKZ3H2hj8HSQ6fGwRKSg2Uurn4PP/kvS/Py4NnghiV/Khky9jv4Kr\nqmphYDDW9BFKv2JNNQCIiHD0F1dyxxPjr0D5JLePNbx1LNA2aIPuSpAIRCQ/MA54FGu2KQp8ms44\nfkstBhGpArwIXAsUD553ZZLnPdZQ11+w5qbE5yuENUFtSkdcGXneXNhntglAVd9S1RZYs1Ac9r6g\nqj+oag+s+e9/wHgRyXeMYx319xBI+jkNCI7TNPj822T2RbmjeSJwiQoBO4E/RaQmcHUYjvkh0EhE\nzhOR3MDNWDtwKGIcC9wiImVFpDgwMK2dVfU3YBbwOvCDqq4K7soL5AG2AgdFpDNwdgZiuEtEiorN\ns7ghyX0FsS/7rVhOvAo7I0i0GSiX2DmegtFAXxGpJyJ5sS/kmaqa6hlWBmLuIiKtgmMPwPp15opI\nTRFpHRxvX3A5hL2A/4hIieAMYmfw2g4d41izgFwickPQwd0daJTk/kLYmcz24DMcfJyvzQU8EbhE\ntwGXY//JX8Y6dUNKVTcDlwBDgW3AKVib8N8hiPFFrC1/KdaROS4dj3kH68w83CykqjuA/wITsQ7X\ni7CElh73Yb961wMfA28med7vgGeBb4N9agBJ29U/A1YBm0UkaRNP4uM/wZpoJgaPr4D1GxwXVV2O\nvecvYkmqA9Al6C/ICzyB9ev8hp2B3B08tBPwvdiotKeAS1T1n2Mc62+sM/gqrFmrGzApyS5Dsf6J\nbcA32HvosoAc3RznXOSISBzWFHGRqs6MdDzOxQo/I3ARJSIdgqaSvMC92GiTbyMclnMxxROBi7Qz\ngLVYs8M5QLegicA5FybeNOScczHOzwiccy7GhazonIi8hs0c3aKqdVLZpxXwDDad/HdVPeYswRIl\nSmilSpWyMFLnnMv5FixY8Luqpjg8O5TVR1/HptC/mdKdIlIUq4fSQVV/FpFS6XnSSpUqMX/+/CwL\n0jnnYoGIpDqbPmRNQ6o6AxtnnZpLgQmq+nOw/5ZQxeKccy51kewjqA4UE5HpIrJARC5LbUcR6S8i\n80Vk/tatW8MYonPO5XyRTAS5sXrs52LDBu8Vkeop7aiqw1U1QVUTSpZMqwKBc865jIrkCmUbgW2q\n+idWO2YGUB+rMe+ciyL79+9n48aN/PXXX5EOxR1Dvnz5KFeuHPHxqZWl+rdIJoL3geeCYmN5sPrm\nT0cwHudcKjZu3EihQoWoVKkSViTWRSNVZdu2bWzcuJHKlSsf+wGBUA4fHQ20AkqIyEas4FY8gKq+\npKrfi8gnwHdYVcJXVXVZqOJxzmXeX3/95UkgGxARihcvTkb7UkOWCFS1Zzr2eRJI11qmzrnI8iSQ\nPWTmc4qdmcUrV8J//wv/pFkJ1znnYk7sJII1a+CZZ2Dy5EhH4pzLoB07dvDCCy9k6rGdOnVix44d\nae4zePBgPv/880w9f3KVKlXi999TXXo7KsVOIujQASpUgJdfjnQkzrkMSisRHDhwIM3HTpkyhaJF\ni6a5z5AhQ2jbtm2m48vuYicRxMVBv37w+eewenWko3HOZcCgQYNYs2YNDRo0YMCAAUyfPp0zzzyT\nLl26UKtWLQDOP/98GjduTO3atRk+fPjhxyb+Ql+/fj01a9bkqquuonbt2rRv3559+/YB0KdPH8aN\nG3d4//vuu49GjRpRt25dVq5cCcDWrVtp164dtWvXpl+/flSsWPGYv/yHDh1KnTp1qFOnDs888wwA\nf/75J+eeey7169enTp06vPvuu4dfY61atahXrx6333571r6BxxDJ4aPh17cvPPAAvPIKPP54pKNx\nLlu65RZYvDhrn7NBA2u5Tc1jjz3GsmXLWBwcePr06SxcuJBly5YdHib52muvceKJJ7Jv3z6aNGnC\nhRdeSPHixY96nlWrVjF69GheeeUVunfvzvjx4+ndu/e/jleiRAkWLlzICy+8wFNPPcWrr77KAw88\nQJs2bbjzzjv55JNPGDFiRJqvacGCBYwcOZK5c+eiqjRr1oyWLVuydu1aypQpw0cffQTAzp072bZt\nGxMnTmTlypWIyDGbsrJa7JwRAJQpA+edByNHeqexc9lc06ZNjxorP2zYMOrXr0/z5s3ZsGEDq1at\n+tdjKleuTIMGDQBo3Lgx69evT/G5L7jggn/tM2vWLHr06AFAhw4dKFasWJrxzZo1i27dulGgQAEK\nFizIBRdcwMyZM6lbty6fffYZAwcOZObMmRQpUoQiRYqQL18++vbty4QJEzjhhBMy+nYcl9g6IwC4\n+mqYNAkmToRLLol0NM5lO2n9cg+nAgUKHL4+ffp0Pv/8c2bPns0JJ5xAq1atUpwFnTdv3sPX4+Li\nDjcNpbZfXFzcMfsgMqp69eosXLiQKVOmcM8993D22WczePBgvv32W7744gvGjRvHc889x5dffpml\nx01LbJ0RALRvD5Uqeaexc9lIoUKF2L17d6r379y5k2LFinHCCSewcuVK5syZk+UxtGjRgrFjxwLw\n6aefsn379jT3P/PMM5k0aRJ79+7lzz//ZOLEiZx55pn88ssvnHDCCfTu3ZsBAwawcOFC9uzZw86d\nO+nUqRNPP/00S5YsyfL40xJ7ZwS5csFVV8Hdd8MPP0CNGpGOyDl3DMWLF6dFixbUqVOHjh07cu65\n5x51f4cOHXjppZeoWbMmNWrUoHnz5lkew3333UfPnj0ZNWoUp512GieffDKFChVKdf9GjRrRp08f\nmjZtCkC/fv1o2LAhU6dOZcCAAeTKlYv4+HhefPFFdu/eTdeuXfnrr79QVYYOHZrl8acl261ZnJCQ\noMe9MM3mzTaU9Kqr4LnnsiYw53Kw77//npo1a0Y6jIj6+++/iYuLI3fu3MyePZtrr732cOd1tEnp\n8xKRBaqakNL+sXdGAHDSSdCzp3UaP/ggHKPTxznnfv75Z7p3786hQ4fIkycPr7zySqRDyjKxmQjA\nxsC98QaMGAFhHrPrnMt+qlWrxqJFiyIdRkjEXmdxogYNoFUrePZZyOJRAc45l53EbiIAOyv4+Wcb\nTuqcczEqthNB585QpUr0DIx2zrkIiO1EEBcHN90EX38N8+ZFOhrnnIuI2E4EAFdcAYUK+VmBczlM\nwYIFAfjll1+46KKLUtynVatWHGs4+jPPPMPevXsP305PWev0uP/++3nqqaeO+3mygieCwoWtGN3Y\nsbBpU6Sjcc5lsTJlyhyuLJoZyRNBespaZzeeCMCahw4dgkwufOGcC61Bgwbx/PPPH76d+Gt6z549\nnH322YdLRr///vv/euz69eupU6cOAPv27aNHjx7UrFmTbt26HVVr6NprryUhIYHatWtz3333AVbI\n7pdffqF169a0bt0aOHrhmZTKTKdV7jo1ixcvpnnz5tSrV49u3bodLl8xbNiww6WpEwveffXVVzRo\n0IAGDRrQsGHDNEtvpJuqZqtL48aNNSS6dVMtVkx1167QPL9z2diKFSuO3Lj5ZtWWLbP2cvPNaR5/\n4cKFetZZZx2+XbNmTf355591//79unPnTlVV3bp1q55yyil66NAhVVUtUKCAqqquW7dOa9euraqq\n//vf//SKK65QVdUlS5ZoXFyczps3T1VVt23bpqqqBw4c0JYtW+qSJUtUVbVixYq6devWw8dOvD1/\n/nytU6eO7tmzR3fv3q21atXShQsX6rp16zQuLk4XLVqkqqoXX3yxjho16l+v6b777tMnn3xSVVXr\n1q2r06dPV1XVe++9V28O3o/SpUvrX3/9paqq27dvV1XVzp0766xZs1RVdffu3bp///5/PfdRn1cA\nmK+pfK/6GUGiQYNg+3Z46aVIR+KcS6Zhw4Zs2bKFX375hSVLllCsWDHKly+PqnLXXXdRr1492rZt\ny6ZNm9i8eXOqzzNjxozD6w/Uq1ePevXqHb5v7NixNGrUiIYNG7J8+XJWrFiRZkyplZmG9Je7BiuY\nt2PHDlq2bAnA5ZdfzowZMw7H2KtXL9566y1y57b5vy1atODWW29l2LBh7Nix4/D24xFTM4t377Z+\n4RQ1bQpt28L//gc33AD584c1NueyjQgNrLj44osZN24cv/32G5cEJeTffvtttm7dyoIFC4iPj6dS\npUoplp8+lnXr1vHUU08xb948ihUrRp8+fTL1PInSW+76WD766CNmzJjBBx98wMMPP8zSpUsZNGgQ\n5557LlOmTKFFixZMnTqVU089NdOxQgz1Ebz3HpQtC2kkZqtIunmz1SByzkWVSy65hDFjxjBu3Dgu\nvvhiwH5NlypVivj4eKZNm8ZPP/2U5nOcddZZvPPOOwAsW7aM7777DoBdu3ZRoEABihQpwubNm/n4\n448PPya1EtiplZnOqCJFilCsWLHDZxOjRo2iZcuWHDp0iA0bNtC6dWsef/xxdu7cyZ49e1izZg11\n69Zl4MCBNGnS5PBSmscjZs4ITjvNFiUbMgReey2VnVq2hNNPhyeesMqk8fFhjdE5l7ratWuze/du\nypYtS+nSpQHo1asX5513HnXr1iUhIeGYv4yvvfZarrjiCmrWrEnNmjVp3LgxAPXr16dhw4aceuqp\nlC9fnhYtWhx+TP/+/enQoQNlypRh2rRph7enVmY6rWag1Lzxxhtcc8017N27lypVqjBy5EgOHjxI\n79692blzJ6rKTTfdRNGiRbn33nuZNm0auXLlonbt2nTs2DHDx0suZGWoReQ1oDOwRVXrpLFfE2A2\n0ENVjznG63jKUN96K/zf/8H330P16qns9NFHNuN45Ejo0ydTx3Eup/Ey1NlLRstQh7Jp6HWgQ1o7\niEgc8DjwaQjjOGzQIMiXD+6/P42dOnWygnSPPOLF6JxzMSFkiUBVZwB/HGO3G4HxwJZQxZFUqVJw\n880wZgwsXZrKTiKWKVatgtdfD0dYzjkXURHrLBaRskA34MV07NtfROaLyPytW7ce13Fvv91GDg0e\nnMZOXbpA8+bwwAOQyd5+53KaUDUju6yVmc8pkqOGngEGquqhY+2oqsNVNUFVE0qWLHlcBz3xREsG\nkybB3Lmp7CQCjz4KGzf6bGPngHz58rFt2zZPBlFOVdm2bRv58uXL0ONCumaxiFQCPkyps1hE1gES\n3CwB7AX6q2qaiwNkxZrFu3dbZ3GVKjBrln3vp6hDB5g/H9asgSJFjuuYzmVn+/fvZ+PGjcc1tt6F\nR758+ShXrhzxyUY9RuWaxapaOfG6iLyOJYywrBBTqBA89BD062fzC7p3T2XHRx6Bxo1tktmQIeEI\nzbmoFB8fT+XKlY+9o8uWQtY0JCKjsWGhNURko4j0FZFrROSaUB0zI/r0gfr1YeBASPVHTqNGliWG\nDoUtYenPds65sAvlqKGeqlpaVeNVtZyqjlDVl1T1X8V8VLVPeuYQZKW4OPuhv369zS1I1YMPWqZ4\n+OFwheacc2EVMyUmUnL22XDeefYdn2qdqurV4corrRhdJmYMOudctIvpRADw1FP2g3/gwDR2GjwY\ncuWyGWnOOZfDxHwiqF4dbrsN3njDRhClqFw5yxTvvgvTp4czPOecC7mQDh8NhawYPprcn39CzZpQ\nrBgsWAAplvfet892KlwYFi5MZSfnnItOkao1lG0UKGAl1r/7DpKshne0/Plt9NDSpfDiMSdDO+dc\ntuFnBAFVqzf39dfwww8QVLn9907t29sksx9/hOOc5eycc+HiZwTpIALDhtmaBTfeeIyd9uyBu+4K\na3zOORcqngiSqFbNCo+OH2+XFNWsCTfdBCNGwLx54QzPOedCwpuGkjlwAJo1g02bYMUKK1L3L7t2\n2XCjihVh9mwbWuqcc1HMm4YyIHduW8py2zZb0SxFhQvDk0/Ct9/Cc8+FNT7nnMtqnghSUL++zR17\n4w2YMiWVnXr3ho4d4c47rTqpc85lU54IUnHPPVC3rlWXSLHenAgMH26nEH37wqFjLqvgnHNRyRNB\nKvLmhXfegR074IorbOTov5QrZ5XrvvrKahE551w25IkgDXXqWC2iKVPSmGjWty+0awd33OFF6Zxz\n2ZIngmO4/nqbaHb77akseC8Cr75q//brl8qpg3PORS9PBMcgAiNHQtGicPHFtszlv1SoYKOIvvgC\nXnkl7DE659zx8ESQDqVKwZgxsGoV9O+fyo/+/v2hdWs7dfj557DH6JxzmeWJIJ1atbJ1jseMgRde\nSGGHXLlstvGhQ2lkC+eciz6eCDJg4EA491z4739h7twUdqhcGR57DKZO9QqlzrlswxNBBuTKBW++\nCWXLQrdu8MsvKex03XXQoYOtdrNsWdhjdM65jPJEkEEnngjvv2/lhs4/39arOUquXPD661CkCPTo\nkcIOzjkXXTwRZEK9evDWW1Z8tG/fFLoDTjrJ6lMsX26dx845F8U8EWTS+efDww/D6NHWLfAv55xj\nVeteeMHWOnbOuSjlieA43Hkn9OwJd98NkyensMOjj0KLFlajYvHisMfnnHPp4YngOIjYiNHGjaFX\nL1vB8ih58tgKN8WLQ9euqVSvc865yApZIhCR10Rki4ikOHRGRHqJyHcislREvhGR+qGKJZTy57fO\n4+LFrSr1Dz8k2+Gkk2DSJEsCF11ka2E651wUCeUZwetAhzTuXwe0VNW6wIPA8BDGElJlysBnn9kZ\nQrt2sGFDsh0aN7ZTh5kz4ZZbIhKjc86lJmSJQFVnAH+kcf83qro9uDkHKBeqWMKhWjWbR7ZzJ7Rv\nD7//nmyHSy+1CqUvvggvvxyRGJ1zLiXR0kfQF/g4tTtFpL+IzBeR+Vu3bg1jWBnTsKF1Gq9fbxVL\n/1Wg7pFHbLLZDTfY2YFzzkWBiCcCEWmNJYKBqe2jqsNVNUFVE0qWLBm+4DKhZUsbLbpwIVxwAfz9\nd5I74+JsvGnlynDhhV6czjkXFSKaCESkHvAq0FVVt0UylqzUpQu89hp8/rktbXzwYJI7ixa104a/\n/7bJCHv3RixO55yDCCYCEakATAD+o6o/RiqOULnsMhg6FMaNg//8Bw4cSHLnqafaOpiLF9scA1/v\n2DkXQblD9cQiMhpoBZQQkY3AfUA8gKq+BAwGigMviAjAAVVNCFU8kfDf/8L+/Va1dP9+++6Pjw/u\nPPdcm5I8cOCRtY+dcy4CQpYIVLXnMe7vB/QL1fGjxR132LyyxKTw7ruQN29w54ABsHGjnTqULu11\niZxzERGyROCOuOUWOxO44QbrQB4/HvLlwyYePPMMbN5sSeGkk6wdyTnnwsgTQZhcf72dGVx9NZx3\nHkyYAIUKcWSRg99/hyuvhJIlbYipc86FScSHj8aSq66ypQqmTbOlLzdvDu7ImxcmToS6dW1YaYrL\nnznnXGh4Igizyy6z0aMrV8Lpp8Pq1cEdhQvDlCnWV9Chg1crdc6FjSeCCOjUCb780spRtGgBCxYE\nd5x8MnzxhbUZtWsHK1ZENE7nXGzwRBAhzZrBN9/ACSfYbOQpU4I7Kla0LBEfD23bJjllcM650PBE\nEEHVq1syqF7dOpCffjpY9rJqVZuWvH8/tG4Na9ZEOlTnXA7miSDCSpe2+nPdutnKllddFSxZUKuW\nNRPt3WvJYN26SIfqnMuhPBFEgQIFYOxYuOceW7agXbugjHW9epYM9uyxZPDTT5EO1TmXA3kiiBK5\ncsGDD8Lbb9vo0aZNYdEioEEDayZK7FlevjzSoTrnchhPBFHm0kthxgzrHjjtNDtDoFEj+OorK053\nxhnw9deRDtM5l4N4IohCTZvaegZnngn9+tmE433V6lnPcqlSNppo8uRIh+mcyyE8EUSpkiXhk0/g\n3nth5Eg7O1h9oBLMmmV9B926BacLzjl3fDwRRLG4OBgyxOYYbNhgS2GO+qSkdSC3a2enCw8/HIw5\ndc65zPFEkA107GgVJxo1shIVva8pyK63P7Dlz+65B266KdkyaM45l36eCLKJ8uVtwvGQITBmDDRo\nEs+ca9+A226D556Dnj2TLZDsnHPp44kgG4mLsz6DGTOCAURn5eL+gk9x8LEn4b33rIjRrl2RDtM5\nl814IsiGTj/dmop69oQHHoBm793OpkfftAxx5pk+8cw5lyGeCLKpokVh1CgYNw7Wr4dT7v8P4/t+\nhP70EzRpYqOLnHMuHTwRZHMXXmiTjc85By56uT29q87lnxOKQps28NprkQ7POZcNeCLIAU46CSZN\nsvkGU9bUoMKvc1lboSX07WudyT6iyDmXBk8EOYQI9OkD338PLc8vRvU1H/N28Ztg6FDo3NlqFTnn\nXAo8EeQwJ58M774LE97PzcB8/8fVvMzBTz/nUJNmtj6mc84l44kgh+rSxVa6zH1df9oc+pw/1vzB\n/kZN0UnvRzo051yU8USQgxUuDM8/D4/MaknPagtYvK8G0u18/rhhsPcbOOcOS1ciEJFTRCRvcL2V\niNwkIkWP8ZjXRGSLiCxL5X4RkWEislpEvhORRhkP36VHixbw8bLyzHliJqNyX8GJzz/I+ipt+GvV\nhkiH5pyLAuk9IxgPHBSRqsBwoDzwzjEe8zrQIY37OwLVgkt/4MV0xuIyIXduuHFAPtr+/BovnvYm\nJX5ewL5TG/Dt3d5U5FysS28iOKSqB4BuwLOqOgAondYDVHUG8Ecau3QF3lQzBygqImk+pzt+pUvD\ntd/8h+WjFvFLnko0feR8Pqh8I+tX/hXp0JxzEZLeRLBfRHoClwMfBtvij/PYZYGkbRMbg23/IiL9\nRWS+iMzfunXrcR7WATTrXY3qW79h/lm3ct7659hVqxnP3/A9e/dGOjLnXLilNxFcAZwGPKyq60Sk\nMjAqdGEdTVWHq2qCqiaULFkyXIfN8eIL5iXhq//x+xsfUTHPr/R5PoHB5Ubw1ijl0KFIR+ecC5d0\nJQJVXaGqN6nqaBEpBhRS1ceP89ibsL6GROWCbS7MSlzWiSLrlvBP49N4ans/cl/WkzaNdzJzZqQj\nc86FQ3pHDU0XkcIiciKwEHhFRIYe57EnA5cFo4eaAztV9dfjfE6XWaVLU2zuVA49/Ajdc43jzaUN\nGHDWHC68EFavjnRwzrlQSm/TUBFV3QVcgHXwNgPapvUAERkNzAZqiMhGEekrIteIyDXBLlOAtcBq\n4BXguky9Apd14uLIdded5Jo1k3JllW9ynUHtDx6jds1D3HYbbN8e6QCdc6Egmo71bkVkKdAeeAO4\nW1Xnich3qlov1AEml5CQoPPnzw/3YWPPjh1w9dUwdiwrypzNOb+8zp6i5Rg40FbGPOGESAfonMsI\nEVmgqgkp3ZfeM4IhwFRgTZAEqgCrsipAF4WKFrU1MV95hVo7ZrOuUF3urvQ2d96pVK0KL74I+/dH\nOkjnXFZIb2fxe6paT1WvDW6vVdULQxuaizgR6NcPFi8md52a3L64N1tbXkyjCr9z3XVw6qnw9tv4\nCCPnsrn0dhaXE5GJQcmILQirab0AAB1tSURBVCIyXkTKhTo4FyWqVYOZM+HRRynxzWQ+WF+HeYM/\noFAh6N0bGjSADz+EdLQyOueiUHqbhkZio3zKBJcPgm0uVsTFwaBBMH8+ctJJJAzpwqKGV/LeiF3s\n3QvnnQdnnGHLJjvnspf0JoKSqjpSVQ8El9cBn9kVi+rVg3nz4K67kDff4KIh9Vj54jReesnWTm7Z\nEjp2hEWLIh2ocy690psItolIbxGJCy69gW2hDMxFsTx54OGH4euvIU8ecrdvw9ULr2b1gp088QTM\nnQuNGkGPHrDKhxQ4F/XSmwiuBLoDvwG/AhcBfUIUk8sumjeHxYvh9tvh1VfJ37gWA079gLVr4e67\n4YMPoGZN6N8fNm6MdLDOudSkd9TQT6raRVVLqmopVT0f8FFDziYUPPkkzJkDxYtDly4UvbYnD920\nhbVr4brr4PXXoWpVyxfb/DzSuahzPCuU3ZplUbjsr0kTmD8fhgyB8eOhVi1O+vxthv2f8uOP1kw0\ndChUqQIPPgh79kQ6YOdcouNJBJJlUbicIU8euPdeay6qVs3GlnbuTKW4Dbz+OixdCm3awODBlhCG\nDsXLXjsXBY4nEfiocZeyWrVg1ix45hmYPt1uv/ACtWseYuJEa0WqVw9uuw0qV7aWJT9DcC5y0kwE\nIrJbRHalcNmNzSdwLmVxcXDzzbBsmXUqX389nH46LF5Ms2bw+ec2R61BA7jjDqhUCR59FHbtinTg\nzsWeNBOBqhZS1cIpXAqpau5wBemyscqV4dNP4a23YN06aNwYbr0Vdu/mjDNg6lSYPRuaNoW77rKE\n8NBDsHNnpAN3LnYcT9OQc+kjAr16wcqVNpb0mWdsXOmECaBK8+YwZQp8+63NTr73XqhYEe6/30tf\nOxcOnghc+BQrZmVLv/kGSpSACy+02hTr1wM28GjyZFi40DqVH3jAzhDuvdeHnToXSp4IXPg1b25D\nTYcOPdKZ/Oij8PffADRsaCcLixdD+/bWVFSpEtx5J2zdGtHIncuRPBG4yMidG/77X/j+eytOdNdd\nULeudRoE6teH996zYaedO8Pjj1tCuPVWn6nsXFbyROAiq3x5m4D2ySd2u0MHuOAC+Omnw7vUqQOj\nR8OKFdaaNGyYzUPo1w9+/DFCcTuXg3gicNHhnHPsp/8jj9hZQc2a1ib011+Hdzn1VHjzTVi9Gq66\nyhbFOfVU6N7dq506dzw8EbjokTevdQR8/z106mS9xLVrw/vvH7XqTaVK8Pzz1sc8cKDljUaN7GTi\nq698gRznMsoTgYs+FSrAuHE2/yBfPjj/fGjXzianJXHSSdbH/PPPdiKxaBG0agUtWljlU08IzqWP\nJwIXvdq1s6FDw4bZmNL69eGGG/41lrRIETuRWL8ennsOfvkFunSxMhbvvAMHDkQmfOeyC08ELrrF\nx8ONN9oKN9dea/MQqlWDZ5+F/fuP2jV/fqtksWqV9SUcOmTz2GrUgJdeOqq7wTmXhCcClz0UL24/\n9xcvtg6Bm26yQkWfffavXePj4T//sb7n99+HkiUth1SuDE884fWMnEvOE4HLXurWtS//SZPsJ377\n9tC1qw0lSiZXLmsimj0bvvzSHjpwoHVBDBzocxGcSxTSRCAiHUTkBxFZLSKDUri/gohME5FFIvKd\niHQKZTwuhxCxL/8VK+Cxx+xbvlYt+3ZP4ee+CLRubX3P8+fbSNWnnrIzhF69YMGCCLwG56JIyBKB\niMQBzwMdgVpATxGplWy3e4CxqtoQ6AG8EKp4XA6UN699+f/4oy2C88QT1n/w8sup9hA3bgzvvgtr\n1ljr0gcfQEKCjTaaPNn6FZyLNaE8I2gKrFbVtar6DzAG6JpsHwUKB9eLAL+EMB6XU5UuDa+9ZuVL\nq1eHa66xIUMffZTqGNJKleB//7PmoaFDbcRR1642Qe2FF+DPP8P6CpyLqFAmgrLAhiS3Nwbbkrof\n6C0iG4EpwI0pPZGI9BeR+SIyf6tXHXOpadIEZsyAiRPtjKBzZ2jbNs1px4ULW8mj1avtTKFYMRt5\nVKEC3H23DUV1LqeLdGdxT+B1VS0HdAJGici/YlLV4aqaoKoJJUuWDHuQLhsRsQloy5fbENMlS6w9\n6PLLYcOGVB+WO7eVqpgzB77+2voUHnvMzhwuv9wGKzmXU4UyEWwCyie5XS7YllRfYCyAqs4G8gEl\nQhiTixXx8Tb5bM0aWwvz3Xet2eiuu9IcPypiK2qOG3dk6sL48VYa++yzrbXJ+xFcThPKRDAPqCYi\nlUUkD9YZPDnZPj8DZwOISE0sEXjbj8s6RYrYT/sffrDSpY8+ClWrWkdAsglpyVWpAv/3f9aP8MQT\n1ifdubOVPxo+HPbtC9NrcC7EQpYIVPUAcAMwFfgeGx20XESGiEiXYLfbgKtEZAkwGuij6hViXAhU\nrGjrJs+bZ0NNr7/e6luPH3/MokRFi8KAAbB2rZWsKFAArr7a+hEGD4bffgvTa3AuRCS7fe8mJCTo\n/PnzIx2Gy85UbdzonXfaXISmTW3Vm1at0v3wmTNttNHkydYK1auXLZhTp05oQ3cus0RkgaompHRf\npDuLnQs/EZtyvGQJjBhhQ4Nat7bS10uWpOvhZ51lk5t/+MEWyBkzxmYun3OOlcXOZr+vXIzzROBi\nV+7ccOWV1vj/xBNWi6JhQ7jsMptYkA7VqtnaCBs3WinspUttXYS6dS3HeD+Cyw48ETiXP/+RToA7\n7rCFkmvUsAkGv/+erqc48cQjpbDffNNyTL9+thLnoEG2ZoJz0coTgXOJihWzEUarVln50mHDrCDR\n4MGwY0e6niJPHnvookUwbRq0bAlPPmlPc8EFts2bjVy08UTgXHLlysGrr9qKaB07woMP2jf5Qw/B\n7t3pegoR63sePx7WrbMTjRkzoE0bq37x8stexsJFD08EzqWmZk0YO9Z+3p91lq2hnLioQQa+xStU\nsOkLGzZYSaT4eCuHVK4c3HabtUg5F0meCJw7lgYNbIWbb7+1ekYDB8Ipp8Azz2Ro2bP8+eGKK6zs\n9axZNsJo2DCb39a5s81aPngwhK/DuVR4InAuvZo0gY8/tm/x2rWtM7lqVVs+859/0v00ItCihQ05\nXb8e7rnHkkPnzpZfHnkENm8O3ctwLjlPBM5lVIsW8MUXtiBOpUpw3XVWx2jEiGOWrUiubFkYMsRG\nFb33niWCu++20UaXXALTp3vnsgs9TwTOZVbr1jbF+JNPoFQpGy9aq5aVsshgG098PFx0keWXlSut\nXt5nn9khatWymkfbt4fodbiY54nAueMhYo39c+davYmCBW38aN261tGciVKlNWpY+YpNm+CNN6zW\n0S232NnDlVdaV4WfJbis5InAuawgAuedZ43948bZ7UsusZnKkyZl6ps7f36b5Dx7tg1cuuwyyy3N\nmtnymq+8Anv2hOC1uJjjicC5rJQrl5W7/u47ePttqzHRrduRjuZM/pRv0ABeesnKIiVW0O7f31bp\nvOoqW1DHzxJcZnkicC4U4uLg0kutuunIkfDHH1bU7rTTrPJpJr+1Cxe2xXKWLLGV1Lp3h9Gj7Wnr\n1LEmJV/N1WWUJwLnQil3bujTx3qAX37ZxoV26WI/8d99N9MTBxJXUhsxAn791ZqJChe2CWply8LF\nF1sfts9LcOnhicC5cMiTx9pyfvzReoD/+Qd69LAhQSNHZnjYaVKFCtmApdmzrSrGDTfYsNOOHY+U\nSlq3Luteist5PBE4F07x8dbru3y5TRwoUMCGAlWtCs89d9x1q2vXPjLi6L337PZDD9mym23b2iS2\nDEyGdjHCE4FzkZArl00cWLAApkyxGWQ33nikllE6i9ulJk8ee/qPP4affrJJa2vWQM+eUKYM3HRT\nutbgcTHCE4FzkSRibTgzZ1p7Tr16VsuoYkW4/37Ytu24D1G+vNXLW7PGJqmdc451VzRoYMNQn3su\n3csuuBzKE4Fz0UDEFi/49FObMXbWWfDAA5YQBgyA33477kPkymXNQ6NHWwfzsGFw4ICdiJQuDV27\n2hQIbzqKPZ4InIs2TZrYJLTvvrNv56FDrabR9dfbz/oscOKJlgAWL7YmoltugXnzbLRR6dJw9dVW\nW8/nJsQG0Wz2SSckJOj8+fMjHYZz4bN6NTz+uI02OnjQljq7/XabYpyFDh60WkejRsGECbB3r3Uy\n9+5tVTOqVs3Sw7kwE5EFqpqQ0n1+RuBctKta1SYKrF9vS5199hk0b27NRx98kKl6RimJi4P27S0R\nbN5seadKFVugrVo1m7fw4otZ0m3hoownAueyizJljix19vTTNhyoSxcbI/rqq1nauF+woI1y/ewz\nK5H9+OOwa5dV3D75ZFs74e23vdZRThHSRCAiHUTkBxFZLSKDUtmnu4isEJHlIvJOKONxLkcoVMga\n9Vevhnfesep0V11l/QgPP5zlQ4DKlbMTkaVLYeFCO/SSJdZkVKqUlbmYONE7mbOzkPURiEgc8CPQ\nDtgIzAN6quqKJPtUA8YCbVR1u4iUUtUtaT2v9xE4l4wqTJsGTz5pdSXy5YNevaw3uH79kBzy0CH4\n5hsbgfTee1bfqHBh677o0QPOPtuqa7joEak+gqbAalVdq6r/AGOArsn2uQp4XlW3AxwrCTjnUiAC\nbdrY7LHly6220ejRNlGgVSvr+T1wIEsPmSsXnHEGPP+8VUSdOtWSwIQJ0KGDtWJdfz189ZXXO8oO\nQpkIygIbktzeGGxLqjpQXUS+FpE5ItIhpScSkf4iMl9E5m/10orOpa5WLevR3bgRnnrK+hEuvNDW\nwHziCauCmsVy57ZO5pEjrZN54kRbWW3kSMtDZcrANddYf8NxlFRyIRTpzuLcQDWgFdATeEVEiibf\nSVWHq2qCqiaULFkyzCE6lw0VK2alSFevtjkJVavajOVy5az43dKlITlsvnxw/vlWWHXLFltIp3Vr\nW72zfXvraL7ySquq8fffIQnBZUIoE8EmoHyS2+WCbUltBCar6n5VXYf1KVQLYUzOxZa4OJuU9sUX\nNkGtd2/7Vq5Xz5qTJk0KWdtNwYI2QW3MGOtDmDTJlmQYPx7OPdc6mnv3tu3HWWvPHadQJoJ5QDUR\nqSwieYAewORk+0zCzgYQkRJYU9HaEMbkXOyqWxeGD7fhp48/brOUu3Wzs4WnnoLt20N26Pz5LR+N\nGmVnCh99dKQoXrduULKkrew5dqwPSY2EkCUCVT0A3ABMBb4HxqrqchEZIiJdgt2mAttEZAUwDRig\nqj5dxblQKl7cxoOuWWM/zxPrGSU2Gy1cGNLD581rZwYjRlgJpc8+szOD6dMtGZQsacs/v/qq9Tm4\n0PMSE845mxjw7LM2L2HfPqt3dO219s18wglhCeHgQatvNGECvP++9XOL2CTqrl3tcuqpYQklR0pr\n+KgnAufcETt2WPvNSy/ZestFisDll9uwn5o1wxaGqnVpvP++XRJPUqpXt87orl2t1FJcXNhCyvY8\nETjnMkbV1kh46SWrTb1/v5XJ7tfPhqPmzx/WcDZsgMmTLSlMm2bTIkqVsk7njh2tvHaxYmENKdvx\nROCcy7wtW2xSwPDhsHatnSX06gV9+0KjRmEPZ+dO62R+/32bSL1jh01wa97cJrN16ACNG9s2d4Qn\nAufc8Tt0yKYKjxhhZwl//22zl/v2tcQQgZ/kBw7YOgqffGKXefPsZKZECZu30KGD/XvSSWEPLep4\nInDOZa3t261jecQIWLTIhgJdeKElhVatIvZz/PffbRRSYmLYEhStadTIksI558Bpp0F8fETCiyhP\nBM650Fm0yBLCW29Zu02FCkdWs4ngMJ9Dh2wwVGJS+PprG5lUuLAVxUtsRqpQIWIhhpUnAudc6O3b\nZ4WGRo2ytZcPHbJhqJddZiVJS5SIaHg7d8KXX1pS+Phj64AGGwyVmBTOOsvKZOREngicc+H1669W\nAfXNN+1nee7cNrznsstsVZsIf9uqwsqVR84WvvrKujzy57eWrcS+hRo1bC5DTuCJwDkXOd99Z2cJ\nb79tCaJIEZuo9p//QIsWUfFNu3evJYPExPDjj7a9dGlLDK1b279Vq0ZFuJniicA5F3kHD1rxu1Gj\nbPrw3r1QuTJceqk1HdWpE+kID1u71kKdNs0uv/1m28uWtaSQeKlcObJxZoQnAudcdNmzx5LBqFHW\ncH/okK2lcMkldqlRI9IRHqZqZwiJSWH69COjkSpWPHLG0Lp1dHc8eyJwzkWvzZut+N2YMVZsSNWW\n2ExMClWqRDrCo6ha9Y3EpDB9OmwLSmWWL2+tXWecYf/WrRs9ZTA8ETjnsodNm2wR5HffhTlzbFuT\nJpYQune3b9ooc+gQLFtmfQxff225bFOw8kqhQjZvITExNGsGBQpEJk5PBM657Oenn2yBgjFjjlSd\na9rUFjC44AKrQBeFVOHnny0hJCaGZctse1wcNGx4JDG0aGEd0uHgicA5l72tXm1JYeJESPz/X6uW\nJYRu3ezbNYqH8+zYAbNnH0kM3357ZFW2U0450px0+uk2ryEUE7M9ETjnco4NG2x9ywkTYMYMa5up\nWNESQrdu9q0aLQ3zqfjnH5uQnZgYvv76SAd04cLWGta8uV2aNbPFeo6XJwLnXM70++/wwQeWFD77\nzGaFlSwJXbrYMmdt20auUT4DVO2kZ/Zs6xqZM8emXyQuJ12liiWEnj3tZWWGJwLnXM63e7fVjpg4\nEaZMgV27rBhe69Y2m/ncc6FSpUhHmW5798KCBZYU5s61JHHddXD33Zl7Pk8EzrnYsn+/tbl8+KGd\nMaxaZdvr1LGk0LmztbtEeRNScgcOWLWOzPBE4JyLbT/+aEnhww9t5bUDB6B4cat/1KkTtGsX8aJ4\noeaJwDnnEu3YYdVRP/zQmpC2bbMRR40b24IF7dvnyEULPBE451xKDh60hvipU+0yZ45tK1QI2rQ5\nkhhOOSXSkR43TwTOOZceO3ZY7YjExLB+vW0/5RRLCmefDS1bWrNSNuOJwDnnMkrVOpmnTrWmpGnT\n4M8/rRmpfv0jlebOOstKa0c5TwTOOXe8/vkH5s2zaqnTpsE339i8hVy5rH+hdWtrTjrjjKicuxCx\nRCAiHYD/A+KAV1X1sVT2uxAYBzRR1TS/5T0ROOeiwl9/2eD+xPrUc+fasNXcua0mUuJqNs2bQ8GC\nkY42MolAROKAH4F2wEZgHtBTVVck268Q8BGQB7jBE4FzLlv680+rFZGYGObNs/IXiZXmzjzTzhbO\nOANKlQp7eGklgkxOTUiXpsBqVV0bBDEG6AqsSLbfg8DjwIAQxuKcc6FVoICNMGrf3m7v2mVnDDNn\n2uS2F1+Ep5+2+6pXt8SQmByqVIlo0bxQJoKywIYktzcCzZLuICKNgPKq+pGIpJoIRKQ/0B+gQjQv\nAeScc4kKF7aRRuecY7f//tvKaScmhgkTYMQIu690aZu7cNpp1pTUuDHkzx+2UEOZCNIkIrmAoUCf\nY+2rqsOB4WBNQ6GNzDnnQiBv3iNf9nfcYc1G339/JDHMmWPJAayfoX79I4mhefOQnjWEMhFsApIu\nJ1Qu2JaoEFAHmC724k4GJotIl2P1EzjnXLaXKxfUrm2Xa66xbVu2WKfznDnWrDRyJDz3nN1XsiQM\nGgS33prloYQyEcwDqolIZSwB9AAuTbxTVXcCh4t7iMh04HZPAs65mFWqlNWZTqw1ffAgLF9+JDGU\nKROSw4YsEajqARG5AZiKDR99TVWXi8gQYL6qTg7VsZ1zLkeIi4N69ezSv3/IDhPSPgJVnQJMSbZt\ncCr7tgplLM4551IWgpUxnXPOZSeeCJxzLsZ5InDOuRjnicA552KcJwLnnItxngiccy7GeSJwzrkY\nl+0WphGRrcBPmXhoCeD3LA4nK3hcGRetsXlcGROtcUH0xnY8cVVU1ZIp3ZHtEkFmicj81GpxR5LH\nlXHRGpvHlTHRGhdEb2yhisubhpxzLsZ5InDOuRgXS4lgeKQDSIXHlXHRGpvHlTHRGhdEb2whiStm\n+gicc86lLJbOCJxzzqXAE4FzzsW4HJ8IRKSDiPwgIqtFZFCEYykvItNEZIWILBeRm4Pt94vIJhFZ\nHFw6RSC29SKyNDj+/GDbiSLymYisCv4tFuaYaiR5TxaLyC4RuSVS75eIvCYiW0RkWZJtKb5HYoYF\nf3ffiUijMMf1pIisDI49UUSKBtsrici+JO/dS2GOK9XPTkTuDN6vH0TknDDH9W6SmNaLyOJgezjf\nr9S+H0L/N6aqOfaCrYy2BqgC5AGWALUiGE9poFFwvRDwI1ALuB9bpjOS79V6oESybU8Ag4Lrg4DH\nI/xZ/gZUjNT7BZwFNAKWHes9AjoBHwMCNAfmhjmu9kDu4PrjSeKqlHS/CLxfKX52wf+DJUBeoHLw\n/zYuXHElu/9/wOAIvF+pfT+E/G8sp58RNAVWq+paVf0HGAN0jVQwqvqrqi4Mru8GvgfKRiqedOgK\nvBFcfwM4P4KxnA2sUdXMzCrPEqo6A/gj2ebU3qOuwJtq5gBFRaR0uOJS1U9V9UBwcw5QLhTHzmhc\naegKjFHVv1V1HbAa+/8b1rhERIDuwOhQHDstaXw/hPxvLKcngrLAhiS3NxIlX7wiUgloCMwNNt0Q\nnN69Fu4mmIACn4rIAhFJXBz1JFX9Nbj+G3BSBOJK1IOj/3NG+v1KlNp7FE1/e1divxwTVRaRRSLy\nlYicGYF4UvrsouX9OhPYrKqrkmwL+/uV7Psh5H9jOT0RRCURKQiMB25R1V3Ai8ApQAPgV+zUNNzO\nUNVGQEfgehE5K+mdaueiERlrLCJ5gC7Ae8GmaHi//iWS71FqRORu4ADwdrDpV6CCqjYEbgXeEZHC\nYQwpKj+7JHpy9A+OsL9fKXw/HBaqv7Gcngg2AeWT3C4XbIsYEYnHPuS3VXUCgKpuVtWDqnoIeIUQ\nnRKnRVU3Bf9uASYGMWxOPNUM/t0S7rgCHYGFqro5iDHi71cSqb1HEf/bE5E+QGegV/AFQtD0si24\nvgBri68erpjS+Oyi4f3KDVwAvJu4LdzvV0rfD4ThbyynJ4J5QDURqRz8quwBTI5UMEH74wjge1Ud\nmmR70na9bsCy5I8NcVwFRKRQ4nWso3EZ9l5dHux2OfB+OONK4qhfaZF+v5JJ7T2aDFwWjOxoDuxM\ncnofciLSAbgD6KKqe5NsLykiccH1KkA1YG0Y40rts5sM9BCRvCJSOYjr23DFFWgLrFTVjYkbwvl+\npfb9QDj+xsLRGx7JC9az/iOWye+OcCxnYKd13wGLg0snYBSwNNg+GSgd5riqYCM2lgDLE98noDjw\nBbAK+Bw4MQLvWQFgG1AkybaIvF9YMvoV2I+1x/ZN7T3CRnI8H/zdLQUSwhzXaqz9OPHv7KVg3wuD\nz3gxsBA4L8xxpfrZAXcH79cPQMdwxhVsfx24Jtm+4Xy/Uvt+CPnfmJeYcM65GJfTm4acc84dgycC\n55yLcZ4InHMuxnkicM65GOeJwDnnYpwnAucCInJQjq52mmXVaoMqlpGc7+BcqnJHOgDnosg+VW0Q\n6SCcCzc/I3DuGIL69E+IrdfwrYhUDbZXEpEvgwJqX4hIhWD7SWJrACwJLqcHTxUnIq8EteY/FZH8\nwf43BTXovxORMRF6mS6GeSJw7oj8yZqGLkly305VrQs8BzwTbHsWeENV62FF3YYF24cBX6lqfazu\n/fJgezXgeVWtDezAZq2C1ZhvGDzPNaF6cc6lxmcWOxcQkT2qWjCF7euBNqq6NigK9puqFheR37ES\nCfuD7b+qagkR2QqUU9W/kzxHJeAzVa0W3B4IxKvqQyLyCbAHmARMUtU9IX6pzh3FzwicSx9N5XpG\n/J3k+kGO9NGdi9WMaQTMC6pgOhc2ngicS59Lkvw7O7j+DVbRFqAXMDO4/gVwLYCIxIlIkdSeVERy\nAeVVdRowECgC/OusxLlQ8l8ezh2RX4JFywOfqGriENJiIvId9qu+Z7DtRmCkiAwAtgJXBNtvBoaL\nSF/sl/+1WLXLlMQBbwXJQoBhqrojy16Rc+ngfQTOHUPQR5Cgqr9HOhbnQsGbhpxzLsb5GYFzzsU4\nPyNwzrkY54nAOedinCcC55yLcZ4InHMuxnkicM65GPf/yXfKEbmsgJMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMh2BG152967"
      },
      "source": [
        "##Plotting train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "27561d40-f05a-4582-cfac-1ae7f2ec35f3",
        "id": "WNEq_n08297J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(epochs, average_acc_history_reduced, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, average_val_acc_history_reduced, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f6a92d0b0b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5BQnjMi93Dgn"
      },
      "source": [
        "##Performances on test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c98b7599-8470-4e7f-8c8b-0ed5ad365c31",
        "id": "yANP0XPF3Dg0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        " model = build_model()\n",
        " model.fit(train_data_stand_reduced, one_hot_train_labels, epochs= num_epochs, batch_size=8, shuffle=True)\n",
        " test_loss, test_acc = model.evaluate(test_data_stand_reduced, one_hot_test_labels)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "131/131 [==============================] - 1s 4ms/step - loss: 1.2255 - acc: 0.3130\n",
            "Epoch 2/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 1.1124 - acc: 0.4122\n",
            "Epoch 3/200\n",
            "131/131 [==============================] - 0s 147us/step - loss: 1.0401 - acc: 0.4885\n",
            "Epoch 4/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.9896 - acc: 0.5496\n",
            "Epoch 5/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.9585 - acc: 0.5802\n",
            "Epoch 6/200\n",
            "131/131 [==============================] - 0s 148us/step - loss: 0.9361 - acc: 0.6031\n",
            "Epoch 7/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.9200 - acc: 0.6260\n",
            "Epoch 8/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.8996 - acc: 0.6183\n",
            "Epoch 9/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.8865 - acc: 0.6260\n",
            "Epoch 10/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.8760 - acc: 0.6260\n",
            "Epoch 11/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.8648 - acc: 0.6336\n",
            "Epoch 12/200\n",
            "131/131 [==============================] - 0s 149us/step - loss: 0.8563 - acc: 0.6565\n",
            "Epoch 13/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.8488 - acc: 0.6565\n",
            "Epoch 14/200\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.8386 - acc: 0.6565\n",
            "Epoch 15/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.8345 - acc: 0.6565\n",
            "Epoch 16/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.8299 - acc: 0.6794\n",
            "Epoch 17/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.8245 - acc: 0.6794\n",
            "Epoch 18/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.8177 - acc: 0.6794\n",
            "Epoch 19/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.8139 - acc: 0.6870\n",
            "Epoch 20/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.8103 - acc: 0.6718\n",
            "Epoch 21/200\n",
            "131/131 [==============================] - 0s 264us/step - loss: 0.8041 - acc: 0.6718\n",
            "Epoch 22/200\n",
            "131/131 [==============================] - 0s 210us/step - loss: 0.7974 - acc: 0.6794\n",
            "Epoch 23/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.7946 - acc: 0.6718\n",
            "Epoch 24/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.7916 - acc: 0.6794\n",
            "Epoch 25/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.7870 - acc: 0.6641\n",
            "Epoch 26/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.7822 - acc: 0.6947\n",
            "Epoch 27/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.7811 - acc: 0.6641\n",
            "Epoch 28/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.7750 - acc: 0.7023\n",
            "Epoch 29/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.7717 - acc: 0.6870\n",
            "Epoch 30/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.7671 - acc: 0.6870\n",
            "Epoch 31/200\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.7666 - acc: 0.6947\n",
            "Epoch 32/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7610 - acc: 0.6870\n",
            "Epoch 33/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7597 - acc: 0.7099\n",
            "Epoch 34/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7538 - acc: 0.7023\n",
            "Epoch 35/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.7499 - acc: 0.7099\n",
            "Epoch 36/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7472 - acc: 0.6947\n",
            "Epoch 37/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.7445 - acc: 0.7023\n",
            "Epoch 38/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.7427 - acc: 0.6947\n",
            "Epoch 39/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.7378 - acc: 0.7023\n",
            "Epoch 40/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.7361 - acc: 0.7023\n",
            "Epoch 41/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.7320 - acc: 0.6794\n",
            "Epoch 42/200\n",
            "131/131 [==============================] - 0s 144us/step - loss: 0.7321 - acc: 0.7099\n",
            "Epoch 43/200\n",
            "131/131 [==============================] - 0s 215us/step - loss: 0.7274 - acc: 0.7176\n",
            "Epoch 44/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.7249 - acc: 0.6870\n",
            "Epoch 45/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.7202 - acc: 0.7099\n",
            "Epoch 46/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.7165 - acc: 0.6947\n",
            "Epoch 47/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.7168 - acc: 0.7176\n",
            "Epoch 48/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7140 - acc: 0.7099\n",
            "Epoch 49/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7093 - acc: 0.7099\n",
            "Epoch 50/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.7063 - acc: 0.7099\n",
            "Epoch 51/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.7037 - acc: 0.7099\n",
            "Epoch 52/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.7022 - acc: 0.7099\n",
            "Epoch 53/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6985 - acc: 0.7099\n",
            "Epoch 54/200\n",
            "131/131 [==============================] - 0s 187us/step - loss: 0.6939 - acc: 0.7328\n",
            "Epoch 55/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6912 - acc: 0.7252\n",
            "Epoch 56/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6871 - acc: 0.7252\n",
            "Epoch 57/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.6858 - acc: 0.6947\n",
            "Epoch 58/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.6828 - acc: 0.7328\n",
            "Epoch 59/200\n",
            "131/131 [==============================] - 0s 224us/step - loss: 0.6771 - acc: 0.7176\n",
            "Epoch 60/200\n",
            "131/131 [==============================] - 0s 216us/step - loss: 0.6745 - acc: 0.7252\n",
            "Epoch 61/200\n",
            "131/131 [==============================] - 0s 184us/step - loss: 0.6709 - acc: 0.7328\n",
            "Epoch 62/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.6680 - acc: 0.7099\n",
            "Epoch 63/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6643 - acc: 0.7099\n",
            "Epoch 64/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6617 - acc: 0.7252\n",
            "Epoch 65/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.6582 - acc: 0.7252\n",
            "Epoch 66/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.6545 - acc: 0.7252\n",
            "Epoch 67/200\n",
            "131/131 [==============================] - 0s 201us/step - loss: 0.6523 - acc: 0.7252\n",
            "Epoch 68/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6498 - acc: 0.7481\n",
            "Epoch 69/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.6457 - acc: 0.7405\n",
            "Epoch 70/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.6455 - acc: 0.7557\n",
            "Epoch 71/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.6399 - acc: 0.7481\n",
            "Epoch 72/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.6355 - acc: 0.7405\n",
            "Epoch 73/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6332 - acc: 0.7405\n",
            "Epoch 74/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.6328 - acc: 0.7557\n",
            "Epoch 75/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.6268 - acc: 0.7634\n",
            "Epoch 76/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.6275 - acc: 0.7481\n",
            "Epoch 77/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6213 - acc: 0.7557\n",
            "Epoch 78/200\n",
            "131/131 [==============================] - 0s 156us/step - loss: 0.6197 - acc: 0.7481\n",
            "Epoch 79/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.6150 - acc: 0.7481\n",
            "Epoch 80/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.6146 - acc: 0.7328\n",
            "Epoch 81/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.6099 - acc: 0.7405\n",
            "Epoch 82/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.6074 - acc: 0.7557\n",
            "Epoch 83/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.6054 - acc: 0.7634\n",
            "Epoch 84/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.6030 - acc: 0.7481\n",
            "Epoch 85/200\n",
            "131/131 [==============================] - 0s 195us/step - loss: 0.6015 - acc: 0.7634\n",
            "Epoch 86/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5978 - acc: 0.7634\n",
            "Epoch 87/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5959 - acc: 0.7710\n",
            "Epoch 88/200\n",
            "131/131 [==============================] - 0s 166us/step - loss: 0.5908 - acc: 0.7557\n",
            "Epoch 89/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5888 - acc: 0.7786\n",
            "Epoch 90/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5887 - acc: 0.7710\n",
            "Epoch 91/200\n",
            "131/131 [==============================] - 0s 205us/step - loss: 0.5848 - acc: 0.7634\n",
            "Epoch 92/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.5814 - acc: 0.7710\n",
            "Epoch 93/200\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.5792 - acc: 0.7863\n",
            "Epoch 94/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5781 - acc: 0.7710\n",
            "Epoch 95/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5750 - acc: 0.7939\n",
            "Epoch 96/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.5727 - acc: 0.7939\n",
            "Epoch 97/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.5713 - acc: 0.7786\n",
            "Epoch 98/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.5707 - acc: 0.7939\n",
            "Epoch 99/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5663 - acc: 0.7786\n",
            "Epoch 100/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5657 - acc: 0.7710\n",
            "Epoch 101/200\n",
            "131/131 [==============================] - 0s 209us/step - loss: 0.5618 - acc: 0.8015\n",
            "Epoch 102/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.5589 - acc: 0.7939\n",
            "Epoch 103/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.5560 - acc: 0.7939\n",
            "Epoch 104/200\n",
            "131/131 [==============================] - 0s 193us/step - loss: 0.5554 - acc: 0.7939\n",
            "Epoch 105/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.5516 - acc: 0.8015\n",
            "Epoch 106/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5502 - acc: 0.7939\n",
            "Epoch 107/200\n",
            "131/131 [==============================] - 0s 181us/step - loss: 0.5473 - acc: 0.7863\n",
            "Epoch 108/200\n",
            "131/131 [==============================] - 0s 188us/step - loss: 0.5450 - acc: 0.7939\n",
            "Epoch 109/200\n",
            "131/131 [==============================] - 0s 191us/step - loss: 0.5426 - acc: 0.8015\n",
            "Epoch 110/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.5417 - acc: 0.8015\n",
            "Epoch 111/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.5370 - acc: 0.8015\n",
            "Epoch 112/200\n",
            "131/131 [==============================] - 0s 183us/step - loss: 0.5391 - acc: 0.8092\n",
            "Epoch 113/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.5320 - acc: 0.8092\n",
            "Epoch 114/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5328 - acc: 0.8015\n",
            "Epoch 115/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.5290 - acc: 0.7939\n",
            "Epoch 116/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5270 - acc: 0.8015\n",
            "Epoch 117/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.5240 - acc: 0.8015\n",
            "Epoch 118/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.5227 - acc: 0.8168\n",
            "Epoch 119/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.5196 - acc: 0.8092\n",
            "Epoch 120/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.5182 - acc: 0.7939\n",
            "Epoch 121/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5140 - acc: 0.8015\n",
            "Epoch 122/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5143 - acc: 0.8092\n",
            "Epoch 123/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.5101 - acc: 0.8092\n",
            "Epoch 124/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.5094 - acc: 0.7939\n",
            "Epoch 125/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.5061 - acc: 0.8092\n",
            "Epoch 126/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.5035 - acc: 0.8244\n",
            "Epoch 127/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4995 - acc: 0.8244\n",
            "Epoch 128/200\n",
            "131/131 [==============================] - 0s 157us/step - loss: 0.5017 - acc: 0.8244\n",
            "Epoch 129/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4974 - acc: 0.8168\n",
            "Epoch 130/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4959 - acc: 0.8244\n",
            "Epoch 131/200\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.4941 - acc: 0.8168\n",
            "Epoch 132/200\n",
            "131/131 [==============================] - 0s 154us/step - loss: 0.4916 - acc: 0.8092\n",
            "Epoch 133/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4889 - acc: 0.8244\n",
            "Epoch 134/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4881 - acc: 0.8092\n",
            "Epoch 135/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4848 - acc: 0.8015\n",
            "Epoch 136/200\n",
            "131/131 [==============================] - 0s 179us/step - loss: 0.4847 - acc: 0.8168\n",
            "Epoch 137/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4811 - acc: 0.8168\n",
            "Epoch 138/200\n",
            "131/131 [==============================] - 0s 198us/step - loss: 0.4782 - acc: 0.8092\n",
            "Epoch 139/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4781 - acc: 0.8244\n",
            "Epoch 140/200\n",
            "131/131 [==============================] - 0s 176us/step - loss: 0.4728 - acc: 0.8092\n",
            "Epoch 141/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.4748 - acc: 0.8092\n",
            "Epoch 142/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4724 - acc: 0.8092\n",
            "Epoch 143/200\n",
            "131/131 [==============================] - 0s 171us/step - loss: 0.4681 - acc: 0.8168\n",
            "Epoch 144/200\n",
            "131/131 [==============================] - 0s 175us/step - loss: 0.4672 - acc: 0.7939\n",
            "Epoch 145/200\n",
            "131/131 [==============================] - 0s 186us/step - loss: 0.4646 - acc: 0.8168\n",
            "Epoch 146/200\n",
            "131/131 [==============================] - 0s 161us/step - loss: 0.4643 - acc: 0.8168\n",
            "Epoch 147/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4629 - acc: 0.8168\n",
            "Epoch 148/200\n",
            "131/131 [==============================] - 0s 196us/step - loss: 0.4610 - acc: 0.8244\n",
            "Epoch 149/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4577 - acc: 0.8244\n",
            "Epoch 150/200\n",
            "131/131 [==============================] - 0s 202us/step - loss: 0.4558 - acc: 0.8092\n",
            "Epoch 151/200\n",
            "131/131 [==============================] - 0s 277us/step - loss: 0.4552 - acc: 0.8244\n",
            "Epoch 152/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4539 - acc: 0.8244\n",
            "Epoch 153/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4503 - acc: 0.8168\n",
            "Epoch 154/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.4489 - acc: 0.8168\n",
            "Epoch 155/200\n",
            "131/131 [==============================] - 0s 172us/step - loss: 0.4451 - acc: 0.8321\n",
            "Epoch 156/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4495 - acc: 0.8397\n",
            "Epoch 157/200\n",
            "131/131 [==============================] - 0s 169us/step - loss: 0.4421 - acc: 0.8321\n",
            "Epoch 158/200\n",
            "131/131 [==============================] - 0s 173us/step - loss: 0.4428 - acc: 0.8168\n",
            "Epoch 159/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4394 - acc: 0.8321\n",
            "Epoch 160/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4371 - acc: 0.8321\n",
            "Epoch 161/200\n",
            "131/131 [==============================] - 0s 192us/step - loss: 0.4357 - acc: 0.8168\n",
            "Epoch 162/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4338 - acc: 0.8244\n",
            "Epoch 163/200\n",
            "131/131 [==============================] - 0s 178us/step - loss: 0.4320 - acc: 0.8321\n",
            "Epoch 164/200\n",
            "131/131 [==============================] - 0s 152us/step - loss: 0.4322 - acc: 0.8244\n",
            "Epoch 165/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4297 - acc: 0.8321\n",
            "Epoch 166/200\n",
            "131/131 [==============================] - 0s 197us/step - loss: 0.4268 - acc: 0.8321\n",
            "Epoch 167/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.4260 - acc: 0.8397\n",
            "Epoch 168/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4249 - acc: 0.8321\n",
            "Epoch 169/200\n",
            "131/131 [==============================] - 0s 167us/step - loss: 0.4233 - acc: 0.8244\n",
            "Epoch 170/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4229 - acc: 0.8321\n",
            "Epoch 171/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4202 - acc: 0.8321\n",
            "Epoch 172/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.4180 - acc: 0.8168\n",
            "Epoch 173/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4152 - acc: 0.8244\n",
            "Epoch 174/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4151 - acc: 0.8321\n",
            "Epoch 175/200\n",
            "131/131 [==============================] - 0s 170us/step - loss: 0.4135 - acc: 0.8321\n",
            "Epoch 176/200\n",
            "131/131 [==============================] - 0s 162us/step - loss: 0.4113 - acc: 0.8244\n",
            "Epoch 177/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.4107 - acc: 0.8321\n",
            "Epoch 178/200\n",
            "131/131 [==============================] - 0s 159us/step - loss: 0.4084 - acc: 0.8473\n",
            "Epoch 179/200\n",
            "131/131 [==============================] - 0s 160us/step - loss: 0.4061 - acc: 0.8397\n",
            "Epoch 180/200\n",
            "131/131 [==============================] - 0s 153us/step - loss: 0.4055 - acc: 0.8321\n",
            "Epoch 181/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.4020 - acc: 0.8244\n",
            "Epoch 182/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.4014 - acc: 0.8397\n",
            "Epoch 183/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3996 - acc: 0.8321\n",
            "Epoch 184/200\n",
            "131/131 [==============================] - 0s 155us/step - loss: 0.3966 - acc: 0.8626\n",
            "Epoch 185/200\n",
            "131/131 [==============================] - 0s 194us/step - loss: 0.3967 - acc: 0.8550\n",
            "Epoch 186/200\n",
            "131/131 [==============================] - 0s 151us/step - loss: 0.3949 - acc: 0.8550\n",
            "Epoch 187/200\n",
            "131/131 [==============================] - 0s 180us/step - loss: 0.3905 - acc: 0.8702\n",
            "Epoch 188/200\n",
            "131/131 [==============================] - 0s 164us/step - loss: 0.3906 - acc: 0.8550\n",
            "Epoch 189/200\n",
            "131/131 [==============================] - 0s 189us/step - loss: 0.3912 - acc: 0.8702\n",
            "Epoch 190/200\n",
            "131/131 [==============================] - 0s 163us/step - loss: 0.3880 - acc: 0.8550\n",
            "Epoch 191/200\n",
            "131/131 [==============================] - 0s 257us/step - loss: 0.3862 - acc: 0.8779\n",
            "Epoch 192/200\n",
            "131/131 [==============================] - 0s 236us/step - loss: 0.3857 - acc: 0.8473\n",
            "Epoch 193/200\n",
            "131/131 [==============================] - 0s 165us/step - loss: 0.3827 - acc: 0.8702\n",
            "Epoch 194/200\n",
            "131/131 [==============================] - 0s 174us/step - loss: 0.3875 - acc: 0.8626\n",
            "Epoch 195/200\n",
            "131/131 [==============================] - 0s 158us/step - loss: 0.3826 - acc: 0.8550\n",
            "Epoch 196/200\n",
            "131/131 [==============================] - 0s 185us/step - loss: 0.3808 - acc: 0.8779\n",
            "Epoch 197/200\n",
            "131/131 [==============================] - 0s 182us/step - loss: 0.3798 - acc: 0.8702\n",
            "Epoch 198/200\n",
            "131/131 [==============================] - 0s 203us/step - loss: 0.3782 - acc: 0.8626\n",
            "Epoch 199/200\n",
            "131/131 [==============================] - 0s 168us/step - loss: 0.3762 - acc: 0.8702\n",
            "Epoch 200/200\n",
            "131/131 [==============================] - 0s 177us/step - loss: 0.3753 - acc: 0.8702\n",
            "34/34 [==============================] - 0s 5ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c5e05c29-c9fa-4dac-fd13-ba29c13c7d1b",
        "id": "kQLgWQtS3DhJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.metrics_names"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['loss', 'acc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "30cfd131-b234-466d-b914-b989b79198c6",
        "id": "_c5TGT4H3DhR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_acc\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2647058823529412"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMbKL5Yw3DhZ"
      },
      "source": [
        "Si comporta molto bene in training e in validation ma si comporta male in test"
      ]
    }
  ]
}